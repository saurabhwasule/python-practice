###Installation 
#Use Anaconda

#Anaconda 
$ conda install scikit-learn
$ conda install statsmodels 
$ conda install plotly
$ conda install -c quantopian ta-lib
$ pip install cufflinks patsy
$ conda update jupyter 
$ conda update nbconvert  #some bug fix 


#if ananconda fails installation do following
# open a DOS prompt with admin rights. cd  Anaconda2\Scripts folder
$ conda update conda
#cd Anaconda2\Lib directory
..\python _nsis.py mkmenus

#To save as pdf in jupyter , install pandoc and MikTex 
# https://github.com/jgm/pandoc/releases/latest
# https://miktex.org/download
# then start jupyter - Note first time while saving, it may download many files, hence may fail couple of time, but keep on trying 

##Using with jupyter/IPython 
$ set PATH=C:\Anaconda2;C:\Anaconda2\Scripts;%PATH%
$ jupyter notebook  #start server 



###Numpy - ndarray Introduction 

# A numpy array(homogeneous)(numpy.ndarray) is created from python list or List of List for higher dimension
#indexed by a tuple of nonnegative integers.

#Stored as colum major way 

#dimensions are called axes. The number of axes(plural of axis) is rank. 
#axis starts from zero , axis=0 means 1st dimension(x) , axis=1 means 2nd dimension(y)...
#OR axis=-1 means last dimension, axis=-2 means 2nd last dimension 

#shape  is a tuple of integers giving the size of the array along each dimension.
#a 1D array can be .reshape(tuple_of_dims) if total elements are same 

#Note many function take 'axis' argument, means the dimension on which function operates
#For 2D , axis=0 means 1st dimension=Row , axis=1 means 2nd dimension=Column
#eg insert, delete etc 

#For certain methods (eg sum, prod etc) axis=0 means, operation along the 1st dimension
#ie 1st dimension  varying 
#ie for 2D,  Row varying which is equivalent to ColumnWise 

#Hence Understand meaning of axis per function from reference documents 

#For  example, np.sum(axis=n),  dimension n is collapsed and deleted, 
#For example, if b has shape (5,6,7,8), and c = b.sum(axis=2), 
#then axis 2 (dimension with size 7) is collapsed, and the result has shape (5,6,8). 
#c[x,y,z] is equal to the sum of all elements c[x,y,:,z]



#Example 

import numpy as np

a = np.array([1, 2, 3])  # Create a rank 1 array
print type(a)            # Prints "<type 'numpy.ndarray'>"
print a.shape            # Prints "(3,)"
print a.ndim 			 # 1
print a[0], a[1], a[2]   # Prints "1 2 3"
a[0] = 5                 # Change an element of the array
print a                  # Prints "[5, 2, 3]"

b = np.array([[1,2,3],[4,5,6]])   # Create a rank 2 array
print b.shape                     # Prints "(2, 3)"
print b[0, 0], b[0, 1], b[1, 0]   # Prints "1 2 4"
b[0,]	#or b[0,:]   			  # array([1, 2, 3])  
b[:,0]                            #array([1, 4])


c = np.array([ [ [1,2], [2,3] ],[ [1,2], [2,3] ],[ [1,2], [2,3] ]])
c.shape            #(3, 2, 2)

>>> c[0,]
array([[1, 2],
       [2, 3]])
	   
>>> c[0,1,]
array([2, 3])

>>> c[0,1,1]
3

#Note the difference of below, one is vector and another is 1x3
>>> x = np.array([[1,2,3]])        
>>> x.shape                         #rank 2 as two dimension 
(1, 3)

>>> x = np.array([1,2,3])           # rank 1, generally called vector 
>>> x.shape
(3,)



### Numpy - Creation of array - these methods take (m,n,...) dimensions 
# similar methods (zeros/ones/full)_like(another_array) which creates based on another_array.shape
import numpy as np

a = np.zeros((2,2))  # Create an array of all zeros 

    
b = np.ones((1,2))   # Create an array of all ones

c = np.full((2,2), 7) # Create a constant array
print c               # Prints "[[ 7.  7.]
                      #          [ 7.  7.]]"

d = np.eye(2)        # Create a 2x2 identity matrix

#random 
e = np.random.random((2,2)) # Create an array filled with random values
print e                     # Might print "[[ 0.91940167  0.08143941]
                            #               [ 0.68744134  0.87236687]]"
#array range 
>>> np.arange(10).reshape(2,5)
array([[0, 1, 2, 3, 4],
       [5, 6, 7, 8, 9]])
       
>>> np.arange(10)[:8].reshape(2,2,2)
array([[[0, 1],
        [2, 3]],

       [[4, 5],
        [6, 7]]])
        
### Numpy - Array indexing - Slice Indexing (can be mutated)
#index can be single number, or start:stop:step (stop exclusive)
#or :(means all elements of that dimension) or array of indexes
#or boolean array(where True indexes are selected)
import numpy as np

# Create the following rank 2 array with shape (3, 4)
a = np.array([[1,2, 3, 4], 
              [5,6, 7, 8], 
              [9,10,11,12]])

# index 0 to 1 and columns 1 and 2; 
#b is the following array of shape (2, 2):
# [[2 3]
#  [6 7]]
b = a[:2, 1:3]

# A slice of an array is a view into the same data, so modifying it
# will modify the original array.
print a[0, 1]   # Prints "2"
b[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1]
print a[0, 1]   # Prints "77"


#Mixing integer indexing with slice indexing.
#yields an array of lower rank than the original array. 

row_r1 = a[1, :]    # Rank 1 view of the second row of a  
row_r2 = a[1:2, :]  # Rank 2 view of the second row of a
print row_r1, row_r1.shape  # Prints "[5 6 7 8] (4,)"
print row_r2, row_r2.shape  # Prints "[[5 6 7 8]] (1, 4)"

# We can make the same distinction when accessing columns of an array:
col_r1 = a[:, 1]
col_r2 = a[:, 1:2]
print col_r1, col_r1.shape  # Prints "[ 2  6 10] (3,)"
print col_r2, col_r2.shape  # Prints "[[ 2]
                            #          [ 6]
                            #          [10]] (3, 1)"


### Numpy - Array indexing - Integer array indexing to create subarray , use [ [] ]

import numpy as np

a = np.array([[1,2], [3, 4], [5, 6]])


# The returned array will have shape (3,) 
print a[[0, 1, 2], [0, 1, 0]]  # Prints "[1 4 5]"   #takes element from (first_array_index1, second_array_index1)  and so on..

# Same as 
print np.array([a[0, 0], a[1, 1], a[2, 0]])  # Prints "[1 4 5]"



##Example -To mutate one element from each row of a matrix:

import numpy as np

# Create a new array from which we will select elements
a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])

# Create an array of indices
b = np.array([0, 2, 0, 1])

# Select one element from each row of a using the indices in b
print a[np.arange(4), b]  # Prints "[ 1  6  7 11]" ie [0,0], [1,2], [2,0], [3,1] # arange is same as range, but returns array 

# Mutate one element from each row of a using the indices in b
a[np.arange(4), b] += 10

print a  # prints "array([[11,  2,  3],
         #                [ 4,  5, 16],
         #                [17,  8,  9],
         #                [10, 21, 12]])


### Numpy - Array indexing - Boolean array indexing

import numpy as np

a = np.array([[1,2], [3, 4], [5, 6]])

bool_idx = (a > 2)  
            
print bool_idx      # Prints "[[False False]
                    #          [ True  True]
                    #          [ True  True]]"

print a[bool_idx]  # Prints "[3 4 5 6]"

# We can do all of the above in a single concise statement:
print a[a > 2]     # Prints "[3 4 5 6]"

>>> a[ (a > 2) & (a<5)]    #Use &, | and ~ for boolean operation , ==, !=, > >= etc for comparison
array([3, 4])
>>> a[ (a > 2) | (a<5)]
array([1, 2, 3, 4, 5, 6])
>>> a[ ~(a > 2) ]
array([1, 2])

>>> a[ a == 2]
array([2])
>>> a[ a != 2]
array([1, 3, 4, 5, 6])


### Numpy - Array indexing - using ... (means all remaining dimensions)

>>> from numpy import arange
>>> a = arange(16).reshape(2,2,2,2)

#you have a 4-dimensional matrix of order 2x2x2x2. 
#To select all first elements in the 4th dimension, you can use the ellipsis notation
>>> a[..., 0].flatten()
array([ 0,  2,  4,  6,  8, 10, 12, 14])

#which is equivalent to
>>> a[:,:,:,0].flatten()
array([ 0,  2,  4,  6,  8, 10, 12, 14])



###numpy - Vectorizing functions (vectorize)

#Use  vectorize to convert an ordinary Python function which accepts scalars 
#and returns scalars into a 'vectorized-function' with the same broadcasting rules 
#as other Numpy functions (i.e. the Universal functions, or ufuncs). 

#behaves elementwise 


def addsubtract(a,b):
    if a > b:
        return a - b
    else:
        return a + b


>>> vec_addsubtract = np.vectorize(addsubtract)

>>> vec_addsubtract([0,3,6,9],[1,3,5,7])
array([1, 6, 1, 2])



###numpy - r_ , c_ , select and where 

#np.r_  : By default: create a array(1D) from comma seperated many slices start:stop:step (stop exclusive)
#or comman seperated numbers (along the first axis ie row)
#it has many other functonalities - check Reference 

#np.c_ : create a array(2D) from comma seperated many 1D arrays or start:stop:step (stop exclusive)
# but  along the second axis(ie column) -> Column stack 
#

#note used with []   not ()

>>> np.c_[np.array([1,2,3]), np.array([4,5,6])]
array([[1, 4],
       [2, 5],
       [3, 6]])
>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]
array([[1, 2, 3, 0, 0, 4, 5, 6]])


>>> x = np.r_[-2:3, 0,0, 3:9]
>>> x
array([-2, -1,  0,  1,  2,  0,  0,  3,  4,  5,  6,  7,  8]

##Difference between .r_, .c_ etc 
#columnwise append 
>>> np.c_[1:3,1:3]  
array([[1, 1],
       [2, 2]])
#rowwise append 
>>> np.r_[1:3,1:3]
array([1, 2, 1, 2])
#columnwise append 
>>> np.column_stack( [[1,2], [1,2]]) #[array1,array2,..] , only 1 positional arg 
array([[1, 1],
       [2, 2]])
#rowwise append       
>>> np.hstack([[1,2],[1,2]]) #[array1,array2,..] , only 1 positional arg 
array([1, 2, 1, 2])
#vertically stacking array*
>>> np.vstack([[1,2],[1,2]]) #[array1,array2,..] , only 1 positional arg 
array([[1, 2],
       [1, 2]])
#vertically stacking array*
>>> np.stack([[1,2],[10,11]])
array([[ 1,  2],
       [10, 11]])
#columnwise append 
>>> np.stack([[1,2],[10,11]], axis=1)
array([[ 1, 10],
       [ 2, 11]])    
#rowwise append    
>>> np.concatenate( [[1,2],[3,4]])
array([1, 2, 3, 4])
#vertically stacking array - note each is 2D 
>>> np.concatenate(( [[1,2],[3,4]] , [[5,6]]  ), axis=0)
array([[1, 2],
       [3, 4],
       [5, 6]])
#2nd array appended columnwise - note each is 2D 
>>> np.concatenate(( [[1,2],[3,4]] , [[5],[6]]  ), axis=1)
array([[1, 2, 5],
       [3, 4, 6]])
#Repeat array to N times       
>>> np.tile([1,2],3)
array([1, 2, 1, 2, 1, 2])
#repeat array to MxN times 
>>> np.tile([1,2],(3,3))
array([[1, 2, 1, 2, 1, 2],
       [1, 2, 1, 2, 1, 2],
       [1, 2, 1, 2, 1, 2]])
#broadcasting 
>>> np.broadcast_to([1,2],(2,2)) #original shape=(2,) => (2,n)
array([[1, 2],
       [1, 2]])
>>> np.broadcast_to([[1,2]],(5,2)) #original shape=(1,2) => (n,2) 
array([[1, 2],
       [1, 2],
       [1, 2],
       [1, 2],
       [1, 2]])
>>> np.broadcast_to([10],(3,3))#original shape=(1,) => (m,n,k,..) 
array([[10, 10, 10],
       [10, 10, 10],
       [10, 10, 10]])
>>> np.broadcast_to(10,(3,3))#original shape=scaler => (m,n,k,..) 
array([[10, 10, 10],
       [10, 10, 10],
       [10, 10, 10]])       

       
       
##numpy.select(condlist, choicelist, default=0)
#if x > 3, then 0, if x>=0 , then x+2 else default =0 
>>> np.select([x > 3, x >= 0], [0, x+2])
array([0, 0, 2, 3, 4, 2, 2, 5, 0, 0, 0, 0, 0])

##numpy.where(condition[, x, y])
#condition : array_like, bool, When True, yield x, otherwise yield y.
#x, y : array_like, optional
#out : ndarray or tuple of ndarrays
#If both x and y are specified, the output array contains elements of x 
#where condition is True, and elements from y elsewhere.
#If only condition is given, return the tuple of indices where condition is True.
 
>>> x = np.arange(9.).reshape(3, 3)
>>> x
array([[ 0.,  1.,  2.],
       [ 3.,  4.,  5.],
       [ 6.,  7.,  8.]])
>>> np.where( x > 5 )
(array([2, 2, 2]), array([0, 1, 2]))     # indices [2,0], [2,1], [2,2] are true
>>> x[np.where( x > 3.0 )]               # Note: result is 1D.
array([ 4.,  5.,  6.,  7.,  8.])
>>> np.where(x < 5, x, -1)               # Note: broadcasting.
array([[ 0.,  1.,  2.],
       [ 3.,  4., -1.],
       [-1., -1., -1.]])



       
### Numpy - Datatypes of array elements 

import numpy as np

x = np.array([1, 2])  # Let numpy choose the datatype
print x.dtype         # Prints "int64"

x = np.array([1.0, 2.0])  # Let numpy choose the datatype
print x.dtype             # Prints "float64"

x = np.array([1, 2], dtype=np.int64)  # Force a particular datatype
print x.dtype                         # Prints "int64"




### Numpy - Array math and all np.methods - operates elementwise on array 
#check by 
dir(np)

#Basic mathematical functions operate elementwise on arrays, 
#and are available both as operator overloads and as functions in the numpy module

import numpy as np

x = np.array([[1,2],[3,4]], dtype=np.float64)
y = np.array([[5,6],[7,8]], dtype=np.float64)

# Elementwise sum; both produce the array
# [[ 6.0  8.0]
#  [10.0 12.0]]
print x + y                 
print np.add(x, y)

# Elementwise
print x - y    #print np.subtract(x, y)
print x * y    #print np.multiply(x, y)
print x / y    #print np.divide(x, y)
print np.sqrt(x)  #other math methods eg np.sin(), np.log() ....

##Use .dot  for inner product of vector or matrix multiplication 
v = np.array([9,10])
w = np.array([11, 12])

# Inner product of vectors; both produce 219
print v.dot(w)
print np.dot(v, w)

# Matrix / vector product; both produce the rank 1 array [29 67]
print x.dot(v)
print np.dot(x, v)

# Matrix / matrix product; both produce the rank 2 array
# [[19 22]
#  [43 50]]
print x.dot(y)
print np.dot(x, y)


### Numpy - Sum -  for performing computations on arrays 

# For , np.sum(axis=n),  then dimension n is collapsed and deleted, 
#For example, if b has shape (5,6,7,8), and c = b.sum(axis=2), 
#then axis 2 (dimension with size 7) is collapsed, and the result has shape (5,6,8). 
#c[x,y,z] is equal to the sum of all elements c[x,y,:,z].

import numpy as np

x = np.array([[1,2],[3,4]])

print np.sum(x)  # Compute sum of all elements; prints "10"
print np.sum(x, axis=0)  # Compute sum of each column; prints "[4 6]"
print np.sum(x, axis=1)  # Compute sum of each row; prints "[3 7]"


### Numpy - Transposing  an array 

import numpy as np

x = np.array([[1,2], [3,4]])
print x    # Prints "[[1 2]
           #          [3 4]]"
print x.T  # Prints "[[1 3]
           #          [2 4]]"

# Note that taking the transpose of a rank 1 array does nothing:
v = np.array([1,2,3])
print v    # Prints "[1 2 3]"
print v.T  # Prints "[1 2 3]"



### Numpy - Record Array(numpy.recarray) vs Structured array(numpy.ndarray with complex dtype)
#Arrays may have a data-types containing fields
#An example is [(x, int), (y, float)], where each entry in the array is a pair of (int, float). 
#Normally, access is eg  arr['x'] and arr['y']. 
#Record arrays allows access using arr.x and arr.y

#Structured Array 
x = np.array([(1.0, 2), (3.0, 4)], dtype=[('x', float), ('y', int)])
>>> x
array([(1.0, 2), (3.0, 4)],
      dtype=[('x', '<f8'), ('y', '<i4')])

>>> x['x']  #all 'x'
array([ 1.,  3.])

#Convert to  record array:
>>> x = x.view(np.recarray)
>>> x.x  #all 'x'
array([ 1.,  3.])

>>> x.y
array([2, 4])

#Create a new, empty two elements record array
#1st arg  , shape tuple(could be xD)
#=> , how many elements where each element is having dtype structure 
#does not initialize array 

# 1D array of two elements, each element having x,y,z
>>> r = np.recarray((2,), dtype=[('x', int), ('y', float), ('z', int)]) 
>>> r.x   #all x 
array([3, 1])
>>> r
rec.array([(3, 2.1219957915e-314, 3), (1, 2.1219957915e-314, 2)],
          dtype=[('x', '<i4'), ('y', '<f8'), ('z', '<i4')])
>>> r.x = np.array([0,0]) #assign to all x 
>>> r
rec.array([(0, 2.1219957915e-314, 3), (0, 2.1219957915e-314, 2)],
          dtype=[('x', '<i4'), ('y', '<f8'), ('z', '<i4')])
>>> r.y[0] = 500  #first y 
>>> r
rec.array([(0, 500.0, 3), (0, 2.1219957915e-314, 2)],
          dtype=[('x', '<i4'), ('y', '<f8'), ('z', '<i4')])
>>> r.z = [2,2]
>>> r
rec.array([(0, 500.0, 2), (0, 2.1219957915e-314, 2)],
          dtype=[('x', '<i4'), ('y', '<f8'), ('z', '<i4')])
          
>>> r['x']
array([0, 0])




### Numpy - Data type - use with prefix numpy.
#The default data type in NumPy is float_

#arrayscalar type      Related Python type
int_                    IntType (Python 2 only) 
float_                  FloatType 
complex_                ComplexType 
str_                    StringType 
unicode_                UnicodeType 


#other types 
bool_           Boolean (True or False) stored as a byte 
int_            Default integer type (same as C long; normally either int64 or int32) 
intc            Identical to C int (normally int32 or int64) 
intp            Integer used for indexing (same as C ssize_t; normally either int32 or int64) 
int8            Byte (-128 to 127) 
int16           Integer (-32768 to 32767) 
int32           Integer (-2147483648 to 2147483647) 
int64           Integer (-9223372036854775808 to 9223372036854775807) 
uint8           Unsigned integer (0 to 255) 
uint16          Unsigned integer (0 to 65535) 
uint32          Unsigned integer (0 to 4294967295) 
uint64          Unsigned integer (0 to 18446744073709551615) 
float_          Shorthand for float64. 
float16         Half precision float: sign bit, 5 bits exponent, 10 bits mantissa 
float32         Single precision float: sign bit, 8 bits exponent, 23 bits mantissa 
float64         Double precision float: sign bit, 11 bits exponent, 52 bits mantissa 
complex_        Shorthand for complex128. 
complex64       Complex number, represented by two 32-bit floats (real and imaginary components) 
complex128      Complex number, represented by two 64-bit floats (real and imaginary components) 


#Array types can also be referred to by character codes,
#Type specifier 
'b' 		boolean 
'i' 		(signed) integer 
'u' 		unsigned integer 
'f' 		floating-point 
'c' 		complex-floating point 
'm' 		timedelta 
'M' 		datetime 
'O' 		(Python) objects 
'S', 'a' 	(byte-)string 
'U' 		Unicode 
'V' 		raw data (void) 


>>> np.array([1, 2, 3], dtype='f')
array([ 1.,  2.,  3.], dtype=float32)

#Example 
>>> import numpy as np
>>> x = np.float32(1.0)
>>> x
1.0
>>> y = np.int_([1,2,4])
>>> y
array([1, 2, 4])
>>> z = np.arange(3, dtype=np.uint8)
>>> z
array([0, 1, 2], dtype=uint8)


##To convert the type of an array

>>> z.astype(float)                 
array([  0.,  1.,  2.])
>>> np.int8(z)
array([0, 1, 2], dtype=int8)

#To determine the type of an array, look at the dtype attribute:
>>> z.dtype
dtype('uint8')



##dtype details 
#A data type object (a numpy.dtype class) contains below 
	1. Type of the data (integer, float, Python object, etc.)
	2. Size of the data (how many bytes is in e.g. the integer)
	3. Byte order of the data (little-endian or big-endian)
	4.If the data type is aggregate 
		1. field names
		2. data-type of each field
		3. which part of the memory block each field takes.
	5.If the data type is a sub-array, what is its shape and data type.



#32-bit big-endian integer
>>> dt = np.dtype('>i4')
>>> dt.byteorder
'>'
>>> dt.itemsize
4
>>> dt.name
'int32'
>>> dt.type is np.int32
True

# structured data type containing a 16-character string (in field 'name) 
#and a sub-array of two 64-bit floating-point number (in field 'grades):

>>> dt = np.dtype([('name', np.str_, 16), ('grades', np.float64, (2,))])
>>> dt['name']
dtype('|S16')
>>> dt['grades']
dtype(('float64',(2,)))

#Array of above 

>>> x = np.array([ ('Sarah', (8.0, 7.0)), ('John', (6.0, 7.0)) ], dtype=dt)
>>> x[1]
('John', [6.0, 7.0])
>>> x[1]['grades']
array([ 6.,  7.])
>>> type(x[1])
<type 'numpy.void'>
>>> type(x[1]['grades'])
<type 'numpy.ndarray'>

##  Specifying and constructing data types
## Syntax: Array-scalar types - Built-in scalar types - 24 types, eg float128, complex128 , etc 

dt = np.dtype(np.int32)      # 32-bit integer
dt = np.dtype(np.complex128) # 128-bit complex floating-point number

## Syntax: Built-in Python types
dt = np.dtype(float)   # Python-compatible floating-point number
dt = np.dtype(int)     # Python-compatible integer
dt = np.dtype(object)  # Python object

## Syntax: One-character strings
#Several kinds of strings can be converted. 
#Recognized strings can be prepended with '>' (big-endian), '<' (little-endian), 
#or '=' (hardware-native, the default), |  for byteorder not applicable 

dt = np.dtype('b')  # byte, native byte order
dt = np.dtype('>H') # big-endian unsigned short
dt = np.dtype('<f') # little-endian single-precision float
dt = np.dtype('d')  # double-precision floating-point number

dt = np.dtype('i4')   # 32-bit signed integer
dt = np.dtype('f8')   # 64-bit floating-point number
dt = np.dtype('c16')  # 128-bit complex floating-point number
dt = np.dtype('a25')  # 25-character string
dt = np.dtype('|S4')   # 4 byte string 

#Type specifier 
'b' 		boolean 
'i' 		(signed) integer 
'u' 		unsigned integer 
'f' 		floating-point 
'c' 		complex-floating point 
'm' 		timedelta 
'M' 		datetime 
'O' 		(Python) objects 
'S', 'a' 	(byte-)string 
'U' 		Unicode 
'V' 		raw data (void) 
 
## Syntax: Strings - combination of one character string for aggregate 
#Numpy generates automatic field name as f0, f1,...

#Example:
#•field named f0 containing a 32-bit integer
#•field named f1 containing a 2 x 3 sub-array of 64-bit floating-point numbers
#•field named f2 containing a 32-bit floating-point number

dt = np.dtype("i4, (2,3)f8, f4")

#•field named f0 containing a 3-character string
#•field named f1 containing a sub-array of shape (3,) containing 64-bit unsigned integers
#•field named f2 containing a 3 x 4 sub-array containing 10-character strings

dt = np.dtype("a3, 3u8, (3,4)a10")


# Syntax: [(field_name, field_dtype, field_shape), ...]
#Data-type with fields big (big-endian 32-bit integer), > and little (little-endian 32-bit integer), <:
dt = np.dtype([('big', '>i4'), ('little', '<i4')])

#Data-type with fields R, G, B, A, each being an unsigned 8-bit integer:
dt = np.dtype([('R','u1'), ('G','u1'), ('B','u1'), ('A','u1')])


# Syntax: {'names': ..., 'formats': ..., 'offsets': ..., 'titles': ..., 'itemsize': ...}
#Data type with fields r, g, b, a, each being a 8-bit unsigned integer:
dt = np.dtype({'names': ['r','g','b','a'], 'formats': [uint8, uint8, uint8, uint8]})






### Numpy - Array creation routines - Ones and zeros - *_like means shape taken from argument array 
empty(shape[, dtype, order]) 						Return a new array of given shape and type, without initializing entries.
empty_like(a[, dtype, order, subok]) 				Return a new array with the same shape and type as a given array.
eye(N[, M, k, dtype]) 								Return a 2-D array with ones on the diagonal and zeros elsewhere.
identity(n[, dtype]) 								Return the identity array.
ones(shape[, dtype, order]) 						Return a new array of given shape and type, filled with ones.
ones_like(a[, dtype, order, subok]) 				Return an array of ones with the same shape and type as a given array.
zeros(shape[, dtype, order]) 						Return a new array of given shape and type, filled with zeros.
zeros_like(a[, dtype, order, subok]) 				Return an array of zeros with the same shape and type as a given array.
full(shape, fill_value[, dtype, order]) 			Return a new array of given shape and type, filled with fill_value.
full_like(a, fill_value[, dtype, order, subok]) 	Return a full array with the same shape and type as a given array.

#empty_like 
>>> a = ([1,2,3], [4,5,6])                         # a is array-like
>>> np.empty_like(a)
array([[-1073741821, -1073741821,           3],    #random
       [          0,           0, -1073741821]])


### Numpy - Array creation routines - From existing data
array(object[, dtype, copy, order, subok, ndmin]) 	Create an array.
asarray(a[, dtype, order]) 							Convert the input to an array.
asanyarray(a[, dtype, order]) 						Convert the input to an ndarray, but pass ndarraysubclasses through.
ascontiguousarray(a[, dtype]) 						Return a contiguous array in memory (C order).
asmatrix(data[, dtype]) 							Interpret the input as a matrix.
copy(a[, order]) 									Return an array copy of the given object.
frombuffer(buffer[, dtype, count, offset]) 			Interpret a buffer as a 1-dimensional array.
fromfile(file[, dtype, count, sep]) 				Construct an array from data in a text or binary file.
fromfunction(function, shape, **kwargs) 			Construct an array by executing a function over each coordinate.
fromiter(iterable, dtype[, count]) 					Create a new 1-dimensional array from an iterable object.
fromstring(string[, dtype, count, sep]) 			A new 1-D array initialized from raw binary or text data in a string.
loadtxt(fname[, dtype, comments, delimiter, ...]) 	Load data from a text file.


#fromfunction  
>>> np.fromfunction(lambda i, j: i + j, (3, 3), dtype=int)
array([[0, 1, 2],
       [1, 2, 3],
       [2, 3, 4]])
       
       
#fromfile 
#Construct an struct ndarray
#two files time and temp whereas time is another struct having min and sec fields 
>>> dt = np.dtype([('time', [('min', int), ('sec', int)]), ('temp', float)])
>>> x = np.zeros((1,), dtype=dt)
>>> x['time']['min'] = 10; x['temp'] = 98.25
>>> x
array([((10, 0), 98.25)],
      dtype=[('time', [('min', '<i4'), ('sec', '<i4')]), ('temp', '<f8')])

#Save the raw data to disk:
import os
fname = os.tmpnam()
x.tofile(fname)

#Read the raw data from disk:
>>> np.fromfile(fname, dtype=dt)
array([((10, 0), 98.25)],
      dtype=[('time', [('min', '<i4'), ('sec', '<i4')]), ('temp', '<f8')])


#The recommended way to store and load data:
>>> np.save(fname, x)
>>> np.load(fname + '.npy')
array([((10, 0), 98.25)],
      dtype=[('time', [('min', '<i4'), ('sec', '<i4')]), ('temp', '<f8')])


#loadtxt 
from io import StringIO   # StringIO behaves like a file object
c = StringIO("0 1\n2 3")
>>> np.loadtxt(c)
array([[ 0.,  1.],
       [ 2.,  3.]])

#fromstring
>>> np.fromstring('1 2', dtype=int, sep=' ')
array([1, 2])



### Numpy - Array creation routines - Numerical ranges
arange([start,] stop[, step,][, dtype]) 			Return evenly spaced values within a given interval.
linspace(start, stop[, num, endpoint, ...]) 		Return evenly spaced numbers over a specified interval.
logspace(start, stop[, num, endpoint, base, ...]) 	Return numbers spaced evenly on a log scale.
meshgrid(*xi, **kwargs) 							Return coordinate matrices from coordinate vectors.
mgrid 												nd_grid instance which returns a dense multi-dimensional 'meshgrid'.
ogrid 												nd_grid instance which returns an open multi-dimensional 'meshgrid'.


#numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)
>>> np.linspace(2.0, 3.0, num=5)
array([ 2.  ,  2.25,  2.5 ,  2.75,  3.  ])
#numpy.arange([start, ]stop, [step, ]dtype=None)
>>> np.arange(3.0)
array([ 0.,  1.,  2.])
#numpy.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None)
#base ** start is the starting value of the sequence.
>>> np.logspace(2.0, 3.0, num=4)
array([  100.        ,   215.443469  ,   464.15888336,  1000.        ])


#Difference of meshgrid, mgrid, ogrid, Scipy.ndgrid, Scipy.Boxgrid
#meshgrid- It is used to vectorise functions of two variables, so that you can write
x = np.array([1, 2, 3])
y = np.array([10, 20, 30]) 
XX, YY = np.meshgrid(x, y)  #XX is row stack of x, YY is column stack of y 
XX                          #
array([[1, 2, 3],
       [1, 2, 3],
       [1, 2, 3]])
YY
array([[10, 10, 10],
       [20, 20, 20],
       [30, 30, 30]])


ZZ = XX + YY    #all the combinations of x and y put into the function
ZZ => array([[11, 12, 13],
             [21, 22, 23],
             [31, 32, 33]])

#equivalent to below 
XX, YY = np.atleast_2d(x, y)
XX  #array([[1, 2, 3]])
YY #array([[10, 20, 30]])
YY = YY.T # transpose to allow broadcasting
ZZ = XX + YY


#mgrid and ogrid are helper classes which use index notation 
#without having to use 'linspace'. 
#Note The order in which the output are generated is reversed.
YY, XX = np.mgrid[10:40:10, 1:4]
XX
array([[1, 2, 3],
       [1, 2, 3],
       [1, 2, 3]])
YY
array([[10, 10, 10],
       [20, 20, 20],
       [30, 30, 30]])
ZZ = XX + YY # These are equivalent to the output of meshgrid

YY, XX = numpy.ogrid[10:40:10, 1:4]
XX  #array([[1, 2, 3]])
YY 
array([[10],
       [20],
       [30]])
ZZ = XX + YY # These are equivalent to the atleast_2d example

#Scipy.ndgrid === meshgrid, 
#Sci.pyBoxGrid is the class to help with this kind of generation.




### Numpy - Array creation routines- Building matrices
diag(v[, k]) 										Extract a diagonal or construct a diagonal array.
diagflat(v[, k]) 									Create a two-dimensional array with the flattened input as a diagonal.
tri(N[, M, k, dtype]) 								An array with ones at and below the given diagonal and zeros elsewhere.
tril(m[, k]) 										Lower triangle of an array.
triu(m[, k]) 										Upper triangle of an array.
vander(x[, N, increasing]) 							Generate a Vandermonde matrix.

#k is interpreted as 
#k : The sub-diagonal at and below which the array is filled. 
#k = 0 is the main diagonal, while k < 0 is below it, and k > 0 is above. The default is 0.

>>> np.tri(3, 5, 2, dtype=int)   # 3x5 matrix with 1s from 3rd diagonal lines and below
array([[1, 1, 1, 0, 0],
       [1, 1, 1, 1, 0],
       [1, 1, 1, 1, 1]])



>>> np.tri(3, 5, -1)
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.],
       [ 1.,  1.,  0.,  0.,  0.]])


#Example of diagflat
>>> np.diagflat([[1,2], [3,4]])
array([[1, 0, 0, 0],
       [0, 2, 0, 0],
       [0, 0, 3, 0],
       [0, 0, 0, 4]])



>>> np.diagflat([1,2], 1)  # k= 0, diagonal line , hence k=1 means 2nd diagonal line 
array([[0, 1, 0],
       [0, 0, 2],
       [0, 0, 0]])


#numpy.diag(v, k=0)
>>> x = np.arange(9).reshape((3,3))
>>> x
array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
#extract 
>>> np.diag(x)
array([0, 4, 8])
>>> np.diag(x, k=1)
array([1, 5])
>>> np.diag(x, k=-1)
array([3, 7])
#Create 
>>> np.diag(np.diag(x))
array([[0, 0, 0],
       [0, 4, 0],
       [0, 0, 8]])


       
       
             
    
### Numpy -  The Matrix class
mat(data[, dtype]) 									Interpret the input as a matrix.
bmat(obj[, ldict, gdict]) 							Build a matrix object from a string, nested sequence, or array.


#Example 

>>> x = np.array([[1, 2], [3, 4]])
>>> m = np.asmatrix(x)
>>> x[0,0] = 5
>>> m
matrix([[5, 2],
        [3, 4]])

#Example of bmat 

>>> A = np.mat('1 1; 1 1')
>>> B = np.mat('2 2; 2 2')
>>> C = np.mat('3 4; 5 6')
>>> D = np.mat('7 8; 9 0')

#All the following expressions construct the same block matrix:
>> np.bmat([[A, B], [C, D]])
matrix([[1, 1, 2, 2],
        [1, 1, 2, 2],
        [3, 4, 7, 8],
        [5, 6, 9, 0]])
>>> np.bmat(np.r_[np.c_[A, B], np.c_[C, D]])
matrix([[1, 1, 2, 2],
        [1, 1, 2, 2],
        [3, 4, 7, 8],
        [5, 6, 9, 0]])
>>> np.bmat('A,B; C,D')
matrix([[1, 1, 2, 2],
        [1, 1, 2, 2],
        [3, 4, 7, 8],
        [5, 6, 9, 0]])


        
        
        
        
### Numpy - Array manipulation routines - use as numpy., some are instance method 

##Basic operations
copyto(dst, src[, casting, where]) 		Copies values from one array to another, broadcasting as necessary.

## Changing arrayshape
reshape(a, newshape[, order]) 			Gives a new shape to an array without changing its data.
                                        One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions
ravel(a[, order]) 						Return a contiguous flattened array.
ndarray.flat 							A 1-D iterator over the array.
ndarray.flatten([order]) 				Return a copy of the array collapsed into one dimension.

#Example 
>>> a = np.arange(6).reshape((3, 2))
>>> a
array([[0, 1],
       [2, 3],
       [4, 5]])
	   
>>> np.ravel(a)
array([0, 1, 2, 3, 4, 5])



### Numpy - Array manipulation routines - Transpose-like operations
moveaxis(a, source, destination) 		Move axes of an array to new positions. 
rollaxis(a, axis[, start]) 				Roll the specified axis backwards, until it lies in a given position.
swapaxes(a, axis1, axis2) 				Interchange two axes of an array.
ndarray.T 								Same as self.transpose(), except that self is returned if self.ndim < 2.
transpose(a[, axes]) 					Permute the dimensions of an array.

#Example 
#moveaxis
#source : int or sequence of int
#Original positions of the axes to move. These must be unique.
#destination : int or sequence of int
#Destination positions for each of the original axes. These must also be unique

>>> x = np.zeros((3, 4, 5))
>>> np.moveaxis(x, 0, -1).shape   #move 0th dimension to last dimension 
(4, 5, 3)
>>> np.moveaxis(x, -1, 0).shape
(5, 3, 4)

#All same 
#transpose
#axes : list of ints, optional
#By default, reverse the dimensions, otherwise permute the axes according to the values given

>>> np.transpose(x).shape
(5, 4, 3)
>>> np.swapaxis(x, 0, -1).shape         #swap 0th dimension with last dimension 
(5, 4, 3)
>>> np.moveaxis(x, [0, 1], [-1, -2]).shape
(5, 4, 3)
>>> np.moveaxis(x, [0, 1, 2], [-1, -2, -3]).shape
(5, 4, 3)

#rollaxis 
#axis : int
#The axis to roll backwards. The positions of the other axes do not change relative to one another.
#start : int, optional
#The axis is rolled until it lies before this position. The default, 0, results in a 'complete' roll.

>>> a = np.ones((3,4,5,6))
>>> np.rollaxis(a, 3, 1).shape
(3, 6, 4, 5)
>>> np.rollaxis(a, 2).shape
(5, 3, 4, 6)
>>> np.rollaxis(a, 1, 4).shape
(3, 5, 6, 4)

##rollaxis vs moveaxis 
#To move one axis to particular index(of axes), use always moveaxis 
import numpy as np

#index:       0  1  2  3  4    
x = np.zeros((1, 2, 3, 4, 5))
for i in range(5):
    print(np.moveaxis(x, 3, i).shape)  #index 3(ie 4)

#Results in:
(4, 1, 2, 3, 5)
(1, 4, 2, 3, 5)
(1, 2, 4, 3, 5)
(1, 2, 3, 4, 5)
(1, 2, 3, 5, 4)

#rollaxis : start argument : The axis is rolled until it lies before this position

x = np.zeros((1, 2, 3, 4, 5))
for i in range(5):
    print(np.rollaxis(x, 3, i).shape)  #index 3(ie 4)
    
#result 
(4, 1, 2, 3, 5)
(1, 4, 2, 3, 5)
(1, 2, 4, 3, 5)
(1, 2, 3, 4, 5)
(1, 2, 3, 4, 5)  #*** note this 
 
#rollaxis :np.rollaxis(a,axis,start)
#after the roll the number at axis before the roll will be placed 
#just before the number at start before the roll.

#index                      :    0 1 2 3 4
a = np.arange(1*2*3*4*5).reshape(1,2,3,4,5)
>>> np.rollaxis(a,0,2).shape # the 1 will be before the 3.
(2, 1, 3, 4, 5)
>>> np.rollaxis(a,0,3).shape # the 1 will be before the 4.
(2, 3, 1, 4, 5)
>>> np.rollaxis(a,1,2).shape # the 2 will be before the 3.
(1, 2, 3, 4, 5)
>>> np.rollaxis(a,1,3).shape # the 2 will be before the 4.
(1, 3, 2, 4, 5)
>>> np.rollaxis(a,1,1).shape # the 2 can't be moved to before the 2.
(1, 2, 3, 4, 5)
>>> np.rollaxis(a,2, 2).shape # the 3 can't be moved to before the 3.
(1, 2, 3, 4, 5)
>>> np.rollaxis(a,1,5).shape # the 2 will be moved to the end.
(1, 3, 4, 5, 2)
>>> np.rollaxis(a,2,5).shape # the 3 will be moved to the end.
(1, 2, 4, 5, 3)
>>> np.rollaxis(a,4,5).shape # the 5 is already at the end.
(1, 2, 3, 4, 5)

#More example 
>>> x = np.zeros((3, 4, 5))
>>> np.moveaxis(x, 0, -1).shape
(4, 5, 3)
>>> np.rollaxis(x, 0, -1).shape
(4, 3, 5)
>>> np.moveaxis(x, 0,2).shape
(4, 5, 3)
>>> np.rollaxis(x, 0,2).shape
(4, 3, 5)
>>> np.moveaxis(x, 0,1).shape
(4, 3, 5)
>>> np.rollaxis(x, 0,1).shape
(3, 4, 5)

>>> a = np.ones((3,4,5,6))
>>> np.rollaxis(a, 3, 1).shape
(3, 6, 4, 5)
>>> np.moveaxis(a, 3, 1).shape
(3, 6, 4, 5)
>>> np.rollaxis(a, 3, 2).shape
(3, 4, 6, 5)
>>> np.moveaxis(a, 3, 2).shape
(3, 4, 6, 5)

### Numpy - Array manipulation routines - Changing number of dimensions
atleast_1d(*arys) 						Convert inputs to arrays with at least one dimension.
atleast_2d(*arys) 						View inputs as arrays with at least two dimensions.
atleast_3d(*arys) 						View inputs as arrays with at least three dimensions.
broadcast 								Produce an object that mimics broadcasting.
broadcast_to(array, shape[, subok]) 	Broadcast an array to a new shape.
broadcast_arrays(*args, **kwargs) 		Broadcast any number of arrays against each other.
expand_dims(a, axis) 					Expand the shape of an array.
squeeze(a[, axis]) 						Remove single-dimensional entries from the shape of an array.

#example 
>>> x = np.array([1, 2, 3])
>>> np.broadcast_to(x, (3, 3))
array([[1, 2, 3],
       [1, 2, 3],
       [1, 2, 3]])

       
#Example 
>>> np.atleast_3d(3.0)
array([[[ 3.]]])

>>> x = np.arange(3.0)  #array([ 0.,  1.,  2.])
>>> np.atleast_3d(x).shape
(1, 3, 1)

#expand dims
>>> x = np.array([1,2])
>>> x.shape
(2,)
>>> y = np.expand_dims(x, axis=0)
>>> y
array([[1, 2]])
>>> y.shape
(1, 2)

>>> y = np.expand_dims(x, axis=1) 
>>> y
array([[1],
       [2]])
>>> y.shape
(2, 1)

#Sequeze
>>> x = np.array([[[0], [1], [2]]])
>>> x.shape
(1, 3, 1)
>>> np.squeeze(x).shape
(3,)
>>> np.squeeze(x, axis=(2,)).shape
(1, 3)


### Numpy - Array manipulation routines - Joining arrays
concatenate((a1, a2, ...)[, axis]) 		Join a sequence of arrays along an existing axis.
stack(arrays[, axis]) 					Join a sequence of arrays along a new axis.
column_stack(tup) 						Stack 1-D arrays as columns into a 2-D array.
dstack(tup) 							Stack arrays in sequence depth wise (along third axis).
hstack(tup) 							Stack arrays in sequence horizontally (column wise).
vstack(tup) 							Stack arrays in sequence vertically (row wise).

#concatenate
>>> a = np.array([[1, 2], [3, 4]])
>>> b = np.array([[5, 6]])
>>> np.concatenate((a, b), axis=0) #row stack 
array([[1, 2],
       [3, 4],
       [5, 6]])
>>> np.concatenate((a, b.T), axis=1) #column stack ie concatenate in row 
array([[1, 2, 5],
       [3, 4, 6]])
       
>>> x = [-2.1, -1,  4.3]
>>> y = [3,  1.1,  0.12]
>>> np.vstack((x,y))   #vertical stack ie row stacking on top of one another
array([[-2.1 , -1.  ,  4.3 ],
       [ 3.  ,  1.1 ,  0.12]])
>>> np.hstack((x,y))  #horizontal stack 
array([-2.1 , -1.  ,  4.3 ,  3.  ,  1.1 ,  0.12])
>>> np.stack( (x,y) )    #axis=0 
array([[-2.1 , -1.  ,  4.3 ],
       [ 3.  ,  1.1 ,  0.12]])
>>> np.stack( (x,y) ,axis=0)  #vstack, rowstack 
array([[-2.1 , -1.  ,  4.3 ],
       [ 3.  ,  1.1 ,  0.12]])
>>> np.stack( (x,y) ,axis=1)  #column stacking 
array([[-2.1 ,  3.  ],
       [-1.  ,  1.1 ],
       [ 4.3 ,  0.12]])
>>> np.concatenate( (x,y) )
array([-2.1 , -1.  ,  4.3 ,  3.  ,  1.1 ,  0.12])
>>> np.concatenate( (x,y) , axis=0)
array([-2.1 , -1.  ,  4.3 ,  3.  ,  1.1 ,  0.12])

### Numpy - Array manipulation routines - Tiling arrays
tile(A, reps) 							Construct an array by repeating A the number of times given by reps.
                                        reps - array_like - The number of repetitions of A along each axis.
repeat(a, repeats[, axis]) 				Repeat elements of an array
                                        repeats : int or array of ints
                                        The number of repetitions for each element. 
                                        repeats is broadcasted to fit the shape of the given axis.


#tile(A, reps)
#reps : array_like,    The number of repetitions of A along each axis.
#If reps has length d, the result will have dimension of max(d, A.ndim)
>>> a = np.array([0, 1, 2])
>>> np.tile(a, 2)
array([0, 1, 2, 0, 1, 2])
>>> np.tile(a, (2, 2))         #each axis= repeat 2 times 
array([[0, 1, 2, 0, 1, 2],
       [0, 1, 2, 0, 1, 2]])
>>> np.tile(a, (2, 1, 2))
array([[[0, 1, 2, 0, 1, 2]],
       [[0, 1, 2, 0, 1, 2]]])
       
       
>>> b = np.array([[1, 2], [3, 4]])
>>> np.tile(b, 2)
array([[1, 2, 1, 2],
       [3, 4, 3, 4]])
>>> np.tile(b, (2, 1))
array([[1, 2],
       [3, 4],
       [1, 2],
       [3, 4]])

>>> c = np.array([1,2,3,4])
>>> np.tile(c,(4,1))
array([[1, 2, 3, 4],
       [1, 2, 3, 4],
       [1, 2, 3, 4],
       [1, 2, 3, 4]])


#repeat(a, repeats, axis=None)[source]
#repeats : int or array of ints
#The number of repetitions for each element. 
#repeats is broadcasted to fit the shape of the given axis. 
#axis : int, optional
#The axis along which to repeat values. By default, use the flattened input array, and return a flat output array.

>>> x = np.array([[1,2],[3,4]])
>>> np.repeat(x, 2)  #each element 2 times 
array([1, 1, 2, 2, 3, 3, 4, 4])
>>> np.repeat(x, 3, axis=1) #each column 3 times along axis=1 (column)
array([[1, 1, 1, 2, 2, 2],
       [3, 3, 3, 4, 4, 4]])
>>> np.repeat(x, [1, 2], axis=0) #along row, 1st row once, 2nd  row twice 
array([[1, 2],
       [3, 4],
       [3, 4]])

### Numpy - Array manipulation routines - Splitting arrays
split(ary, indices_or_sections[, axis]) Split an array into multiple sub-arrays.
array_split(ary, indices_or_sections[, axis]) 	Split an array into multiple sub-arrays.
dsplit(ary, indices_or_sections) 		Split array into multiple sub-arrays along the 3rd axis (depth).
hsplit(ary, indices_or_sections) 		Split an array into multiple sub-arrays horizontally (column-wise).
vsplit(ary, indices_or_sections) 		Split an array into multiple sub-arrays vertically (row-wise).

#split 
>>> x = np.arange(9.0)
>>> np.split(x, 3)
[array([ 0.,  1.,  2.]), array([ 3.,  4.,  5.]), array([ 6.,  7.,  8.])]

>>> x = np.arange(8.0)
>>> np.split(x, [3, 5, 6, 10])
[array([ 0.,  1.,  2.]),
 array([ 3.,  4.]),
 array([ 5.]),
 array([ 6.,  7.]),
 array([], dtype=float64)]
#hsplit 
>>> x = np.arange(16.0).reshape(4, 4)
>>> x
array([[  0.,   1.,   2.,   3.],
       [  4.,   5.,   6.,   7.],
       [  8.,   9.,  10.,  11.],
       [ 12.,  13.,  14.,  15.]])
>>> np.hsplit(x, 2)
[array([[  0.,   1.],
       [  4.,   5.],
       [  8.,   9.],
       [ 12.,  13.]]),
 array([[  2.,   3.],
       [  6.,   7.],
       [ 10.,  11.],
       [ 14.,  15.]])]
	   
#vsplit 
>>> x = np.arange(16.0).reshape(4, 4)
>>> x
array([[  0.,   1.,   2.,   3.],
       [  4.,   5.,   6.,   7.],
       [  8.,   9.,  10.,  11.],
       [ 12.,  13.,  14.,  15.]])
>>> np.vsplit(x, 2)
[array([[ 0.,  1.,  2.,  3.],
       [ 4.,  5.,  6.,  7.]]),
 array([[  8.,   9.,  10.,  11.],
       [ 12.,  13.,  14.,  15.]])]




	   
### Numpy - Array manipulation routines - Adding and removing elements
delete(arr, obj[, axis]) 				Return a new array with sub-arrays along an axis deleted.
insert(arr, obj, values[, axis]) 		Insert values along the given axis before the given indices.
append(arr, values[, axis]) 			Append values to the end of an array.
resize(a, new_shape) 					Return a new array with the specified shape.
trim_zeros(filt[, trim]) 				Trim the leading and/or trailing zeros from a 1-D array or sequence.
unique(ar[, return_index, return_inverse, ...]) 	Find the unique elements of an array.

#delete 
>>> arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])
>>> arr
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12]])
>>> np.delete(arr, 1, axis=0)        #axis=0, row, index = 1
array([[ 1,  2,  3,  4],
       [ 9, 10, 11, 12]])
>>> np.delete(arr, 1, axis=1)
array([[ 1,  3,  4],
       [ 5,  7,  8],
       [ 9, 11, 12]])
       
#insert 	   
>>> a = np.array([[1, 1], [2, 2], [3, 3]])
>>> a
array([[1, 1],
       [2, 2],
       [3, 3]])
>>> np.insert(a, 1, 5)             #insert 5 at index 1
array([1, 5, 1, 2, 2, 3, 3])
>>> np.insert(a, 1, 5, axis=1)  #column, index 1 , insert 5 
array([[1, 5, 1],
       [2, 5, 2],
       [3, 5, 3]])
	   
#append 
>>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])
array([1, 2, 3, 4, 5, 6, 7, 8, 9])

#When axis is specified, values must have the correct shape.
>>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0) #row 
array([[1, 2, 3],
       [4, 5, 6],
       [7, 8, 9]])





### Numpy - Broadcasting - Recycling elements of lower rank array during operations with two arrays  

## Various ways to add one array(mxn) with one vector(1xn) 
#OPTION 1 - row iteration(bad way )

import numpy as np

# We will add the vector v to each row of the matrix x,
# storing the result in the matrix y
x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
v = np.array([1, 0, 1])
y = np.empty_like(x)   # Create an empty matrix with the same shape as x

# Add the vector v to each row of the matrix x with an explicit loop
for i in range(4):
    y[i, :] = x[i, :] + v

# Now y is the following
# [[ 2  2  4]
#  [ 5  5  7]
#  [ 8  8 10]
#  [11 11 13]]



#OPTION 2 - Using high level function np.tile

import numpy as np

# We will add the vector v to each row of the matrix x,
# storing the result in the matrix y
x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
v = np.array([1, 0, 1])
vv = np.tile(v, (4, 1))  # axis=0, 4 times, axis=1, 1 times 
print vv                 # Prints "[[1 0 1]
                         #          [1 0 1]
                         #          [1 0 1]
                         #          [1 0 1]]"
y = x + vv  # Add x and vv elementwise
print y  # Prints "[[ 2  2  4
         #          [ 5  5  7]
         #          [ 8  8 10]
         #          [11 11 13]]"


##OPTION 3 - Broadcasting - recycling

import numpy as np

# We will add the vector v to each row of the matrix x,
# storing the result in the matrix y
x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
v = np.array([1, 0, 1])
y = x + v  # Add v to each row of x - Numpy recycles v(1x4)  4 times to make 4x4 array 
print y  # Prints "[[ 2  2  4]
         #          [ 5  5  7]
         #          [ 8  8 10]
         #          [11 11 13]]"



## Rules of Broadcasting 
1. Shape manipulation : Convert lower rank array's shape to higher rank shape by prepending 1
						For example vector with shape (3,) is converted to rank 2 array as (1,3) 
                        or rank 3 as (1,1,3)
						array with (2,3) is converted to rank 3 by (1,2,3)
						
2. Compatibility of arrays : Compatible if they have the same size in a dimension, 
                             or if one of the arrays has size 1 in that dimension.
							 For example , shape (2,3) is compatible with other array with shape (2,3) 
                             or (1,3) or (2,1) or (1,1) or (1,) 
                             then Broadcasting is possible for other array  (check below rule) 

3. Boradcasting  : If a dimesion is 1 , 
                   other dimesions are copied along that dimension 
                   when doing operations between arrays 
                   eg (1,3) is hstacked 2 times to get (2,3) ie np.tile(x13,(2,1))
                   (2,1) is vstacked three times to get (2,3) ie np.repeat(x21, 3, axis=1) or np.tile(x21, (1,3))
                   (1,1) or (1,) can be tiled ie np.tile(x11, (2,3))

## Rules of Broadcasting - Example:
If a.shape is (5,1), b.shape is (1,6), c.shape is (6,) 
and d.shape is () (ie d scalar), 
then a, b, c, and d are all broadcastable to dimension (5,6); 
	•a acts like a (5,6) array where a[:,0] is broadcast to the other columns,
	•b acts like a (5,6) array where b[0,:] is broadcast to the other rows,
	•c acts like a (1,6) array and therefore like a (5,6) array where c[:] is broadcast to every row
	•d acts like a (5,6) array where the single value is repeated.


#Example 
x1 = np.array([[1,2,3]])
x2 = np.array([1,2,3])
x1 + x2			#array([[2, 4, 6]])

x3 = np.array([1])
x1 + x3			#array([[2, 3, 4]])
x1 + 1			#array([[2, 3, 4]])


#Example 

import numpy as np

# Compute outer product of vectors
v = np.array([1,2,3])  # v has shape (3,)
w = np.array([4,5])    # w has shape (2,)
# To compute an outer product, we first reshape v to be a column
# vector of shape (3, 1)(broadcasting not possible as broadcast makes it (1,3);
# w (2,) is broadcast to (1,2) to yield
# an output of shape (3, 2), which is the outer product of v and w:
# [[ 4  5]
#  [ 8 10]
#  [12 15]]
print np.reshape(v, (3, 1)) * w

# Add a vector to each row of a matrix
x = np.array([[1,2,3], [4,5,6]])
# x has shape (2, 3) and v has shape (3,), convert to (1x3) ->(2x3) 
# giving the following matrix:
# [[2 4 6]
#  [5 7 9]]
print x + v

# Add a vector to each column of a matrix
# x has shape (2, 3) and w has shape (2,).
# x (2,3) --> transpose -> (3, 2) , w (2,) -> broadcast -> (1,2) -> (3,2)
# result (3,2) -> transpose -> (2,3) 
# [[ 5  6  7]
#  [ 9 10 11]]
print (x.T + w).T
# Another solution is to reshape w to be a row vector of shape (2, 1) and then broadcast -> (2,3)
# 
print x + np.reshape(w, (2, 1))

# Multiply a matrix by a constant:
# x has shape (2, 3). Numpy treats scalars as arrays of shape (), broadcast (1,1) -> (2,3)
# [[ 2  4  6]
#  [ 8 10 12]]
print x * 2


###Numpy - numpy.newaxis
#Either ndarray.reshape or numpy.newaxis can be used to add a new dimension to an array. 
>>> b
array([ 1.,  1.,  1.,  1.])
>>> c = b.reshape((1,4))
>>> c *= 2
>>> c
array([[ 2.,  2.,  2.,  2.]])
>>> c.shape
(1, 4)
>>> b
array([ 2.,  2.,  2.,  2.])
>>> d = b[np.newaxis,...]
>>> d
array([[ 2.,  2.,  2.,  2.]])
>>> d.shape
(1, 4)
>>> d *= 2
>>> b
array([ 4.,  4.,  4.,  4.])
>>> c
array([[ 4.,  4.,  4.,  4.]])
>>> d
array([[ 4.,  4.,  4.,  4.]])
>>> 



#the newaxis is used to increase the dimension of the existing array 
#by one more dimension, when used once. 
•1D array will become 2D array
•2D array will become 3D array
•3D array will become 4D array

##Scenario-1: 
#Example
# 1D array
In [7]: arr = np.arange(4)
In [8]: arr.shape
Out[8]: (4,)

# make it as row vector by inserting an axis along first dimension
In [9]: row_vec = arr[np.newaxis, :]
In [10]: row_vec.shape
Out[10]: (1, 4)

# make it as column vector by inserting an axis along second dimension
In [11]: col_vec = arr[:, np.newaxis]
In [12]: col_vec.shape
Out[12]: (4, 1)


##Scenario-2: 

#to add the following two arrays:
x1 = np.array([1, 2, 3, 4, 5])
x2 = np.array([5, 4, 3])

In [2]: x1_new = x1[:, np.newaxis]
# now, the shape of x1_new is (5, 1)
# array([[1],
#        [2],
#        [3],
#        [4],
#        [5]])

In [3]: x1_new + x2
Out[3]:
array([[ 6,  5,  4],
       [ 7,  6,  5],
       [ 8,  7,  6],
       [ 9,  8,  7],
       [10,  9,  8]])


#Alternatively, you can also add new axis to the array x2:
In [6]: x2_new = x2[:, np.newaxis]
In [7]: x2_new     # shape is (3, 1)
Out[7]: 
array([[5],
       [4],
       [3]])

Now, add:
In [8]: x1 + x2_new
Out[8]: 
array([[ 6,  7,  8,  9, 10],
       [ 5,  6,  7,  8,  9],
       [ 4,  5,  6,  7,  8]])

#Example
In [13]: A = np.ones((3,4,5,6))
In [14]: B = np.ones((4,6))
In [15]: (A + B[:, np.newaxis, :]).shape
Out[15]: (3, 4, 5, 6)

#You can also use None in place of np.newaxis; 
In [13]: np.newaxis is None
Out[13]: True


### Numpy - ufunc
##  Functions that support broadcasting are known as universal functions( instance of ufunc, a class) 
#these functions have below attributes 

## Available ufuncs - use with prefix numpy.

#x1,x2 are ndarray 
#The optional output arguments can be used to save memory for large calculations. 
#If arrays are large, complicated expressions can take longer than absolutely necessary 
#due to the creation and (later) destruction of temporary calculation spaces. 
#For example, the expression G = a * b + c is equivalent to t1 = A * B; G = T1 + C; del t1. 
#Use out param like as G = A * B; add(G, C, G) which is the same as G = A * B; G += C.

#elementwise 
add(x1, x2[, out]) 				Add arguments element-wise. 
subtract(x1, x2[, out]) 		Subtract arguments, element-wise. 
multiply(x1, x2[, out]) 		Multiply arguments element-wise. 
divide(x1, x2[, out]) 			Divide arguments element-wise. 

logaddexp(x1, x2[, out]) 		Logarithm of the sum of exponentiations of the inputs. 
logaddexp2(x1, x2[, out]) 		Logarithm of the sum of exponentiations of the inputs in base-2. 

true_divide(x1, x2[, out]) 		Returns a true division of the inputs, element-wise. 
floor_divide(x1, x2[, out]) 	Return the largest integer smaller or equal to the division of the inputs. 

negative(x[, out]) 				Numerical negative, element-wise. 
power(x1, x2[, out]) 			First array elements raised to powers from second array, element-wise. 
remainder(x1, x2[, out]) 		Return element-wise remainder of division. 
mod(x1, x2[, out]) 				Return element-wise remainder of division. 
fmod(x1, x2[, out]) 			Return the element-wise remainder of division. 
absolute(x[, out]) 				Calculate the absolute value element-wise. 

rint(x[, out]) 					Round elements of the array to the nearest integer. 
sign(x[, out]) 					Returns an element-wise indication of the sign of a number. 
conj(x[, out]) 					Return the complex conjugate, element-wise. 

exp(x[, out]) 					Calculate the exponential of all elements in the input array. 
exp2(x[, out]) 					Calculate 2**p for all p in the input array. 
log(x[, out]) 					Natural logarithm, element-wise. 
log2(x[, out]) 					Base-2 logarithm of x. 
log10(x[, out]) 				Return the base 10 logarithm of the input array, element-wise. 
expm1(x[, out]) 				Calculate exp(x) - 1 for all elements in the array. 
log1p(x[, out]) 				Return the natural logarithm of one plus the input array, element-wise. 
sqrt(x[, out]) 					Return the positive square-root of an array, element-wise. 
square(x[, out]) 				Return the element-wise square of the input. 

reciprocal(x[, out]) 			Return the reciprocal of the argument, element-wise. 
ones_like(a[, dtype, order, subok]) Return an array of ones with the same shape and type as a given array. 


#All trigonometric functions use radians as measurement of angle 
#elementwise 
sin(x[, out]) 					Trigonometric sine, element-wise. 
cos(x[, out]) 					Cosine element-wise. 
tan(x[, out]) 					Compute tangent element-wise. 
arcsin(x[, out]) 				Inverse sine, element-wise. 
arccos(x[, out]) 				Trigonometric inverse cosine, element-wise. 
arctan(x[, out]) 				Trigonometric inverse tangent, element-wise. 
arctan2(x1, x2[, out]) 			Element-wise arc tangent of x1/x2 choosing the quadrant correctly. 
hypot(x1, x2[, out]) 			Given the 'legs' of a right triangle, return its hypotenuse. 
sinh(x[, out]) 					Hyperbolic sine, element-wise. 
cosh(x[, out]) 					Hyperbolic cosine, element-wise. 
tanh(x[, out]) 					Compute hyperbolic tangent element-wise. 
arcsinh(x[, out]) 				Inverse hyperbolic sine element-wise. 
arccosh(x[, out]) 				Inverse hyperbolic cosine, element-wise. 
arctanh(x[, out]) 				Inverse hyperbolic tangent element-wise. 
deg2rad(x[, out]) 				Convert angles from degrees to radians. 
rad2deg(x[, out]) 				Convert angles from radians to degrees. 


#Bit-twiddling functions 
#These function all require integer arguments and they manipulate the bit-pattern of those arguments.
bitwise_and(x1, x2[, out]) 			Compute the bit-wise AND of two arrays element-wise. 
bitwise_or(x1, x2[, out]) 			Compute the bit-wise OR of two arrays element-wise. 
bitwise_xor(x1, x2[, out]) 			Compute the bit-wise XOR of two arrays element-wise. 
invert(x[, out]) 					Compute bit-wise inversion, or bit-wise NOT, element-wise. 
left_shift(x1, x2[, out]) 			Shift the bits of an integer to the left. 
right_shift(x1, x2[, out]) 			Shift the bits of an integer to the right. 

#Comparison functions - elementwise 
#it returns myltidimensional True or False based on elementwise comparison
#to return single True or False, use with np.all() ie all() means all elements True
#ge np.equal(x, x ).all()
greater(x1, x2[, out]) 				Return the truth value of (x1 > x2) element-wise. 
greater_equal(x1, x2[, out]) 		Return the truth value of (x1 >= x2) element-wise. 
less(x1, x2[, out]) 				Return the truth value of (x1 < x2) element-wise. 
less_equal(x1, x2[, out]) 			Return the truth value of (x1 =< x2) element-wise. 
not_equal(x1, x2[, out]) 			Return (x1 != x2) element-wise. 
equal(x1, x2[, out]) 				Return (x1 == x2) element-wise. 

#Do not use the Python keywords 'and' 'or' to combine logical array expressions. 
#'and' 'or' keywords will test the truth value of the entire array (not element-by-element)
#it returns myltidimensional True or False based on elementwise comparison
#to return single True or False, use with np.all()
logical_and(x1, x2[, out]) 		Compute the truth value of x1 AND x2 element-wise. 
logical_or(x1, x2[, out]) 		Compute the truth value of x1 OR x2 element-wise. 
logical_xor(x1, x2[, out]) 		Compute the truth value of x1 XOR x2, element-wise. 
logical_not(x[, out]) 			Compute the truth value of NOT x element-wise. 

maximum(x1, x2[, out]) 			Element-wise maximum of array elements. Dont use builtin max()
minimum(x1, x2[, out]) 			Element-wise minimum of array elements. Don't use builtin min()
#the behavior of maximum(a, b) is different than that of max(a, b). 
#As a ufunc, maximum(a, b) performs an element-by-element comparison of a and b 
#and chooses each element of the result according to which element in the two arrays is larger. 

fmax(x1, x2[, out]) 			Element-wise maximum of array elements. 
fmin(x1, x2[, out]) 			Element-wise minimum of array elements. 

isfinite(x[, out]) 				Test element-wise for finiteness (not infinity or not Not a Number). 
isinf(x[, out]) 				Test element-wise for positive or negative infinity. 
isnan(x[, out]) 				Test element-wise for NaN and return result as a boolean array. 
signbit(x[, out]) 				Returns element-wise True where signbit is set (less than zero). 
copysign(x1, x2[, out]) 		Change the sign of x1 to that of x2, element-wise. 
nextafter(x1, x2[, out]) 		Return the next floating-point value after x1 towards x2, element-wise. 
modf(x[, out1, out2]) 			Return the fractional and integral parts of an array, element-wise. 
ldexp(x1, x2[, out]) 			Returns x1 * 2**x2, element-wise. 
frexp(x[, out1, out2]) 			Decompose the elements of x into mantissa and twos exponent. 
fmod(x1, x2[, out]) 			Return the element-wise remainder of division. 
floor(x[, out]) 				Return the floor of the input, element-wise. 
ceil(x[, out]) 					Return the ceiling of the input, element-wise. 
trunc(x[, out]) 				Return the truncated value of the input, element-wise. 






### Numpy- ndarray - attributes 

# A 2-dimensional array of size 2 x 3, composed of 4-byte integer elements:

>>> x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)
>>> type(x)
<type 'numpy.ndarray'>
>>> x.shape
(2, 3)
>>> x.dtype
dtype('int32')


# ndarray objects can accommodate any strided indexing scheme.
# The column-major order (used, for example, in the Fortran language and in Matlab) 
# and row-major order (used in C) schemes are just specific kinds of strided scheme

## Array(ndarray) attributes
# Memory layout
ndarray.flags 			Information about the memory layout of the array. 
ndarray.shape 			Tuple of array dimensions. 
ndarray.strides 		Tuple of bytes to step in each dimension when traversing an array. 
ndarray.ndim 			Number of array dimensions. 
ndarray.data 			Python buffer object pointing to the start of the arrays data. 
ndarray.size 			Number of elements in the array. 
ndarray.itemsize 		Length of one array element in bytes. 
ndarray.nbytes 			Total bytes consumed by the elements of the array. 
ndarray.base 			Base object if memory is from some other object. 

#example 
>>> x = np.zeros((3, 5, 2), dtype=np.complex128)
>>> x.size
30
>>> np.prod(x.shape)
30


#Data type
ndarray.dtype 			Data-type of the arrays elements. 

# Other attributes

ndarray.T 				Same as self.transpose(), except that self is returned if self.ndim < 2. 
ndarray.real 			The real part of the array. 
ndarray.imag 			The imaginary part of the array. 
ndarray.flat			A 1-D iterator over the array. 
ndarray.ctypes 			An object to simplify the interaction of the array with the ctypes module. 

#Example 

>>> x = np.sqrt([1+0j, 0+1j])
>>> x.real
array([ 1.        ,  0.70710678])
>>> x.real.dtype
dtype('float64')




### Numpy- ndarray- instance methods - note almost are available as Global method on numpy.

## Arithmetic, matrix multiplication, and comparison operations
#element-wise operations, and yield ndarray objects as results.

#equivalent to the corresponding universal function (or ufunc)
(+, -, *, /, //, %, divmod(), ** or pow(), <<, >>, &, ^, |, ~) 
(==, <, >, <=, >=, !=) 


#  Unary operations:  -, +, abs(), ~


#Arithmetic, in-place: +=, -=, *=, /= , %=, <<=, **=, >>=, &=, |=, ^=


# Special methods - ndarray.copy(), copy.deepcopy(ndarray)


#container operations - x[y], x[i]=y, x[i:j], x[i:j] = y, y in x 

#Conversion; the operations complex, int, long, float, oct, and hex. 
#They work only on arrays that have one element in them and return the appropriate scalar.

# String representations: str(x), repr(x)



# Array conversion
ndarray.item(*args) 							Copy an element of an array to a standard Python scalar and return it. 
ndarray.tolist() 								Return the array as a (possibly nested) list. 
ndarray.itemset(*args) 							Insert scalar into an array (scalar is cast to arrays dtype, if possible) 
ndarray.tostring([order]) 						Construct Python bytes containing the raw data bytes in the array. 
ndarray.tobytes([order]) 						Construct Python bytes containing the raw data bytes in the array. 
ndarray.tofile(fid[, sep, format]) 				Write array to a file as text or binary (default). 
ndarray.dump(file) 								Dump a pickle of the array to the specified file. 
ndarray.dumps() 								Returns the pickle of the array as a string. 
ndarray.byteswap(inplace) 						Swap the bytes of the array elements 
ndarray.copy([order]) 							Return a copy of the array. 
ndarray.view([dtype, type]) 					New view of array with the same data. 
ndarray.getfield(dtype[, offset]) 				Returns a field of the given array as a certain type. 
ndarray.setflags([write, align, uic]) 			Set array flags WRITEABLE, ALIGNED, and UPDATEIFCOPY, respectively. 
ndarray.fill(value) 							Fill the array with a scalar value. 

#Example 
>>> a = np.array([1, 2])
>>> a.fill(0)
>>> a
array([0, 0])
>>> a = np.empty(2)
>>> a.fill(1)
>>> a
array([ 1.,  1.])

#Conversion/casting of each element of ndarray to a new type 
ndarray.astype(dtype[, order, casting, ...]) 	Copy of the array, cast to a specified type. 


x = np.array([1, 2, 2.5])
>>> x
array([ 1. ,  2. ,  2.5])

x.astype(int)
array([1, 2, 2])
>>> x.astype(str)
array(['1.0', '2.0', '2.5'],
      dtype='|S32')



# Shape manipulation - instance methods
ndarray.reshape(shape[, order]) 				Returns an array containing the same data with a new shape. 
ndarray.resize(new_shape[, refcheck]) 			Change shape and size of array in-place. 
ndarray.transpose(*axes) 						Returns a view of the array with axes transposed. 
ndarray.swapaxes(axis1, axis2) 					Return a view of the array with axis1 and axis2 interchanged. 
ndarray.flatten([order]) 						Return a copy of the array collapsed into one dimension. 
ndarray.ravel([order]) 							Return a flattened array. 
ndarray.squeeze([axis]) 						Remove single-dimensional entries from the shape of a. 

#Example 

>>> a = np.array([[0, 1], [2, 3]], order='C')
>>> a.resize((2, 1))
>>> a
array([[0],
       [1]])

	   
>>> b = np.array([[0, 1], [2, 3]])
>>> b.resize(2, 3) # new_shape parameter doesn't have to be a tuple
>>> b
array([[0, 1, 2],
       [3, 0, 0]])



# Item selection and manipulation
ndarray.take(indices[, axis, out, mode]) 		Return an array formed from the elements of a at the given indices. 
ndarray.put(indices, values[, mode]) 			Set a.flat[n] = values[n] for all n in indices. 
ndarray.repeat(repeats[, axis]) 				Repeat elements of an array. 
ndarray.choose(choices[, out, mode]) 			Use an index array to construct a new array from a set of choices. 
ndarray.sort([axis, kind, order]) 				Sort an array, in-place. 
ndarray.argsort([axis, kind, order]) 			Returns the indices that would sort this array. 
ndarray.partition(kth[, axis, kind, order]) 	Rearranges the elements in the array in such a way that value of the element in kth position is in the position it would be in a sorted array. 
ndarray.argpartition(kth[, axis, kind, order]) 	Returns the indices that would partition this array. 
ndarray.searchsorted(v[, side, sorter]) 		Find indices where elements of v should be inserted in a to maintain order. 
ndarray.nonzero() 								Return the indices of the elements that are non-zero. 
ndarray.compress(condition[, axis, out]) 		Return selected slices of this array along given axis. 
ndarray.diagonal([offset, axis1, axis2]) 


#take 
>>> a = [4, 3, 5, 7, 6, 8]
>>> indices = [0, 1, 4]
>>> np.take(a, indices)
array([4, 3, 6])

#sort - inplace 
>>> a = np.array([[1,4], [3,1]])
>>> a.sort(axis=1) #column varying ie rowwise 
>>> a
array([[1, 4],
       [1, 3]])
>>> a.sort(axis=0) #row varying ie columnwise 
>>> a
array([[1, 3],
       [1, 4]])


#Use the order keyword to specify a field to use when sorting a structured array:
>>> a = np.array([('a', 2), ('c', 1)], dtype=[('x', 'S1'), ('y', int)])
>>> a.sort(order='y')
>>> a
array([('c', 1), ('a', 2)],
      dtype=[('x', '|S1'), ('y', '<i4')])


#More ndarray methods 
ndarray.argmax([axis, out]) 						Return indices of the maximum values along the given axis. 
ndarray.min([axis, out, keepdims]) 					Return the minimum along a given axis. 
ndarray.argmin([axis, out]) 						Return indices of the minimum values along the given axis of a. 
ndarray.ptp([axis, out]) 							Peak to peak (maximum - minimum) value along a given axis. 
ndarray.clip([min, max, out]) 						Return an array whose values are limited to [min, max]. 
ndarray.conj() 										Complex-conjugate all elements. 
ndarray.round([decimals, out]) 						Return a with each element rounded to the given number of decimals. 
ndarray.trace([offset, axis1, axis2, dtype, out]) 	Return the sum along diagonals of the array. 

ndarray.sum([axis, dtype, out, keepdims]) 			Return the sum of the array elements over the given axis. 
ndarray.cumsum([axis, dtype, out]) 					Return the cumulative sum of the elements along the given axis. 
ndarray.mean([axis, dtype, out, keepdims]) 			Returns the average of the array elements along given axis. 
ndarray.var([axis, dtype, out, ddof, keepdims]) 	Returns the variance of the array elements, along given axis. 
ndarray.std([axis, dtype, out, ddof, keepdims]) 	Returns the standard deviation of the array elements along given axis. 
ndarray.prod([axis, dtype, out, keepdims]) 			Return the product of the array elements over the given axis 
ndarray.cumprod([axis, dtype, out]) 				Return the cumulative product of the elements along the given axis. 

ndarray.all([axis, out, keepdims]) 					Returns True if all elements evaluate to True. 
ndarray.any([axis, out, keepdims]) 					Returns True if any of the elements of a evaluate to True. 








### NumPy- Iterating Over Arrays

# Default iteration

#default iterator selects a sub-array of dimension N-1 from the array. 
#To loop over the entire array requires N for-loops.

#each element is  (2ndDimension, 3rdDimension....) array 
#and iterated 1stDimension times

>>> a = arange(24).reshape(3,2,4)+10
>>> for val in a:
        print 'item:', val
item: [[10 11 12 13]
 [14 15 16 17]]
item: [[18 19 20 21]
 [22 23 24 25]]
item: [[26 27 28 29]
 [30 31 32 33]]



# Flat iteration
ndarray.flat 				A 1-D iterator over the array. 

>>> for i, val in enumerate(a.flat):
       if i%5 == 0: print i, val
0 10
5 15
10 20
15 25
20 30


#N-dimensional enumeration
ndenumerate(arr)            Multidimensional ie (i,j,k,..) index iterator. 

>>> for i, val in ndenumerate(a):
			if sum(i)%5 == 0: print i, val
(0, 0, 0) 10
(1, 1, 3) 25
(2, 0, 3) 29
(2, 1, 2) 32


#https://docs.scipy.org/doc/numpy/reference/generated/numpy.nditer.html
#Single element  Iteration by using nditer
>>> a = np.arange(6).reshape(2,3)
>>> a
array([[0, 1, 2],
       [3, 4, 5]])
>>> for x in np.nditer(a):
...     print x,
...
0 1 2 3 4 5

# Controlling Iteration Order - by default layout order(row wise) 
#override with  with order=C for C order(row wise) and order=F for Fortran order(columnwise)

>>> a = np.arange(6).reshape(2,3)
>>> for x in np.nditer(a, order='F'):
      print x,
...
0 3 1 4 2 5
>>> for x in np.nditer(a.T, order='C'):
      print x,
...
0 3 1 4 2 5

#Modifying Array Values
#By default, the nditer treats the input array as a read-only object.

>>> a = np.arange(6).reshape(2,3)
>>> a
array([[0, 1, 2],
       [3, 4, 5]])
       
#Note here x[...] , x is used not a 
>>> for x in np.nditer(a, op_flags=['readwrite']):
      x[...] = 2 * x

>>> a
array([[ 0,  2,  4],
       [ 6,  8, 10]])

#Using an External Loop
#the elements of a  are provided by the iterator one at a time

>>> a = np.arange(6).reshape(2,3)
>>> for x in np.nditer(a, flags=['external_loop']):
      print x,
...
[0 1 2 3 4 5]

>>> for x in np.nditer(a):
      print x
...
0
1
2
3
4
5


>>> for x in np.nditer(a, flags=['external_loop'], order='F'):
         print x,
...
[0 3] [1 4] [2 5]


# Tracking an Index or Multi-Index
#During iteration, you may want to use the index of the current element in a computation. 

 
>>> a = np.arange(6).reshape(2,3)
>>> it = np.nditer(a, flags=['f_index'])
>>> while not it.finished:
       print "%d <%d>" % (it[0], it.index),
       it.iternext()
...
0 <0> 1 <2> 2 <4> 3 <1> 4 <3> 5 <5>



>> it = np.nditer(a, flags=['multi_index'])
>>> while not it.finished:
        print "%d <%s>" % (it[0], it.multi_index),
        it.iternext()
...
0 <(0, 0)> 1 <(0, 1)> 2 <(0, 2)> 3 <(1, 0)> 4 <(1, 1)> 5 <(1, 2)>



>>>> it = np.nditer(a, flags=['multi_index'], op_flags=['writeonly'])
>>> while not it.finished:
        it[0] = it.multi_index[1] - it.multi_index[0]
        it.iternext()
...
>>> a
array([[ 0,  1,  2],
       [-1,  0,  1]])



# Buffering the Array Elements
#By enabling buffering mode, the chunks provided by the iterator to the inner loop can be made larger, 
#significantly reducing the overhead of the Python interpreter. 

>> a = np.arange(6).reshape(2,3)
>>> for x in np.nditer(a, flags=['external_loop'], order='F'):
       print x,
...
[0 3] [1 4] [2 5]

>> for x in np.nditer(a, flags=['external_loop','buffered'], order='F'):
      print x,

[0 3 1 4 2 5]



### NumPy - Global Binary operations - use with prefix numpy.
#Note many are available in ndarray instance  as well 


#Elementwise bit operations
bitwise_and(x1, x2[, out]) 				Compute the bit-wise AND of two arrays element-wise.
bitwise_or(x1, x2[, out]) 				Compute the bit-wise OR of two arrays element-wise.
bitwise_xor(x1, x2[, out]) 				Compute the bit-wise XOR of two arrays element-wise.
invert(x[, out]) 						Compute bit-wise inversion, or bit-wise NOT, element-wise.
left_shift(x1, x2[, out]) 				Shift the bits of an integer to the left.
right_shift(x1, x2[, out]) 				Shift the bits of an integer to the right.

#Bit packing
packbits(myarray[, axis]) 				Packs the elements of a binary-valued array into bits in a uint8 array.
unpackbits(myarray[, axis]) 			Unpacks elements of a uint8 array into a binary-valued output array.

#Output formatting
binary_repr(num[, width]) 				Return the binary representation of the input number as a string.


#Example

#The number 13 is represented by 00001101. Likewise, 17 is represented by 00010001.
#The bit-wise AND of 13 and 17 is therefore 000000001, or 1:


>>> np.bitwise_and(13, 17)
1

>>> np.bitwise_and(14, 13)
12
>>> np.binary_repr(12)
'1100'
>>> np.bitwise_and([14,3], 13)
array([12,  1])


>>> np.bitwise_and([11,7], [4,25])
array([0, 1])

>>> np.bitwise_and(np.array([2,5,255]), np.array([3,14,16]))
array([ 2,  4, 16])

>>> np.bitwise_and([True, True], [False, True])
array([False,  True], dtype=bool)


### Numpy -  Global Logic functions - use with numpy.

#Truth value testing
all(a[, axis, out, keepdims]) 	Test whether all array elements along a given axis evaluate to True.
any(a[, axis, out, keepdims]) 	Test whether any array element along a given axis evaluates to True.


#Array contents
isfinite(x[, out]) 				Test element-wise for finiteness (not infinity or not Not a Number).
isinf(x[, out]) 				Test element-wise for positive or negative infinity.
isnan(x[, out]) 				Test element-wise for NaN and return result as a boolean array.
isneginf(x[, y]) 				Test element-wise for negative infinity, return result as bool array.
isposinf(x[, y]) 				Test element-wise for positive infinity, return result as bool array.

#Array type testing
iscomplex(x) 					Returns a bool array, where True if input element is complex.
iscomplexobj(x) 				Check for a complex type or an array of complex numbers.
isfortran(a) 					Returns True if array is arranged in Fortran-order in memory and not C-order.
isreal(x) 						Returns a bool array, where True if input element is real.
isrealobj(x) 					Return True if x is a not complex type or an array of complex numbers.
isscalar(num) 					Returns True if the type of num is a scalar type.

#Logical operations
#it returns myltidimensional True or False based on elementwise comparison
#to return single True or False, use with np.all() ie all() means all elements True

logical_and(x1, x2[, out]) 		Compute the truth value of x1 AND x2 element-wise.
logical_or(x1, x2[, out]) 		Compute the truth value of x1 OR x2 element-wise.
logical_not(x[, out]) 			Compute the truth value of NOT x element-wise.
logical_xor(x1, x2[, out]) 		Compute the truth value of x1 XOR x2, element-wise.

#Comparison
#it returns myltidimensional True or False based on elementwise comparison
#to return single True or False, use with np.all() ie all() means all elements True

allclose(a, b[, rtol, atol, equal_nan]) 	Returns True if two arrays are element-wise equal within a tolerance.
isclose(a, b[, rtol, atol, equal_nan]) 		Returns a boolean array where two arrays are element-wise equal within a tolerance.
array_equal(a1, a2) 						True if two arrays have the same shape and elements, False otherwise.
array_equiv(a1, a2) 						Returns True if input arrays are shape consistent and all elements equal.
greater(x1, x2[, out]) 						Return the truth value of (x1 > x2) element-wise.
greater_equal(x1, x2[, out]) 				Return the truth value of (x1 >= x2) element-wise.
less(x1, x2[, out]) 						Return the truth value of (x1 < x2) element-wise.
less_equal(x1, x2[, out]) 					Return the truth value of (x1 =< x2) element-wise.
equal(x1, x2[, out]) 						Return (x1 == x2) element-wise.
not_equal(x1, x2[, out]) 					Return (x1 != x2) element-wise.


#Example of all - each element must be True 
>>> np.all([[True,False],[True,True]])
False


>>> np.all([[True,False],[True,True]], axis=0) #axis =0 means along row varying/columnwise 
array([ True, False], dtype=bool)

>>> np.all([-1, 4, 5])
True

>>> np.all([1.0, np.nan])
True


>>> o=np.array([False])
>>> z=np.all([-1, 4, 5], out=o)
>>> id(z), id(o),                              z
(28293632, 28293632, array([ True], dtype=bool))

#Example of any - any of element is True ?
>>> np.any([[True, False], [True, True]])
True

>>> np.any([[True, False], [False, False]], axis=0)
array([ True, False], dtype=bool)

>>> np.any([-1, 0, 5])
True

>>> np.any(np.nan)
True

>>> o=np.array([False])
>>> z=np.any([-1, 4, 5], out=o)
>>> z, o
(array([ True], dtype=bool), array([ True], dtype=bool))
>>> # Check now that z is a reference to o
>>> z is o
True
>>> id(z), id(o) # identity of z and               o
(191614240, 191614240)




### Numpy- Global Mathematical functions - use with numpy.
#Many are available in instance 

#Below are supported
Trigonometric functions
Hyperbolic functions
Rounding
Exponents and logarithms
Arithmetic operations

### Numpy- Global Mathematical functions - Sums, products, differences
#Note axis=n means operation along nth dimension 
#which means nth dimension varying 
prod(a[, axis, dtype, out, keepdims]) 		Return the product of array elements over a given axis.
sum(a[, axis, dtype, out, keepdims]) 		Sum of array elements over a given axi s.
cumprod(a[, axis, dtype, out]) 				Return the cumulative product of elements along a given axi s.
cumsum(a[, axis, dtype, out]) 				Return the cumulative sum of the elements along a given axi s.
diff(a[, n, axis]) 							Calculate the n-th discrete difference along given axi s.
ediff1d(ary[, to_end, to_begin]) 			The differences between consecutive elements of an array.
gradient(f, *varargs, **kwargs) 			Return the gradient of an N-dimensional array.
cross(a, b[, axisa, axisb, axisc, axis]) 	Return the cross product of two (arrays of) vector s.
trapz(y[, x, dx, axis]) 					Integrate along the given axis using the composite trapezoidal rul e.


#Example of prod


>>> np.prod([[1.,2.],[3.,4.]])
24.0


#we can also specify the axis over which to multiply:
>>> np.prod([[1.,2.],[3.,4.]], axis=1)  #2nd dimension varying means rowwise 
array([  2.,  12.])

>>> np.sum([[0, 1], [0, 5]], axis=0)#1st dimension varying means columnwise 
array([0, 6])
>>> np.sum([[0, 1], [0, 5]], axis=1) #2nd dimension varying means rowwise 
array([1, 5])

#Example of cumsum
>>> a = np.array([[1,2,3], [4,5,6]])
>>> a
array([[1, 2, 3],       
[4, 5, 6]])
>>> np.cumsum(a)
array([ 1,  3,  6, 10, 15, 21])
>>> np.cumsum(a, dtype=float)     # specifies type of output value(s)
array([  1.,   3.,   6.,  10.,  15.,  21.])


>>> np.cumsum(a,axis=0)      #1st dimension varying means columnwise 
array([[1, 2, 3],       
[5, 7, 9]])
>>> np.cumsum(a,axis=1)      #2nd dimension varying means rowwise 
array([[ 1,  3,  6],       
[ 4,  9, 15]])
	   

#Example of diff
>>> x = np.array([1, 2, 4, 7, 0])
>>> np.diff(x)
array([ 1,  2,  3, -7])
>>> np.diff(x, n=2)
array([  1,   1, -10])

#Example of ediff1d
>>> x = np.array([1, 2, 4, 7, 0])
>>> np.ediff1d(x)
array([ 1,  2,  3, -7])

#Example of gradient
>>> x = np.array([1, 2, 4, 7, 11, 16], dtype=np.float)
>>> np.gradient(x)
array([ 1. ,  1.5,  2.5,  3.5,  4.5,  5. ])
>>> np.gradient(x, 2)
array([ 0.5 ,  0.75,  1.25,  1.75,  2.25,  2.5 ])


#For two dimensional arrays, the return will be two arrays ordered by axis.
#In this example the first arraystands for the gradient in rows and the second one in columns direction:

>>> np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=np.float))
[array([[ 2.,  2., -1.],        
[ 2.,  2., -1.]]), array([[ 1. ,  2.5,  4. ],        
[ 1. ,  1. ,  1. ]])]


>>> x = np.array([0, 1, 2, 3, 4])
>>> y = x**2
>>> np.gradient(y, edge_order=2)
array([-0.,  2.,  4.,  6.,  8.])


#Example of Vector cross-product.
>>> x = [1, 2, 3]
>>> y = [4, 5, 6]
>>> np.cross(x, y)
array([-3,  6, -3])


#One vector with dimension 2.
>>> x = [1, 2]
>>> y = [4, 5, 6]
>>> np.cross(x, y)
array([12, -6, -3])


#Equivalently:
>>> x = [1, 2, 0]
>>> y = [4, 5, 6]
>>> np.cross(x, y)
array([12, -6, -3])


#Both vectors with dimension 2.
>>> x = [1,2]
>>> y = [4,5]
>>> np.cross(x, y)
-3

#Example of trapz

>>> np.trapz([1,2,3])
4.0
>>> np.trapz([1,2,3], x=[4,6,8])
8.0
>>> np.trapz([1,2,3], dx=2)
8.0
>>> a = np.arange(6).reshape(2, 3)
>>> a
array([[0, 1, 2],       
[3, 4, 5]])
>>> np.trapz(a, axis=0)
array([ 1.5,  2.5,  3.5])
>>> np.trapz(a, axis=1)
array([ 2.,  8.])





### Numpy- Global Mathematical functions - Handling complex numbers
angle(z[, deg]) 			Return the angle of the complex argumen t.
real(val) 					Return the real part of the elements of the array.
imag(val) 					Return the imaginary part of the elements of the array.
conj(x[, out]) 				Return the complex conjugate, element-wis e.


#Example of angle

>>> np.angle([1.0, 1.0j, 1+1j])               # in radians
array([ 0.        ,  1.57079633,  0.78539816])
>>> np.angle(1+1j, deg=True)                  # in degrees
45.0

#Example of conjuga te
>>> x = np.eye(2) + 1j * np.eye(2)
>>> np.conjugate(x)
array([[ 1.-1.j,  0.-0.j],       
[ 0.-0.j,  1.-1.j]])



###Numpy -  Miscellaneous Global function - use with numpy.

convolve(a, v[, mode]) 						Returns the discrete, linear convolution of two one-dimensional sequence s.
clip(a, a_min, a_max[, out]) 				Clip (limit) the values in an array.
sqrt(x[, out]) 								Return the positive square-root of an array, element-wis e.
square(x[, out]) 							Return the element-wise square of the inpu t.
absolute(x[, out]) 							Calculate the absolute value element-wis e.
fabs(x[, out]) 								Compute the absolute values element-wis e.
sign(x[, out]) 								Returns an element-wise indication of the sign of a numbe r.
maximum(x1, x2[, out]) 						Element-wise maximum of array element s.
minimum(x1, x2[, out]) 						Element-wise minimum of array element s.
fmax(x1, x2[, out]) 						Element-wise maximum of array element s.
fmin(x1, x2[, out]) 						Element-wise minimum of array element s.
nan_to_num(x) 								Replace nan with zero and inf with finite number s.
real_if_close(a[, tol]) 					If complex input returns a real array if complex parts are close to zer o.
interp(x, xp, fp[, left, right, period]) 	One-dimensional linear interpolation


#Note how the convolution operator flips the second array before 'sliding' the two across one another:

>>> np.convolve([1, 2, 3], [0, 1, 0.5])
array([ 0. ,  1. ,  2.5,  4. ,  1.5])

#Only return the middle values of the convolution. Contains boundary effects, where zeros are taken into account:

>>> np.convolve([1,2,3],[0,1,0.5], 'same')
array([ 1. ,  2.5,  4. ])

#The two arrays are of the same length, so there is only one position where they completely overlap:
>>> np.convolve([1,2,3],[0,1,0.5], 'valid')
array([ 2.5])

#Example interp
#interp(x, xp, fp[, left, right, period]) 	One-dimensional linear interpolation

>>> xp = [1, 2, 3]
>>> fp = [3, 2, 0]
>>> np.interp(2.5, xp, fp)
1.0
>>> np.interp([0, 1, 1.5, 2.72, 3.14], xp, fp)
array([ 3. ,  3. ,  2.5 ,  0.56,  0. ])
>>> UNDEF = -99.0
>>> np.interp(3.14, xp, fp, right=UNDEF)
-99.0

#Plot an interpolant to the sine function:
#interp(x, xp, fp[, left, right, period]) 	One-dimensional linear interpolation
***

>>> x = np.linspace(0, 2*np.pi, 10)
>>> y = np.sin(x)
>>> xvals = np.linspace(0, 2*np.pi, 50)
>>> yinterp = np.interp(xvals, x, y)
>>> import matplotlib.pyplot as plt
>>> plt.plot(x, y, 'o')
[<matplotlib.lines.Line2D object at 0x...>]
>>> plt.plot(xvals, yinterp, '-x')
[<matplotlib.lines.Line2D object at 0x...>]
>>> plt.show()





### NumPy - Sorting, searching, and counting Global Methods - use with numpy.


#Sorting
#Note axis=n means operation along nth dimension 
#which means nth dimension varying 
sort(a[, axis, kind, order]) 				Return a sorted copy of an array.
lexsort(keys[, axis]) 						Perform an indirect sort using a sequence of keys.
argsort(a[, axis, kind, order]) 			Returns the indices that would sort an array.
ndarray.sort([axis, kind, order]) 			Sort an array, in-place.
msort(a) 									Return a copy of an arraysorted along the first axis.
sort_complex(a) 							Sort a complex array using the real part first, then the imaginary part.
partition(a, kth[, axis, kind, order]) 		Return a partitioned copy of an array.
argpartition(a, kth[, axis, kind, order]) 	Perform an indirect partition along the given axis using the algorithm specified by the kind keyword.


#Example of partition
>>> a = np.array([3, 4, 2, 1])
>>> np.partition(a, 3)
array([2, 1, 3, 4])

>>> np.partition(a, (1, 3))
array([1, 2, 3, 4])

#Example of sort - immutable 
#numpy.sort(a, axis=-1, kind='quicksort', order=None)	Return a sorted copy of an array.
#-1 means along last axis 
kind        speed       worst case      work space      stable
'quicksort'     1       O(n^2)          0               no 
'mergesort'     2       O(n*log(n))     ~n/2            yes 
'heapsort'      3       O(n*log(n))     0               no 

>>> a = np.array([[1,4],[3,1]])
>>> np.sort(a)                # sort along the last axis
array([[1, 4],       
[1, 3]])
>>> np.sort(a, axis=None)     # sort the flattened array
array([1, 1, 3, 4])
>>> np.sort(a, axis=0)        # sort along the first axis
array([[1, 1],       
[3, 4]])

#Use the order keyword to specify a field to use when sorting a structured array:
>>> dtype = [('name', 'S10'), ('height', float), ('age', int)]
>>> values = [('Arthur', 1.8, 41), ('Lancelot', 1.9, 38),     ('Galahad', 1.7, 38)]
>>> a = np.array(values, dtype=dtype)       # create a structured array
>>> np.sort(a, order='height')
array([('Galahad', 1.7, 38), ('Arthur', 1.8, 41),       
('Lancelot', 1.8999999999999999, 38)],      
dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])

#Sort by age, then height if ages are equal:
>>> np.sort(a, order=['age', 'height'])
array([('Galahad', 1.7, 38), ('Lancelot', 1.8999999999999999, 38),       
('Arthur', 1.8, 41)],      
dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])

#Sort names: first by surname, then by name.


>>> surnames =    ('Hertz',    'Galilei', 'Hertz')
>>> first_names = ('Heinrich', 'Galileo', 'Gustav')
>>> ind = np.lexsort((first_names, surnames))
>>> ind
array([1, 2, 0])

>>> [surnames[i] + ", " + first_names[i] for i in ind]
['Galilei, Galileo', 'Hertz, Gustav', 'Hertz, Heinrich']


#Sort two columns of numbers:


>>> a = [1,5,1,4,3,4,4] # First column
>>> b = [9,4,0,4,0,2,1] # Second column
>>> ind = np.lexsort((b,a)) # Sort by a, then by b
>>> print(ind)
[2 0 4 6 5 3 1]

>>> [(a[i],b[i]) for i in ind]
[(1, 0), (1, 9), (3, 0), (4, 1), (4, 2), (4, 4), (5, 4)]





# Searching
#Note axis=n means operation along nth dimension 
#which means nth dimension varying 
#When no axis is given, either array is flattened or based on last axis/dimension(ie -1), check ref 

argmax(a[, axis, out]) 				Returns the indices of the maximum values along an axis.
nanargmax(a[, axis]) 				Return the indices of the maximum values in the specified axis ignoring NaNs.
argmin(a[, axis, out]) 				Returns the indices of the minimum values along an axis.
nanargmin(a[, axis]) 				Return the indices of the minimum values in the specified axis ignoring NaNs.
argwhere(a) 						Find the indices of array elements that are non-zero, grouped by element.
nonzero(a) 							Return the indices of the elements that are non-zero.
flatnonzero(a) 						Return indices that are non-zero in the flattened version of a.
where(condition, [x, y]) 			Return elements, either from x or y, depending on condition.
searchsorted(a, v[, side, sorter]) 	Find indices where elements should be inserted to maintain order.
extract(condition, arr) 			Return the elements of an array that satisfy some condition.

#example of argmax - returns indices 
>>> a = np.arange(6).reshape(2,3)
>>> a
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.argmax(a)   #falttens 
5
>>> np.argmax(a, axis=0)  #1st dimension varying ie columnwise 
array([1, 1, 1])
>>> np.argmax(a, axis=1) #2nd dimension varying ie rowwise 
array([2, 2])


#Example of amax - returns elements (reference under NumPy - stats)
>>> a = np.arange(4).reshape((2,2))
>>> a
array([[0, 1],
       [2, 3]])
>>> np.amax(a)           # Maximum of the flattened array
3
>>> np.amax(a, axis=0)   # Maxima along the first axis
array([2, 3])
>>> np.amax(a, axis=1)   # Maxima along the second axis
array([1, 3])


#Example of extract
>>> arr = np.arange(12).reshape((3, 4))
>>> arr
array([[ 0,  1,  2,  3],       
[ 4,  5,  6,  7],       
[ 8,  9, 10, 11]])
>>> condition = np.mod(arr, 3)==0
>>> condition
array([[ True, False, False,  True],       
[False, False,  True, False],       
[False,  True, False, False]], dtype=bool)
>>> np.extract(condition, arr)
array([0, 3, 6, 9])


#If condition is boolean:
>>> arr[condition]
array([0, 3, 6, 9])



#Counting
count_nonzero(a) 	Counts the number of non-zero values in the array a.

#Example of Counti ng
>>> np.count_nonzero(np.eye(4))
4
>>> np.count_nonzero([[0,1,7,0,0],[3,0,0,2,19]])
5





### NumPy- Set Global routines - use with numpy.


#Making proper sets
unique(ar[, return_index, return_inverse, ...]) Find the unique elements of an array.


#Boolean operations
in1d(ar1, ar2[, assume_unique, invert]) 		Test whether each element of a 1-D array is also present in a second array.
intersect1d(ar1, ar2[, assume_unique]) 			Find the intersection of two arrays.
setdiff1d(ar1, ar2[, assume_unique]) 			Find the set difference of two array s.
setxor1d(ar1, ar2[, assume_unique]) 			Find the set exclusive-or of two array s.
union1d(ar1, ar2) 								Find the union of two array s.


#Example of intersect1d
>>> np.intersect1d([1, 3, 4, 3], [3, 1, 2, 1])
array([1, 3])


#To intersect more than two arrays, use functools.reduce:
>>> from functools import reduce
>>> reduce(np.intersect1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2]))
array([3])





###Numpy - Input and output - use with numpy.

#Numpy binary files (NPY, NPZ)
load(file[, mmap_mode, allow_pickle, ...]) 		Load arrays or pickled objects from .npy, .npz or pickled file s.
save(file, arr[, allow_pickle, fix_imports]) 	Save an array to a binary file in NumPy .npy forma t.
savez(file, *args, **kwds) 						Save several arrays into a single file in uncompressed .npz forma t.
savez_compressed(file, *args, **kwds) 			Save several arrays into a single file in compressed .npz forma t.

#The format of these binary file types is documented in http://docs.scipy.org/doc/numpy/neps/npy-format.html


#Text files
loadtxt(fname[, dtype, comments, delimiter, ...]) 		Load data from a text fil e.
savetxt(fname, X[, fmt, delimiter, newline, ...]) 		Save an array to a text fil e.
genfromtxt(fname[, dtype, comments, ...]) 				Load data from a text file, with missing values handled as specifie d.
fromregex(file, regexp, dtype) 							Construct an array from a text file, using regular expression parsin g.
fromstring(string[, dtype, count, sep]) 				A new 1-D array initialized from raw binary or text data in a strin g.
ndarray.tofile(fid[, sep, format]) 						Write array to a file as text or binary (default ).
ndarray.tolist() 										Return the array as a (possibly nested) lis t.


#Example from fromregex
>>> f = open('test.dat', 'w')
>>> f.write("1312 foo\n1534  bar\n444   qux")
>>> f.close()



>>> regexp = r"(\d+)\s+(...)"  # match [digits, whitespace, anything]
>>> output = np.fromregex('test.dat', regexp,    [('num', np.int64), ('key', 'S3')])
>>> output
array([(1312L, 'foo'), (1534L, 'bar'), (444L, 'qux')],      
dtype=[('num', '<i8'), ('key', '|S3')])
>>> output['num']
array([1312, 1534,  444], dtype=int64)


#Raw binary files
fromfile(file[, dtype, count, sep]) 			Construct an array from data in a text or binary fil e.
ndarray.tofile(fid[, sep, format]) 				Write array to a file as text or binary (default ).

#Example 
>>> dt = np.dtype([('time', [('min', int), ('sec', int)]), ('temp', float)])
>>> x = np.zeros((1,), dtype=dt)
>>> x['time']['min'] = 10; x['temp'] = 98.25
>>> x
array([((10, 0), 98.25)],
      dtype=[('time', [('min', '<i4'), ('sec', '<i4')]), ('temp', '<f8')])

#Save the raw data to disk:
import os
fname = os.tmpnam()
x.tofile(fname)

#Read the raw data from disk:
>>> np.fromfile(fname, dtype=dt)
array([((10, 0), 98.25)],
      dtype=[('time', [('min', '<i4'), ('sec', '<i4')]), ('temp', '<f8')])


#The recommended way to store and load data:
>>> np.save(fname, x)
>>> np.load(fname + '.npy')
array([((10, 0), 98.25)],
      dtype=[('time', [('min', '<i4'), ('sec', '<i4')]), ('temp', '<f8')])


#String formatting
array2string(a[, max_line_width, precision, ...]) 	Return a string representation of an arra y.
array_repr(arr[, max_line_width, precision, ...]) 	Return the string representation of an arra y.
array_str(a[, max_line_width, precision, ...]) 	Return a string representation of the data in an arra y.

#Example of array2string
>>> x = np.array([1e-16,1,2,3])
>>> print(np.array2string(x, precision=2, separator=',',    suppress_small=True))
[ 0., 1., 2., 3.]


#Memory mapping files
memmap 						Create a memory-map to an array stored in a binary file on dis k.


#Text formatting options
set_printoptions([precision, threshold, ...]) 			Set printing option s.
get_printoptions() 										Return the current print option s.
set_string_function(f[, repr]) 							Set a Python function to be used when pretty printing array s.


#Base-n representations
binary_repr(num[, width]) 						Return the binary representation of the input number as a strin g.
base_repr(number[, base, padding]) 				Return a string representation of a number in the given base syste m.


#Data sources
DataSource([destpath]) 							A generic data source file (file, http, ftp, ... ).






### Numpy- Standard array subclasses -  Matrix objects, use with prefix numpy.

#matrix objects inherit from the ndarray and therefore, 
#they have the same attributes and methods of ndarrays. 

#1. Matrix objects can be created using a string notation to allow Matlab-style syntax 
#where spaces separate columns and semicolons (';) separate rows.

#2. Matrix objects are always two-dimensional.

#3. Matrices have special attributes which make calculations easier. These are
matrix.T Returns the transpose of the matrix. 
matrix.H Returns the (complex) conjugate transpose of self. 
matrix.I Returns the (multiplicative) inverse of invertible self. 
matrix.A Return self as an ndarray object. 

 
#4. Matrix objects over-ride multiplication, *, and power, **, 
#to be matrix-multiplication and matrix power

	   
#Creation of matrix    
#The name 'mat' is an alias for 'matrix' in NumPy.
matrix 							Returns a matrix from an array-like object, or from a string of data. 
asmatrix(data[, dtype]) 		Interpret the input as a matrix. 
bmat(obj[, ldict, gdict]) 		Build a matrix object from a string, nested sequence, or array. 

# Matrix creation from a string
>> a=np.mat('1 2 3; 4 5 3')
>>> print (a*a.T).I
[[ 0.2924 -0.1345]
 [-0.1345  0.0819]]


# Matrix creation from nested sequence
>>> np.mat([[1,5,10],[1.0,3,4j]])
matrix([[  1.+0.j,   5.+0.j,  10.+0.j],
        [  1.+0.j,   3.+0.j,   0.+4.j]])


#Matrix creation from an array
>>> np.mat(random.rand(3,3)).T
matrix([[ 0.7699,  0.7922,  0.3294],
        [ 0.2792,  0.0101,  0.9219],
        [ 0.3398,  0.7571,  0.8197]])

		
##Attributes
A           Return self as an ndarray object. 
A1          Return self as a flattened ndarray. 
H           Returns the (complex) conjugate transpose of self. 
I           Returns the (multiplicative) inverse of invertible self. 
T           Returns the transpose of the matrix. 

base        Base object if memory is from some other object. 
ctypes      An object to simplify the interaction of the array with the ctypes module. 
data        Python buffer object pointing to the start of the array's data. 
dtype       Data-type of the array's elements. 
flags       Information about the memory layout of the array. 

flat        A 1-D iterator over the array. 
imag        The imaginary part of the array. 
itemsize    Length of one array element in bytes. 
nbytes      Total bytes consumed by the elements of the array. 
ndim        Number of array dimensions. 
real        The real part of the array. 
shape       Tuple of array dimensions. 
size        Number of elements in the array. 
strides     Tuple of bytes to step in each dimension when traversing an array. 

##Methods
#axis=0, row , axis=1, column 
#Note many methods operates along a axis, means, that axis is varying 
#eg axis=0, row varying means columnwise , axis=1, column varying means rowwise

all([axis, out])                        Test whether all matrix elements along a given axis evaluate to True. 
any([axis, out])                        Test whether any array element along a given axis evaluates to True. 
argmax([axis, out])                     Indexes of the maximum values along an axis. 
argmin([axis, out])                     Indexes of the minimum values along an axis. 
argpartition(kth[, axis, kind, order])  Returns the indices that would partition this array. 
argsort([axis, kind, order])            Returns the indices that would sort this array. 
astype(dtype[, order, casting, subok, copy])    Copy of the array, cast to a specified type. 

byteswap(inplace)                   Swap the bytes of the array elements 
choose(choices[, out, mode])        Use an index array to construct a new array from a set of choices. 
clip([min, max, out])               Return an array whose values are limited to [min, max]. 
compress(condition[, axis, out])    Return selected slices of this array along given axis. 
conj()                              Complex-conjugate all elements. 
conjugate()                         Return the complex conjugate, element-wise. 
copy([order])                       Return a copy of the array. 
cumprod([axis, dtype, out])         Return the cumulative product of the elements along the given axis. 
cumsum([axis, dtype, out])          Return the cumulative sum of the elements along the given axis. 

diagonal([offset, axis1, axis2])    Return specified diagonals. 
dot(b[, out])                       Dot product of two arrays. 
dump(file)                          Dump a pickle of the array to the specified file. 
dumps()                             Returns the pickle of the array as a string. 
fill(value)                         Fill the array with a scalar value. 
flatten([order])                    Return a flattened copy of the matrix. 

getA()                              Return self as an ndarray object. 
getA1()                             Return self as a flattened ndarray. 
getH()                              Returns the (complex) conjugate transpose of self. 
getI()                              Returns the (multiplicative) inverse of invertible self. 
getT()                              Returns the transpose of the matrix. 

getfield(dtype[, offset])           Returns a field of the given array as a certain type. 
item(*args)                         Copy an element of an array to a standard Python scalar and return it. 
itemset(*args)                      Insert scalar into an array (scalar is cast to array's dtype, if possible) 

max([axis, out])                    Return the maximum value along an axis. 
mean([axis, dtype, out])            Returns the average of the matrix elements along the given axis. 
min([axis, out])                    Return the minimum value along an axis. 
newbyteorder([new_order])           Return the array with the same data viewed with a different byte order. 
nonzero()                           Return the indices of the elements that are non-zero. 
partition(kth[, axis, kind, order]) Rearranges the elements in the array in such a way that value of the element in kth position is in the position it would be in a sorted array. 
prod([axis, dtype, out])            Return the product of the array elements over the given axis. 
ptp([axis, out])                    Peak-to-peak (maximum - minimum) value along the given axis. 
put(indices, values[, mode])        Set a.flat[n] = values[n] for all n in indices. 

ravel([order])                      Return a flattened matrix. 
repeat(repeats[, axis])             Repeat elements of an array. 
reshape(shape[, order])             Returns an array containing the same data with a new shape. 
resize(new_shape[, refcheck])       Change shape and size of array in-place. 
round([decimals, out])              Return a with each element rounded to the given number of decimals. 
searchsorted(v[, side, sorter])     Find indices where elements of v should be inserted in a to maintain order. 
setfield(val, dtype[, offset])      Put a value into a specified place in a field defined by a data-type. 
setflags([write, align, uic])       Set array flags WRITEABLE, ALIGNED, and UPDATEIFCOPY, respectively. 
sort([axis, kind, order])           Sort an array, in-place. 
squeeze([axis])                     Return a possibly reshaped matrix. 
std([axis, dtype, out, ddof])       Return the standard deviation of the array elements along the given axis. 
sum([axis, dtype, out])             Returns the sum of the matrix elements, along the given axis. 
swapaxes(axis1, axis2)              Return a view of the array with axis1 and axis2 interchanged. 
take(indices[, axis, out, mode])    Return an array formed from the elements of a at the given indices. 
tobytes([order])                    Construct Python bytes containing the raw data bytes in the array. 
tofile(fid[, sep, format])          Write array to a file as text or binary (default). 
tolist()                            Return the matrix as a (possibly nested) list. 
tostring([order])                   Construct Python bytes containing the raw data bytes in the array. 

trace([offset, axis1, axis2, dtype, out]) Return the sum along diagonals of the array. 
transpose(*axes)                    Returns a view of the array with axes transposed. 
var([axis, dtype, out, ddof])       Returns the variance of the matrix elements, along the given axis. 
view([dtype, type])                 New view of array with the same data. 
        
        
        
        
        
		
		
### Numpy- Standard array subclasses - Memory-mapped file arrays

memmap 					Create a memory-map to an array stored in a binary file on disk. 
memmap.flush() 			Write any changes in the array to the file on disk. 

>>> a = memmap('newfile.dat', dtype=float, mode='w+', shape=1000)
>>> a[10] = 10.0
>>> a[30] = 30.0
>>> del a
>>> b = fromfile('newfile.dat', dtype=float)
>>> print b[10], b[30]
10.0 30.0
>>> a = memmap('newfile.dat', dtype=float)
>>> print a[10], a[30]
10.0 30.0




### Numpy- Standard array subclasses - Masked arrays - numpy.ma
##check reference 






### Numpy - Datetimes and Timedeltas - subclass of numpy.ndarray 
#Starting in NumPy 1.7, there are core array data types which natively support datetime functionality. 
#The data type is called 'datetime64'

#A simple ISO date: YYY-MM-DD
>>> np.datetime64('2005-02-25')
numpy.datetime64('2005-02-25')


#Using months for the unit:
>>> np.datetime64('2005-02')
numpy.datetime64('2005-02')


#Specifying just the month, but forcing a 'days unit:
>>> np.datetime64('2005-02', 'D')
numpy.datetime64('2005-02-01')


#From a date and time:
>>> np.datetime64('2005-02-25T03:30')
numpy.datetime64('2005-02-25T03:30')


>>> np.array(['2007-07-13', '2006-01-13', '2010-08-13'], dtype='datetime64')
array(['2007-07-13', '2006-01-13', '2010-08-13'], dtype='datetime64[D]')

>>> np.array(['2001-01-01T12:00', '2002-02-03T13:56:03.172'], dtype='datetime64')
array(['2001-01-01T12:00:00.000-0600', '2002-02-03T13:56:03.172-0600'], dtype='datetime64[ms]')


#Using arange - All the dates for one month:
>>> np.arange('2005-02', '2005-03', dtype='datetime64[D]')
array(['2005-02-01', '2005-02-02', '2005-02-03', '2005-02-04',
       '2005-02-05', '2005-02-06', '2005-02-07', '2005-02-08',
       '2005-02-09', '2005-02-10', '2005-02-11', '2005-02-12',
       '2005-02-13', '2005-02-14', '2005-02-15', '2005-02-16',
       '2005-02-17', '2005-02-18', '2005-02-19', '2005-02-20',
       '2005-02-21', '2005-02-22', '2005-02-23', '2005-02-24',
       '2005-02-25', '2005-02-26', '2005-02-27', '2005-02-28'],
       dtype='datetime64[D]')


#The datetime object represents a single moment in time, 
#if two datetimes have different units,
>>> np.datetime64('2005') == np.datetime64('2005-01-01')
True
>>> np.datetime64('2010-03-14T15Z') == np.datetime64('2010-03-14T15:00:00.00Z')
True



##Datetime and Timedelta Arithmetic
#NumPy allows the subtraction of two Datetime values, 
#an operation which produces a number with a time unit. 

>>> np.datetime64('2009-01-01') - np.datetime64('2008-01-01')
numpy.timedelta64(366,'D')

>>> np.datetime64('2009') + np.timedelta64(20, 'D')
numpy.datetime64('2009-01-21')



>>> np.datetime64('2011-06-15T00:00') + np.timedelta64(12, 'h')
numpy.datetime64('2011-06-15T12:00-0500')

>>> np.timedelta64(1,'W') / np.timedelta64(1,'D')
7.0


#There are two Timedelta units ('Y', years and 'M,' months) 
#which are treated specially,
#While a timedelta day unit is equivalent to 24 hours, there is no way to convert a month unit into days, 
#because different months have different numbers of days
>>> a = np.timedelta64(1, 'Y')

#converts to Month 
>>> np.timedelta64(a, 'M')
numpy.timedelta64(12,'M')
#but can not be converted to Days 
>>> np.timedelta64(a, 'D')
Traceback (most recent call last):  
File "<stdin>", line 1, in <module>
TypeError: Cannot cast NumPy timedelta64 scalar from metadata [Y] to [D] according to the rule 'same_kind'

##date units:
# the time span for 'W' (week) is exactly 7 times longer than the time span for 'D' (day), 
#and the time span for 'D' (day) is exactly 24 times longer than the time span for 'h' (hour).

#Code   Meaning     Time span (relative)    Time span (absolute)
#dtype is written like dtype='datetime64[D]'
Y       year        +/- 9.2e18 years        [9.2e18 BC, 9.2e18 AD] 
M       month       +/- 7.6e17 years        [7.6e17 BC, 7.6e17 AD] 
W       week        +/- 1.7e17 years        [1.7e17 BC, 1.7e17 AD] 
D       day         +/- 2.5e16 years        [2.5e16 BC, 2.5e16 AD] 

##time units:
#dtype is written like dtype='timedelta64[ns]'
#Code   Meaning     Time span (relative)    Time span (absolute)
h       hour        +/- 1.0e15 years        [1.0e15 BC, 1.0e15 AD] 
m       minute      +/- 1.7e13 years        [1.7e13 BC, 1.7e13 AD] 
s       second      +/- 2.9e11 years        [2.9e11 BC, 2.9e11 AD] 
ms      millisecond +/- 2.9e8 years         [ 2.9e8 BC, 2.9e8 AD] 
us      microsecond +/- 2.9e5 years         [290301 BC, 294241 AD] 
ns      nanosecond  +/- 292 years           [ 1678 AD, 2262 AD] 
ps      picosecond  +/- 106 days            [ 1969 AD, 1970 AD] 
fs      femtosecond +/- 2.6 hours           [ 1969 AD, 1970 AD] 
as      attosecond  +/- 9.2 seconds         [ 1969 AD, 1970 AD] 

##conversion is done as belows - use .astype()
>>> x = np.timedelta64(2069211000000000, 'ns')
>>> days = x.astype('timedelta64[D]')
>>> days / np.timedelta64(1, 'D')
23
#from python datetime 
>>> from datetime import datetime
>>> import numpy as np
>>> dt = datetime.utcnow()
>>> dt
datetime.datetime(2012, 12, 4, 19, 51, 25, 362455)
>>> dt64 = np.datetime64(dt)
>>> ts = (dt64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')
>>> ts
1354650685.3624549
>>> datetime.utcfromtimestamp(ts)
datetime.datetime(2012, 12, 4, 19, 51, 25, 362455)


>>> np.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)
1025222400000000000L


## Business Day Functionality

#To allow the datetime to be used in contexts where only certain days of the week are valid,
#NumPy includes a set of 'busday' (business day) functions.

>>> np.busday_offset('2011-06-23', 1)  # YYY-MM-DD
numpy.datetime64('2011-06-24')

>>> np.busday_offset('2011-06-23', 2)
numpy.datetime64('2011-06-27')

>>>>> np.busday_offset('2011-06-25', 2)
Traceback (most recent call last):  
File "<stdin>", line 1, in <module>
ValueError: Non-business day date in busday_offset

>>>>> np.busday_offset('2011-06-25', 0, roll='forward')
numpy.datetime64('2011-06-27')

>>> np.busday_offset('2011-06-25', 2, roll='forward')
numpy.datetime64('2011-06-29')

>>> np.busday_offset('2011-06-25', 0, roll='backward')
numpy.datetime64('2011-06-24')

>>> np.busday_offset('2011-06-25', 2, roll='backward')
numpy.datetime64('2011-06-28')

#The first business day on or after a date:
>>> np.busday_offset('2011-03-20', 0, roll='forward')
numpy.datetime64('2011-03-21','D')
>>> np.busday_offset('2011-03-22', 0, roll='forward')
numpy.datetime64('2011-03-22','D')


#The first business day strictly after a date:
>>> np.busday_offset('2011-03-20', 1, roll='backward')
numpy.datetime64('2011-03-21','D')
>>> np.busday_offset('2011-03-22', 1, roll='backward')
numpy.datetime64('2011-03-23','D')


#In Canada and the U.S., Mothers day is on the second Sunday in May, 
#which can be computed with a custom weekmask.
>>> np.busday_offset('2012-05', 1, roll='forward', weekmask='Sun')
numpy.datetime64('2012-05-13','D')

##Few Methods
#np.is_busday():  To test a datetime64 value to see if it is a valid day, use is_busday.

>>> np.is_busday(np.datetime64('2011-07-15'))  # a Friday
True
>>> np.is_busday(np.datetime64('2011-07-16')) # a Saturday
False
>>> np.is_busday(np.datetime64('2011-07-16'), weekmask="Sat Sun")
True
>>> a = np.arange(np.datetime64('2011-07-11'), np.datetime64('2011-07-18'))
>>> np.is_busday(a)
array([ True,  True,  True,  True,  True, False, False], dtype='bool')



#np.busday_count():   To find how many valid days there are in a specified range of datetime64 dates, use busday_count:
>>> np.busday_count(np.datetime64('2011-07-11'), np.datetime64('2011-07-18'))
5
>>> np.busday_count(np.datetime64('2011-07-18'), np.datetime64('2011-07-11'))
-5


#to count of how many of them are valid date s,
>>> a = np.arange(np.datetime64('2011-07-11'), np.datetime64('2011-07-18'))
>>> np.count_nonzero(np.is_busday(a))
5



## Custom Weekmasks
# Positional sequences; positions are Monday through Sunday.
# Length of the sequence must be exactly 7.
weekmask = [1, 1, 1, 1, 1, 0, 0]
# list or other sequence; 0 == invalid day, 1 == valid day
weekmask = "1111100"
# string '0' == invalid day, '1' == valid day

# string abbreviations from this list: Mon Tue Wed Thu Fri Sat Sun
weekmask = "Mon Tue Wed Thu Fri"
# any amount of whitespace is allowed; abbreviations are case-sensitive.
weekmask = "MonTue Wed  Thu\tFri"
  
	    
	   
	   
	   
	   
	   
	   
	   



### NumPy - String operations - Use with prefix numpy.char 
#Most of them mimic python str methods, 
#but operate with array of strings and return array of strings
#after operation done on elementwise , hence returns result array 

add(x1, x2) 						Return element-wise string concatenation for two arrays of str or unicode.
multiply(a, i) 						Return (a * i), that is string multiple concatenation, element-wise.
mod(a, values) 						Return (a % i), that is pre-Python 2.6 string formatting (iterpolation), element-wise for a pair of array_likes of str or unicode.

capitalize(a) 						Return a copy of a with only the first character of each element capitalized.
center(a, width[, fillchar]) 		Return a copy of a with its elements centered in a string of length width.
decode(a[, encoding, errors]) 		Calls str.decode element-wise.
encode(a[, encoding, errors]) 		Calls str.encode element-wise.

join(sep, seq) 						Return a string which is the concatenation of the strings in the sequence seq.
ljust(a, width[, fillchar]) 		Return an array with the elements of a left-justified in a string of length width.
lower(a) 							Return an array with the elements converted to lowercase.
lstrip(a[, chars]) 					For each element in a, return a copy with the leading characters removed.
partition(a, sep) 					Partition each element in a around sep.

replace(a, old, new[, count]) 		For each element in a, return a copy of the string with all occurrences of substring old replaced by new.
rjust(a, width[, fillchar]) 		Return an array with the elements of a right-justified in a string of length width.
rpartition(a, sep) 					Partition (split) each element around the right-most separator.
rsplit(a[, sep, maxsplit]) 			For each element in a, return a list of the words in the string, using sep as the delimiter string.
rstrip(a[, chars]) 					For each element in a, return a copy with the trailing characters removed.

split(a[, sep, maxsplit]) 			For each element in a, return a list of the words in the string, using sep as the delimiter string.
splitlines(a[, keepends]) 			For each element in a, return a list of the lines in the element, breaking at line boundaries.
strip(a[, chars]) 					For each element in a, return a copy with the leading and trailing characters removed.
swapcase(a) 						Return element-wise a copy of the string with uppercase characters converted to lowercase and vice versa.

title(a) 							Return element-wise title cased version of string or unicode.
translate(a, table[, deletechars]) 	For each element in a, return a copy of the string where all characters occurring in the optional argument deletechars are removed, and the remaining characters have been mapped through the given translation table.
upper(a) 							Return an array with the elements converted to uppercase.
zfill(a, width) 					Return the numeric string left-filled with zeros Calls str.zfill element-wise.


# Comparison
#these functions strip trailing whitespace characters before performing the comparison.
#it returns myltidimensional True or False based on elementwise comparison
#to return single True or False, use with np.all()
equal(x1, x2) 						Return (x1 == x2) element-wise.
not_equal(x1, x2) 					Return (x1 != x2) element-wise.
greater_equal(x1, x2) 				Return (x1 >= x2) element-wise.
less_equal(x1, x2) 					Return (x1 <= x2) element-wise.
greater(x1, x2) 					Return (x1 > x2) element-wise.
less(x1, x2) 						Return (x1 < x2) element-wise.


# String information
#operation done on elementwise , hence returns result array 
count(a, sub[, start, end]) 		Returns an array with the number of non-overlapping occurrences of substring sub in the range [start, end].
find(a, sub[, start, end]) 			For each element, return the lowest index in the string where substring sub is found.
index(a, sub[, start, end]) 		Like find, but raises ValueError when the substring is not found.

isalpha(a) 							Returns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise.
isdecimal(a) 						For each element, return True if there are only decimal characters in the element.
isdigit(a) 							Returns true for each element if all characters in the string are digits and there is at least one character, false otherwise.
islower(a) 							Returns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.
isnumeric(a) 						For each element, return True if there are only numeric characters in the element.
isspace(a) 							Returns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise.
istitle(a) 							Returns true for each element if the element is a titlecased string and there is at least one character, false otherwise.
isupper(a) 							Returns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.

rfind(a, sub[, start, end]) 		For each element in a, return the highest index in the string where substring sub is found, such that sub is contained within [start, end].
rindex(a, sub[, start, end]) 		Like rfind, but raises ValueError when the substring sub is not found.
startswith(a, prefix[, start, end]) Returns a boolean array which is True where the string element in a starts with prefix, otherwise False.


#Example of strip
>>> c = np.array(['aAaAaA', '  aA  ', 'abBABba'])
>>> c
array(['aAaAaA', '  aA  ', 'abBABba'],    
dtype='|S7')	

>>> np.char.strip(c)
array(['aAaAaA', 'aA', 'abBABba'],    
dtype='|S7')	

>>> np.char.strip(c, 'a') # 'a' unstripped from c[1] because whitespace leads
array(['AaAaA', '  aA  ', 'bBABb'],    
dtype='|S7')	

>>> np.char.strip(c, 'A') # 'A' unstripped from c[1] because (unprinted) ws trails
array(['aAaAa', '  aA  ', 'abBABba'],    
dtype='|S7')

#Example of count	

>>> c = np.array(['aAaAaA', '  aA  ', 'abBABba'])
>>> c
array(['aAaAaA', '  aA  ', 'abBABba'],    
dtype='|S7')	

>>> np.char.count(c, 'A')
array([3, 1, 1])
>>> np.char.count(c, 'aA')
array([3, 1, 0])

>>> np.char.count(c, 'A', start=1, end=4)
array([2, 1, 1])
>>> np.char.count(c, 'A', start=1, end=3)
array([1, 0, 0])











### Numpy- Functional programming - use with numpy.
#axis=0, 1st dimension (ie 1st dimension varying eg for 2D it is columnwise)
apply_along_axis(func1d, axis, arr, *args, ...) 	Apply a function to 1-D slices along the given axis.
apply_over_axes(func, a, axes) 						Apply a function repeatedly over multiple axes.
vectorize(pyfunc[, otypes, doc, excluded, cache]) 	Generalized function class.
frompyfunc(func, nin, nout) 						Takes an arbitrary Python function and returns a Numpy ufunc.
piecewise(x, condlist, funclist, *args, **kw) 		Evaluate a piecewise-defined function


#Example of apply_along_axis
>>> def my_func(a):
        """Average first and last element of a 1-D array"""
        return (a[0] + a[-1]) * 0.5
>>> b = np.array([[1,2,3], [4,5,6], [7,8,9]])
>>> np.apply_along_axis(my_func, 0, b)  #axis=0 means row varying/columnswise 
array([ 4.,  5.,  6.])
>>> np.apply_along_axis(my_func, 1, b)  #axis=1 means column varying/rowwise
array([ 2.,  5.,  8.])


#For a function that doesnt return a scalar, 
#the number of dimensions in outarr is the same as arr.
>>> b = np.array([[8,1,7], [4,3,9], [5,2,6]])
>>> np.apply_along_axis(sorted, 1, b)
array([[1, 7, 8],       
[3, 4, 9],       
[2, 5, 6]])

#apply_over_axes(func, a, axes)
#Apply a function repeatedly over multiple axes.
#func is called as res = func(a, axis), where axis is the first element of axes. 
#The result res of the function call must have either the same dimensions as a 
#or one less dimension. 
#If res has one less dimension than a, a dimension is inserted before axis. 
#The call to func is then repeated for each axis in axes, with res as the first argument.

>>> a = np.arange(24).reshape(2,3,4)
>>> a
array([[[ 0,  1,  2,  3],        
[ 4,  5,  6,  7],        
[ 8,  9, 10, 11]],       
[[12, 13, 14, 15],        
[16, 17, 18, 19],        
[20, 21, 22, 23]]])


#Sum over axes 0 and 2. 
#The result has same number of dimensions as the original array:
>>> np.apply_over_axes(np.sum, a, [0,2])
array([[[ 60],        
[ 92],        
[124]]])


#Tuple axis arguments to ufuncs are equivalent:
>>> np.sum(a, axis=(0,2), keepdims=True)
array([[[ 60],        
[ 92],        
[124]]])
		

#Example of frompyfunc(func, nin, nout)
#nin : int,The number of input arguments.
#nout : int,The number of objects returned by func.
#Takes an arbitrary Python function and returns a Numpy ufunc.
#Use frompyfunc to add broadcasting to the Python function oct:
>>> oct_array = np.frompyfunc(oct, 1, 1)
>>> oct_array(np.array((10, 30, 100)))
array([012, 036, 0144], dtype=object)
>>> np.array((oct(10), oct(30), oct(100))) # for comparison
array(['012', '036', '0144'],      
dtype='|S4')

#Example of vectorize 
#cpnvert a scalar function into a function which operates on Array elementwise (with broadcasting)
>>> def myfunc(a, b):
        "Return a-b if a>b, otherwise return a+b"
        if a > b:
            return a - b
        else:
            return a + b



>>> vfunc = np.vectorize(myfunc)
>>> vfunc([1, 2, 3, 4], 2)
array([3, 4, 1, 2])



#Example of piecewise(x, condlist, funclist,*args, **kw)
#Given a set of conditions and corresponding functions, 
#evaluate each function on the input data wherever its condition is true.

#Wherever condlist[i] is True, funclist[i](x) is used as the output value.
#funclist : list of callables, f(x,*args,**kw), or scalars
#If, instead of a callable, a scalar is provided then a constant function (lambda x: scalar) is assumed

#Define the sigma function, which is -1 for x < 0 and +1 for x >= 0.
>>> x = np.linspace(-2.5, 2.5, 6)
>>> np.piecewise(x, [x < 0, x >= 0], [-1, 1])
array([-1., -1., -1.,  1.,  1.,  1.])


#Define the absolute value, which is -x for x <0 and x for x >= 0.
>>> np.piecewise(x, [x < 0, x >= 0], [lambda x: -x, lambda x: x])
array([ 2.5,  1.5,  0.5,  0.5,  1.5,  2.5])







### Numpy - Linear algebra - Use with prefix numpy.

#Matrix and vector products
dot(a, b[, out]) 									Dot product of two arrays.
linalg.multi_dot(arrays)                            Compute the dot product of two or more arrays in a single function call 
vdot(a, b) 											Return the dot product of two vectors.
inner(a, b) 										Inner product of two arrays.
outer(a, b[, out]) 									Compute the outer product of two vectors.
matmul(a, b[, out]) 								Matrix product of two arrays.
tensordot(a, b[, axes]) 							Compute tensor dot product along specified axes for arrays >= 1-D.
einsum(subscripts, *operands[, out, dtype, ...]) 	Evaluates the Einstein summation convention on the operands.
linalg.matrix_power(M, n) 							Raise a square matrix to the (integer) power n.
kron(a, b) 											Kronecker product of two arrays.


#Decompositions
linalg.cholesky(a) 									Cholesky decomposition.
linalg.qr(a[, mode]) 								Compute the qr factorization of a matrix.
linalg.svd(a[, full_matrices, compute_uv]) 			Singular Value Decomposition.

#Matrix eigenvalues
linalg.eig(a) 										Compute the eigenvalues and right eigenvectors of a square array.
linalg.eigh(a[, UPLO]) 								Return the eigenvalues and eigenvectors of a Hermitian or symmetric matrix.
linalg.eigvals(a) 									Compute the eigenvalues of a general matrix.
linalg.eigvalsh(a[, UPLO]) 							Compute the eigenvalues of a Hermitian or real symmetric matrix.


#Norms and other numbers
linalg.norm(x[, ord, axis, keepdims]) 				Matrix or vector norm.
linalg.cond(x[, p]) 								Compute the condition number of a matrix.
linalg.det(a) 										Compute the determinant of an array.
linalg.matrix_rank(M[, tol]) 						Return matrix rank of array using SVD method Rank of the array is the number of SVD singular values of the array that are greater than tol.
linalg.slogdet(a) 									Compute the sign and (natural) logarithm of the determinant of an array.
trace(a[, offset, axis1, axis2, dtype, out]) 		Return the sum along diagonals of the array.


#Solving equations and inverting matrices
linalg.solve(a, b) 									Solve a linear matrix equation, or system of linear scalar equations.
linalg.tensorsolve(a, b[, axes]) 					Solve the tensor equation a x = b for x.
linalg.lstsq(a, b[, rcond]) 						Return the least-squares solution to a linear matrix equation.
linalg.inv(a) 										Compute the (multiplicative) inverse of a matrix.
linalg.pinv(a[, rcond]) 							Compute the (Moore-Penrose) pseudo-inverse of a matrix.
linalg.tensorinv(a[, ind]) 							Compute the 'inverse of an N-dimensional array.

#Exceptions
linalg.LinAlgError 	G						eneric Python-exception-derived object raised by linalg functions.


##Linear algebra on several matrices at once (New in version 1.8.0)

#This is indicated in the documentation via input parameter specifications 
#such as a : (..., M, M) array_like. 
#This means that if for instance given an input array a.shape == (N, M, M), 
#it is interpreted as a 'stack' of N matrices, each of size M-by-M. 

#Similar specification applies to return values, 
#for instance the determinant has det : (...) 
#and will in this case return an array of shape det(a).shape == (N,). 


#Examples
# Solve the system of equations 3 * x0 + x1 = 9 and x0 + 2 * x1 = 8:

>>> a = np.array([[3,1], [1,2]])
>>> b = np.array([9,8])
>>> x = np.linalg.solve(a, b)
>>> x
array([ 2.,  3.])

#Check that the solution is correct:
>>> np.allclose(np.dot(a, x), b)
True


#Examples
#Fit a line, y = mx + c, through some noisy data-points:
>>>

>>> x = np.array([0, 1, 2, 3])
>>> y = np.array([-1, 0.2, 0.9, 2.1])

#y = Ap, where A = [[x 1]] and p = [[m], [c]]. Now use lstsq to solve for p:

>>> A = np.vstack([x, np.ones(len(x))]).T
>>> A
array([[ 0.,  1.],       
[ 1.,  1.],       
[ 2.,  1.],       
[ 3.,  1.]])

>>>

>>> m, c = np.linalg.lstsq(A, y)[0]
#Return of  np.linalg.lstsq(A, y)
#https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq
(array([ 1.  , -0.95]), array([0.05]), 2, array([4.10003045, 1.09075677]))

>>> print m, c
1.0 -0.95

#Plot the data along with the fitted line:
>>> import matplotlib.pyplot as plt
>>> plt.plot(x, y, 'o', label='Original data', markersize=10)
>>> plt.plot(x, m*x + c, 'r', label='Fitted line')
>>> plt.legend()
>>> plt.show()


#inv - Compute the (multiplicative) inverse of a matrix.
>>> from numpy.linalg import inv
>>> a = np.array([[1., 2.], [3., 4.]])
>>> ainv = inv(a)
>>> np.allclose(np.dot(a, ainv), np.eye(2))
True
>>> np.allclose(np.dot(ainv, a), np.eye(2))
True

#for matrix 
>>> ainv = inv(np.matrix(a))
>>> ainv
matrix([[-2. ,  1. ],
        [ 1.5, -0.5]])


#Inverses of several matrices can be computed at once:
>>> a = np.array([[[1., 2.], [3., 4.]], [[1, 3], [3, 5]]])
>>> inv(a)
array([[[-2. ,  1. ],
        [ 1.5, -0.5]],
       [[-5. ,  2. ],
        [ 3. , -1. ]]])


#det 
>>> a = np.array([[1, 2], [3, 4]])
>>> np.linalg.det(a)
-2.0
#Computing determinants for a stack of matrices:
>>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])
>>> a.shape
(3, 2, 2)
>>> np.linalg.det(a)
array([-2., -3., -8.])



### Numpy - Polynomial - numpy.polynomial.polynomial


#c,c1,c2 : 1D array of polynomial coefficients 
#(including coefficients equal to zero) 
#Array of coefficients ordered so that the coefficients for terms of degree n are contained in c[n]. 
#x,y : array_like 

##Basics
polyval(x, c[, tensor])             Evaluate a polynomial at points x. 
polyval2d(x, y, c)                  Evaluate a 2-D polynomial at points (x, y). 
polyval3d(x, y, z, c)               Evaluate a 3-D polynomial at points (x, y, z). 
polygrid2d(x, y, c)                 Evaluate a 2-D polynomial on the Cartesian product of x and y. 
polygrid3d(x, y, z, c)              Evaluate a 3-D polynomial on the Cartesian product of x, y and z. 
polyroots(c)                        Compute the roots of a polynomial. 
polyfromroots(roots)                Generate a monic polynomial with given roots. 
polyvalfromroots(x, r[, tensor])    Evaluate a polynomial specified by its roots at points x. 

#Example 
>>> from numpy.polynomial.polynomial import polyval
>>> polyval(1, [1,2,3])
6.0
>>> import numpy.polynomial.polynomial as poly
>>> poly.polyroots(poly.polyfromroots((-1,0,1)))
array([-1.,  0.,  1.])



##Fitting
polyfit(x, y, deg[, rcond, full, w])    Least-squares fit of a polynomial to data. 
polyvander(x, deg)                      Vandermonde matrix of given degree. 
polyvander2d(x, y, deg)                 Pseudo-Vandermonde matrix of given degrees. 
polyvander3d(x, y, z, deg)              Pseudo-Vandermonde matrix of given degrees. 

#Example 
>>> from numpy.polynomial import polynomial as P
>>> x = np.linspace(-1,1,51) # x "data": [-1, -0.96, ..., 0.96, 1]
>>> y = x**3 - x + np.random.randn(len(x)) # x^3 - x + N(0,1) "noise"
>>> c, stats = P.polyfit(x,y,3,full=True) #True, diagnostic information
>>> c # c[0], c[2] should be approx. 0, c[1] approx. -1, c[3] approx. 1
array([ 0.01909725, -1.30598256, -0.00577963,  1.02644286])
>>> stats # [residuals, rank, singular_values, rcond], note the large SSR, explaining the rather poor results
[array([ 38.06116253]), 4, array([ 1.38446749,  1.32119158,  0.50443316,
0.28853036]), 1.1324274851176597e-014]



##Calculus
polyder(c[, m, scl, axis])          Differentiate a polynomial. 
polyint(c[, m, k, lbnd, scl, axis]) Integrate a polynomial. 

#Example 
#m : int, optional,Order of integration, must be positive. (Default: 1)
#k : {[], list, scalar}, optional
#Integration constant(s). The value of the first integral at zero is the first value in the list, the value of the second integral at zero is the second value, etc

>>> from numpy.polynomial import polynomial as P
>>> c = (1,2,3,4) # 1 + 2x + 3x**2 + 4x**3
>>> P.polyder(c) # (d/dx)(c) = 2 + 6x + 12x**2
array([  2.,   6.,  12.])
>>> P.polyder(c,3) # (d**3/dx**3)(c) = 24
array([ 24.])

>>> from numpy.polynomial import polynomial as P
>>> c = (1,2,3)
>>> P.polyint(c) # should return array([0, 1, 1, 1])
array([ 0.,  1.,  1.,  1.])
>>> P.polyint(c,3) # should return array([0, 0, 0, 1/6, 1/12, 1/20])
array([ 0.        ,  0.        ,  0.        ,  0.16666667,  0.08333333,
        0.05      ])
>>> P.polyint(c,k=3) # should return array([3, 1, 1, 1])
array([ 3.,  1.,  1.,  1.])


##Algebra
polyadd(c1, c2)             Add one polynomial to another. 
polysub(c1, c2)             Subtract one polynomial from another. 
polymul(c1, c2)             Multiply one polynomial by another. 
polymulx(c)                 Multiply a polynomial by x. 
polydiv(c1, c2)             Divide one polynomial by another. 
polypow(c, pow[, maxpower]) Raise a polynomial  
polyzero                    = array([0])
polyx                       = array([0, 1])
polyone                     = array([1])

#Example 
>>> from numpy.polynomial import polynomial as P
>>> c1 = (1,2,3)
>>> c2 = (3,2,1)
>>> sum = P.polyadd(c1,c2); sum
array([ 4.,  4.,  4.])
>>> P.polyval(2, sum) # 4 + 4(2) + 4(2**2)
28.0

>>> P.polymul(c1,c2)
array([  3.,   8.,  14.,   8.,   3.])

>>> P.polydiv(c1,c2)
(array([ 3.]), array([-8., -4.]))


##numpy.poly1d
class numpy.poly1d(c_or_r, r=False, variable=None)
#A one-dimensional polynomial class.
#Polynomials can be added, subtracted, multiplied, and divided

c_or_r : array_like
    The polynomials coefficients, in decreasing powers, 
    or if the value of the second parameter is True, the polynomials roots 
    For example, poly1d([1, 2, 3]) returns an object that represents x^2 + 2x + 3, 
    whereas poly1d([1, 2, 3], True) returns one that represents (x-1)(x-2)(x-3) = x^3 - 6x^2 + 11x -6.

variable : str, optional
    Changes the variable used when printing p from x to variable (see Examples).
 
 
#Attributes of numpy.poly1d
c                   A copy of the polynomial coefficients 
coef                A copy of the polynomial coefficients 
coefficients        A copy of the polynomial coefficients 
coeffs              A copy of the polynomial coefficients 
o                   The order or degree of the polynomial 
order               The order or degree of the polynomial 
r                   The roots of the polynomial, where self(x) == 0 
roots               The roots of the polynomial, where self(x) == 0 
variable            The name of the polynomial variable 

# instance Methods
__call__(val)       Evaulate the poly at val 
deriv([m])          Return a derivative of this polynomial. mth order 
integ([m, k])       Return integration , mth order 



#Construct the polynomial x^2 + 2x + 3:
>>> p = np.poly1d([1, 2, 3])
>>> print(np.poly1d(p))
   2
1 x + 2 x + 3


#Evaluate the polynomial at x = 0.5:
>>> p(0.5)
4.25


#Find the roots:
>>> p.r
array([-1.+1.41421356j, -1.-1.41421356j])
>>> p(p.r)
array([ -4.44089210e-16+0.j,  -4.44089210e-16+0.j])


#Show the coefficients:
>>> p.c
array([1, 2, 3])


#Display the order (the leading zero-coefficients are removed):
>>> p.order
2


#Show the coefficient of the k-th power in the polynomial (which is equivalent to p.c[-(i+1)]):
>>> p[1]
2

#Polynomials can be added, subtracted, multiplied, and divided (returns quotient and remainder):
>>> p * p
poly1d([ 1,  4, 10, 12,  9])

>>> (p**3 + 4) / p
(poly1d([  1.,   4.,  10.,  12.,   9.]), poly1d([ 4.]))


#asarray(p) gives the coefficient array, 
#so polynomials can be used in all functions that accept arrays:

>>> p**2 # square of polynomial
poly1d([ 1,  4, 10, 12,  9])

>>> np.square(p) # square of individual coefficients
array([1, 4, 9])


#The variable used in the string representation of p can be modified, using the variable parameter:
>>> p = np.poly1d([1,2,3], variable='z')
>>> print(p)
   2
1 z + 2 z + 3


#Construct a polynomial from its roots:
>>> np.poly1d([1, 2], True)
poly1d([ 1, -3,  2])

#This is the same polynomial as obtained by:
>>> np.poly1d([1, -1]) * np.poly1d([1, -2])
poly1d([ 1, -3,  2])


##numpy.polyfit(x, y, deg, rcond=None, full=False, w=None, cov=False)
#Least squares polynomial fit.
#Fit a polynomial p(x) = p[0] * x**deg + ... + p[deg] of degree deg to points (x, y). 
#Returns a vector of coefficients p that minimises the squared error.


>>> x = np.array([0.0, 1.0, 2.0, 3.0,  4.0,  5.0])
>>> y = np.array([0.0, 0.8, 0.9, 0.1, -0.8, -1.0])
>>> z = np.polyfit(x, y, 3)
>>> z
array([ 0.08703704, -0.81349206,  1.69312169, -0.03968254])
>>> p = np.poly1d(z)
>>> p(0.5)
0.6143849206349179
>>> p(3.5)
-0.34732142857143039
>>> p(10)
22.579365079365115

#High-order polynomials may oscillate wildly:
>>> p30 = np.poly1d(np.polyfit(x, y, 30))
/... RankWarning: Polyfit may be poorly conditioned...
>>> p30(4)
-0.80000000000000204
>>> p30(5)
-0.99999999999999445
>>> p30(4.5)
-0.10547061179440398

#plot 
>>> import matplotlib.pyplot as plt
>>> xp = np.linspace(-2, 6, 100)
>>> _ = plt.plot(x, y, '.', xp, p(xp), '-', xp, p30(xp), '--')
>>> plt.ylim(-2,2)
(-2, 2)
>>> plt.show()





### Numpy - Discrete Fourier Transform (numpy.fft)

#Standard FFTs, a is ndarray, can be of compl ex
fft(a[, n, axis, norm]) 		Compute the one-dimensional discrete Fourier Transform.
ifft(a[, n, axis, norm]) 		Compute the one-dimensional inverse discrete Fourier Transform.
fft2(a[, s, axes, norm]) 		Compute the 2-dimensional discrete Fourier Transform This function computes the n-dimensional discrete Fourier Transform over any axes in an M-dimensional array by means of the Fast Fourier Transform (FFT).
ifft2(a[, s, axes, norm]) 		Compute the 2-dimensional inverse discrete Fourier Transform.
fftn(a[, s, axes, norm]) 		Compute the N-dimensional discrete Fourier Transform.
ifftn(a[, s, axes, norm]) 		Compute the N-dimensional inverse discrete Fourier Transform.
#Real FFTs
rfft(a[, n, axis, norm]) 		Compute the one-dimensional discrete Fourier Transform for real input.
irfft(a[, n, axis, norm]) 		Compute the inverse of the n-point DFT for real input.
rfft2(a[, s, axes, norm]) 		Compute the 2-dimensional FFT of a real array.
irfft2(a[, s, axes, norm]) 		Compute the 2-dimensional inverse FFT of a real array.
rfftn(a[, s, axes, norm]) 		Compute the N-dimensional discrete Fourier Transform for real input.
irfftn(a[, s, axes, norm]) 		Compute the inverse of the N-dimensional FFT of real input.
#Hermitian FFTs
hfft(a[, n, axis, norm]) 		Compute the FFT of a signal which has Hermitian symmetry (real spectrum).
ihfft(a[, n, axis, norm]) 		Compute the inverse FFT of a signal which has Hermitian symmetry.
#Helper routines
fftfreq(n[, d]) 				Return the Discrete Fourier Transform sample frequencies.
rfftfreq(n[, d]) 				Return the Discrete Fourier Transform sample frequencies (for usage with rfft, irfft).
fftshift(x[, axes]) 			Shift the zero-frequency component to the center of the spectrum.
ifftshift(x[, axes]) 			The inverse of fftshift.


#Coeff placements

If A = fft(a, n), then A[0] contains the zero-frequency term (the mean of the signal), which is always real for real input s.
Then A[1:n/2] contains the positive-frequency terms, and A[n/2+1:] contains the negative-frequency terms, in order of decreasingly negative frequenc y.

For an even number of input points, A[n/2] represents both positive and negative Nyquist frequenc y,
and is also purely real for real inpu t.

For an odd number of input points, A[(n-1)/2] contains the largest positive frequenc y,
while A[(n+1)/2] contains the largest negative frequenc y.

The routine np.fft.fftfreq(n) returns an array giving the frequencies of corresponding elements in the outpu t.
The routine np.fft.fftshift(A) shifts transforms and their frequencies to put the zero-frequency components in the middl e,
and np.fft.ifftshift(A) undoes that shift.

When the input a is a time-domain signal and A = fft(a ),
np.abs(A) is its amplitude spectr um
np.abs(A)**2 is its power spectrum and  phase spectrum is obtained by np.angle(A).

#Normalization

The default normalization has the direct transforms unscaled 
and the inverse transforms are scaled by 1/ n.

To obtain unitary transforms by setting the keyword argument norm to "ortho" (default is None)
so that both direct and inverse transforms will be scaled by 1/sqrt(n).

#Real and Hermitian transforms

When the input is purely real, its transform is Hermitian,
i.e., the component at frequency f_k(a complex number) 
is the complex conjugate of the component at frequency -f_ k,
which means that for real inputs there is no new information in the negative frequency components

The family of rfft functions is designed to operate on real inputs, 
and exploits this symmetry by computing only the positive frequency components, up to and including the Nyquist frequency.
Thus, n input points produce n/2+1 complex output points.

The inverses of this family assumes the same symmetry of its input, 
and for an output of n points uses n/2+1 input points.

when the spectrum is purely real, the signal is Hermitian.
i.e., the component at time t_k(a complex number) is the complex conjugate of the component at time -t_ k,
The hfft family of functions exploits this symmetry by using n/2+1 complex points in the input (time) doma in
for n real points in the frequency domain.

In higher dimensions, FFTs are used, 
e.g., for image analysis and filtering.
The computational efficiency of the FFT means that it can also be a faster way to compute large convolutions,
using the property that a convolution in the time domain is equivalent to a point-by-point multiplication in the frequency domain.

## Example of fft

>>> np.fft.fft(np.exp(2j * np.pi * np.arange(8) / 8))
array([ -3.44505240e-16 +1.14383329e-17j,         
8.00000000e+00 -5.71092652e-15j,         
2.33482938e-16 +1.22460635e-16j,         
1.64863782e-15 +1.77635684e-15j,         
9.95839695e-17 +2.33482938e-16j,         
0.00000000e+00 +1.66837030e-15j,         
1.14383329e-17 +1.22460635e-16j,         
-1.64863782e-15 +1.77635684e-15j])

#For real input has an FFT which is Hermitian

import matplotlib.pyplot as plt
t = np.arange(256)
sp = np.fft.fft(np.sin(t))                #ndarray type, has real and imag attribute to give each element's real or im ag
freq = np.fft.fftfreq(t.shape[-1       ])
plt.plot(freq, sp.real, freq, sp.imag)     #freq vs real and freq.im ag
plt.show()


## Example of numpy.fft.ifft

np.fft.ifft([0, 4, 0, 0])   #array([ 1.+0.j,  0.+1.j, -1.+0.j,  0.-1.j])

#Create and plot a band-limited signal with random phases:
import matplotlib.pyplot as plt
t = np.arange(400)
n = np.zeros((400,), dtype=complex)
n[40:60] = np.exp(1j*np.random.uniform(0, 2*np.pi, (20,)))
s = np.fft.ifft(n)
plt.plot(t, s.real, 'b-', t, s.imag, 'r--')
plt.legend(('real', 'imaginary'))
plt.show()






###Numpy- Simple financial functions - use with prefix numpy.
fv(rate, nper, pmt, pv[, when]) 				Compute the future value.
pv(rate, nper, pmt[, fv, when]) 				Compute the present value.
npv(rate, values) 								Returns the NPV (Net Present Value) of a cash flow series.
pmt(rate, nper, pv[, fv, when]) 				Compute the payment against loan principal plus interest.
ppmt(rate, per, nper, pv[, fv, when]) 			Compute the payment against loan principal.
ipmt(rate, per, nper, pv[, fv, when]) 			Compute the interest portion of a payment.
irr(values) 									Return the Internal Rate of Return (IRR).
mirr(values, finance_rate, reinvest_rate) 		Modified internal rate of return.
nper(rate, pmt, pv[, fv, when]) 				Compute the number of periodic payments.
rate(nper, pmt, pv, fv[, when, guess, tol, ...]) Compute the rate of interest per period.


#By convention, the negative sign represents cash flow out (i.e. money not available today).

#What is the future value after 10 years of saving $100 now, 
#with an additional monthly savings of $100. 
#Assume the interest rate is 5% (annually) compounded monthly?
#         monthly rate nmonths monthly savings  initial deposit
>>> np.fv(0.05/12,     10*12,  -100,            -100)
15692.928894335748

 

#What is the present value (e.g., the initial investment) 
#of an investment that needs to total $15692.93 after 10 years 
#of saving $100 every month? 
#Assume the interest rate is 5% (annually) compounded monthly.

>>> np.pv(0.05/12, 10*12, -100, 15692.93)
-100.00067131625819

#numpy.irr(values) - Return the Internal Rate of Return (IRR).
#values : Input cash flows per time period. 
#By convention, net 'deposits' are negative 
#and net 'withdrawals' are positive. 

#Suppose one invests 100 units 
#and then makes the following withdrawals at regular (fixed) intervals: 
#39, 59, 55, 20. 
#Assuming the ending value is 0, ones 100 unit investment yields 173 units
>>> round(irr([-100, 39, 59, 55, 20]), 5)
0.28095



#numpy.nper(rate, pmt, pv, fv=0, when='end')
#Compute the number of periodic payments.

#If you only had $150/month to pay towards the loan, 
#how long would it take to pay-off a loan of $8,000 
#at 7% annual interest?

>>> print(round(np.nper(0.07/12, -150, 8000), 5))
64.07335
#So, over 64 months would be required to pay off the loan.


#The total payment is made up of payment against principal plus interest.
pmt(Compute the payment against loan principal plus interest) 
    = ppmt(Compute the payment against loan principal.) 
        + ipmt(Compute the interest portion of a payment)
        
pmt(rate, nper, pv[, fv, when]) 				Compute the payment against loan principal plus interest.
ppmt(rate, per, nper, pv[, fv, when]) 			Compute the payment against loan principal.
ipmt(rate, per, nper, pv[, fv, when]) 			Compute the interest portion of a payment.

#What is the monthly payment needed to pay off a $200,000 loan 
#in 15 years at an annual interest rate of 7.5%?

>>> np.pmt(0.075/12, 12*15, 200000)
-1854.0247200054619


#Full calculation
#What is the amortization schedule for a 1 year loan of $2500 
#at 8.24% interest per year compounded monthly?

>>> principal = 2500.00

#The 'per variable represents the periods of the loan. 
#Remember that financial equations start the period count at 1!

>>> per = np.arange(1*12) + 1
>>> ipmt = np.ipmt(0.0824/12, per, 1*12, principal)
>>> ppmt = np.ppmt(0.0824/12, per, 1*12, principal)


#Each element of the sum of the 'ipmt and 'ppmt 
#arrays should equal 'pmt.

>>> pmt = np.pmt(0.0824/12, 1*12, principal)
>>> np.allclose(ipmt + ppmt, pmt)
True

>>> fmt = '{0:2d} {1:8.2f} {2:8.2f} {3:8.2f}'
>>> for payment in per:
        index = payment - 1
        principal = principal + ppmt[index]
        print(fmt.format(payment, ppmt[index], ipmt[index], principal))
 1  -200.58   -17.17  2299.42
 2  -201.96   -15.79  2097.46
 3  -203.35   -14.40  1894.11
 4  -204.74   -13.01  1689.37
 5  -206.15   -11.60  1483.22
 6  -207.56   -10.18  1275.66
 7  -208.99    -8.76  1066.67
 8  -210.42    -7.32   856.25
 9  -211.87    -5.88   644.38
10  -213.32    -4.42   431.05
11  -214.79    -2.96   216.26
12  -216.26    -1.49    -0.00

>>> interestpd = np.sum(ipmt)
>>> np.round(interestpd, 2)
-112.98





### NumPy - stats - use with prefix numpy.

##Order statistics
#Note axis=n means operation along nth dimension 
#which means nth dimension varying 
#When no axis is given, either array is flattened or based on last axis/dimension(ie -1), check ref 

amin(a[, axis, out, keepdims]) 			Return the minimum of an array or minimum along an axis.
amax(a[, axis, out, keepdims]) 			Return the maximum of an array or maximum along an axis.
nanmin(a[, axis, out, keepdims]) 		Return minimum of an array or minimum along an axis, ignoring any NaNs.
nanmax(a[, axis, out, keepdims]) 		Return the maximum of an array or maximum along an axis, ignoring any NaNs.
ptp(a[, axis, out]) 					Range of values (maximum - minimum) along an axis.
percentile(a, q[, axis, out, ...]) 		Compute the qth percentile of the data along the specified axis.
nanpercentile(a, q[, axis, out, ...]) 	Compute the qth percentile of the data along the specified axis, while ignoring nan values.

#Example of percentile
>>> a = np.array([[10, 7, 4], [3, 2, 1]])
>>> a
array([[10,  7,  4],       
[ 3,  2,  1]])
>>> np.percentile(a, 50) #a is flattened 
3.5
>>> np.percentile(a, 50, axis=0)
array([[ 6.5,  4.5,  2.5]])
>>> np.percentile(a, 50, axis=1)
array([ 7.,  2.])
>>> np.percentile(a, 50, axis=1, keepdims=True)
array([[ 7.],       
[ 2.]])


>>> m = np.percentile(a, 50, axis=0)
>>> out = np.zeros_like(m)
>>> np.percentile(a, 50, axis=0, out=out)
array([[ 6.5,  4.5,  2.5]])
>>> m
array([[ 6.5,  4.5,  2.5]])


>>> b = a.copy()
>>> np.percentile(b, 50, axis=1, overwrite_input=True)
array([ 7.,  2.])
>>> assert not np.all(a == b)


#Example of ptp
>>> x = np.arange(4).reshape((2,2))
>>> x
array([[0, 1],       
[2, 3]])



>>> np.ptp(x, axis=0)
array([2, 2])

>>> np.ptp(x, axis=1)
array([1, 1])





## Averages and variances
#Note axis=n means operation along nth dimension 
#which means nth dimension varying 
#When no axis is given, either array is flattened or based on last axis/dimension(ie -1), check ref 
median(a[, axis, out, overwrite_input, keepdims]) 	Compute the median along the specified axis.
average(a[, axis, weights, returned]) 				Compute the weighted average along the specified axis.
mean(a[, axis, dtype, out, keepdims]) 				Compute the arithmetic mean along the specified axis.
std(a[, axis, dtype, out, ddof, keepdims]) 			Compute the standard deviation along the specified axis.
var(a[, axis, dtype, out, ddof, keepdims]) 			Compute the variance along the specified axis.
nanmedian(a[, axis, out, overwrite_input, ...]) 	Compute the median along the specified axis, while ignoring NaNs.
nanmean(a[, axis, dtype, out, keepdims]) 			Compute the arithmetic mean along the specified axis, ignoring NaNs.
nanstd(a[, axis, dtype, out, ddof, keepdims]) 		Compute the standard deviation along the specified axis, while ignoring NaNs.
nanvar(a[, axis, dtype, out, ddof, keepdims]) 		Compute the variance along the specified axis, while ignoring NaNs.

#Example of var
>>> a = np.array([[1, 2, 3], [3, 4, 5]])
>>> np.var(a)        #a  flattened 
1.667
>>> np.var(a, axis=0)  #1st dimension varying , columnwise 
array([ 1.,  1., 1.])
>>> np.var(a, axis=1) #2nd dimension varying , rowwise 
array([0.66666667, 0.66666667])


#In single precision, var() can be inaccurate:
>>> a = np.zeros((2, 512*512), dtype=np.float32)
>>> a[0, :] = 1.0
>>> a[1, :] = 0.1
>>> np.var(a)
0.20250003

#Computing the variance in float64 is more accurate:
>>> np.var(a, dtype=np.float64)
0.20249999932944759
>>> ((1-0.55)**2 + (0.1-0.55)**2)/2
0.2025

#Example of mean
>>> a = np.array([[1, 2], [3, 4]])
>>> np.mean(a)  #a  flattened 
2.5
>>> np.mean(a, axis=0) #1st dimension varying , columnwise 
array([ 2.,  3.])
>>> np.mean(a, axis=1) #2nd dimension varying , rowwise 
array([ 1.5,  3.5])


#In single precision, mean can be inaccurate:
>>> a = np.zeros((2, 512*512), dtype=np.float32)
>>> a[0, :] = 1.0
>>> a[1, :] = 0.1
>>> np.mean(a)
0.546875


#Computing the mean in float64 is more accurate:
>>> np.mean(a, dtype=np.float64)
0.55000000074505806



## Correlating
corrcoef(x[, y, rowvar, bias, ddof]) 			Return Pearson product-moment correlation coefficients.
correlate(a, v[, mode]) 						Cross-correlation of two 1-dimensional sequences.
cov(m[, y, rowvar, bias, ddof, fweights, ...]) 	Estimate a covariance matrix, given data and weights.


#Example of cov
#Consider two variables, x_0 and x_1, which correlate perfectly, but in opposite directions:
>>> x = np.array([[0, 2], [1, 1], [2, 0]]).T
>>> x
array([[0, 1, 2],       #x_0
[2, 1, 0]])             #x_1


#Note how x_0 increases while x_1 decreases. 
#The covariance matrix shows this clearly:
>>> np.cov(x)
array([[ 1., -1.],       #X_0 vs X_0        X_0 vs X_1
[-1.,  1.]])             #X_1 vs X_0        X_1 vs X_1 


#Note that element C_{0,1}, which shows the correlation between x_0 and x_1, is negative.

#Further, note how x and y are combined:
>>> x = [-2.1, -1,  4.3]
>>> y = [3,  1.1,  0.12]
>>> X = np.vstack((x,y))
array([[-2.1 , -1.  ,  4.3 ],
       [ 3.  ,  1.1 ,  0.12]])
>>> print(np.cov(X))
[[ 11.71        -4.286     ] 
[ -4.286        2.14413333]]
>>> print(np.cov(x, y))
[[ 11.71        -4.286     ] 
[ -4.286        2.14413333]]
>>> print(np.cov(x))
11.71


#Example of correlate
>>> np.correlate([1, 2, 3], [0, 1, 0.5])
array([ 3.5])
>>> np.correlate([1, 2, 3], [0, 1, 0.5], "same")
array([ 2. ,  3.5,  3. ])
>>> np.correlate([1, 2, 3], [0, 1, 0.5], "full")
array([ 0.5,  2. ,  3.5,  3. ,  0. ])

#Compare with convolution 
#Note how the convolution operator flips the second array 
#before 'sliding' the two across one another:
>>> np.convolve([1, 2, 3], [0, 1, 0.5])
array([ 0. ,  1. ,  2.5,  4. ,  1.5])

#Only return the middle values of the convolution. 
#Contains boundary effects, where zeros are taken into account:
>>> np.convolve([1,2,3],[0,1,0.5], 'same')
array([ 1. ,  2.5,  4. ])

#The two arrays are of the same length, 
#so there is only one position where they completely overlap:
>>> np.convolve([1,2,3],[0,1,0.5], 'valid')
array([ 2.5])

#Using complex sequences:
>>> np.correlate([1+1j, 2, 3-1j], [0, 1, 0.5j], 'full')
array([ 0.5-0.5j,  1.0+0.j ,  1.5-1.5j,  3.0-1.j ,  0.0+0.j ])

#Note that you get the time reversed, complex conjugated result when the two input sequences change place s,
#i.e., c_{va}[k] = c^{*}_{av}[-k]:
>>> np.correlate([0, 1, 0.5j], [1+1j, 2, 3-1j], 'full')
array([ 0.0+0.j ,  3.0+1.j ,  1.5+1.5j,  1.0+0.j ,  0.5+0.5j])



#Histograms
histogram(a[, bins, range, normed, weights, ...]) 	Compute the histogram of a set of data.
histogram2d(x, y[, bins, range, normed, weights]) 	Compute the bi-dimensional histogram of two data samples.
histogramdd(sample[, bins, range, normed, ...]) 	Compute the multidimensional histogram of some data.
bincount(x[, weights, minlength]) 					Count number of occurrences of each value in array of non-negative ints.
digitize(x, bins[, right]) 							Return the indices of the bins to which each value in input array belongs.


#Example of histogram
>>> np.histogram([1, 2, 1], bins=[0, 1, 2, 3])
(array([0, 2, 1]), array([0, 1, 2, 3])) #hist-freq of each, bin_edges
>>> np.histogram(np.arange(4), bins=np.arange(5), density=True)
(array([ 0.25,  0.25,  0.25,  0.25]), array([0, 1, 2, 3, 4]))
>>> np.histogram([[1, 2, 1], [1, 0, 1]], bins=[0,1,2,3])
(array([1, 4, 1]), array([0, 1, 2, 3]))



>>> a = np.arange(5)
>>> hist, bin_edges = np.histogram(a, density=True)
>>> hist
array([ 0.5,  0. ,  0.5,  0. ,  0. ,  0.5,  0. ,  0.5,  0. ,  0.5])
>>> hist.sum()
2.4999999999999996
>>> np.sum(hist*np.diff(bin_edges))
1.0


#Example of digitize

>>> x = np.array([0.2, 6.4, 3.0, 1.6])
>>> bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])
>>> inds = np.digitize(x, bins)
>>> inds
array([1, 4, 3, 2])
>>> for n in range(x.size):
        print(bins[inds[n]-1], "<=", x[n], "<", bins[inds[n]])
...
0.0 <= 0.2 < 1.0
4.0 <= 6.4 < 10.0
2.5 <= 3.0 < 4.0
1.0 <= 1.6 < 2.5


#Example of 2D hist
>>> import matplotlib as mpl
>>> import matplotlib.pyplot as plt

#Construct a 2-D histogram with variable bin width. First define the bin edges:
>>> xedges = [0, 1, 1.5, 3, 5]
>>> yedges = [0, 2, 3, 4, 6]

#Next we create a histogram H with random bin content:
>>> x = np.random.normal(3, 1, 100)
>>> y = np.random.normal(1, 1, 100)
>>> H, xedges, yedges = np.histogram2d(y, x, bins=(xedges, yedges))


#Or we fill the histogram H with a determined bin content:
>>> H = np.ones((4, 4)).cumsum().reshape(4, 4)
>>> print(H[::-1])  # This shows the bin content in the order as plotted
[[ 13.  14.  15.  16.] 
[  9.  10.  11.  12.] 
[  5.   6.   7.   8.] 
[  1.   2.   3.   4.]]


#Imshow can only do an equidistant representation of bins:
>>> fig = plt.figure(figsize=(7, 3))
>>> ax = fig.add_subplot(131)
>>> ax.set_title('imshow: equidistant')
>>> im = plt.imshow(H, interpolation='nearest', origin='low',
            extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])


#pcolormesh can display exact bin edges:
>>> ax = fig.add_subplot(132)
>>> ax.set_title('pcolormesh: exact bin edges')
>>> X, Y = np.meshgrid(xedges, yedges)
>>> ax.pcolormesh(X, Y, H)
>>> ax.set_aspect('equal')


#NonUniformImage displays exact bin edges with interpolation:
>>> ax = fig.add_subplot(133)
>>> ax.set_title('NonUniformImage: interpolated')
>>> im = mpl.image.NonUniformImage(ax, interpolation='bilinear')
>>> xcenters = xedges[:-1] + 0.5 * (xedges[1:] - xedges[:-1])
>>> ycenters = yedges[:-1] + 0.5 * (yedges[1:] - yedges[:-1])
>>> im.set_data(xcenters, ycenters, H)
>>> ax.images.append(im)
>>> ax.set_xlim(xedges[0], xedges[-1])
>>> ax.set_ylim(yedges[0], yedges[-1])
>>> ax.set_aspect('equal')
>>> plt.show()




###Numpy- Random sampling (use with prefix numpy.random.)

# Simple random data
#size could tuple of dimensions 
rand(d0, d1, ..., dn) 				Random values in a given shape.
randn(d0, d1, ..., dn) 				Return a sample (or samples) from the 'standard normal' distributio n.
randint(low[, high, size, dtype]) 	Return random integers from low (inclusive) to high (exclusive ).
random_integers(low[, high, size]) 	Random integers of type np.int between low and high, inclusive.
random_sample([size]) 				Return random floats in the half-open interval [0.0, 1.0 ).
random([size]) 						Return random floats in the half-open interval [0.0, 1.0 ).
ranf([size]) 						Return random floats in the half-open interval [0.0, 1.0 ).
sample([size]) 						Return random floats in the half-open interval [0.0, 1.0 ).
choice(a[, size, replace, p]) 		Generates a random sample from a given 1-D array
bytes(length) 						Return random byte s.

#Example
>>> np.random.randn()
2.1923875335537315 #random

#Two-by-four array of samples from N(3, 6.25):
#sd = sqrt(var)
>>> 2.5 * np.random.randn(2, 4) + 3
array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],  #random       
[ 0.39924804,  4.68456316,  4.99394529,  4.84057254]]) #random



#Permutations
shuffle(x) 							Modify a sequence in-place by shuffling its content s.
permutation(x) 						Randomly permute a sequence, or return a permuted rang e.

#Example of permutation
>>> np.random.permutation(10)
array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6])

>>>>> np.random.permutation([1, 4, 9, 12, 15])
array([15,  1,  9,  4, 12])



>>> arr = np.arange(9).reshape((3, 3))
>>> np.random.permutation(arr)
array([[6, 7, 8],       
[0, 1, 2],       
[3, 4, 5]])

	   

#Distributions
#size could tuple of dimensions 
beta(a, b[, size]) 							Draw samples from a Beta distributio n.
binomial(n, p[, size]) 						Draw samples from a binomial distributio n.
chisquare(df[, size]) 						Draw samples from a chi-square distributio n.
dirichlet(alpha[, size]) 					Draw samples from the Dirichlet distributio n.
exponential([scale, size]) 					Draw samples from an exponential distributio n.
f(dfnum, dfden[, size]) 					Draw samples from an F distributio n.
gamma(shape[, scale, size]) 				Draw samples from a Gamma distributio n.
geometric(p[, size]) 						Draw samples from the geometric distributio n.
gumbel([loc, scale, size]) 					Draw samples from a Gumbel distributio n.
hypergeometric(ngood, nbad, nsample[, size]) Draw samples from a Hypergeometric distributio n.
laplace([loc, scale, size]) 				Draw samples from the Laplace or double exponential distribution with specified location (or mean) and scale (decay ).
logistic([loc, scale, size]) 				Draw samples from a logistic distributio n.
lognormal([mean, sigma, size]) 				Draw samples from a log-normal distributio n.
logseries(p[, size]) 						Draw samples from a logarithmic series distributio n.
multinomial(n, pvals[, size]) 				Draw samples from a multinomial distributio n.
multivariate_normal(mean, cov[, size]) 		Draw random samples from a multivariate normal distributio n.
negative_binomial(n, p[, size]) 			Draw samples from a negative binomial distributio n.
noncentral_chisquare(df, nonc[, size]) 		Draw samples from a noncentral chi-square distributio n.
noncentral_f(dfnum, dfden, nonc[, size]) 	Draw samples from the noncentral F distributio n.
normal([loc, scale, size]) 					Draw random samples from a normal (Gaussian) distributio n.
pareto(a[, size]) 							Draw samples from a Pareto II or Lomax distribution with specified shap e.
poisson([lam, size]) 						Draw samples from a Poisson distributio n.
power(a[, size]) 							Draws samples in [0, 1] from a power distribution with positive exponent a -  1.
rayleigh([scale, size]) 					Draw samples from a Rayleigh distributio n.
standard_cauchy([size]) 					Draw samples from a standard Cauchy distribution with mode =  0.
standard_exponential([size]) 				Draw samples from the standard exponential distributio n.
standard_gamma(shape[, size]) 				Draw samples from a standard Gamma distributio n.
standard_normal([size]) 					Draw samples from a standard Normal distribution (mean=0, stdev=1 ).
standard_t(df[, size]) 						Draw samples from a standard Students t distribution with df degrees of freedo m.
triangular(left, mode, right[, size]) 		Draw samples from the triangular distribution over the interval [left, right ].
uniform([low, high, size]) 					Draw samples from a uniform distributio n.
vonmises(mu, kappa[, size]) 				Draw samples from a von Mises distributio n.
wald(mean, scale[, size]) 					Draw samples from a Wald, or inverse Gaussian, distributio n.
weibull(a[, size]) 							Draw samples from a Weibull distributio n.
zipf(a[, size]) 							Draw samples from a Zipf distributio n.


#Example of normal
>>> mu, sigma = 0, 0.1 # mean and standard deviation
>>> s = np.random.normal(mu, sigma, (10,10) )


#Verify the mean and the variance:
>>> abs(mu - np.mean(s, axis=1)) < 0.01
True

>>> abs(sigma - np.std(s, ddof=1, axis=1)) < 0.01
True

#Display the histogram of the samples, along with the probability density function:
>>> mu, sigma = 0, 0.1 # mean and standard deviation
>>> s = np.random.normal(mu, sigma, 1000)
>>> abs(mu - np.mean(s)) < 0.01
True
>>> abs(sigma - np.std(s, ddof=1)) < 0.01
True
>>> import matplotlib.pyplot as plt
#The values of the histogram bins, The edges of the bins, patches
>>> count, bins, ignored = plt.hist(s, bins=30, normed=True)
>>> plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2) ),  linewidth=2, color='r')
>>> plt.show()




#Random generator
RandomState 		Container for the Mersenne Twister pseudo-random number generato r.
seed([seed]) 		Seed the generato r.
get_state() 		Return a tuple representing the internal state of the generato r.
set_state(state) 	Set the internal state of the generator from a tupl e.


###Numpy - Window functions - from module numpy.
#Various windows
bartlett(M) 			Return the Bartlett window.
blackman(M) 			Return the Blackman window.
hamming(M) 				Return the Hamming window.
hanning(M) 				Return the Hanning window.
kaiser(M, beta) 		Return the Kaiser window.


>>> np.bartlett(12)
array([ 0.        ,  0.18181818,  0.36363636,  0.54545455,  0.72727273,        
0.90909091,  0.90909091,  0.72727273,  0.54545455,  0.36363636,        
0.18181818,  0.        ])


#Plot the window and its frequency response (requires SciPy and matplotlib):


>>> from numpy.fft import fft, fftshift
>>> window = np.bartlett(51)
>>> plt.plot(window)
[<matplotlib.lines.Line2D object at 0x...>]
>>> plt.title("Bartlett window")
<matplotlib.text.Text object at 0x...>
>>> plt.ylabel("Amplitude")
<matplotlib.text.Text object at 0x...>
>>> plt.xlabel("Sample")
<matplotlib.text.Text object at 0x...>
>>> plt.show()


>>> plt.figure()
<matplotlib.figure.Figure object at 0x...>
>>> A = fft(window, 2048) / 25.5
>>> mag = np.abs(fftshift(A))
>>> freq = np.linspace(-0.5, 0.5, len(A))
>>> response = 20 * np.log10(mag)
>>> response = np.clip(response, -100, 100)
>>> plt.plot(freq, response)
[<matplotlib.lines.Line2D object at 0x...>]
>>> plt.title("Frequency response of Bartlett window")
<matplotlib.text.Text object at 0x...>
>>> plt.ylabel("Magnitude [dB]")
<matplotlib.text.Text object at 0x...>
>>> plt.xlabel("Normalized frequency [cycles per sample]")
<matplotlib.text.Text object at 0x...>
>>> plt.axis('tight')
(-0.5, 0.5, -100.0, ...)
>>> plt.show()







###Numpy- Test Support (numpy.testing)

#Common test support for all numpy test scripts.

#Asserts
assert_almost_equal(actual, desired[, ...]) 			Raises an AssertionError if two items are not equal up to desired precisio n.
assert_approx_equal(actual, desired[, ...]) 			Raises an AssertionError if two items are not equal up to significant digit s.
assert_array_almost_equal(x, y[, decimal, ...]) 		Raises an AssertionError if two objects are not equal up to desired precisio n.
assert_allclose(actual, desired[, rtol, ...]) 			Raises an AssertionError if two objects are not equal up to desired toleranc e.
assert_array_almost_equal_nulp(x, y[, nulp]) 			Compare two arrays relatively to their spacin g.
assert_array_max_ulp(a, b[, maxulp, dtype]) 			Check that all items of arrays differ in at most N Units in the Last Plac e.
assert_array_equal(x, y[, err_msg, verbose]) 			Raises an AssertionError if two array_like objects are not equa l.
assert_array_less(x, y[, err_msg, verbose]) 			Raises an AssertionError if two array_like objects are not ordered by less tha n.
assert_equal(actual, desired[, err_msg, verbose]) 		Raises an AssertionError if two objects are not equa l.
assert_raises(exception_class, callable, ...) 			Fail unless an exception of class exception_class is thrown by callable when invoked with arguments args and keyword arguments kwarg s.
assert_raises_regex(exception_class, ...[, ...]) 		Fail unless an exception of class exception_class and with message that matches expected_regexp is thrown by callable when invoked with arguments args and keyword arguments kwarg s.
assert_warns(warning_class, *args, **kwargs) 			Fail unless the given callable throws the specified warnin g.
assert_string_equal(actual, desired) 					Test if two strings are equa l.

#Examp le
>>> np.testing.assert_equal([4,5], [4,6])
...
<type 'exceptions.AssertionError'>:
Items are not equal:
item=1 
ACTUAL: 5 
DESIRED: 6
 


#Decorators
decorators.deprecated([conditional]) 				Filter deprecation warnings while running the test suit e.
decorators.knownfailureif(fail_condition[, msg]) 	Make function raise KnownFailureException exception if given condition is tru e.
decorators.setastest([tf]) 							Signals to nose that this function is or is not a tes t.
decorators.skipif(skip_condition[, msg]) 			Make function raise SkipTest exception if a given condition is tru e.
decorators.slow(t) 									Label a test as 'slow†ٮ
decorate_methods(cls, decorator[, testmatch]) 		Apply a decorator to all methods in a class matching a regular expressio n.


#Test Running
Tester 									alias of NoseTest er
run_module_suite([file_to_run, argv]) 	Run a test modul e.
rundocs([filename, raise_on_error ])





























     

###SciPy  - scientific and engineering functions based on NumPy array 

#below modules re present 
•Clustering package                                 (scipy.cluster)
•Constants                                          (scipy.constants)
•Discrete Fourier transforms                        (scipy.fftpack)
•Integration and ODEs                               (scipy.integrate)
•Interpolation                                      (scipy.interpolate)
•Input and output                                   (scipy.io)
•Linear algebra                                     (scipy.linalg)
•Miscellaneous routines                             (scipy.misc)
•Multi-dimensional image processing                 (scipy.ndimage)
•Orthogonal distance regression                     (scipy.odr)
•Optimization and root finding                      (scipy.optimize)
•Signal processing                                  (scipy.signal)
•Sparse matrices                                    (scipy.sparse)
•Sparse linear algebra                              (scipy.sparse.linalg)
•Compressed Sparse Graph Routines                   (scipy.sparse.csgraph)
•Spatial algorithms and data structures             (scipy.spatial)
•Special functions                                  (scipy.special)
•Statistical functions                              (scipy.stats)
•Statistical functions for masked arrays            (scipy.stats.mstats)


### SciPi - Quick matplotlib plot 
#https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot

import matplotlib.pyplot as plt


#For multi lines, an arbitrary number of x, y, fmt groups can be specified
#x,y are numpy array or list  , fmt = 'One_char_fromColor_one_char_fromLineStyle'
plt.plot(x1, y1, 'g^', x2, y2, 'g-', ....)

#or for single line 
plt.plot(x, y, color='green', linestyle='dashed', marker='o',  markerfacecolor='blue', markersize=12).
#colors can be full names ('green'), hex strings ('#008000'), 
#RGB or RGBA tuples ((0,1,0,1)) or grayscale intensities as a string ('0.8')

#then show 
plt.show()

## line style or marker
'-'     solid line style 
'--'    dashed line style 
'-.'    dash-dot line style 
':'     dotted line style 
'.'     point marker 
','     pixel marker 
'o'     circle marker 
'v'     triangle_down marker 
'^'     triangle_up marker 
'<'     triangle_left marker 
'>'     triangle_right marker 
'1'     tri_down marker 
'2'     tri_up marker 
'3'     tri_left marker 
'4'     tri_right marker 
's'     square marker 
'p'     pentagon marker 
'*'     star marker 
'h'     hexagon1 marker 
'H'     hexagon2 marker 
'+'     plus marker 
'x'     x marker 
'D'     diamond marker 
'd'     thin_diamond marker 
'|'     vline marker 
'_'     hline marker 

#color style 
'b'     blue 
'g'     green 
'r'     red 
'c'     cyan 
'm'     magenta 
'y'     yellow 
'k'     black 
'w'     white 

## few properties that can be passsed to  plt.plot or ax.plot or other plot functions 
alpha                   float (0.0 transparent through 1.0 opaque) 
animated                [True | False] 
antialiased or aa       [True | False] 
clip_box                a matplotlib.transforms.Bbox instance 
clip_on                 [True | False] 
color or c              any matplotlib color 
dash_capstyle           ['butt' | 'round' | 'projecting'] 
dash_joinstyle          ['miter' | 'round' | 'bevel'] 
drawstyle               ['default' | 'steps' | 'steps-pre' | 'steps-mid' | 'steps-post'] 
fillstyle               ['full' | 'left' | 'right' | 'bottom' | 'top' | 'none'] 
label                   string or anything printable with '%s' conversion. 
linestyle or ls         ['solid' | 'dashed', 'dashdot', 'dotted' | (offset, on-off-dash-seq) | '-' | '--' | '-.' | ':' | 'None' | ' ' | ''] 
linewidth or lw         float value in points 
marker                  A valid marker style 
markeredgecolor or mec  any matplotlib color 
markeredgewidth or mew  float value in points 
markerfacecolor or mfc  any matplotlib color 
markersize or ms        float 
solid_capstyle          ['butt' | 'round' | 'projecting'] 
solid_joinstyle         ['miter' | 'round' | 'bevel'] 
visible                 [True | False] 
zorder                  any number 

##Few other plot functions on plt. or on ax., 
#Most of the above key words can be given 
.scatter(x, y, s=None, c=None, marker=None, **kwargs)                           Make a scatter plot of x vs y. 
.plot_date(x, y, fmt='o', tz=None, xdate=True, ydate=False, *, data=None, **kwargs)  A plot with data that contains dates. 
.step(x, y,**kwargs)       Make a step plot. 

.loglog(x, y,**kwargs)     Make a plot with log scaling on both the x and y axis. 
.semilogx(x, y,**kwargs)   Make a plot with log scaling on the x axis. 
.semilogy(x, y,**kwargs)   Make a plot with log scaling on the y axis. 

.bar(x, height, width_default: 0.8, *, align='center', **kwargs)        Make a bar plot,
.barh(y, width, height_default: 0.8, *, align='center', **kwargs)       Make a horizontal bar plot. 

.pie(x, explode=None, labels=None, colors=None, ..)                     Plot a pie chart. 
.fill(sequence of x, y, color like plot, **kwargs)  Plot filled polygons. 

.vlines(x, ymin, ymax, colors='k', linestyles='solid', label='', *, data=None, **kwargs)            Plot vertical lines. 
.hlines(y, xmin, xmax, colors='k', linestyles='solid', label='', *, data=None, **kwargs)            Plot horizontal lines at each y from xmin to xmax. 

.axhline(y=0, xmin=0, xmax=1, **kwargs)             Add a horizontal line across the axis.
.axhspan(ymin, ymax, xmin=0, xmax=1, **kwargs)      Add a horizontal span (rectangle) across the axis.
.axvline(x=0, ymin=0, ymax=1, **kwargs)             Add a vertical line across the axes
.axvspan(xmin, xmax, ymin=0, ymax=1, **kwargs)      Add a vertical span (rectangle) across the axes

.hist(x, bins=None, range=None, density=None, weights=None, cumulative=False,..)        Plot a histogram.
.imshow(X_shape (n, m) or (n, m, 3) or (n, m, 4) ,..)   Display an image on the axes.
.matshow(Z_shape (n, m), **kwargs)                      Plot a matrix or array as an image.


##With subplots 
#Draw Window contains many Figure 
#Each Figure can have multiple axes on them
#Axes are used to draw 

##Option-1 
#note subplots(..) returns (figure,axes)
#where axes is numpy.ndarray, hence access like axes[0,0], axes[0,1],... for complex subplots 
#for simple , can destructure immediately
figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=True,...  )
ax1.plot(x,y,fmt,.....)  
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)
#then show 
plt.show()

##Option-2 
fig = plt.figure(figsize=(15,15))        #create a figure 
ax1 = fig.add_subplot(211)               ## nrows,ncols, which_Axes_tomake_current ie 1  
ax1.plot(x,y,fmt,.....)  
ax2 = fig.add_subplot(212)
ax2.plot(x,y,fmt,.....) 
#then show 
plt.show()

##Option-3 
plt.figure(1)                # the first figure
plt.subplot(211)             # nrows,ncols, which_Axes_tomake_current ie 1                             
plt.plot(x,y,fmt,.....)      # all plot/scatter/box/hist etc goes subplot 1 
plt.subplot(212)             # nrows,ncols, which_Axes_tomake_current ie 2
plt.plot(x,y,fmt,.....)      # all plot/scatter/box/hist etc goes subplot 2

plt.figure(2)                # a second figure, sets to this figure, all plot commands go here
plt.plot(x,y,fmt,.....)      # creates a subplot(111) by default

plt.figure(1)                # figure 1 current; subplot(212) still current
plt.subplot(211)             # make subplot(211) in figure1 current
plt.title('Easy as 1, 2, 3') # subplot 211 title
#then show 
plt.show()

##Few utility methods 
ax = plt.gca()  #Get the current Axes instance on the current figure 
fig = plt.gcf()  #Get a reference to the current figure.
ax.cla() #clear 
fig.clf() # clear 

##Few figure methods 
fig.legend(handles=(line1, line2, line3), labels=('label1', 'label2', 'label3'), loc='upper right') #loc can be (x,y) or predefined string , linen are matplotlib line instance
fig.text(x, y, s, *args, **kwargs) #Add text to figure.
fig.savefig(fname, **kwargs) #Save the current figure
fig.sca(a)          #Set the current axes to be a and return a
fig.set_dpi(val)            #Set the dots-per-inch of the figure, val is float 
fig.set_edgecolor(color)    #any matplotlib color 
fig.set_facecolor(color)    #any matplotlib color 
fig.set_figheight(val, forward=False) #val is float 
fig.set_figwidth(val, forward=False)  #val is float 
fig.set_size_inches(w, h=None, forward=True)
fig.subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None) #returns axes as ndarray


##Few plt. methods, Note below  operates on the current axis.
#Note below relevant methods withour arg gives current value eg xlim() gives current xlimit 
plt.xlabel('x axis label', fontsize=14, color='red')
plt.ylabel('y axis label')
plt.title('Sine and Cosine')
plt.legend(['Sine', 'Cosine'])
plt.axis([0, 4, 0, 10]) #Set the axis, [xmin, xmax, ymin, ymax] or 'off' or 'equal' or 'scaled' or 'tight' etc 
plt.text(60, .025, r'$\mu=100,\ \sigma=15$')  #Any text  or  '$any LaTex code$'
plt.grid(True)
plt.ylim(-2,2)
plt.xlim(-2,2)
plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),arrowprops=dict(facecolor='black', shrink=0.05),) #xytext=location of text, xy=location for annotation
plt.yscale('linear') #log, symlog, logit
plt.xscale('linear')

##Few Axes methods  , many are available on plt. as well 
#Note below relevant methods withour arg or get_*() gives current value 
#eg xlim() gives current xlimit , get_xlabel() gives current xlabel 
ax.set_xlabel('x axis label')
ax.set_ylabel('y axis label')
ax.set_title('Simple plot')
ax.legend(['Sine', 'Cosine'])

ax.axis([0, 4, 0, 10]) #Set the axis, [xmin, xmax, ymin, ymax] or 'off' or 'equal' or 'scaled' or 'tight' etc 
ax.set_axis_off() #Turn off the axis. 
ax.set_axis_on()  # Turn on the axis. 

ax.text(60, .025, r'$\mu=100,\ \sigma=15$')  #Any text 
ax.annotate('local max', xy=(2, 1), xytext=(3, 1.5),arrowprops=dict(facecolor='black', shrink=0.05),) #xytext=location of text, xy=location for annotation
ax.arrow(x, y, dx, dy, **kwargs)    #Add an arrow to the axes.
ax.grid(b=True|False, color='r', linestyle='-', linewidth=2)
ax.set_label(s)       #Set the label to s for auto legend.
 
ax.set_ylim(-2,2)
ax.set_xlim(-2,2)
ax.set_yscale('linear') #log, symlog, logit
ax.set_xscale('linear')
ax.set_visible(b)     #Set the artist's visibility.
ax.set_zorder(level)  #Set the zorder for the artist. Artists with lower zorder values are drawn first.

ax.set_xticks(ticks, minor=False)     #Set the x ticks with list of ticks
ax.set_xticklabels(labels, fontdict=None, minor=False, **kwargs)#Set the x-tick labels with list of string labels.
ax.set_yticks(ticks, minor=False)#Set the y ticks with list of ticks
ax.set_yticklabels(labels, fontdict=None, minor=False, **kwargs)#Set the y-tick labels with list of strings labels.

ax.xaxis_date(tz=None)  #Sets up x-axis ticks and labels that treat the x data as dates.
ax.yaxis_date(tz=None)  #Sets up y-axis ticks and labels that treat the y data as dates.
ax.minorticks_off()     #Remove minor ticks from the axes.
ax.minorticks_on()      #Remove minor ticks from the axes.
new_ax = ax.twinx()     #Create a twin Axes sharing the xaxis
new_ax = ax.twiny()     #Create a twin Axes sharing the yaxis



###Scipy - scipy.linalg - https://docs.scipy.org/doc/scipy/reference/linalg.html
#similar to Python numpy.linalg

##scipy.linalg.inv(a, overwrite_a=False, check_finite=True)

>>> from scipy import linalg
>>> a = np.array([[1., 2.], [3., 4.]])
>>> linalg.inv(a)
array([[-2. ,  1. ],
       [ 1.5, -0.5]])
>>> np.dot(a, linalg.inv(a))
array([[ 1.,  0.],
       [ 0.,  1.]])


##scipy.linalg.solve(a, b, sym_pos=False, lower=False, overwrite_a=False, overwrite_b=False, debug=None, check_finite=True, assume_a='gen', transposed=False)

 >>> a = np.array([[3, 2, 0], [1, -1, 0], [0, 5, 1]])
>>> b = np.array([2, 4, -1])
>>> from scipy import linalg
>>> x = linalg.solve(a, b)
>>> x
array([ 2., -2.,  9.])
>>> np.dot(a, x) == b
array([ True,  True,  True], dtype=bool)


##scipy.linalg.eig(a, b=None, left=False, right=True, overwrite_a=False, overwrite_b=False, check_finite=True, homogeneous_eigvals=False)
a : (M, M) array_like
    A complex or real matrix whose eigenvalues and eigenvectors will be computed.

b : (M, M) array_like, optional
    Right-hand side matrix in a generalized eigenvalue problem. 
    Default is None, identity matrix is assumed.

Returns:
    w : (M,) or (2, M) double or complex ndarray
    The eigenvalues, each repeated according to its multiplicity. 
    The shape is (M,) unless homogeneous_eigvals=True.

vl : (M, M) double or complex ndarray
    The normalized left eigenvector corresponding to the eigenvalue w[i] is the column vl[:,i]. 
    Only returned if left=True.

vr : (M, M) double or complex ndarray
    The normalized right eigenvector corresponding to the eigenvalue w[i] is the column vr[:,i]. 
    Only returned if right=True.
 
 
>>> import numpy as np
>>> import scipy as sp
>>> import scipy.linalg
>>> K = np.mat([[1.8, -1.097+0.995j], [-1.097-0.955j, 1.8]])
>>> M = np.mat([[209., 0.], [0., 209.]])
>>> M
matrix([[ 209.,    0.],
        [   0.,  209.]])
>>> K
matrix([[ 1.800+0.j   , -1.097+0.955j],
        [-1.097-0.955j,  1.800+0.j   ]])
>>> D, V = sp.linalg.eig(K, b=M)
>>> D
array([ 0.00165333 -1.99202696e-19j,  0.01557155 +0.00000000e+00j])
>>> V
array([[ 0.70710678 +0.00000000e+00j, -0.53332494 +4.64289256e-01j],
       [ 0.53332494 +4.64289256e-01j,  0.70710678 -8.38231384e-18j]])







###Scipy - optimize - https://docs.scipy.org/doc/scipy/reference/optimize.html


##scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)
#Minimization of scalar function of one or more variables
fun : callable
    The objective function to be minimized. 
    Must be in the form f(x, *args). 
    The optimizing argument, x, is a 1-D array of points, 
    and args is a tuple of any additional fixed parameters needed to completely specify the function
bounds : sequence, optional
    Bounds for variables (only for L-BFGS-B, TNC and SLSQP). 
    (min, max) pairs for each element in x, defining the bounds on that parameter. 
    Use None for one of min or max when there is no bound in that direction.
constraints : dict or sequence of dict, optional
    Constraints definition (only for COBYLA and SLSQP). 
    Each constraint is defined in a dictionary with fields:
    type : str
           Constraint type: 'eq' for equality, 'ineq' for inequality.
    fun : callable
          The function defining the constraint.
method : Type of solver. Should be one of
    •'Nelder-Mead' 
    •'Powell' 
    •'CG' 
    •'BFGS' 
    •'Newton-CG' 
    •'L-BFGS-B' 
    •'TNC' 
    •'COBYLA' 
    •'SLSQP' 
    •'dogleg' 
    •'trust-ncg' 
    •'trust-exact' 
    •'trust-krylov' 
Returns:res : OptimizeResult
    Attributes are 
        x (ndarray) The solution of the optimization. 
        success (bool) Whether or not the optimizer exited successfully. 
        status (int) Termination status of the optimizer. 
        message (str) Description of the cause of the termination. 
        fun, jac, hess: ndarray Values of objective function, its Jacobian and its Hessian (if available). 
        hess_inv (object): Inverse of the objective function's Hessian; 
        nfev, njev, nhev: (int) Number of evaluations of the objective functions and of its Jacobian and Hessian. 
        nit (int): Number of iterations performed by the optimizer. 
        maxcv (float): The maximum constraint violation 

     
#Example - The objective function is, two variables= x[0], x[1]
>>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2

#There are three constraints defined as:
>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},
        {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},
        {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})
#And variables must be positive, hence the following bounds:
>>> bnds = ((0, None), (0, None))
#The optimization problem is solved using the SLSQP method as:
>>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,  constraints=cons)
>>> res.x
>>> print(res.message)
>>> res.hess_inv

#Example - minimizing the Rosenbrock function. 
#This function (and its respective derivatives) is implemented in rosen 
from scipy.optimize import minimize, rosen, rosen_der
#Option-1 
>>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]
>>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)
>>> res.x
array([ 1.,  1.,  1.,  1.,  1.])
##Option-2 
>>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,
...                options={'gtol': 1e-6, 'disp': True})
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 26
         Function evaluations: 31
         Gradient evaluations: 31
>>> res.x
array([ 1.,  1.,  1.,  1.,  1.])
>>> print(res.message)
Optimization terminated successfully.
>>> res.hess_inv




##scipy.optimize.minimize_scalar(fun, bracket=None, bounds=None, args=(), method='brent', tol=None, options=None)
#Minimization of scalar function of one variable.
    method : Type of solver. Should be one of:
        •'Brent' 
        •'Bounded' 
        •'Golden' 


>>> def f(x):
        return (x - 2) * x * (x + 2)**2


#Using the Brent method, we find the local minimum as:
>>> from scipy.optimize import minimize_scalar
>>> res = minimize_scalar(f)
>>> res.x
1.28077640403
#Using the Bounded method, we find a local minimum with specified bounds as:
>>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')
>>> res.x
-2.0000002026


##Equation (Local) Minimizers
nnls(A, b)                                          Solve argmin_x || Ax - b ||_2 for x>=0. 
                                                    A : ndarray,Matrix A 
                                                    b : ndarray,Right-hand side vector.
                                                    Returns:
                                                        x : ndarray,Solution vector.
                                                        rnorm : float,The residual, || Ax-b ||_2.
lsq_linear(A, b[, bounds, method, tol, ...])        Solve a linear  
least_squares(fun, x0[, jac, bounds, ...])          Solve a nonlinear least-squares problem with bounds on the variables. 


##scipy.optimize.lsq_linear(A, b, bounds=(-inf, inf), method='trf', tol=1e-10, lsq_solver=None, lsmr_tol=None, max_iter=None, verbose=0)
#Solve a linear least-squares problem with bounds on the variables.
#Given a m-by-n design matrix A and a target vector b with m elements
    method : 'trf' or 'bvls'
    lsq_solver : {None, 'exact', 'lsmr'}, optional
    Returns:OptimizeResult with the following fields defined:
        x : ndarray, shape (n,)
            Solution found.
        cost : float
            Value of the cost function at the solution.
        fun : ndarray, shape (m,)
            Vector of residuals at the solution.
        nit : int
            Number of iterations. Zero if the unconstrained solution is optimal.
        status : int
            Reason for algorithm termination:
            •-1 : the algorithm was not able to make progress on the last iteration.
            •0 : the maximum number of iterations is exceeded.
            •1 : the first-order optimality measure is less than tol.
            •2 : the relative change of the cost function is less than tol.
            •3 : the unconstrained solution is optimal.
        message : str
            Verbal description of the termination reason.
        success : bool
            True if one of the convergence criteria is satisfied (status > 0).
 
#Example 
from scipy.sparse import rand
from scipy.optimize import lsq_linear

np.random.seed(0)
m = 20000
n = 10000
A = rand(m, n, density=1e-4)
b = np.random.randn(m)
lb = np.random.randn(n)
ub = lb + 1

>>> res = lsq_linear(A, b, bounds=(lb, ub), lsmr_tol='auto', verbose=1)
>>> res.x


##scipy.optimize.least_squares(fun, x0, jac='2-point', bounds=(-inf, inf), method='trf', ftol=1e-08, xtol=1e-08, gtol=1e-08, x_scale=1.0, loss='linear', f_scale=1.0, diff_step=None, tr_solver=None, tr_options={}, jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={})
#Solve a nonlinear least-squares problem with bounds on the variables.
fun : callable
    Function which computes the vector of residuals, 
    with the signature fun(x, *args, **kwargs), 
    i.e., the minimization proceeds with respect to its first argument. 
    The argument x passed to this function is an ndarray of shape (n,) 
    (never a scalar, even for n=1). 
    It must return a 1-d array_like of shape (m,) or a scalar. 
bounds : 2-tuple of array_like, optional
    Lower and upper bounds on independent variables. 
    Defaults to no bounds.
    Each array must match the size of x0 or be a scalar, 
    in the latter case a bound will be the same for all variables. 
    Use np.inf with an appropriate sign to disable bounds on all or some variables.
method : 
    {'trf', 'dogbox', 'lm'}, optional
loss : str or callable, optional
    Determines the loss function. The following keyword values are allowed:
    •'linear' (default) : rho(z) = z. Gives a standard least-squares problem.
    •'soft_l1' : rho(z) = 2 * ((1 + z)**0.5 - 1). The smooth approximation of l1 (absolute value) loss. Usually a good choice for robust least squares.
    •'huber' : rho(z) = z if z <= 1 else 2*z**0.5 - 1. Works similarly to 'soft_l1'.
    •'cauchy' : rho(z) = ln(1 + z). Severely weakens outliers influence, but may cause difficulties in optimization process.
    •'arctan' : rho(z) = arctan(z). Limits a maximum loss on a single residual, has properties similar to 'cauchy'.
    If callable, it must take a 1-d ndarray z=f**2 
    and return an array_like with shape (3, m) 
    where row 0 contains function values, 
    row 1 contains first derivatives
    and row 2 contains second derivatives. 
    Method 'lm' supports only 'linear' loss.
Returns:OptimizeResult with the following fields defined:
    x : ndarray, shape (n,)
        Solution found.
    cost : float
        Value of the cost function at the solution.
    fun : ndarray, shape (m,)
        Vector of residuals at the solution.
    jac : ndarray, sparse matrix or LinearOperator, shape (m, n)
        Modified Jacobian matrix at the solution
    grad : ndarray, shape (m,)
        Gradient of the cost function at the solution.
    status : int
        The reason for algorithm termination:
        •-1 : improper input parameters status returned from MINPACK.
        •0 : the maximum number of function evaluations is exceeded.
        •1 : gtol termination condition is satisfied.
        •2 : ftol termination condition is satisfied.
        •3 : xtol termination condition is satisfied.
        •4 : Both ftol and xtol termination conditions are satisfied.
    message : str
        Verbal description of the termination reason.
    success : bool
        True if one of the convergence criteria is satisfied (status > 0).
 



#Example -  solve a curve fitting problem using robust loss function 
#to take care of outliers in the data. 

#Define the model function as y = a + b * exp(c * t), 
#where t is a predictor variable, 
#y is an observation and a, b, c are parameters to estimate.

#generate data:
>>> def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0):
        y = a + b * np.exp(t * c)    
        rnd = np.random.RandomState(random_state)
        error = noise * rnd.randn(t.size)
        outliers = rnd.randint(0, t.size, n_outliers)
        error[outliers] *= 10    
        return y + error
    
a = 0.5
b = 2.0
c = -1
t_min = 0
t_max = 10
n_points = 15

t_train = np.linspace(t_min, t_max, n_points)
y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)


#Define function for computing residuals and initial estimate of parameters.
#a=x[0], b= x[1], c = x[2]
>>> def fun(x, t, y):
        return x[0] + x[1] * np.exp(x[2] * t) - y

>>> x0 = np.array([1.0, 1.0, 0.0])
#Compute a standard least-squares solution:
>>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))


#compute two solutions with two different robust loss functions. 
#The parameter f_scale is set to 0.1, meaning that inlier residuals should not significantly exceed 0.1 
#(the noise level used).
>>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1, args=(t_train, y_train))
>>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,    args=(t_train, y_train))


#plot all the curves. 
#by selecting an appropriate loss 
#we can get estimates close to optimal even in the presence of strong outliers. 

#it is recommended to try 'soft_l1' or 'huber' losses first (if at all necessary)
#as the other two options may cause difficulties in optimization process

>>> t_test = np.linspace(t_min, t_max, n_points * 10)
>>> y_true = gen_data(t_test, a, b, c)
>>> y_lsq = gen_data(t_test, *res_lsq.x)
>>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)
>>> y_log = gen_data(t_test, *res_log.x)

>>> import matplotlib.pyplot as plt
>>> plt.plot(t_train, y_train, 'o')
>>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')
>>> plt.plot(t_test, y_lsq, label='linear loss')
>>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')
>>> plt.plot(t_test, y_log, label='cauchy loss')
>>> plt.xlabel("t")
>>> plt.ylabel("y")
>>> plt.legend()
>>> plt.show()


##scipy.optimize.curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=True, bounds=(-inf, inf), method=None, jac=None, **kwargs)
#Use non-linear least squares to fit a function, f, to data.
    f : callable
        The model function, f(x, ...). 
        It must take the independent variable as the first argument 
        and the parameters to fit as separate remaining arguments.
        xdata : An M-length sequence or an (k,M)-shaped array for functions 
        with k predictors,The independent variable where the data is measured.
    ydata : M-length sequence
        
    p0 : None, scalar, or N-length sequence, optional
        Initial guess for the parameters. 
        If None, then the initial values will all be 1 
    bounds : 2-tuple of array_like, optional
        Lower and upper bounds on independent variables. 
        Defaults to no bounds. 
        Each element of the tuple must be either an array 
        with the length equal to the number of parameters, 
        or a scalar (in which case the bound is taken to be the same for all parameters.) 
        Use np.inf with an appropriate sign to disable bounds on all or some parameters.
    Returns:
        popt : array
            Optimal values for the parameters 
            so that the sum of the squared residuals of f(xdata, *popt) - ydata is minimized
        pcov : 2d array
            The estimated covariance of popt. 
            The diagonals provide the variance of the parameter estimate. 
            To compute one standard deviation errors on the parameters 
            use perr = np.sqrt(np.diag(pcov)).
 
#Example 
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit



def func(x, a, b, c):
    return a * np.exp(-b * x) + c


#define the data to be fit with some noise
>>> xdata = np.linspace(0, 4, 50)
>>> y = func(xdata, 2.5, 1.3, 0.5)  #a,b,c
>>> y_noise = 0.2 * np.random.normal(size=xdata.size)
>>> ydata = y + y_noise
>>> plt.plot(xdata, ydata, 'b-', label='data')


#Fit for the parameters a, b, c of the function func
>>> popt, pcov = curve_fit(func, xdata, ydata) #popt is array [a,b,c]
>>> plt.plot(xdata, func(xdata, *popt), 'r-', label='fit')


#Constrain the optimization to the region of 0 < a < 3, 0 < b < 2 and 0 < c < 1:
>>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 2., 1.]))
>>> plt.plot(xdata, func(xdata, *popt), 'g--', label='fit-with-bounds')
>>> plt.xlabel('x')
>>> plt.ylabel('y')
>>> plt.legend()
>>> plt.show()


##Global Optimization
basinhopping(func, x0[, niter, T, stepsize, ...]) Find the global minimum of a function using the basin-hopping algorithm 
brute(func, ranges[, args, Ns, full_output, ...]) Minimize a function over a given range by brute force. 
differential_evolution(func, bounds[, args, ...]) Finds the global minimum of a multivariate function 

##scipy.optimize.brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin>, disp=False
Uses the 'brute force' method, 
i.e. computes the function's value at each point of a multidimensional 
grid of points, to find the global minimum of the function.
    func : callable
        The objective function to be minimized. 
        Must be in the form f(x, *args), 
        where x is the argument in the form of a 1-D array (ie x,y,z,..)
        and args is a tuple of any additional fixed parameters needed to completely 
        specify the function.
    ranges : tuple
        Each component of the ranges tuple must be either a 'slice object' 
        or a range tuple of the form (low, high). 
        The program uses these to create the grid of points 
        on which the objective function will be computed
    Returns:
    x0 : ndarray
        A 1-D array containing the coordinates of a point 
        at which the objective function had its minimum value. 
    fval : float
        Function value at the point x0. 
 
 
#Example - define the objective function f as the sum of three other functions, 
#f = f1 + f2 + f3. 
#We suppose each of these has a signature (z, *params), 
#where z = (x, y), and params and the functions are as defined below.


params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)
def f1(z, *params):
    x, y = z
    a, b, c, d, e, f, g, h, i, j, k, l, scale = params
    return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)

def f2(z, *params):
    x, y = z
    a, b, c, d, e, f, g, h, i, j, k, l, scale = params
    return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))

def f3(z, *params):
    x, y = z
    a, b, c, d, e, f, g, h, i, j, k, l, scale = params
    return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))



def f(z, *params):
    return f1(z, *params) + f2(z, *params) + f3(z, *params)


rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))
from scipy import optimize
resbrute = optimize.brute(f, rranges, args=params, full_output=True, finish=optimize.fmin)
>>>resbrute[0]  # global minimum
array([-1.05665192,  1.80834843])
>>> resbrute[1]  # function value at global minimum
-3.4085818767

##scipy.optimize.differential_evolution
scipy.optimize.differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=1000, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0)[source]
    Finds the global minimum of a multivariate function. 
    Differential Evolution is stochastic in nature (does not use gradient methods) 
    to find the minimium, and can search large areas of candidate space, 
    but often requires larger numbers of function evaluations than conventional gradient based techniques.
Parameters:
    func : callable
        The objective function to be minimized. 
        Must be in the form f(x, *args), 
        where x is the argument in the form of a 1-D array 
        and args is a tuple of any additional fixed parameters needed to completely specify the function.
    bounds : sequence
        Bounds for variables. 
        (min, max) pairs for each element in x, defining the lower and upper bounds for the optimizing argument of func. 
        It is required to have len(bounds) == len(x). 
        len(bounds) is used to determine the number of parameters in x.
    args : tuple, optional
        Any additional fixed parameters needed to completely specify the objective function.
    strategy : str, optional
        •'best1bin'
        •'best1exp'
        •'rand1exp'
        •'randtobest1exp'
        •'best2exp'
        •'rand2exp'
        •'randtobest1bin'
        •'best2bin'
        •'rand2bin'
        •'rand1bin'
        The default is 'best1bin'.
Returns:res : OptimizeResult
    Important attributes are: x the solution array, 
    success a Boolean flag indicating if the optimizer exited successfully 
    message which describes the cause of the termination
#Example - find the minimum of the Ackley function (http://en.wikipedia.org/wiki/Test_functions_for_optimization).
from scipy.optimize import differential_evolution
import numpy as np
def ackley(x):  #function of two variables 
    arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))
    arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))
    return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e

bounds = [(-5, 5), (-5, 5)]
result = differential_evolution(ackley, bounds)
>>> result.x, result.fun
(array([ 0.,  0.]), 4.4408920985006262e-16)




##Linear Programming
scipy.optimize.linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method='simplex', callback=None, options=None)[source]
    Minimize a linear objective function subject to linear equality 
    and inequality constraints.
    Minimize:     c^T * x
    Subject to:   A_ub * x <= b_ub
                  A_eq * x == b_eq
    Parameters:
    c : array_like
        Coefficients of the linear objective function to be minimized.
    A_ub : array_like, optional
        2-D array which, when matrix-multiplied by x, 
        gives the values of the upper-bound inequality constraints at x.

    b_ub : array_like, optional
        1-D array of values representing the upper-bound of each inequality constraint 
        (row) in A_ub.
    A_eq : array_like, optional
        2-D array which, when matrix-multiplied by x, 
        gives the values of the equality constraints at x.
    b_eq : array_like, optional
        1-D array of values representing the RHS of each equality constraint (row) in A_eq.
    bounds : sequence, optional
        (min, max) pairs for each element in x, defining the bounds on that parameter. 
        Use None for one of min or max when there is no bound in that direction. 
        By default bounds are (0, None) (non-negative) 
        If a sequence containing a single tuple is provided, 
        then min and max will be applied to all variables in the problem.
    method : str, optional
        Type of solver. 'simplex' and 'interior-point' are supported.     
    Returns:A scipy.optimize.OptimizeResult consisting of the following fields:
        x : ndarray
            The independent variable vector which optimizes the linear programming problem.
        fun : float
            Value of the objective function.
        slack : ndarray
            The values of the slack variables. Each slack variable corresponds to an inequality constraint. If the slack is zero, then the corresponding constraint is active.
        success : bool
            Returns True if the algorithm succeeded in finding an optimal solution.
        status : int
            An integer representing the exit status of the optimization
        nit : int
        The number of iterations performed.
        message : str
            A string descriptor of the exit status of the optimization.

#Example 

#Minimize: f = -1*x[0] + 4*x[1]
#Subject to: -3*x[0] + 1*x[1] <= 6
#            1*x[0] + 2*x[1] <= 4
#where:           
#            x[1] >= -3
#            -inf <= x[0] <= inf


c = [-1, 4]
A = [[-3, 1], [1, 2]]  #row1= -3*x[0] + 1*x[1], row2= 1*x[0] + 2*x[1] 
b = [6, 4]  #<= 6, <= 4
x0_bounds = (None, None) #-inf <= x[0] <= inf
x1_bounds = (-3, None) #x[1] >= -3
from scipy.optimize import linprog
res = linprog(c, A_ub=A, b_ub=b, bounds=(x0_bounds, x1_bounds), options={"disp": True})
Optimization terminated successfully.
     Current function value: -22.000000
     Iterations: 1
>>> print(res)
     fun: -22.0
 message: 'Optimization terminated successfully.'
     nit: 1
   slack: array([ 39.,   0.])
  status: 0
 success: True
       x: array([ 10.,  -3.])

       
       
##Root finding - Scalar functions
# Ridders' method is faster than bisection, 
#but not generally as fast as the Brent rountines
brentq(f, a, b[, args, xtol, rtol, maxiter, ...]) Find a root of a function in a bracketing interval using Brent's method. 
brenth(f, a, b[, args, xtol, rtol, maxiter, ...]) Find root of f in [a,b]. 
ridder(f, a, b[, args, xtol, rtol, maxiter, ...]) Find a root of a function in an interval. 
bisect(f, a, b[, args, xtol, rtol, maxiter, ...]) Find root of a function within an interval. 
newton(func, x0[, fprime, args, tol, ...]) Find a zero using the Newton-Raphson or secant method. 

#Example - newton 
#fprime2 : The second order derivative of the function when available and convenient
>>> def f(x):
        return (x**3 - 1)  # only one real root at x = 1

from scipy import optimize
#fprime and fprime2 not provided, use secant method
root = optimize.newton(f, 1.5)
>>> root
1.0000000000000016

#Only fprime provided, use Newton Raphson method
root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2)
>>> root
1.0

#fprime2 provided, fprime provided/not provided use parabolic Halley's method
root = optimize.newton(f, 1.5, fprime2=lambda x: 6 * x)
>>> root
1.0000000000000016
root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2,fprime2=lambda x: 6 * x)
>>> root
1.0

#Example - of brenth

>>> def f(x):
        return (x**2 - 1)
        
from scipy import optimize
root = optimize.brenth(f, -2, 0)
>>> root
-1.0
root = optimize.brenth(f, 0, 2)
>>> root
1.0

#Example of bisect 
#f(a) and f(b) cannot have the same signs. Slow but sure.
>>> def f(x):
        return (x**2 - 1)

from scipy import optimize
root = optimize.bisect(f, 0, 2)
>>> root
1.0
root = optimize.bisect(f, -2, 0)
>>> root
-1.0




##Fixed point finding:
fixed_point(func, x0[, args, xtol, maxiter, ...]) Find a fixed point of the function. 
#Given a function of one or more variables and a starting point, 
#find a fixed-point of the function: i.e. where func(x0) == x0.                             
from scipy import optimize
def func(x, c1, c2):
    return np.sqrt(c1/(x+c2))

c1 = np.array([10,12.])
c2 = np.array([3, 5.])
>>> xx = optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))
array([ 1.4920333 ,  1.37228132])
>>> func(xx,c1,c2)
array([1.4920333 , 1.37228132])
    
       
       
##Multidimensional root finding - General nonlinear solvers:
scipy.optimize.root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)[source]
    Find a root of a vector function.
    Parameters:
        fun : callable
            A vector function to find a root of.
        x0 : ndarray
            Initial guess.
        args : tuple, optional
            Extra arguments passed to the objective function and its Jacobian.
        method : str, optional
            •Method Krylov uses Krylov approximation for inverse Jacobian. 
                It is suitable for large-scale problem
            Type of solver. Should be one of
            •'hybr' 
            •'lm' 
            •'broyden1' 
            •'broyden2' 
            •'anderson' 
            •'linearmixing' 
            •'diagbroyden' 
            •'excitingmixing' 
            •'krylov' 
            •'df-sane' 
        jac : bool or callable, optional
            If jac is a Boolean and is True, 
            fun is assumed to return the value of Jacobian along with the objective function. 
            If False, the Jacobian will be estimated numerically. 
            jac can also be a callable returning the Jacobian of fun. 
            In this case, it must accept the same arguments as fun.
    Returns:
        sol : OptimizeResult
        Important attributes are: x the solution array, 
        success a Boolean flag indicating if the algorithm exited successfully 
        message which describes the cause of the termination
 
#Example , with two variable, x[0], x[1]
def fun(x):
    return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,
         0.5 * (x[1] - x[0])**3 + x[1]]


#this is optional 
def jac(x):
    return np.array([[1 + 1.5 * (x[0] - x[1])**2,
                   -1.5 * (x[0] - x[1])**2],
                  [-1.5 * (x[1] - x[0])**2,
                   1 + 1.5 * (x[1] - x[0])**2]])


from scipy import optimize
sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')
>>> sol.x
array([ 0.8411639,  0.1588361])






### Scipy - Integration (scipy.integrate)

from scipy import integrate

>>> help(integrate) 
Methods for Integrating Functions given function object.
quad          -- General purpose integration.   
dblquad       -- General purpose double integration.   
tplquad       -- General purpose triple integration.   
fixed_quad    -- Integrate func(x) using Gaussian quadrature of order n.   
quadrature    -- Integrate with given tolerance using Gaussian quadrature.   
romberg       -- Integrate func using Romberg integration.
 
Methods for Integrating Functions given fixed samples.
trapz         -- Use trapezoidal rule to compute integral from samples.   
cumtrapz      -- Use trapezoidal rule to cumulatively compute integral.   
simps         -- Use Simpson's rule to compute integral from samples.   
romb          -- Use Romberg Integration to compute integral from  (2**k + 1) evenly-spaced samples.

#Example - scipy.integrate.trapz(y, x=None, dx=1.0, axis=-1)
>>> np.trapz([1,2,3])
4.0
>>> np.trapz([1,2,3], x=[4,6,8])
8.0
>>> np.trapz([1,2,3], dx=2)
8.0
>>> a = np.arange(6).reshape(2, 3)
>>> a
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.trapz(a, axis=0)
array([ 1.5,  2.5,  3.5])
>>> np.trapz(a, axis=1)
array([ 2.,  8.])



##General integration (quad)
#scipy.integrate.quad(func, a, b, args=(), full_output=0, epsabs=1.49e-08, epsrel=1.49e-08, limit=50, points=None, weight=None, wvar=None, wopts=None, maxp1=50, limlst=50)
#to integrate a function of one variable between two point s.
#The points can be +/-inf to indicate infinite limits.
a : float
    Lower limit of integration (use -numpy.inf for -infinity).
b : float
    Upper limit of integration (use numpy.inf for +infinity).
Returns:
    y : float The integral of func from a to b.
    abserr : float ,An estimate of the absolute error in the result.
 
#Example 
>>> from scipy import integrate
>>> x2 = lambda x: x**2
>>> integrate.quad(x2, 0, 4)
(21.333333333333332, 2.3684757858670003e-13)
>>> print(4**3 / 3.)  # analytical result
21.3333333333


#Calculate ∫ ∞ 0 e −x dx 
>>> invexp = lambda x: np.exp(-x)
>>> integrate.quad(invexp, 0, np.inf)
(1.0, 5.842605999138044e-11)


#with addl args 
>>> f = lambda x,a : a*x
>>> y, err = integrate.quad(f, 0, 1, args=(1,))
>>> y
0.5
>>> y, err = integrate.quad(f, 0, 1, args=(3,))
>>> y
1.5




##General multiple integration (dblquad, tplquad, nquad)
scipy.integrate.dblquad(func, a, b, gfun, hfun, args=(), epsabs=1.49e-08, epsrel=1.49e-08)
    Return the double (definite) integral of func(y, x) from x = a..b 
    and y = gfun(x)..hfun(x).
scipy.integrate.tplquad(func, a, b, gfun, hfun, qfun, rfun, args=(), epsabs=1.49e-08, epsrel=1.49e-08)[source]
    Return the triple integral of func(z, y, x) from x = a..b, 
    y = gfun(x)..hfun(x), and z = qfun(x,y)..rfun(x,y).
scipy.integrate.nquad(func, ranges, args=None, opts=None, full_output=False)[source]
    Integration over multiple variables.
    func : The function to be integrated,func(x0, x1, ..., xn, t0, t1, ..., tm)
        where integration is carried out over x0, ... xn, which must be floats. 
        That is, integration over x0 is the innermost integral, and xn is the outermost.
    ranges : iterable object
        Each element of ranges may be either a sequence of 2 numbers, 
        or else a callable that returns such a sequence. 
        ranges[0] corresponds to integration over x0, and so on. 
        If an element of ranges is a callable, 
        then it will be called with all of the integration arguments available, 
        as well as any parametric arguments. 
        e.g. if func = f(x0, x1, x2, t0, t1), 
        then ranges[0] may be defined as either (a, b) 
        or else as (a, b) = range0(x1, x2, t0, t1).
    args : iterable object, optional
        Additional arguments t0, ..., tn, required by func, ranges, and opts.
Returns:
    result/y : float,The resultant integral.
    abserr : float,An estimate of the error.
 

>>> from scipy.integrate import quad, dblquad
>>> def I(n):        
        return dblquad(lambda t, x: np.exp(-x*t)/t**n, 0, np.inf, lambda x: 1, lambda x: np.inf)

>>> print(I(4))
(0.25000000000435768, 1.0518245707751597e-09)
>>> print(I(3))
(0.33333333325010883, 2.8604069919261191e-09)
>>> print(I(2))
(0.49999999999857514, 1.8855523253868967e-09)

#Example 

from scipy import integrate
func = lambda x0,x1,x2,x3 : x0**2 + x1*x2 - x3**3 + np.sin(x0) + (
                                1 if (x0-.2*x3-.5-.25*x1>0) else 0)
                                
points = [[lambda x1,x2,x3 : 0.2*x3 + 0.5 + 0.25*x1], [], [], []]
def opts0(*args, **kwargs):
     return {'points':[0.2*args[2] + 0.5 + 0.25*args[0]]}
     
>>> integrate.nquad(func, [[0,1], [-1,1], [.13,.8], [-.15,1]],
                    opts=[opts0,{},{},{}], full_output=True)
(1.5267454070738633, 2.9437360001402324e-14, {'neval': 388962})


scale = .1
def func2(x0, x1, x2, x3, t0, t1):
    return x0*x1*x3**2 + np.sin(x2) + 1 + (1 if x0+t1*x1-t0>0 else 0)
def lim0(x1, x2, x3, t0, t1):
    return [scale * (x1**2 + x2 + np.cos(x3)*t0*t1 + 1) - 1,
            scale * (x1**2 + x2 + np.cos(x3)*t0*t1 + 1) + 1]
def lim1(x2, x3, t0, t1):
    return [scale * (t0*x2 + t1*x3) - 1,
            scale * (t0*x2 + t1*x3) + 1]
def lim2(x3, t0, t1):
    return [scale * (x3 + t0**2*t1**3) - 1,
            scale * (x3 + t0**2*t1**3) + 1]
def lim3(t0, t1):
    return [scale * (t0+t1) - 1, scale * (t0+t1) + 1]
def opts0(x1, x2, x3, t0, t1):
    return {'points' : [t0 - t1*x1]}
def opts1(x2, x3, t0, t1):
    return {}
def opts2(x3, t0, t1):
    return {}
def opts3(t0, t1):
    return {}
>>> integrate.nquad(func2, [lim0, lim1, lim2, lim3], args=(0,0),
...                 opts=[opts0, opts1, opts2, opts3])
(25.066666666666666, 2.7829590483937256e-13)






###Scipy - interpolation or extraploation , use with scipy.interpolate.


interp1d(x, y[, kind, axis, copy, ...])     Interpolate a 1-D function. 
                                            kind : str or int, optional
                                            Specifies the kind of interpolation as a string 
                                            ('linear, 'nearest', 'zero, 'slinear', 'quadratic, 'cubic')
                                            where 'zero', 'slinear', 'quadratic and 'cubic' 
                                            refer to a spline interpolation of zeroth, first, second or third order) 
                                            or as an integer specifying the order of the spline interpolator to use. 
                                            Default is 'linear'.

interp2d(x, y, z[, kind, copy, ...])        Interpolate over a 2-D grid. 
                                            kind : {'linear, 'cubic, 'quintic'}, optional
                                            The kind of spline interpolation to use. 
                                            Default is 'linear'.


#it has __call__(x,y,..) for getting interpolated values 
#x, y : array_like,Arrays defining the data point coordinates.

#Note that calling interp1d/interp2d with NaNs present in input values results in undefined behaviour.

#example 
>>> import matplotlib.pyplot as plt
>>> from scipy import interpolate
>>> x = np.arange(0, 10)
>>> y = np.exp(-x/3.0)
>>> f = interpolate.interp1d(x, y)



>>> xnew = np.arange(0, 9, 0.1)
>>> ynew = f(xnew)   # use interpolation function returned by `interp1d`
>>> plt.plot(x, y, 'o', xnew, ynew, '-')
>>> plt.show()


#Example 

>>> from scipy import interpolate
>>> x = np.arange(-5.01, 5.01, 0.25)
>>> y = np.arange(-5.01, 5.01, 0.25)
>>> xx, yy = np.meshgrid(x, y) #xx is row stack of x, yy is column stack of y 
>>> z = np.sin(xx**2+yy**2)
>>> f = interpolate.interp2d(x, y, z, kind='cubic')

#Now use the obtained interpolation function and plot the result:
>>> import matplotlib.pyplot as plt
>>> xnew = np.arange(-5.01, 5.01, 1e-2)
>>> ynew = np.arange(-5.01, 5.01, 1e-2)
>>> znew = f(xnew, ynew)
>>> plt.plot(x, z[0, :], 'ro-', xnew, znew[0, :], 'b-')
>>> plt.show()


##Multidimensional interpolation on regular grids.
scipy.interpolate.interpn(points, values, xi, method='linear', bounds_error=True, fill_value=nan)[source]
    Parameters:
        points : tuple of ndarray of float, with shapes (m1, ), ..., (mn, )
            The points defining the regular grid in n dimensions.
        values : array_like, shape (m1, ..., mn, ...)
            The data on the regular grid in n dimensions.
        xi : ndarray of shape (..., ndim)
            The coordinates to sample the gridded data at
        method : str, optional
            The method of interpolation to perform. 
            Supported are 'linear' and 'nearest', and 'splinef2d'. 
            'splinef2d' is only supported for 2-dimensional data.
    Returns:
        values_x : ndarray, shape xi.shape[:-1] + values.shape[ndim:]
            Interpolated values at input coordinates.
 

#Example 
import numpy as np

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

# Set up grid and array of values
x1 = np.arange(10)
x2 = np.arange(10)
arr = x1 + x2[:, np.newaxis]  #x2 is converted to (10,1)

# Set up grid for plotting
X, Y = np.meshgrid(x1, x2) #xx is row stack of x, yy is column stack of y 

# Plot the values as a surface plot to depict
fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_surface(X, Y, arr, rstride=1, cstride=1, cmap=cm.jet,
                       linewidth=0, alpha=0.8)
fig.colorbar(surf, shrink=0.5, aspect=5)

# to interpolate along a line, i.e., one point along the first dimension, 
#but all points along the second dimension
from scipy.interpolate import interpn

interp_x = 3.5           # Only one value on the x1-axis
interp_y = np.arange(10) # A range of values on the x2-axis

# Note the following two lines that are used to set up the
# interpolation points as a 10x2 array!
interp_mesh = np.array(np.meshgrid(interp_x, interp_y))
>>> interp_mesh
array([[[3.5],
        [3.5],
        [3.5],
        [3.5],
        [3.5],
        [3.5],
        [3.5],
        [3.5],
        [3.5],
        [3.5]],

       [[0. ],
        [1. ],
        [2. ],
        [3. ],
        [4. ],
        [5. ],
        [6. ],
        [7. ],
        [8. ],
        [9. ]]])
>>> interp_mesh.shape
(2, 10, 1)  
>>> np.rollaxis(interp_mesh, 0, 3).shape
(10, 1, 2)
interp_points = np.rollaxis(interp_mesh, 0, 3).reshape((10, 2))
>>> interp_points
array([[3.5, 0. ],
       [3.5, 1. ],
       [3.5, 2. ],
       [3.5, 3. ],
       [3.5, 4. ],
       [3.5, 5. ],
       [3.5, 6. ],
       [3.5, 7. ],
       [3.5, 8. ],
       [3.5, 9. ]])
#or 
interp_points = np.array( [ [3.5,i] for i in range(10)  ])
# Perform the interpolation
interp_arr = interpn((x1, x2), arr, interp_points)

# Plot the result
ax.scatter(interp_x * np.ones(interp_y.shape), interp_y, interp_arr, s=20,
           c='k', depthshade=False)
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()




















###SciPy - Parse 
#There are seven available sparse matrix types: 
#each suitable for some tasks
1.csc_matrix: Compressed Sparse Column format
2.csr_matrix: Compressed Sparse Row format
3.bsr_matrix: Block Sparse Row format
4.lil_matrix: List of Lists format
5.dok_matrix: Dictionary of Keys format
6.coo_matrix: COOrdinate format (aka IJV, triplet format)
7.dia_matrix: DIAgonal format

#don't use np.methods() on sparse matrix directly 
#Use Scipy parseMatrix instance methods if available 
#or use np version after conversion to array
#with sparseInstance.toarray(), sparseInstance.todense()

#To construct a matrix efficiently, use either dok_matrix or lil_matrix. 

#The lil_matrix class supports basic slicing and fancy indexing with a similar syntax to NumPy arrays.

##Parse - Arithmatic Operation 
#To perform  arithmetic operations(elementwise)  *, -, +, - ,/, ** 
#first convert the matrix to either CSC(.tocsc()) or CSR format(.tocsr()) 
#Or convery to np.ndarray array via toarray() or todense() and then use np methods
#The lil_matrix format is row-based, so conversion to CSR is efficient


##Common instance attributes and Methods 
•mtx.A - same as mtx.toarray()
•mtx.T - transpose (same as mtx.transpose())
•mtx.H - Hermitian (conjugate) transpose
•mtx.real - real part of complex matrix
•mtx.imag - imaginary part of complex matrix
•mtx.size - the number of nonzeros (same as self.getnnz())
•mtx.shape - the number of rows and columns (tuple)

#Below instance methods for arithmatic operations 
power(n[, dtype])	This function performs element-wise power.
multiply(other)	    Point-wise multiplication by another matrix
dot(other)	        Ordinary dot product

#Nonzero elements 
count_nonzero()	    Number of non-zero entries,
nonzero()	        nonzero indices

#each element datatype conversion, instance method
asfptype()	                    Upcast matrix to a floating point format (if necessary)
astype(dtype[, casting, copy])	Cast the matrix elements to a specified type.

#Formating instance methods 
asformat(format)	Return this matrix in a given sparse format
getformat()	        Format of a matrix representation as a string.
getmaxprint()	    Maximum number of elements to display when printed.

#Getting row or column 
getcol(j)	Returns a copy of column j of the matrix, as an (m x 1) sparse matrix (column vector).
getrow(i)	Returns a copy of row i of the matrix, as a (1 x n) sparse matrix (row vector).

#shape change 
reshape(shape[, order])	    Gives a new shape to a sparse matrix without changing its data.

#Instances have following methods 
maximum(other)	            Element-wise maximum between this and another matrix.
mean([axis, dtype, out])	Compute the arithmetic mean along the specified axis.
minimum(other)	            Element-wise minimum between this and another matrix.
copy()	                    Returns a copy of this matrix.
sum([axis, dtype, out])	    Sum the matrix elements over a given axis.
transpose([axes, copy])	    Reverses the dimensions of the sparse matrix.
#only for CSR,CSC,COO, BSR not for DIA,LIL and DOK
argmax([axis, out])         Return indices of maximum elements along an axis
argmin([axis, out])         Return indices of minimum elements along an axis
max([axis, out])            Return the maximum of the matrix or maximum along an axis. 
min([axis, out])            Return the minimum of the matrix or maximum along an axis. 


#Diagonal manipulation methods 
diagonal([k])	        Returns the k-th diagonal of the matrix.
setdiag(values[, k])	Set diagonal or off-diagonal elements of the array.

#Instances have below mathematical functions 
tan()	    Element-wise tan.
tanh()	    Element-wise tanh.
sign()	    Element-wise sign.
sin()	    Element-wise sin.
sinh()	    Element-wise sinh.
sqrt()	    Element-wise sqrt.
rint()	    Element-wise rint.
arcsin()	Element-wise arcsin.
arcsinh()	Element-wise arcsinh.
arctan()	Element-wise arctan.
arctanh()	Element-wise arctanh.
ceil()	    Element-wise ceil.
conj()	    Element-wise complex conjugation.
conjugate()	Element-wise complex conjugation.
expm1()	    Element-wise expm1.
floor()	    Element-wise floor.
log1p()	    Element-wise log1p.
rad2deg()	Element-wise rad2deg.
deg2rad()	Element-wise deg2rad.
trunc()	    Element-wise trunc.

#Instances Have below methods for conversion 
toarray([order, out])	    Return a dense ndarray representation of this matrix.
tobsr([blocksize, copy])	Convert this matrix to Block Sparse Row format.
tocoo([copy])	            Convert this matrix to COOrdinate format.
tocsc([copy])	            Convert this matrix to Compressed Sparse Column format.
tocsr([copy])	            Convert this matrix to Compressed Sparse Row format.
todense([order, out])	    Return a dense matrix representation of this matrix.
todia([copy])	            Convert this matrix to sparse DIAgonal format.
todok([copy])	            Convert this matrix to Dictionary Of Keys format.
tolil([copy])	            Convert this matrix to LInked List format.

##scipy.parse  - Building sparse matrices:
eye(m[, n, k, dtype, format])   Sparse matrix with ones on diagonal 
identity(n[, dtype, format])    Identity matrix in sparse format 
hstack(blocks[, format, dtype]) Stack sparse matrices horizontally (column wise) 
vstack(blocks[, format, dtype]) Stack sparse matrices vertically (row wise) 

tril(A[, k, format])            Return the lower triangular portion of a matrix in sparse format 
triu(A[, k, format])            Return the upper triangular portion of a matrix in sparse format 

kron(A, B[, format])            kronecker product of sparse matrices A and B 
kronsum(A, B[, format])         kronecker sum of sparse matrices A and B 

diags(diagonals[, offsets, shape, format, dtype]) Construct a sparse matrix from diagonals. 

spdiags(data, diags, m, n[, format])        Return a sparse matrix from diagonals. 
block_diag(mats[, format, dtype])           Build a block diagonal sparse matrix from provided matrices. 

bmat(blocks[, format, dtype])               Build a sparse matrix from sparse sub-blocks 

rand(m, n[, density, format, dtype, ...])   Generate a sparse matrix of the given shape and density with uniformly distributed values. 
random(m, n[, density, format, dtype, ...]) Generate a sparse matrix of the given shape and density with randomly distributed values. 

#Sparse matrix tools:
find(A)        Return the indices and values of the nonzero elements of a matrix 

#Identifying sparse matrices:
issparse(x)  
isspmatrix(x)  
isspmatrix_csc(x)  
isspmatrix_csr(x)  
isspmatrix_bsr(x)  
isspmatrix_lil(x)  
isspmatrix_dok(x)  
isspmatrix_coo(x)  
isspmatrix_dia(x) 





##Sparse matrix type - DIAgonal format - scipy.sparse.dia_matrix 
#Creation 
dia_matrix(D)
    with a dense matrix
dia_matrix(S)
    with another sparse matrix S (equivalent to S.todia())
dia_matrix((M, N), [dtype])
    to construct an empty matrix with shape (M, N), dtype is optional, 
dia_matrix((data, offsets), shape=(M, N))
    where the data[k,:] stores the diagonal entries for diagonal offsets[k]
    •offset for each diagonal
        • 0 is the main diagonal, a[i,i]
        • negative offset = below eg -1 is next diagonal below main diagonal  
        • positive offset = above eg 1 is next  diagonal above main diagonal 

#Example 
>>> import numpy as np
>>> from scipy import sparse
>>> import matplotlib.pyplot as plt
>>> data = np.array([[1, 2, 3, 4]]).repeat(3, axis=0)
>>> data
array([[1, 2, 3, 4],
       [1, 2, 3, 4],
       [1, 2, 3, 4]])
>>> offsets = np.array([0, -1, 2])
>>> mtx = sparse.dia_matrix((data, offsets), shape=(4, 4))
>>> mtx.todense()
matrix([[1, 0, 3, 0],
        [1, 2, 0, 4],
        [0, 2, 3, 0],
        [0, 0, 3, 4]])
>>> data = np.arange(12).reshape((3, 4)) + 1
>>> data
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12]])
>>> mtx = sparse.dia_matrix((data, offsets), shape=(4, 4))
>>> mtx.data   
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12]]...)
>>> mtx.offsets
array([ 0, -1,  2], dtype=int32)
>>> print(mtx)   
  (0, 0)        1
  (1, 1)        2
  (2, 2)        3
  (3, 3)        4
  (1, 0)        5
  (2, 1)        6
  (3, 2)        7
  (0, 2)        11
  (1, 3)        12
>>> mtx.todense()
matrix([[ 1,  0, 11,  0],
        [ 5,  2,  0, 12],
        [ 0,  6,  3,  0],
        [ 0,  0,  7,  4]])


#Note arithmatic operations not supported 
>>> vec = np.ones((4, ))
>>> vec
array([ 1.,  1.,  1.,  1.])
>>> mtx * vec
array([ 12.,  19.,   9.,  11.])
>>> mtx.toarray() * vec
array([[  1.,   0.,  11.,   0.],
       [  5.,   2.,   0.,  12.],
       [  0.,   6.,   3.,   0.],
       [  0.,   0.,   7.,   4.]])


       
##Sparse matrix type - scipy.sparse.lil_matrix - List of Lists format
#Creation 
lil_matrix(D)
    with a dense matrix or rank-2 ndarray D
lil_matrix(S)
    with another sparse matrix S (equivalent to S.tolil())
lil_matrix((M, N), [dtype])
    to construct an empty matrix with shape (M, N) dtype is optional, defaulting to dtype='d'.
      
#Advantages of the LIL format
•supports flexible slicing
•changes to the matrix sparsity structure are efficient
#Disadvantages of the LIL format
•arithmetic operations LIL + LIL are slow (consider CSR or CSC)
•slow column slicing (consider CSC)
•slow matrix vector products (consider CSR or CSC)

#Example - create an empty LIL matrix:
>>> mtx = sparse.lil_matrix((4, 5))
#prepare random data:
>>> from numpy.random import rand
>>> data = np.round(rand(2, 3))
>>> data
array([[ 1.,  1.,  1.],
       [ 1.,  0.,  1.]])
#assign the data using fancy indexing:
>>> mtx[:2, [1, 2, 3]] = data # indices: row=[0,1], column=[1,2,3]
>>> print(mtx) 
  (0, 1)  1.0
  (0, 2)  1.0
  (0, 3)  1.0
  (1, 1)  1.0
  (1, 3)  1.0
>>> mtx.todense()
matrix([[ 0.,  1.,  1.,  1.,  0.],
        [ 0.,  1.,  0.,  1.,  0.],
        [ 0.,  0.,  0.,  0.,  0.],
        [ 0.,  0.,  0.,  0.,  0.]])
>>> mtx.toarray()
array([[ 0.,  1.,  1.,  1.,  0.],
       [ 0.,  1.,  0.,  1.,  0.],
       [ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.]])
#more slicing and indexing:
>>> mtx = sparse.lil_matrix([[0, 1, 2, 0], [3, 0, 1, 0], [1, 0, 0, 1]])
>>> mtx.todense()
matrix([[0, 1, 2, 0],
        [3, 0, 1, 0],
        [1, 0, 0, 1]]...)
>>> print(mtx) 
  (0, 1)    1
  (0, 2)    2
  (1, 0)    3
  (1, 2)    1
  (2, 0)    1
  (2, 3)    1
>>> mtx[:2, :]  
<2x4 sparse matrix of type '<... 'numpy.int64'>'
  with 4 stored elements in LInked List format>
>>> mtx[:2, :].todense()
matrix([[0, 1, 2, 0],
        [3, 0, 1, 0]]...)
>>> mtx[1:2, [0,2]].todense()
matrix([[3, 1]]...)
>>> mtx.todense()    
matrix([[0, 1, 2, 0],
        [3, 0, 1, 0],
        [1, 0, 0, 1]]...)


##Sparse matrix type - Dictionary of Keys format - scipy.sparse.dok_matrix
#Creation 
dok_matrix(D)
    with a dense matrix, D
dok_matrix(S)
    with a sparse matrix, S
dok_matrix((M,N), [dtype])
    create the matrix with initial shape (M,N) dtype is optional, defaulting to dtype='d'
#Advantage and disadvantage 
•efficient O(1) access to individual elements
•flexible slicing, changing sparsity structure is efficient
•can be efficiently converted to a coo_matrix once constructed
•slow arithmetics (for loops with dict.iteritems())


#Example 
>>> mtx = sparse.dok_matrix((5, 5), dtype=np.float64)
>>> mtx     
<5x5 sparse matrix of type '<... 'numpy.float64'>'
        with 0 stored elements in Dictionary Of Keys format>
>>> for ir in range(5):
        for ic in range(5):
            mtx[ir, ic] = 1.0 * (ir != ic)
>>> mtx     
<5x5 sparse matrix of type '<... 'numpy.float64'>'
        with 20 stored elements in Dictionary Of Keys format>
>>> mtx.todense()
matrix([[ 0.,  1.,  1.,  1.,  1.],
        [ 1.,  0.,  1.,  1.,  1.],
        [ 1.,  1.,  0.,  1.,  1.],
        [ 1.,  1.,  1.,  0.,  1.],
        [ 1.,  1.,  1.,  1.,  0.]])
#slicing and indexing:
>>>>>> mtx[1, 1]
0.0
>>> mtx[1, 1:3]     
<1x2 sparse matrix of type '<... 'numpy.float64'>'
      with 1 stored elements in Dictionary Of Keys format>
>>> mtx[1, 1:3].todense()
matrix([[ 0.,  1.]])
>>> mtx[[2,1], 1:3].todense()
matrix([[ 1.,  0.],
        [ 0.,  1.]])    
        
    
##Sparse matrix type - COOrdinate format -  scipy.sparse.coo_matrix
#Creation 
coo_matrix(D)
    with a dense matrix D
coo_matrix(S)
    with another sparse matrix S (equivalent to S.tocoo())
coo_matrix((M, N), [dtype])
    to construct an empty matrix with shape (M, N) dtype is optional, defaulting to dtype='d'.
coo_matrix((data, (i, j)), [shape=(M, N)])
    to construct from three arrays:
    1.data[:] the entries of the matrix, in any order
    2.i[:] the row indices of the matrix entries
    3.j[:] the column indices of the matrix entries
    Where A[i[k], j[k]] = data[k]. 


#Advantages of the COO format
•facilitates fast conversion among sparse formats
•permits duplicate entries (see example)
•very fast conversion to and from CSR/CSC formats
#Disadvantages of the COO format
•does not directly support:
•arithmetic operations
•slicing

#Example - create empty COO matrix:
>>> mtx = sparse.coo_matrix((3, 4), dtype=np.int8)
>>> mtx.todense()
matrix([[0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0]], dtype=int8)
#Example - create using (data, ij) tuple:
#nonzero data at index [0,0], [3,3], [1,1], [0,2]
>>> row = np.array([0, 3, 1, 0])
>>> col = np.array([0, 3, 1, 2])
>>> data = np.array([4, 5, 7, 9])
>>> mtx = sparse.coo_matrix((data, (row, col)), shape=(4, 4))
>>> mtx     
<4x4 sparse matrix of type '<... 'numpy.int64'>'
        with 4 stored elements in COOrdinate format>
>>> mtx.todense()
matrix([[4, 0, 9, 0],
        [0, 7, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 5]])
#duplicates entries are summed together:
#note three data elements are at [0,0]
>>>>>> row = np.array([0, 0, 1, 3, 1, 0, 0])
>>> col = np.array([0, 2, 1, 3, 1, 0, 0])
>>> data = np.array([1, 1, 1, 1, 1, 1, 1])
>>> mtx = sparse.coo_matrix((data, (row, col)), shape=(4, 4))
>>> mtx.todense()
matrix([[3, 0, 1, 0],
        [0, 2, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 1]])
#no slicing…:
>>> mtx[2, 3]   
Traceback (most recent call last):
...
TypeError: 'coo_matrix' object ...     


##Sparse matrix type - Compressed Sparse Row format- csr_matrix
#creation 
csr_matrix(D)
    with a dense matrix or rank-2 ndarray D
csr_matrix(S)
    with another sparse matrix S (equivalent to S.tocsr())
csr_matrix((M, N), [dtype])
    to construct an empty matrix with shape (M, N) dtype is optional, 
    defaulting to dtype=d.
csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)])
    where data, row_ind and col_ind (all arrays) satisfy the relationship 
    a[row_ind[k], col_ind[k]] = data[k].
csr_matrix((data, indices, indptr), [shape=(M, N)])
    is the standard CSR representation 
    where the column indices for row i are stored in 
    indices[indptr[i]:indptr[i+1]] 
    and their corresponding values are stored in 
    data[indptr[i]:indptr[i+1]]. 
        •indices is array of column indices
        •data is array of corresponding nonzero values
        •indptr points to row starts in indices and data
        •length is n_row + 1, last item = number of values = length of both indices and data
        •item (i, j) can be accessed as data[indptr[i]+k], where k is position of j in indices[indptr[i]:indptr[i+1]]

#Advantages of the CSR format
    •efficient arithmetic operations CSR + CSR, CSR * CSR, etc.
    •efficient row slicing
    •fast matrix vector product
#Disadvantages of the CSR format
    •slow column slicing operations (consider CSC)
    •changes to the sparsity structure are expensive (consider LIL or DOK)

    
#Example - create using (data, ij) tuple:
#non-zero value at index [0,0], [0,2], [1,2], [2,0],[2,1],[2,2]
>>> row = np.array([0, 0, 1, 2, 2, 2])
>>> col = np.array([0, 2, 2, 0, 1, 2])  #indices
#index              0  1  2  3  4  5
#indptr:            0     2  3       6    #from row 
#indptr: points to row starts in indices and data
#any item in indptr = index of row where row's value change (Note atleast one item in a row is required evenif =0)
#last item in indptr = len(row)
>>> data = np.array([1, 2, 3, 4, 5, 6])
>>> mtx = sparse.csr_matrix((data, (row, col)), shape=(3, 3))
>>> mtx.todense()   
matrix([[1, 0, 2],
        [0, 0, 3],
        [4, 5, 6]]...)
>>> mtx.data
array([1, 2, 3, 4, 5, 6]...)
>>> mtx.indices  #array of column indices, same as above col 
array([0, 2, 2, 0, 1, 2], dtype=int32)
>>> mtx.indptr #points to row starts in indices and data
array([0, 2, 3, 6], dtype=int32)
#create using (data, indices, indptr) tuple:
>>> data = np.array([1, 2, 3, 4, 5, 6])
>>> indices = np.array([0, 2, 2, 0, 1, 2])
>>> indptr = np.array([0, 2, 3, 6])
>>> mtx = sparse.csr_matrix((data, indices, indptr), shape=(3, 3))
>>> mtx.todense()
matrix([[1, 0, 2],
        [0, 0, 3],
        [4, 5, 6]])
>>> (mtx + mtx).toarray()
array([[ 2,  0,  4],
       [ 0,  0,  6],
       [ 8, 10, 12]], dtype=int32)
>>> mtx[1,1]
0
>>> mtx[0:2,0:2]
<2x2 sparse matrix of type '<class 'numpy.int32'>'
        with 1 stored elements in Compressed Sparse Row format>
>>> mtx[0:2,0:2].todense()
matrix([[1, 0],
        [0, 0]], dtype=int32)
>> mtx.sqrt().todense()
matrix([[1.        , 0.        , 1.41421356],
       [0.        , 0.        , 1.73205081],
       [2.        , 2.23606798, 2.44948974]])
       
>>> mtx.power(3).todense()
matrix([[  1,   0,   8],
        [  0,   0,  27],
        [ 64, 125, 216]], dtype=int32)      
       
##Sparse matrix type - Compressed Sparse Column format - scipy.sparse.csc_matrix
#Creation 
csc_matrix(D)
    with a dense matrix or rank-2 ndarray D
csc_matrix(S)
    with another sparse matrix S (equivalent to S.tocsc())
csc_matrix((M, N), [dtype])
    to construct an empty matrix with shape (M, N) dtype is optional, defaulting to dtype='d'.
csc_matrix((data, (row_ind, col_ind)), [shape=(M, N)])
    where data, row_ind and col_ind satisfy the relationship 
    a[row_ind[k], col_ind[k]] = data[k].
csc_matrix((data, indices, indptr), [shape=(M, N)])
    is the standard CSC representation 
    where the row indices for column i are stored in 
    indices[indptr[i]:indptr[i+1]] 
    and their corresponding values are stored in 
    data[indptr[i]:indptr[i+1]]. 
    This is similar to CSR but indptr is for column 
        •indices is array of row indices
        •data is array of corresponding nonzero values
        •indptr points to column starts in indices and data
        •length is n_col + 1, last item = number of values = length of both indices and data
         •item (i, j) can be accessed as data[indptr[j]+k], where k is position of i in indices[indptr[j]:indptr[j+1]]


#Example - create using (data, ij) tuple:
>>> row = np.array([0, 2, 2, 0, 1, 2]) #indices 
>>> col = np.array([0, 0, 1, 2, 2, 2])
#index              0  1  2  3  4  5
#indptr:            0  2  3  6
#indptr: points to col starts in indices and data
#any item in indptr= index of col where col's value change (Note atleast one item in a col is required evenif =0)
#last item in indptr = len(col)

>>> data = np.array([1, 2, 3, 4, 5, 6])
>>> mtx = sparse.csc_matrix((data, (row, col)), shape=(3, 3))
>>> mtx         
<3x3 sparse matrix of type '<... 'numpy.int64'>'
        with 6 stored elements in Compressed Sparse Column format>
>>> mtx.todense()   
array([[1, 0, 4],
       [0, 0, 5],
       [2, 3, 6]])
>>> mtx.data   
array([1, 4, 5, 2, 3, 6]...)
>>> mtx.indices     #array of row indices, same as above row 
array([0, 2, 2, 0, 1, 2], dtype=int32)
>>> mtx.indptr  #points to column starts in indices and data
array([0, 2, 3, 6], dtype=int32)
#create using (data, indices, indptr) tuple:
>>> indptr = np.array([0, 2, 3, 6])
>>> indices = np.array([0, 2, 2, 0, 1, 2])
>>> data = np.array([1, 2, 3, 4, 5, 6])
>>> csc_matrix((data, indices, indptr), shape=(3, 3)).toarray()
array([[1, 0, 4],
       [0, 0, 5],
       [2, 3, 6]])
    
    
    
##Sparse matrix type - Block Sparse Row format - scipy.sparse.bsr_matrix
#basically a CSR with dense sub-matrices of fixed shape instead of scalar items
#block size (R, C) must evenly divide the shape of the matrix (M, N)
#Creation 
bsr_matrix(D, [blocksize=(R,C)])
    where D is a dense matrix or 2-D ndarray.
bsr_matrix(S, [blocksize=(R,C)])
    with another sparse matrix S (equivalent to S.tobsr())
bsr_matrix((M, N), [blocksize=(R,C), dtype])
    to construct an empty matrix with shape (M, N) dtype is optional, defaulting to dtype='d'.
bsr_matrix((data, ij), [blocksize=(R,C), shape=(M, N)])
    where data and ij satisfy a[ij[0, k], ij[1, k]] = data[k]
bsr_matrix((data, indices, indptr), [shape=(M, N)])
    is the standard BSR representation 
    where the block column indices for row i are stored in 
    indices[indptr[i]:indptr[i+1]] 
    and their corresponding block values are stored in 
    data[ indptr[i]: indptr[i+1] ]. 
        •indices is array of column indices for each block
        •data is array of corresponding nonzero values of shape (nnz, R, C)
        •indptr points to row starts in indices and data

#Example - create empty BSR matrix with (1, 1) block size (like CSR…):
>>> mtx = sparse.bsr_matrix((3, 4), dtype=np.int8)
>>> mtx  
<3x4 sparse matrix of type '<... 'numpy.int8'>'
        with 0 stored elements (blocksize = 1x1) in Block Sparse Row format>
>>> mtx.todense()
matrix([[0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0]], dtype=int8)
#Example - create empty BSR matrix with (3, 2) block size:
>>>>>> mtx = sparse.bsr_matrix((3, 4), blocksize=(3, 2), dtype=np.int8)
>>> mtx  
<3x4 sparse matrix of type '<... 'numpy.int8'>'
        with 0 stored elements (blocksize = 3x2) in Block Sparse Row format>
>>> mtx.todense()
matrix([[0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0]], dtype=int8)
#Example - create using (data, ij) tuple with (1, 1) block size (like CSR…):
>>> row = np.array([0, 0, 1, 2, 2, 2])
>>> col = np.array([0, 2, 2, 0, 1, 2])
>>> data = np.array([1, 2, 3, 4, 5, 6])
>>> mtx = sparse.bsr_matrix((data, (row, col)), shape=(3, 3))
>>> mtx  
<3x3 sparse matrix of type '<... 'numpy.int64'>'
        with 6 stored elements (blocksize = 1x1) in Block Sparse Row format>
>>> mtx.todense()   
matrix([[1, 0, 2],
        [0, 0, 3],
        [4, 5, 6]]...)
>>> mtx.data    
array([[[1]],
       [[2]],
       [[3]],
       [[4]],
       [[5]],
       [[6]]]...)
>>> mtx.indices
array([0, 2, 2, 0, 1, 2], dtype=int32)
>>> mtx.indptr
array([0, 2, 3, 6], dtype=int32)
#Example - create using (data, indices, indptr) tuple with (2, 2) block size:
>>> indptr = np.array([0, 2, 3, 6])
>>> indices = np.array([0, 2, 2, 0, 1, 2])
>>> data = np.array([1, 2, 3, 4, 5, 6]).repeat(4).reshape(6, 2, 2)
>>> mtx = sparse.bsr_matrix((data, indices, indptr), shape=(6, 6))
>>> mtx.todense()
matrix([[1, 1, 0, 0, 2, 2],
        [1, 1, 0, 0, 2, 2],
        [0, 0, 0, 0, 3, 3],
        [0, 0, 0, 0, 3, 3],
        [4, 4, 5, 5, 6, 6],
        [4, 4, 5, 5, 6, 6]])
>>> data
array([[[1, 1],
        [1, 1]],
       [[2, 2],
        [2, 2]],
       [[3, 3],
        [3, 3]],
       [[4, 4],
        [4, 4]],
       [[5, 5],
        [5, 5]],
       [[6, 6],
        [6, 6]]])
        
        
        



###SciPy - Parse - Sparse linear algebra (scipy.sparse.linalg)
#Matrix Operations
inv(A)              Compute the inverse of a sparse matrix 
expm(A)             Compute the matrix exponential using Pade approximation. 
expm_multiply(A, B[, start, stop, num, endpoint])   Compute the action of the matrix exponential of A on B. 

#Matrix norms
norm(x[, ord, axis])    Norm of a sparse matrix 
onenormest(A[, t, itmax, compute_v, compute_w])     Compute a lower bound of the 1-norm of a sparse matrix. 

#Direct methods for solving linear equation systems:
spsolve(A, b[, permc_spec, use_umfpack])    Solve the sparse linear system Ax=b, where b may be a vector or a matrix. 
spsolve_triangular(A, b[, lower, ...])      Solve the equation A x = b for x, assuming A is a triangular matrix. 

#Iterative methods for solving linear equation systems:
bicg(A, b[, x0, tol, maxiter, M, callback])     Use BIConjugate Gradient iteration to solve Ax = b. 
bicgstab(A, b[, x0, tol, maxiter, M, callback]) Use BIConjugate Gradient STABilized iteration to solve Ax = b. 
cg(A, b[, x0, tol, maxiter, M, callback])       Use Conjugate Gradient iteration to solve Ax = b. 
cgs(A, b[, x0, tol, maxiter, M, callback])      Use Conjugate Gradient Squared iteration to solve Ax = b. 
gmres(A, b[, x0, tol, restart, maxiter, M, ...]) Use Generalized Minimal RESidual iteration to solve Ax = b. 
lgmres(A, b[, x0, tol, maxiter, M, ...])        Solve a matrix equation using the LGMRES algorithm. 
minres(A, b[, x0, shift, tol, maxiter, M, ...]) Use MINimum RESidual iteration to solve Ax=b 
qmr(A, b[, x0, tol, maxiter, M1, M2, callback]) Use Quasi-Minimal Residual iteration to solve Ax = b. 
gcrotmk(A, b[, x0, tol, maxiter, M, ...])       Solve a matrix equation using flexible GCROT(m,k) algorithm. 

#Iterative methods for least-squares problems:
lsqr(A, b[, damp, atol, btol, conlim, ...])     Find the least-squares solution to a large, sparse, linear system of equations. 
lsmr(A, b[, damp, atol, btol, conlim, ...])     Iterative solver for least-squares problems. 

#Eigenvalue problems:
eigs(A[, k, M, sigma, which, v0, ncv, ...])     Find k eigenvalues and eigenvectors of the square matrix A. 
eigsh(A[, k, M, sigma, which, v0, ncv, ...])    Find k eigenvalues and eigenvectors of the real symmetric square matrix or complex hermitian matrix A. 
lobpcg(A, X[, B, M, Y, tol, maxiter, ...])      Locally Optimal Block Preconditioned Conjugate Gradient Method (LOBPCG) 

#Singular values problems:
svds(A[, k, ncv, tol, which, v0, maxiter, ...]) Compute the largest k singular values/vectors for a sparse matrix. 

#Complete or incomplete LU factorizations
splu(A[, permc_spec, diag_pivot_thresh, ...])       Compute the LU decomposition of a sparse, square matrix. 
spilu(A[, drop_tol, fill_factor, drop_rule, ...])   Compute an incomplete LU decomposition for a sparse, square matrix. 



##Example -  Matrix vector product
import numpy as np
from scipy.sparse import csr_matrix
A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])
v = np.array([1, 0, -1])
>>> A.dot(v)   #or np.dot(A.toarray(), v)
array([ 1, -3, -1], dtype=int64)

##Example - Construct a 1000x1000 lil_matrix and add some values to it:
from scipy.sparse import lil_matrix
from scipy.sparse.linalg import spsolve
from numpy.linalg import solve, norm
from numpy.random import rand

A = lil_matrix((1000, 1000))
A[0, :100] = rand(100)   #first row, column till 100
A[1, 100:200] = A[0, :100]  #2nd row, column 10 to 200 set from above 
A.setdiag(rand(1000))  #set diag elements 

#convert it to CSR format and solve A x = b for x:
A = A.tocsr()
b = rand(1000)
x = spsolve(A, b)

#Convert it to a dense matrix and solve, and check that the result is the same:
x_ = solve(A.toarray(), b)

#compute norm of the error with:
err = norm(x-x_)
err < 1e-10 #True


##Example - inv 
>>> from scipy.sparse import csc_matrix
>>> from scipy.sparse.linalg import inv
>>> A = csc_matrix([[1., 0.], [1., 2.]])
>>> Ainv = inv(A)
>>> Ainv
<2x2 sparse matrix of type '<class 'numpy.float64'>'
    with 3 stored elements in Compressed Sparse Column format>
>>> A.dot(Ainv)
<2x2 sparse matrix of type '<class 'numpy.float64'>'
    with 2 stored elements in Compressed Sparse Column format>
>>> A.dot(Ainv).todense()
matrix([[ 1.,  0.],
        [ 0.,  1.]])


##Example - linear solver 
>>> from scipy.sparse import csc_matrix
>>> from scipy.sparse.linalg import gmres
>>> A = csc_matrix([[3, 2, 0], [1, -1, 0], [0, 5, 1]], dtype=float)
>>> b = np.array([2, 4, -1], dtype=float)
>>> x, exitCode = gmres(A, b)
>>> print(exitCode)            # 0 indicates successful convergence
0
>>> np.allclose(A.dot(x), b)
True

##Example - least square , lsqr and lsmr
Returns:
    x : ndarray of float
        The final solution.
    istop : int
        Gives the reason for termination. 1 means x is an approximate solution to Ax = b. 2 means x approximately solves the least-squares problem.
    itn : int
        Iteration number upon termination.
    r1norm : float
        norm(r), where r = b - Ax.
    r2norm : float
        sqrt( norm(r)^2  +  damp^2 * norm(x)^2 ). Equal to r1norm if damp == 0.
    anorm : float
        Estimate of Frobenius norm of Abar = [[A]; [damp*I]].
    acond : float
        Estimate of cond(Abar).
    arnorm : float
        Estimate of norm(A'*r - damp^2*x).
    xnorm : float
        norm(x)
    var : ndarray of float
        If calc_var is True, estimates all diagonals of (A'A)^{-1} (if damp == 0) or more generally (A'A + damp^2*I)^{-1}. This is well defined if A has full column rank or damp > 0. (Not sure what var means if rank(A) < n and damp = 0.)
     
from scipy.sparse import csc_matrix
from scipy.sparse.linalg import lsqr
A = csc_matrix([[1., 0.], [1., 1.], [0., 1.]], dtype=float)

#The first example has the trivial solution [0, 0]
>>> b = np.array([0., 0., 0.], dtype=float)
>>> x, istop, itn, normr = lsqr(A, b)[:4]
The exact solution is  x = 0
>>> istop
0
>>> x
array([ 0.,  0.])

#The next example has a non-trivial solution:
>>> b = np.array([1., 0., -1.], dtype=float)
>>> x, istop, itn, r1norm = lsqr(A, b)[:4]
>>> istop  #As indicated by istop=1, lsqr found a solution obeying the tolerance limits. 
1
>>> x
array([ 1., -1.])
>>> itn
1
>>> r1norm
4.440892098500627e-16


#in the case where there is no solution for the equation:
>>> b = np.array([1., 0.01, -1.], dtype=float)
>>> x, istop, itn, r1norm = lsqr(A, b)[:4]
>>> istop  #istop indicates that the system is inconsistent and thus x is rather an approximate solution
2
>>> x
array([ 1.00333333, -0.99666667])
>>> A.dot(x)-b
array([ 0.00333333, -0.00333333,  0.00333333])
>>> r1norm
0.005773502691896255


##Find 6 eigenvectors of the identity matrix:
>>> import scipy.sparse as sparse
>>> id = np.eye(13)
>>> vals, vecs = sparse.linalg.eigs(id, k=6)
>>> vals
array([ 1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j])
>>> vecs.shape
(13, 6)

##Compute the largest k singular values/vectors for a sparse matrix.
>>> from scipy.sparse import csc_matrix
>>> from scipy.sparse.linalg import svds, eigs
>>> A = csc_matrix([[1, 0, 0], [5, 0, 2], [0, -1, 0], [0, 0, 3]], dtype=float)
>>> u, s, vt = svds(A, k=2)
>>> s
array([ 2.75193379,  5.6059665 ])
>>> np.sqrt(eigs(A.dot(A.T), k=2)[0]).real
array([ 5.6059665 ,  2.75193379])

#Compute the LU decomposition of a sparse, square matrix- splu and spilu
Returns:
    invA : scipy.sparse.linalg.SuperLU
    Object, which has a solve method.
 
>>> from scipy.sparse import csc_matrix
>>> from scipy.sparse.linalg import splu
>>> A = csc_matrix([[1., 0., 0.], [5., 0., 2.], [0., -1., 0.]], dtype=float)
>>> B = splu(A)
>>> x = np.array([1., 2., 3.], dtype=float)
>>> B.solve(x)
array([ 1. , -3. , -1.5])
>>> A.dot(B.solve(x))
array([ 1.,  2.,  3.])
>>> B.solve(A.dot(x))
array([ 1.,  2.,  3.])
#Compute an incomplete LU decomposition for a sparse, square matrix
#The resulting object is an approximation to the inverse of A.
>>> from scipy.sparse import csc_matrix
>>> from scipy.sparse.linalg import spilu
>>> A = csc_matrix([[1., 0., 0.], [5., 0., 2.], [0., -1., 0.]], dtype=float)
>>> B = spilu(A)
>>> x = np.array([1., 2., 3.], dtype=float)
>>> B.solve(x)
array([ 1. , -3. , -1.5])
>>> A.dot(B.solve(x))
array([ 1.,  2.,  3.])
>>> B.solve(A.dot(x))
array([ 1.,  2.,  3.])



###SciPy - Parse - Compressed Sparse Graph Routines (scipy.sparse.csgraph)

connected_components(csgraph[, directed, ...])  Analyze the connected components of a sparse graph 
laplacian(csgraph[, normed, return_diag, ...])  Return the Laplacian matrix of a directed graph. 
shortest_path(csgraph[, method, directed, ...]) Perform a shortest-path graph search on a positive directed or undirected graph. 
dijkstra(csgraph[, directed, indices, ...])     Dijkstra algorithm using Fibonacci Heaps 
floyd_warshall(csgraph[, directed, ...])        Compute the shortest path lengths using the Floyd-Warshall algorithm 
bellman_ford(csgraph[, directed, indices, ...]) Compute the shortest path lengths using the Bellman-Ford algorithm. 
johnson(csgraph[, directed, indices, ...])      Compute the shortest path lengths using Johnson's algorithm. 

breadth_first_order(csgraph, i_start[, ...])    Return a breadth-first ordering starting with specified node. 
depth_first_order(csgraph, i_start[, ...])      Return a depth-first ordering starting with specified node. 
breadth_first_tree(csgraph, i_start[, directed]) Return the tree generated by a breadth-first search 
depth_first_tree(csgraph, i_start[, directed])  Return a tree generated by a depth-first search. 
minimum_spanning_tree(csgraph[, overwrite])     Return a minimum spanning tree of an undirected graph 
reverse_cuthill_mckee(graph[, symmetric_mode])  Returns the permutation array that orders a sparse CSR or CSC matrix in Reverse-Cuthill McKee ordering. 
maximum_bipartite_matching(graph[, perm_type])  Returns an array of row or column permutations that makes the diagonal of a nonsingular square CSC sparse matrix zero free. 
structural_rank(graph)                          Compute the structural rank of a graph (matrix) with a given sparsity pattern. 

construct_dist_matrix(graph, predecessors[, ...])   Construct distance matrix from a predecessor matrix 
csgraph_from_dense(graph[, null_value, ...])        Construct a CSR-format sparse graph from a dense matrix. 
csgraph_from_masked(graph)                          Construct a CSR-format graph from a masked array. 
csgraph_masked_from_dense(graph[, ...])             Construct a masked array graph representation from a dense matrix. 
csgraph_to_dense(csgraph[, null_value])             Convert a sparse graph representation to a dense representation 
csgraph_to_masked(csgraph)                          Convert a sparse graph representation to a masked array representation 
reconstruct_path(csgraph, predecessors[, ...])      Construct a tree from a graph and a predecessor list. 



##Graph Representations

#A graph with N nodes can be represented by an (N x N) adjacency matrix G. 
#If there is a connection from node i to node j, then G[i, j] = w, 
#where w is the weight of the connection. 

#For nodes i and j which are not connected, 
#the value depends on the representation:
•for dense array representations, non-edges are represented by G[i, j] = 0, infinity, or NaN.
•for dense masked representations (of type np.ma.MaskedArray), non-edges are represented by masked values. 
 This can be useful when graphs with zero-weight edges are desired.
•for sparse array representations, non-edges are represented 
 by non-entries in the matrix. 
 This sort of sparse representation also allows for edges with zero weights.

#Example graph 
      G
     (0)
    /   \
   1     2
  /       \
(2)       (1)


#This graph has three nodes, 
#where node 0 and 1 are connected by an edge of weight 2, 
#and nodes 0 and 2 are connected by an edge of weight 1. 


>>> G_dense = np.array([[0, 2, 1],
                        [2, 0, 0],
                        [1, 0, 0]])
>>> G_masked = np.ma.masked_values(G_dense, 0)
>>> from scipy.sparse import csr_matrix
>>> G_sparse = csr_matrix(G_dense)


#This becomes more difficult when zero edges are significant. 
     G2
     (0)
    /   \
   0     2
  /       \
(2)       (1)


#nodes 0 and 2 are connected by an edge of zero weight. 
#use either a masked or sparse representation 

>>> G2_data = np.array([[np.inf, 2,      0     ],
                        [2,      np.inf, np.inf],
                        [0,      np.inf, np.inf]])
>>> G2_masked = np.ma.masked_invalid(G2_data)
>>> from scipy.sparse.csgraph import csgraph_from_dense
>>> # G2_sparse = csr_matrix(G2_data) would give the wrong result
>>> G2_sparse = csgraph_from_dense(G2_data, null_value=np.inf)
>>> G2_sparse.data
array([ 2.,  0.,  2.,  0.])


##Directed vs. Undirected
#This is specified throughout the csgraph module by a boolean keyword. 
#Graphs are assumed to be directed by default. 

#In a directed graph, traversal from node i to node j can be accomplished 
#over the edge G[i, j], but not the edge G[j, i]. 

#In a non-directed graph, traversal from node i to node j 
#can be accomplished over either G[i, j] or G[j, i]. 

#If both edges are not null, and the two have unequal weights, 
#then the smaller of the two is used



##Examples
#computation of a minimum spanning tree over a simple four-component graph:
# input graph             minimum spanning tree

     (0)                         (0)
    /   \                       /
   3     8                     3
  /       \                   /
(3)---5---(1)               (3)---5---(1)
  \       /                           /
   6     2                           2
    \   /                           /
     (2)                         (2)

>>> from scipy.sparse import csr_matrix
>>> from scipy.sparse.csgraph import minimum_spanning_tree
>>> X = csr_matrix([[0, 8, 0, 3],
                    [0, 0, 2, 5],
                    [0, 0, 0, 6],
                    [0, 0, 0, 0]])
>>> Tcsr = minimum_spanning_tree(X)
>>> Tcsr.toarray().astype(int)
array([[0, 0, 0, 3],
       [0, 0, 2, 5],
       [0, 0, 0, 0],
       [0, 0, 0, 0]])

##Examples
#computation of a depth-first tree over a simple four-component graph, 
#starting at node 0:
# input graph          breadth first tree from (0)

     (0)                         (0)
    /   \                       /   \
   3     8                     3     8
  /       \                   /       \
(3)---5---(1)               (3)       (1)
  \       /                           /
   6     2                           2
    \   /                           /
     (2)                         (2)


>>> from scipy.sparse import csr_matrix
>>> from scipy.sparse.csgraph import breadth_first_tree
>>> X = csr_matrix([[0, 8, 0, 3],
                    [0, 0, 2, 5],
                    [0, 0, 0, 6],
                    [0, 0, 0, 0]])
>>> Tcsr = breadth_first_tree(X, 0, directed=False)
>>> Tcsr.toarray().astype(int)
array([[0, 8, 0, 3],
       [0, 0, 2, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0]])




##scipy.sparse.csgraph.shortest_path
scipy.sparse.csgraph.shortest_path(csgraph, method='auto', directed=True, return_predecessors=False, unweighted=False, overwrite=False, indices=None)
Parameters:
    csgraph : array, matrix, or sparse matrix, 2 dimensions
        The N x N array of distances representing the input graph.
    method : string ['auto'|'FW'|'D'], optional
        Algorithm to use for shortest paths. Options are:
        'auto' – (default) select the best among 'FW', 'D', 'BF', or 'J' based on the input data.
        'FW' – Floyd-Warshall algorithm. 
               Computational cost is approximately O[N^3]. 
               The input csgraph will be converted to a dense representation.
        'D' – Dijkstra's algorithm with Fibonacci heaps. 
             Computational cost is approximately O[N(N*k + N*log(N))], 
             where k is the average number of connected edges per node. 
             The input csgraph will be converted to a csr representation.
        'BF' – Bellman-Ford algorithm. 
            This algorithm can be used when weights are negative. 
            If a negative cycle is encountered, an error will be raised. 
            Computational cost is approximately O[N(N^2 k)], where k is the average number of connected edges per node. 
            The input csgraph will be converted to a csr representation.
        'J' – Johnson's algorithm. Like the Bellman-Ford algorithm,
            Johnson's algorithm is designed for use when the weights are negative. 
            It combines the Bellman-Ford algorithm with Dijkstra's algorithm for faster computation.
    directed : bool, optional
        If True (default), then find the shortest path on a directed graph: 
        only move from point i to point j along paths csgraph[i, j]. 
        If False, then find the shortest path on an undirected graph: 
        the algorithm can progress from point i to j along csgraph[i, j] or csgraph[j, i]
    return_predecessors : bool, optional
        If True, return the size (N, N) predecesor matrix
    unweighted : bool, optional
        If True, then find unweighted distances. 
        That is, rather than finding the path between each point 
        such that the sum of weights is minimized, 
        find the path such that the number of edges is minimized.
    overwrite : bool, optional
        If True, overwrite csgraph with the result. 
        This applies only if method == 'FW' and csgraph is a dense, c-ordered array with dtype=float64.
    indices : array_like or int, optional
        If specified, only compute the paths for the points at the given indices. 
        Incompatible with method == 'FW'.
Returns:
    dist_matrix : ndarray
        The N x N matrix of distances between graph nodes. 
        dist_matrix[i,j] gives the shortest distance from point i to point j 
        along the graph.
    predecessors : ndarray
        Returned only if return_predecessors == True. 
        The N x N matrix of predecessors, which can be used to reconstruct the shortest paths. 
        Row i of the predecessor matrix contains information on the shortest paths 
        from point i: each entry predecessors[i, j] 
        gives the index of the previous node in the path from point i to point j. 
        If no path exists between point i and j, then predecessors[i, j] = -9999
 
#Example 
from __future__ import division, print_function, absolute_import

import numpy as np
from numpy.testing import assert_array_almost_equal, assert_array_equal
from pytest import raises as assert_raises
from scipy.sparse.csgraph import (shortest_path, dijkstra, johnson,
    bellman_ford, construct_dist_matrix, NegativeCycleError)


directed_G = np.array([[0, 3, 3, 0, 0],
                       [0, 0, 0, 2, 4],
                       [0, 0, 0, 0, 0],
                       [1, 0, 0, 0, 0],
                       [2, 0, 0, 2, 0]], dtype=float)

undirected_G = np.array([[0, 3, 3, 1, 2],
                         [3, 0, 0, 2, 4],
                         [3, 0, 0, 0, 0],
                         [1, 2, 0, 0, 2],
                         [2, 4, 0, 2, 0]], dtype=float)

unweighted_G = (directed_G > 0).astype(float)

directed_SP = [[0, 3, 3, 5, 7],
               [3, 0, 6, 2, 4],
               [np.inf, np.inf, 0, np.inf, np.inf],
               [1, 4, 4, 0, 8],
               [2, 5, 5, 2, 0]]

directed_pred = np.array([[-9999, 0, 0, 1, 1],
                          [3, -9999, 0, 1, 1],
                          [-9999, -9999, -9999, -9999, -9999],
                          [3, 0, 0, -9999, 1],
                          [4, 0, 0, 4, -9999]], dtype=float)

undirected_SP = np.array([[0, 3, 3, 1, 2],
                          [3, 0, 6, 2, 4],
                          [3, 6, 0, 4, 5],
                          [1, 2, 4, 0, 2],
                          [2, 4, 5, 2, 0]], dtype=float)

undirected_SP_limit_2 = np.array([[0, np.inf, np.inf, 1, 2],
                                  [np.inf, 0, np.inf, 2, np.inf],
                                  [np.inf, np.inf, 0, np.inf, np.inf],
                                  [1, 2, np.inf, 0, 2],
                                  [2, np.inf, np.inf, 2, 0]], dtype=float)

undirected_SP_limit_0 = np.ones((5, 5), dtype=float) - np.eye(5)
undirected_SP_limit_0[undirected_SP_limit_0 > 0] = np.inf

undirected_pred = np.array([[-9999, 0, 0, 0, 0],
                            [1, -9999, 0, 1, 1],
                            [2, 0, -9999, 0, 0],
                            [3, 3, 0, -9999, 3],
                            [4, 4, 0, 4, -9999]], dtype=float)

methods = ['auto', 'FW', 'D', 'BF', 'J']


def test_directed():
    def check(method):
        SP = shortest_path(directed_G, method=method, directed=True,
                           overwrite=False)
        assert_array_almost_equal(SP, directed_SP)

    for method in methods:
        check(method)


def test_undirected():
    def check(method, directed_in):
        if directed_in:
            SP1 = shortest_path(directed_G, method=method, directed=False,
                                overwrite=False)
            assert_array_almost_equal(SP1, undirected_SP)
        else:
            SP2 = shortest_path(undirected_G, method=method, directed=True,
                                overwrite=False)
            assert_array_almost_equal(SP2, undirected_SP)

    for method in methods:
        for directed_in in (True, False):
            check(method, directed_in)









###SciPy stats (scyipy.stats) - Continuous dist 
#has many, https://docs.scipy.org/doc/scipy/reference/stats.html
#Note all these have addl 'params' (mentioned against each)
#along with loc(mean),scale(sd) which can be supplied in args 
#By default loc, scale are 0,1 ie standardised   
beta                A beta continuous random variable. params =  a, b
cauchy              A Cauchy continuous random variable. params = none
chi                 A chi continuous random variable. params= df
expon               An exponential continuous random variable. param=none 
f                   An F continuous random variable. params = dfn, dfd, ie df1, df2
gamma               A gamma continuous random variable, params= a
norm                A normal continuous random variable. params=none
pareto              A Pareto continuous random variable. parms = b
t                   A Students T continuous random variable , params= df
uniform             A uniform continuous random variable. params = none

#Each distribution  has below methods , **kwds depends on particular dist 
Density (.pdf)				Returns probability Pr of a random variable X, ie Pr(x), 
Distribution (.cdf)  		Returns cummulative Pr ie Pr(x <= q) or Pr(x >=q) with lower.tail = FALSE
Quantile (.ppf)				Inverse of cdf, Given Probability p, returns x, value of X ie what is the value of x given p
Random generation(.rvs)		Generates n random number based on this distribution 

#x is taken as X variable, axis x and  q, Pr is taken as y axis in PDF graph
#Common Methods of distributions 
#Note all these have addl 'params' (mentioned against distribution each above)
#along with loc(mean),scale(sd) which can be supplied in args 
#By default loc, scale are 0,1 ie standardised   
#for example for norm, params=None, hence complete signature pdf(x, loc=0, scale=1)
pdf(x, params)       Probability density function at x of the given RV. 
cdf(x, params)       Cumulative distribution function of the given RV. 
ppf(q, params)       Percent point function (inverse of cdf) at q of the given RV. 
rvs(params, size)    Generates size(could be tuple for MD) random number based on this distribution 

logpdf(x, params)    Log of the probability density function at x of the given RV. 
logcdf(x, params)    Log of the cumulative distribution function at x of the given RV. 
sf(x, params)        Survival function (1 - cdf) at x of the given RV. 
logsf(x, params)     Log of the survival function of the given RV. 
isf(q, params)       Inverse survival function (inverse of sf) at q of the given RV. 

moment(n, params)    n-th order non-central moment of distribution. 
stats(params, mask)  Some statistics of the given RV. 
                     Mask is string , Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
                     Can be combined. eg 'mv'
entropy(params)      Differential entropy of the RV. 

median(params)       Median of the distribution. 
mean(params)         Mean of the distribution. 
std(params)          Standard deviation of the distribution. 
var(params)          Variance of the distribution. 

fit(data, params)    Return MLEs for shape, location, and scale parameters from data. 
nnlf(theta, x)       Return negative loglikelihood function. 

fit_loc_scale(data, params)                     Estimate loc and scale parameters from data using 1st and 2nd moments. 
expect([func, args, loc, scale, lb, ub, ...])   Calculate expected value of a function with respect to the distribution. 
interval(alpha, params)                         Confidence interval with equal areas around the median. 



##Example of norm distribution 

from scipy.stats import norm
import matplotlib.pyplot as plt
fig, ax = plt.subplots(1, 1)

#Calculate a few first moments:
mean, var, skew, kurt = norm.stats(moments='mvsk')


#Display the probability density function (pdf):
#ppf - given cummulative p, output x , pdf= Pr(x) given x , cdf = cummulative Pr
x = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)
ax.plot(x, norm.pdf(x),'r-', lw=5, alpha=0.6, label='norm pdf')

#Check accuracy of cdf and ppf
vals = norm.ppf([0.001, 0.5, 0.999])
np.allclose([0.001, 0.5, 0.999], norm.cdf(vals)) #True

#Generate random numbers:
r = norm.rvs(size=1000)

#And compare the histogram:
ax.hist(r, normed=True, histtype='stepfilled', alpha=0.2)
ax.legend(loc='best', frameon=False)
plt.show()

#example - fit 
import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt


# Generate some data for this demonstration.
data = norm.rvs(10.0, 2.5, size=500) #loc,scale, size 

# Fit a normal distribution to the data:
>>> mu, std = norm.fit(data)  
(9.811694740931252, 2.6595778507346335)
# Plot the histogram.
plt.hist(data, bins=25, normed=True, alpha=0.6, color='g')

# Plot the PDF.
xmin, xmax = plt.xlim() #get xlimit 
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mu, std)
plt.plot(x, p, 'k', linewidth=2)
title = "Fit results: mu = %.2f,  std = %.2f" % (mu, std)
plt.title(title)

plt.show()



        
###SciPy stats (scyipy.stats) - Discrete dist 
#has many, few are 
#Note all these have addl 'params' (mentioned against each)
#along with loc which can be supplied in args 
#By default loc is  0 ie standardised   

bernoulli       A Bernoulli discrete random variable. params= p
binom           A binomial discrete random variable. , params =n, p
poisson         A Poisson discrete random variable. params= mu
randint         A uniform discrete random variable, params= low, high 



#All have below methods  (note pdf is replaced by pmf)
#k is taken as X variable, axis x, and q is taken as y axis in PMF graph
#Note all these have addl 'params' (mentioned against each given above)
#along with loc which can be supplied in args 
#By default loc is  0 ie standardised   
pmf(k, params )      Probability mass function at k of the given RV. 
cdf(k, params)       Cumulative distribution function of the given RV. 
ppf(q, params )      Percent point function (inverse of cdf) at q of the given RV. 
rvs(params, size)    Generates size(could be tuple for MD) random number based on this distribution 


logpmf(k, params)    Log of the probability mass function at k of the given RV. 
logcdf(k, params)    Log of the cumulative distribution function at k of the given RV. 
sf(k, params)        Survival function (1 - cdf) at k of the given RV. 
logsf(k, params)     Log of the survival function of the given RV. 
isf(q, params)       Inverse survival function (inverse of sf) at q of the given RV. 

moment(n, params)    n-th order non-central moment of distribution. 
stats(params, mask)  Some statistics of the given RV. 
                     mask: Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
                     can be combined
entropy(params)      Differential entropy of the RV. 

median(params)       Median of the distribution. 
mean(params)         Mean of the distribution. 
std(params)          Standard deviation of the distribution. 
var(params)          Variance of the distribution. 

interval(alpha, *args, **kwds)          Confidence interval with equal areas around the median. 
expect([func, args, loc, lb, ub, ...])  Calculate expected value of a function with respect to the distribution for discrete distribution. 

#Example 

from scipy.stats import binom
import matplotlib.pyplot as plt
fig, ax = plt.subplots(1, 1)

#Calculate a few first moments:
n, p = 5, 0.4
mean, var, skew, kurt = binom.stats(n, p, moments='mvsk')


#Display the probability mass function (pmf):
#ppf - given cumulative p, output x , pmf= Pr(x) given x , cdf = cummulative Pr
x = np.arange(binom.ppf(0.01, n, p),binom.ppf(0.99, n, p))
ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')
ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)

#Check accuracy of cdf and ppf:
prob = binom.cdf(x, n, p)
np.allclose(x, binom.ppf(prob, n, p))#True

#Generate random numbers:
r = binom.rvs(n, p, size=1000)









###SciPy stats (scyipy.stats) -   Meaning of p-value and H0 - null hypothesis 

"null hypothesis" is a general statement or default position 
stated as an objective of a particular test 
eg coefficient of regression is not significant(does not add value to the effect )
e.g. no difference between blood pressures in group A and group B(ie same mean)

The p-value is the probability of the test statistic being at least 
as extreme as the one observed given that the null hypothesis is true
And is used to accept or reject truthness of null hypothesis
    •High P values: your data are likely with a true null.
    •Low P values: your data are unlikely with a true null.


    
The null hypothesis is generally assumed to be true 
until evidence indicates otherwise( ie rejectig H0 when p-value < 0.05)

A result is said to be statistically significant 
if it allows us to reject the null hypothesis

If p value > .10    -> 'not significant'
If p value <= .10   -> 'marginally significant'
If p value <= .05   -> 'significant'
If p value <= .01   -> 'highly significant.'

Set the significance level, α, (based on domain expert's prior knowledge)
-probability of making a Type I error, to be small
for example  0.05(5%), 
implying that it is acceptable to have a 5% probability of incorrectly rejecting the null hypothesis

Compare the P-value to α. 
If the P-value is less than (or equal to) α, reject the null hypothesis 
in favor of the alternative hypothesis. 
If the P-value is greater than α, do not reject the null hypothesis.

Critical values are essentially cut-off values 
that define regions where the test statistic is unlikely to lie; 
for example, a region where the critical value is exceeded 
with probability α  if the null hypothesis is true.
 
We decide either to reject the null hypothesis 
if the test statistic exceeds the critical value (for α = 0.05) 
or analagously to reject the null hypothesis if the p-value is smaller than 0.05

Type I error is the incorrect rejection of a true null hypothesis 
(also known as a "false positive" finding)
A test's probability of making a type I error is denoted by α

Type II error is incorrectly retaining a false null hypothesis 
(also known as a "false negative" finding
A test's probability of making a type II error is denoted by β
and related to the power of a test (which equals 1−β).

For any given sample set, 
the effort to reduce one type of error generally results in 
increasing the other type of error. 

For a given test, the only way to reduce both error rates is 
to increase the sample size
 

A two-tailed('two-sided') test is appropriate 
if the estimated value may be more than or less than the reference value
For example, to compare the mean of a sample to a given value x using a t-test.  
Our null hypothesis is that the mean is equal to x. 
A two-tailed test will test both if the mean is significantly greater than x 
and if the mean significantly less than x. 
The mean is considered significantly different from x 
if the test statistic is in the top 2.5% or bottom 2.5% of its probability distribution, 
resulting in a p-value less than 0.05(resulting rejection of H0)    
 

A one-tailed/sided test is appropriate 
if the estimated value may depart from the reference value in only one direction,
for example either:
    'greater' : above or equal to a certain value, or
    'less' :below/less or equal to a certain value.
For example, to compare the mean of a sample to a given value x using a t-test.  
Our null hypothesis is that the mean is equal to x.  
A one-tailed test will test either if the mean is significantly greater than x 
or if the mean is significantly less than x, but not both. 
Then, depending on the chosen tail, the mean is significantly greater than 
or less than x if the test statistic is in the top 5% of its probability distribution 
or bottom 5% of its probability distribution, 
resulting in a p-value less than 0.05.


Note in Scipy, few tests have 
alternative : {'two-sided', 'less','greater'} function parameter 
which means , 
For 'two-sided', Alternative hypothesis is two-sided meaning H0 is 'equal-to'
For 'less','greater': Alternate Hypothesis is less or greater than test parameter 
However, when Alternate Hypothesis is 'greater', meaning H0 is 'less-than' 
and when Alternate Hypothesis is 'less', meaning H0 is 'greater-than'
And rejecting H0 means truthifying  Alternate Hypothesis
and vice versa 


If the test statistic follows a Student's t-distribution in the null hypothesis 
– when underlying variable follows a normal distribution 
with unknown scaling factor(var), 
then the test is referred to as a one-tailed or two-tailed t-test. 

If the test is performed using the actual population mean and variance, 
rather than an estimate from a sample, 
it would be called a one-tailed or two-tailed Z-test.

Note in Scipy, p-value of t-test  are always two-sided
This means that given p and t values from a two-tailed test, 
you would reject the null hypothesis of a greater-than test 
when p/2 < alpha and t > 0, 
and of a less-than test when p/2 < alpha and t < 0.


### Scipy - statistical tests - Kolmogorov-Smirnov test for goodness of fit
#(non-parametric)(for continuous dist)
#non-parameteric - no assumption of pdf (parameteric means normal dist)
scipy.stats.kstest(rvs, cdf, args=(), N=20, alternative='two-sided', mode='approx')
#H0:rvs is drawn from cdf
#cdf: If a string, it should be the name of a distribution in scipy.stats
#or a python callable which gives the cdf
#Returns: (D, p-value)

 

#Example 
from scipy import stats
x = np.linspace(-15, 15, 9)
>>> stats.kstest(x, 'norm')
(0.44435602715924361, 0.038850142705171065)  #<0.05, reject H0
np.random.seed(987654321)
>>> stats.kstest(stats.norm.rvs(size=100), 'norm')  
(0.058352892479417884, 0.88531190944151261)  #>0.05, dont reject H0


#Test against one-sided alternative hypothesis
#In the one-sided test, 
#the alternative is True pdf, G(x)  is 'less' or 'greater' than the given F(x) of the hypothesis
#ie G(x)<=F(x), resp. G(x)>=F(x).
np.random.seed(987654321)
x = stats.norm.rvs(loc=0.2, size=100)
stats.kstest(x,'norm', alternative = 'less') #H0:greater as alternative is less
(0.12464329735846891, 0.040989164077641749) #<0.05, reject H0



### Scipy - statistical tests - Kolmogorov-Smirnov statistic on 2 samples 
#(non parametric test)(continuous distribution)
scipy.stats.ks_2samp(data1, data2)
#non-parameteric - no assumption of pdf (parameteric means normal dist)
#H0: data1,data2 are drawn from the same continuous distribution.
#Returns (statistic, pvalue)

#Example 
from scipy import stats
np.random.seed(12345678)  #fix random seed to get the same result
n1 = 200  # size of first sample
n2 = 300  # size of second sample

rvs1 = stats.norm.rvs(size=n1, loc=0., scale=1)
rvs2 = stats.norm.rvs(size=n2, loc=0.5, scale=1.5)
>>> stats.ks_2samp(rvs1, rvs2)
(0.20833333333333337, 4.6674975515806989e-005)  #<0.05, reject H0 



### Scipy - statistical tests - Mann-Whitney rank test on samples x and y
#(non-parametric, n > 20)
#non-parameteric - no assumption of pdf (parameteric means normal dist)
scipy.stats.mannwhitneyu(x, y, use_continuity=True, alternative=None)
#H0: x,y come from the same population(ie same distribution/mean etc)
#returns (statistics, p-value)


#Frank Wilcoxon's 1945 paperdescribed two tests -- 
#for "Unpaired Experiments" and "Paired Comparisons" 
#which have come to be called the (Wilcoxon) rank sum test 
#and the (Wilcoxon) signed rank test respectively
#The test for comparing unpaired samples was extended by Mann and Whitney in 1947

from scipy.stats import mannwhitneyu
import numpy as np 
a = np.arange(100)
b = np.arange(100)
np.random.shuffle(b)
>>> np.corrcoef(a,b)
   array([[ 1.        , -0.07155116],
          [-0.07155116,  1.        ]])
>>> mannwhitneyu(a, b, alternative='two-sided')
MannwhitneyuResult(statistic=5000.0, pvalue=0.9990251925510822) # result for almost not correlated, >0.05, don't reject H0
>>> mannwhitneyu(a, a, alternative='two-sided')
MannwhitneyuResult(statistic=5000.0, pvalue=0.9990251925510822) # result for perfectly correlated


### Scipy - statistical tests - Wilcoxon signed-rank test
#(non-parametric , n>20)
scipy.stats.wilcoxon(x, y=None, zero_method='wilcox', correction=False)
#H0: x,y,  paired samples come from the same distribution(ie same mean etc)
#paired means same objects are subjected to two tests 
#returns (statistics, p-value)



### Scipy - statistical tests - Kruskal-Wallis H-test 
#(non-parametric, like anova,Analysis of variance , n>5)
scipy.stats.kruskal(sample1, sample2,... )
#H0: population median of all of the groups are equal.
#returns (statistics, p-value)

#Example 
from scipy import stats
x = [1, 3, 5, 7, 9]
y = [2, 4, 6, 8, 10]
>>> stats.kruskal(x, y)
KruskalResult(statistic=0.27272727272727337, pvalue=0.60150813444058948) #>0.05, dont reject H0


### Scipy - statistical tests - Friedman test 
#(non-parametric, like one way anova with repeated measures, n>10 and repeated>6)
scipy.stats.friedmanchisquare(sample1, sample2,...  )
#H0: repeated measurements of the same individuals have the same distribution
#repeated measures is used to compare three or more group(atleast three samples must be given)
#where the participants are the same in each group

#For example, if two measurement techniques are used on the same set of individuals, 
#the Friedman test can be used to determine if the two measurement techniques are consistent.
#returns (statistics, p-value)





### Scipy - statistical tests - Combine p-values 
scipy.stats.combine_pvalues(pvalues, method='fisher', weights=None)
#method : {'fisher, 'stouffer}, 
#returns (statistics, p-value)




### Scipy - statistical tests - one-way chi square test 
#( non-parametric, n>5)
scipy.stats.chisquare(f_obs, f_exp=None, ddof=0, axis=0)
#H0: f_obs data has the f_exp frequencies.
#"One way" means that units are classified according to a single categorical variable
#ie f_obs is array (dimension-1)
#returns (statistics, p-value)

#Example 
# with only f_obs is given,  expected frequencies are uniform 
#and = the mean of the observed frequencies.

from scipy.stats import chisquare
>>> chisquare([16, 18, 16, 14, 12, 12])
(2.0, 0.84914503608460956)  #last one is p_value 


#With f_exp the expected frequencies can be given.
>>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])
(3.5, 0.62338762774958223)


#When f_obs is 2-D, by default the test is applied to each column.
obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T
>>> obs.shape
(6, 2)
>>> chisquare(obs)
(array([ 2.        ,  6.66666667]), array([ 0.84914504,  0.24663415])) #last one is array of p_value 

#By setting axis=None, 
#the test is applied to all data in the array, 
#which is equivalent to applying the test to the flattened array.
>>> chisquare(obs, axis=None)
(23.31034482758621, 0.015975692534127565)
>>> chisquare(obs.ravel())
(23.31034482758621, 0.015975692534127565)

#ddof is the change to make to the default degrees of freedom.
>>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)
(2.0, 0.73575888234288467)






### Scipy - statistical tests - Cressie-Read power divergence statistic and goodness of fit test
#( non-parametric, n>5)
#generalization of pearson chi square test(when lambda_ = 1)
scipy.stats.power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None)
#H0: f_obs data has the f_exp frequencies.
#returns (statistics, p-value)
#lambda_              Value Description
"pearson"             1     Pearson's chi-squared statistic.
                            In this case, the function is
                            equivalent to `stats.chisquare`.
"log-likelihood"      0     Log-likelihood ratio. Also known as
                            the G-test [R557].
"freeman-tukey"      -1/2   Freeman-Tukey statistic.
"mod-log-likelihood" -1     Modified log-likelihood ratio.
"neyman"             -2     Neyman's statistic.
"cressie-read"        2/3   The power recommended in [R559]


#Example 
# with onlyf_obs is given,  expected frequencies are uniform 
#and given by the mean of the observed frequencies.

from scipy.stats import power_divergence
>>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')
(2.006573162632538, 0.84823476779463769) #last one is p-value 


#The expected frequencies can be given with the f_exp argument:
>>> power_divergence([16, 18, 16, 14, 12, 12],
            f_exp=[16, 16, 16, 16, 16, 8],
            lambda_='log-likelihood')
(3.3281031458963746, 0.6495419288047497) #last one is p-value 




### Scipy - statistical tests - Chi-square test of independence of variables in a contingency table.
scipy.stats.chi2_contingency(contingency_table, correction=True, lambda_=None)
#(non-parameteric, n>5)
#H0:observed frequencies in the contingency table are independent
#lambda_ as same as from power_divergence
#contingency_table could be RxC in 2 dimensions or higher dimensions
#Returns:
    chi2 : float,The test statistic.
    p : float,The p-value of the test
    dof : int,Degrees of freedom
    expected : ndarray, same shape as observed,
              The expected frequencies, based on the marginal sums of the table
 
#Example of contigency table 
#Suppose that we have two variables, 
#sex (male or female) and handedness (right or left handed). 
#Further suppose that 100 individuals are randomly sampled 
#from a very large population as part of a study of sex differences in handedness
        Handedness
Gender  Right handed    Left handed     Total
Male        43          9               52
Female      44          4               48 
Total       87          13              100 
#H0: male-female's  right or left handed are independent 
obs = np.array([[43,9], [44,4]])
>>> stats.chi2_contingency(obs)
(1.0724852071005921, 
0.300384770390566,  #>0.05, don't reject H0 
1, array([[45.24,  6.76],
       [41.76,  6.24]]))
       
       

#A two-way example (2 x 3)- 2D
from scipy.stats import chi2_contingency
obs = np.array([[10, 10, 20], [20, 20, 20]])
>>> chi2_contingency(obs)
(2.7777777777777777,
 0.24935220877729619,   #p-value 
 2,
 array([[ 12.,  12.,  16.],
        [ 18.,  18.,  24.]]))


#Perform the test using the log-likelihood ratio (i.e. the 'G-test') 
g, p, dof, expctd = chi2_contingency(obs, lambda_="log-likelihood")
>>> g, p
(2.7688587616781319, 0.25046668010954165)

#A four-way example (2 x 2 x 2 x 2)-4D
obs = np.array(
        [[[[12, 17],
        [11, 16]],
        [[11, 12],
        [15, 16]]],
        [[[23, 15],
        [30, 22]],
        [[14, 17],
        [15, 16]]]])
>>> chi2_contingency(obs)
(8.7584514426741897,
 0.64417725029295503,  #p value 
 11,
 array([[[[ 14.15462386,  14.15462386],
          [ 16.49423111,  16.49423111]],
         [[ 11.2461395 ,  11.2461395 ],
          [ 13.10500554,  13.10500554]]],
        [[[ 19.5591166 ,  19.5591166 ],
          [ 22.79202844,  22.79202844]],
         [[ 15.54012004,  15.54012004],
          [ 18.10873492,  18.10873492]]]]))




### Scipy - statistical tests - Fisher exact test on a 2x2 contingency table
scipy.stats.fisher_exact(contingency_table, alternative='two-sided')
#H0:observed frequencies in the contingency table are independent
#returns (statistics, p-value)

#Example 
        Atlantic  Indian
whales     8        2
sharks     1        5
#H0: whales-sharks from Atlantic and Indian ocean are independent

import scipy.stats as stats
oddsratio, pvalue = stats.fisher_exact([[8, 2], [1, 5]])
>>> pvalue   #<0.05, reject H0
0.0349...




### Scipy - statistical tests - Jarque-Bera goodness of fit test on sample data
#(parametric, n>2000)
scipy.stats.jarque_bera(x)
#H0: sample data has the skewness and kurtosis matching a normal distribution.
#returns (statistics, p-value)

#Example 
from scipy import stats
np.random.seed(987654321)
x = np.random.normal(0, 1, 100000)
y = np.random.rayleigh(1, 100000)
>>> stats.jarque_bera(x)
(4.7165707989581342, 0.09458225503041906)  #last one is p-value 
>>> stats.jarque_bera(y)
(6713.7098548143422, 0.0)


### Scipy - statistical tests - Shapiro-Wilk test for normality
scipy.stats.shapiro(x, a=None, reta=False)
#H0: x is from normal distribution
#returns (statistics, p-value)

from scipy import stats
np.random.seed(12345678)
x = stats.norm.rvs(loc=5, scale=3, size=100)
>>> stats.shapiro(x)
(0.9902572631835938, 0.685400664806366)  #last one is p-value


### Scipy - statistical tests - Anderson-Darling test- 1 sample 
scipy.stats.anderson(x, dist='norm')
#H0: x is from 'dist'
#dist : {'norm,expon,logistic,gumbel,extreme1}
#returns (statistic, critical_values=array([one float for each significance_level]), 
#significance_level=array([15. , 10. ,  5. ,  2.5,  1.]))
#significance_level is in % 

>>> x = stats.norm.rvs(loc=5, scale=3, size=100)
>>> stats.shapiro(x)
(0.9902572631835938, 0.685400664806366)   #>0.5, don't reject H0 
>>> stats.anderson(x)
AndersonResult(statistic=0.20901515717881125, critical_values=array([0.555, 0.63
2, 0.759, 0.885, 1.053]), significance_level=array([15. , 10. ,  5. ,  2.5,  1.
])) #statistic < critical_values, hence don't reject H0


### Scipy - statistical tests - Anderson-Darling test for k-samples
#(non-Paramteric, continuous and discrete data)
scipy.stats.anderson_ksamp(array_of_samples, midrank=True)
#H0: k-samples are drawn from the same population
#Returns 
statistic : float
    Normalized k-sample Anderson-Darling test statistic.
critical_values : array
    The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%.
significance_level : float
    An approximate significance level 
    at which the null hypothesis for the provided samples can be rejected.
 
#statistic < critical_values, hence don't reject H0
#statistic > critical_values, hence reject H0
#Example 
>>> stats.anderson_ksamp([np.random.normal(size=50),
    np.random.normal(loc=0.5, size=30)])
(2.4615796189876105,
  array([ 0.325,  1.226,  1.961,  2.718,  3.752]),
  0.03134990135800783)  #last one: 3% significance level where H0 can be rejected



### Scipy - statistical tests - Binomial test
scipy.stats.binom_test(x, n=None, p=0.5, alternative='two-sided')
#H0: probability of success given x out of n trials is p for 'two-sided'
#probability of success given x out of n trials is <= p for 'greater'
#probability of success given x out of n trials is >= p for 'less'
#x: the number of successes, and give n 
#or if x has length 2, it is the number of successes and the number of failures, and ignore n 
#Returns p-value 

#Example 
#Problem - A soft drink company has invented a new drink, 
#and would like to find out if it will be as popular as the existing favorite drink. 
#For this purpose, its research department arranges 18 participants for taste testing. 
#Each participant tries both drinks in random order before giving his or her opinion. 

#It turns out that 5 of the participants like the new drink better, and the rest prefer the old one. 
#At .05 significance level, can we reject the notion that the two drinks are equally popular? 

 
> scipy.stats.binom_test(5, 18)              #H0 : two drinks are equally popular ie p=0.5
0.096252441406249986             #>0.05, accept H0




### Scipy - statistical tests - Moods test for equal scale parameters
#(non-parametric)
scipy.stats.mood(x, y, axis=0)
#H0: x,y have same distribution with same scale(for norm, it is var) parameter
#For multi-dimensional arrays, if the inputs are of shapes (n0, n1, n2, n3) 
#and (n0, m1, n2, n3), then if axis=1, 
#the resulting z and p values will have shape (n0, n2, n3). 
#Note that n1 and m1 don't have to be equal, but the other dimensions do.

#Returns:
z : scalar or ndarray
    The z-score for the hypothesis test. For 1-D inputs a scalar is returned.
p-value : scalar ndarray
    The p-value for the hypothesis test.
 
 
#Example 

from scipy import stats
np.random.seed(1234)
x2 = np.random.randn(2, 45, 6, 7) #from the 'standard normal' distribution., args: The dimensions of the returned array
x1 = np.random.randn(2, 30, 6, 7) #The dimensions of the returned array
z, p = stats.mood(x1, x2, axis=1)
>>> p.shape
(2, 6, 7)


#Find the number of points where the difference in scale is not significant(p-value> 0.05)
>>> (p > 0.1).sum()
74

#Perform the test with different scales:
x1 = np.random.randn(2, 30)
x2 = np.random.randn(2, 35) * 10.0
>>> stats.mood(x1, x2, axis=1)
(array([-5.7178125 , -5.25342163]), array([  1.07904114e-08,   1.49299218e-07])) #last one p-value 




### Scipy - statistical tests - Ansari-Bradley test for equal scale parameters
#(A non-parametric test for the equality of 2 variances, exact for n<55, else approx)
scipy.stats.ansari(x, y)
#H0: x,y have equal scale parameter (similar in shape ie variances)
#Returns (statistic, p-value)



### Scipy - statistical tests - Bartletts test for equal variances of k samples 
#(A parametric(normal) test for equality of k variances in normal samples)
scipy.stats.bartlett(sample1, sample2,... )
#H0: all have equal variance
#For samples from significantly non-normal populations,
#Levenes test is more robust.
#Returns (statistic, p-value)



### Scipy - statistical tests - Levene test for equal variances of k samples 
#(A parametric test for equality of k variances)
scipy.stats.levene(sample1, sample2, ...,center ,proportiontocut =0.05 )
#H0: all have equal variance
#Returns (statistic, p-value)
#center : {'mean', 'median', 'trimmed'}, optional
    •'median' : Recommended for skewed (non-normal) distributions>
    •'mean' : Recommended for symmetric, moderate-tailed distributions.
    •'trimmed' : Recommended for heavy-tailed distributions.



### Scipy - statistical tests - Fligner-Killeen test for equality of variance of k samples 
#(A non-parametric test for the equality of k variances)
scipy.stats.fligner(sample1, sample2, ...,center ,proportiontocut =0.05)
#H0: all have equal variance
#Returns (statistic, p-value)
#center : {'mean', 'median', 'trimmed'}, optional
    •'median' : Recommended for skewed (non-normal) distributions>
    •'mean' : Recommended for symmetric, moderate-tailed distributions.
    •'trimmed' : Recommended for heavy-tailed distributions.




### Scipy - statistical tests - Moods median test 
#(non-parametric)
scipy.stats.median_test(sample1, sample2, ..., lambda_ )
#H0: two or more samples come from populations with the same median
#By default, the statistic computed in this test is Pearson's chi-squared statistic. 
#lambda_ allows a statistic from the Cressie-Read power divergence family to be used instead
#Returns 
stat : float
    The test statistic. The statistic that is returned is determined by lambda_. 
    The default is Pearson's chi-squared statistic.
p : float
    The p-value of the test.
m : float
    The grand median.
table : ndarray
    The contingency table. The shape of the table is (2, n), 
    where n is the number of samples. 
    The first row holds the counts of the values above the grand median, 
    and the second row holds the counts of the values below the grand median




#Example 
#A biologist runs an experiment in which there are three groups of plants. 
#Group 1 has 16 plants, group 2 has 15 plants, and group 3 has 17 plants. 
#Each plant produces a number of seeds. The seed counts for each group are:
Group 1: 10 14 14 18 20 22 24 25 31 31 32 39 43 43 48 49
Group 2: 28 30 31 33 34 35 36 40 44 55 57 61 91 92 99
Group 3:  0  3  9 22 23 25 25 33 34 34 40 45 46 48 62 67 84
#Ho: g1, g2, g3 have same median 

g1 = [10, 14, 14, 18, 20, 22, 24, 25, 31, 31, 32, 39, 43, 43, 48, 49]
g2 = [28, 30, 31, 33, 34, 35, 36, 40, 44, 55, 57, 61, 91, 92, 99]
g3 = [0, 3, 9, 22, 23, 25, 25, 33, 34, 34, 40, 45, 46, 48, 62, 67, 84]
from scipy.stats import median_test
stat, p, med, tbl = median_test(g1, g2, g3)
#The median is
>>> med
34.0
#and the contingency table is
>>> tbl
array([[ 5, 10,  7],
       [11,  5, 10]])

#p is too large to conclude that the medians are not the same
>>> p
0.12609082774093244  #>0.5, don't reject H0 


#The 'G-test' can be performed by passing lambda_="log-likelihood" to median_test.
g, p, med, tbl = median_test(g1, g2, g3, lambda_="log-likelihood")
>>> p
0.12224779737117837




### Scipy - statistical - boxcox transformations to convert to normal dist 
#Normality is an important assumption for many statistical techniques
#A Box Cox transformation is a way to transform non-normal dependent variables into a normal shape
boxcox(x[, lmbda, alpha])           Return a positive dataset transformed by a Box-Cox power transformation. 
boxcox_normmax(x[, brack, method])  Compute optimal Box-Cox transform parameter for input data. 
boxcox_llf(lmb, data)               The boxcox log-likelihood function. 
entropy(pk[, qk, base])             Calculate the entropy of a distribution for given probability values. 
boxcox_normplot(x, la, lb[, plot, N]) Compute parameters for a Box-Cox normality plot, optionally show it. 
probplot(x[, sparams, dist, fit, plot, rvalue])     Calculate quantiles for a probability plot, and optionally show the plot. 

#boxcox: Transformation depends on lambda (λ), which varies from -5 to 5
#selects the best lambda 
#actula transformation is given by 
y = (x**lmbda - 1) / lmbda,  for lmbda > 0
    log(x),                  for lmbda = 0
#If alpha is not None, 
#return the 100 * (1-alpha)% confidence interval for lmbda 
#Returns 
boxcox : ndarray
    Box-Cox power transformed array.
maxlog : float, optional
    If the lmbda parameter is None, 
    the second returned argument is the lambda that maximizes the log-likelihood function.
(min_ci, max_ci) : tuple of float, optional, 
    minimum and maximum confidence limits



#Example 
from scipy import stats
import matplotlib.pyplot as plt
np.random.seed(1234)  # make this example reproducible


#Generate some data and determine optimal lmbda in various ways:
x = stats.loggamma.rvs(5, size=30) + 5
y, lmax_mle = stats.boxcox(x)  #y is transformed of x
lmax_pearsonr = stats.boxcox_normmax(x)
>>> lmax_mle
7.177...
>>> lmax_pearsonr
7.916...

>>> stats.boxcox_normmax(x, method='all')
array([ 7.91667384,  7.17718692])


fig = plt.figure()
ax = fig.add_subplot(111)
prob = stats.boxcox_normplot(x, -10, 10, plot=ax)
ax.axvline(lmax_mle, color='r')
ax.axvline(lmax_pearsonr, color='g', ls='--')
plt.show()

#Or draw stats.probplot
scipy.stats.probplot(x, sparams=(), dist='norm', fit=True, plot=None, rvalue=False)
#Generates a probability plot of sample data 
#against the quantiles of a specified theoretical distribution 
#(the normal distribution by default). 
#Parameters:
    x : array_like
        Sample/response data from which probplot creates the plot.
    sparams : tuple, optional
        Distribution-specific shape parameters (shape parameters plus location and scale).
#Even if plot is given, 
#the figure is not shown or saved by probplot; plt.show() or plt.savefig('figname.png') should be used after calling probplot.
#probplot generates a probability plot, 
#which should not be confused with a Q-Q or a P-P plot
#Returns:
(osm, osr) : tuple of ndarrays
    Tuple of theoretical quantiles (osm, or order statistic medians) 
    and ordered responses (osr). 
    osr is simply sorted input x. 
(slope, intercept, r) : tuple of floats, optional
    Tuple containing the result of the least-squares fit, 
    if that is performed by probplot. 
    r is the square root of the coefficient of determination. 
    If fit=False and plot=None, this tuple is not returned.
 
#Example 
from scipy import stats
import matplotlib.pyplot as plt
nsample = 100
np.random.seed(7654321)
fig = plt.figure()
ax1 = fig.add_subplot(511)
x = stats.loggamma.rvs(5, size=500) + 5
prob = stats.probplot(x, dist=stats.norm, plot=ax1)
ax1.set_xlabel('')
ax1.set_title('Probplot against normal distribution')
#after transformation 
ax2 = fig.add_subplot(512)
xt, _ = stats.boxcox(x)
prob = stats.probplot(xt, dist=stats.norm, plot=ax2)
ax2.set_title('Probplot after Box-Cox transformation')
#Then QQ plot 
#qqplot(data, dist=<scipy.stats._continuous_distns.norm_gen object>, distargs=(), a=0, loc=0, scale=1, fit=False, line=None, ax=None)
#dist : A scipy.stats or statsmodels distribution
ax2 = fig.add_subplot(513)
import statsmodels.api as sm
sm.qqplot(xt, line='45', ax=ax2)
ax3.set_title('Q-Q after Box-Cox transformation')
#qqplot of the residuals against quantiles of t-distribution with 4 degrees of freedom:
import scipy.stats as stats
ax3 = fig.add_subplot(514)
fig = sm.qqplot(xt, stats.t, distargs=(4,), , ax=ax3)
#qqplot against same as above, but with mean 3 and std 10:
ax4 = fig.add_subplot(515)
fig = sm.qqplot(xt, stats.t, distargs=(4,), loc=3, scale=10, , ax=ax4)
plt.show()











### Scipy - statistical tests - 1-way(2way) ANOVA(F-test) 
#(parametric/assumes normal)
scipy.stats.f_oneway(sample1, sample2,...)
#H0:two or more groups have the same population mean.
#"One way"(1D) means that units are classified according to a single categorical variable
#"Two way"(2D) means that units are classified according to a Two categorical variable(ie contigency table)
#Returns (statistic, p-value)

ANOVA is a means of comparing the ratio of systematic variance 
to unsystematic variance in an experimental study. 

Variance in the ANOVA is partitioned in to total variance(SS-T), 
variance due to groups(SS-Between or SS-A), 
and variance due to individual differences(SS-Within)

The "Between Groups"  represents what is often called "explained variance" 
or "systematic variance". 
We can think of this as variance that is due to the independent variable, 
the difference among  groups of IV/treatment 

The "Within Groups" variance represents what is often called "error variance". 
This is the variance within the groups, 
variance that is not due to the independent variable/treatment . 


The ratio obtained when doing this comparison is known as the F-ratio. 

A one-way ANOVA can be seen as a regression model 
with a single categorical predictor. 
This predictor usually has two plus categories. 
A one-way ANOVA has a single factor with J levels

Sum of Squares Between((SS-Between) is the variability due to interaction between the groups
Also called Treatment Sum of Squares or "Within groups Sum of Squares"

Sum of Squares Within(SS-Within) - The variability in the data due to differences within people
Also called Error Sum of Squares or Residual Sum of Squares

Sum of Squares Total(SS-T)- This is the total variability in the data.

Note: Effect size(eta-squared) is a value which allows you to see 
how much your independent variable (IV)(also called treatment)
has affected the dependent variable (DV) in an experimental study
•Small effect: 0.01
•Medium effect: 0.059
•Large effect: 0.138
For example , eta-squared=0.498
Means- a very large effect size; 49.8% of the variance was caused by the IV (treatment).
#Effect size for a between groups ANOVA
eta_sqrd = SSbetween/SStotal

Omega squared is widely viewed as a lesser biased alternative to eta-squared, 
especially when sample sizes are small.
•Omega squared can have values between ± 1. 
•Zero indicates no effect. 
•If the observed F is less than one, Omega squared will be negative.



##Assumptions
1.The samples are independent.
2.Each sample is from a normally distributed population.
3.The population standard deviations of the groups are all equal(homoscedasticity)

#Example 

import scipy.stats as stats

#Here are some data on a shell measurement 
#(the length of the anterior adductor muscle scar, standardized by dividing by length) 
#in the mussel Mytilus trossulus from five locations:
#Tillamook, Oregon; Newport, Oregon; Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, 
#taken from a much larger data set used in McDonald et al. (1991).

#H0: all shells from these have same mean 
tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,
    0.0659, 0.0923, 0.0836]
newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,
    0.0725]
petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]
magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,
        0.0689]
tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]
>>> stats.f_oneway(tillamook, newport, petersburg, magadan, tvarminne)
(7.1210194716424473, 0.00028122423145345439) #<0.5, reject H0


##Comparison Scipy vs  statsmodel 
#PlantGrowth.csv - "weight" , "group"("ctrl", "trt1","trt2")
#H0: all groups have same mean weight 
import pandas as pd
datafile="PlantGrowth.csv"
data = pd.read_csv(datafile)
 
#Create a boxplot- shows each group's weight with minimum, first quartile, median, third quartile, and maximum
data.boxplot('weight', by='group', figsize=(12, 8))
 
ctrl = data['weight'][data.group == 'ctrl']
 
grps = pd.unique(data.group.values)
d_data = {grp:data['weight'][data.group == grp] for grp in grps}
 
k = len(pd.unique(data.group))  # number of conditions
N = len(data.values)  # conditions times participants
n = data.groupby('group').size()[0] #Participants in each condition

#scipy 
from scipy import stats
F, p = stats.f_oneway(d_data['ctrl'], d_data['trt1'], d_data['trt2'])
#above gives  p-value 
#Note ANOVA assumes all groups have same variance 
#test it via scipy.stats.bartlett,
F, p =stats.bartlett(d_data['ctrl'], d_data['trt1'], d_data['trt2'])
#p <0.05, reject H0=all groups have same variance 
#or via scipy.stats.obrientransform
from scipy.stats import obrientransform
tx, ty, tz = obrientransform(d_data['ctrl'], d_data['trt1'], d_data['trt2'])
from scipy.stats import f_oneway
F, p = f_oneway(tx, ty, tz)
#p <0.05, reject H0=all groups have same variance 




##To get manually 
DFbetween = k - 1
DFwithin = N - k
DFtotal = N - 1
#Sum  Square 
SSbetween = (sum(data.groupby('group').sum()['weight']**2)/n) - (data['weight'].sum()**2)/N
sum_y_squared = sum([value**2 for value in data['weight'].values])
SSwithin = sum_y_squared - sum(data.groupby('group').sum()['weight']**2)/n
SStotal = sum_y_squared - (data['weight'].sum()**2)/N
#Mean Square 
MSbetween = SSbetween/DFbetween
MSwithin = SSwithin/DFwithin

F = MSbetween/MSwithin
p = stats.f.sf(F, DFbetween, DFwithin) #p-value 
#Effect size for a between groups ANOVA
eta_sqrd = SSbetween/SStotal
#we can use the less biased effect size measure Omega squared
om_sqrd = (SSbetween - (DFbetween * MSwithin))/(SStotal + MSwithin)
#Reporting style 
F(2, 27) = 4.846, p =  .016, eta-squared =  .264, omega-squared = .204



##statsmodels 
#H0: all groups have same mean weight 
import statsmodels.api as sm
from statsmodels.formula.api import ols
#IV(treatment) - group, DV- weight 
mod = ols('weight ~ group',data=data).fit()
aov_table = sm.stats.anova_lm(mod, typ=2)
effect_size = aov_table['sum_sq'][0]/(aov_table['sum_sq'][0]+aov_table['sum_sq'][1])
print(aov_table)
            sum_sq      df      F           PR(>F)
group       3.76634     2       4.846088    0.01591  
Residual    10.49209    27   
#PR<0.05 reject H0, means all groups are not same 
#To know which group has effect , Turky HSD is required(statsmodels) 
#reject (array of boolean, True if we reject Null(Ho=same mean pairwise) for group pair) 
import numpy as np
from scipy import stats
from statsmodels.stats.multicomp import (pairwise_tukeyhsd,MultiComparison)
res2 = pairwise_tukeyhsd(data['weight'], data['group']) #TukeyHSDResults
res2.groupsunique #to get group index 
print(res2.summary())
#'crt' group is taken reference 
#Plot a universal confidence interval of each group mean
#If some mean box overlapps, mean difference is not significant 
res2.plot_simultaneous(comparison_name='crtl', ax=None, figsize=(10, 6), xlabel=None, ylabel=None)
#Or 
mod = MultiComparison(dta2['StressReduction'], dta2['Treatment'])
mod.groupsunique 
res3 = mod.tukeyhsd() #TukeyHSDResults
res3.groupsunique #to get group index 
print(res3.summary())
res3.plot_simultaneous(comparison_name='ctrl', ax=None, figsize=(10, 6), xlabel=None, ylabel=None)
#Or 
print(mod.allpairtest(stats.ttest_rel, method='Holm'))
#or 
print(mod.allpairtest(stats.ttest_rel, method='b'))





#or use pyvttbl(not maintained)
#Requires numpy 1.11.x 
#setup conda/virtualenv with below 
#pip install virtualenv
#mkdir pyvttbl_env
#cd pyvttbl_env
#virtualenv ./
#pyvttbl_env\Scripts\activate
#pip pip install numpy-1.11.3+mkl-cp27-cp27m-win32.whl scipy-0-19.1-cp27-cp27m-win32.whl
#pip install jupyter matplotlib seaborn
#to run our ANOVA in a notebook 
#ipython kernel install --name "pyvttbl_env" --user


from pyvttbl import DataFrame
 
df=DataFrame()
df.read_tbl(datafile)
aov_pyvttbl = df.anova1way('weight', 'group')
print aov_pyvttbl  #O'BRIEN TEST , H0: all groups have same variance 
Anova: Single Factor on weight
 
SUMMARY
Groups   Count    Sum     Average   Variance 
============================================
ctrl        10   50.320     5.032      0.340 
trt1        10   46.610     4.661      0.630 
trt2        10   55.260     5.526      0.196 
 
O'BRIEN TEST FOR HOMOGENEITY OF VARIANCE
Source of Variation    SS     df    MS       F     P-value   eta^2   Obs. power 
===============================================================================
Treatments            0.977    2   0.489   1.593     0.222   0.106        0.306 
Error                 8.281   27   0.307                                        
===============================================================================
Total                 9.259   29                                                
 
ANOVA
Source of Variation     SS     df    MS       F     P-value   eta^2   Obs. power 
================================================================================
Treatments             3.766    2   1.883   4.846     0.016   0.264        0.661 
Error                 10.492   27   0.389                                        
================================================================================
Total                 14.258   29                                                
 
POSTHOC MULTIPLE COMPARISONS
 
Tukey HSD: Table of q-statistics
       ctrl     trt1       trt2   
=================================
ctrl   0      1.882 ns   2.506 ns 
trt1          0          4.388 *  
trt2                     0        
=================================
  + p < .10 (q-critical[3, 27] = 3.0301664694)
  * p < .05 (q-critical[3, 27] = 3.50576984879)
 ** p < .01 (q-critical[3, 27] = 4.49413305084)
ns - non significant 
#above show that trt1's mean is different than trt2  significantly 
 





##Note 2way annova not part of Scipy, use statsmodel 
#Unlike One-Way ANOVA, 
#it enables us to test the effect of two factors at the same time
#The only restriction is that the number of observations in each cell has to be equal 
import pandas as pd
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
from statsmodels.graphics.factorplots import interaction_plot
import matplotlib.pyplot as plt
from scipy import stats

datafile="ToothGrowth.csv" #"len","supp","dose"
data = pd.read_csv(datafile)

#visualising factorial data
fig = interaction_plot(data.dose, data.supp, data.len,
             colors=['red','blue'], markers=['D','^'], ms=10)
             
def eta_squared(aov):
    aov['eta_sq'] = 'NaN'
    aov['eta_sq'] = aov[:-1]['sum_sq']/sum(aov['sum_sq'])
    return aov
 
def omega_squared(aov):
    mse = aov['sum_sq'][-1]/aov['df'][-1]
    aov['omega_sq'] = 'NaN'
    aov['omega_sq'] = (aov[:-1]['sum_sq']-(aov[:-1]['df']*mse))/(sum(aov['sum_sq'])+mse)
    return aov


#C(data) means Categorical values in data 
#A:B means take interaction terms betwen A and B 
formula = 'len ~ C(supp) + C(dose) + C(supp):C(dose)'
model = ols(formula, data).fit()
aov_table = anova_lm(model, typ=2)
eta_squared(aov_table)
omega_squared(aov_table)
print(aov_table)
#note all terms are significant , does effect is largest 
Output ANOVA table
                sum_sq      df    F         PR(>F)          eta_sq      omega_sq
C(supp)         205.350000  1   15.571979   2.311828e-04    0.059484    0.055452 
C(dose)         2426.434333 2   91.999965   4.046291e-18    0.702864    0.692579 
C(supp):C(dose) 108.319000  2   4.106991    2.186027e-02    0.031377    0.023647 
Residual        712.106000  54  

#residual must be normal, check 
res = model.resid 
fig = sm.qqplot(res, line='s', ax=None)
plt.show()


#or 
from pyvttbl import DataFrame
df=DataFrame()
df.read_tbl(datafile)
df['id'] = xrange(len(df['len']))
print(df.anova('len', sub='id', bfactors=['supp', 'dose']))



















### Scipy - statistical tests - T-Test 
#(parametric)(equal variances)
#All returns (statistic, p_value)

ttest_1samp(a, popmean)
#H0: onesameple a has mean 'popmean'

ttest_ind(a, b, equal_var=True)
#H0:2 independent samples have identical average (expected) values

ttest_ind_from_stats(mean1, std1, nobs1, mean2, std2, nobs2)
#H0: 2 independent samples have identical average (expected) values
#mean1, std1, nobs1, mean2, std2, nobs2 are arraylike

ttest_rel(a,  b)
#H0:TWO RELATED(paired) samples  a and b have equal mean

#Example of ttest_1samp 
from scipy import stats
np.random.seed(7654567)  # fix seed to get the same result
rvs = stats.norm.rvs(loc=5, scale=10, size=(50,2))
>>> stats.ttest_1samp(rvs,5.0)
(array([-0.68014479, -0.04323899]), array([ 0.49961383,  0.96568674])) #last one is p-value 
>>> stats.ttest_1samp(rvs,0.0)
(array([ 2.77025808,  4.11038784]), array([ 0.00789095,  0.00014999]))


#Example of ttest_ind 
#Test with sample with identical means:
rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)
rvs2 = stats.norm.rvs(loc=5,scale=10,size=500)
>>> stats.ttest_ind(rvs1,rvs2)
(0.26833823296239279, 0.78849443369564776)  #last one is p-value 
>>> stats.ttest_ind(rvs1,rvs2, equal_var = False)
(0.26833823296239279, 0.78849452749500748)

#ttest_ind underestimates p for unequal variances:
rvs3 = stats.norm.rvs(loc=5, scale=20, size=500)
>>> stats.ttest_ind(rvs1, rvs3)
(-0.46580283298287162, 0.64145827413436174)
>>> stats.ttest_ind(rvs1, rvs3, equal_var = False)
(-0.46580283298287162, 0.64149646246569292)

#When n1 != n2, 
#the equal variance t-statistic is no longer equal to the unequal variance t-statistic:
rvs4 = stats.norm.rvs(loc=5, scale=20, size=100)
>>> stats.ttest_ind(rvs1, rvs4)
(-0.99882539442782481, 0.3182832709103896)
>>> stats.ttest_ind(rvs1, rvs4, equal_var = False)
(-0.69712570584654099, 0.48716927725402048)


#T-test with different means, variance, and n:
rvs5 = stats.norm.rvs(loc=8, scale=20, size=100)
>>> stats.ttest_ind(rvs1, rvs5)
(-1.4679669854490653, 0.14263895620529152)
>>> stats.ttest_ind(rvs1, rvs5, equal_var = False)
(-0.94365973617132992, 0.34744170334794122)

#Example of ttest_rel (paired)
from scipy import stats
np.random.seed(12345678) # fix random seed to get same numbers
rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)
rvs2 = (stats.norm.rvs(loc=5,scale=10,size=500) +
            stats.norm.rvs(scale=0.2,size=500))
>>> stats.ttest_rel(rvs1,rvs2)
(0.24101764965300962, 0.80964043445811562)
>>> rvs3 = (stats.norm.rvs(loc=8,scale=10,size=500) +
            stats.norm.rvs(scale=0.2,size=500))
>>> stats.ttest_rel(rvs1,rvs3)
(-3.9995108708727933, 7.3082402191726459e-005)







### Scipy - statistical - Correlation coefficient
#returns (r, p_value)
# r is from -1 to +1, ie inverse propotional to direct proportional, 
#=0 means not related 
pearsonr(x, y)          parametric, 
spearmanr(a,b)          non-Paramteric
pointbiserialr(x, y)    parametric, same value as pearsonr
kendalltau(x, y)        Calculates Kendalls tau, a correlation measure for ordinal dat a.
np.corrcoef(a, b)       parametric


>>> from scipy import stats
>>> stats.spearmanr([1,2,3,4,5], [5,6,7,8,7])
(0.82078268166812329, 0.088587005313543798)
>>> np.random.seed(1234321)
>>> x2n = np.random.randn(100, 2)
>>> y2n = np.random.randn(100, 2)
>>> stats.spearmanr(x2n)
(0.059969996999699973, 0.55338590803773591)



### Scipy - statistical tests - Other tests
kurtosistest(a) H0:a has normal kurtosis
normaltest(a)   H0:a is normal distribution
skewtest(a)     H0:a has normal skew

#Example 
>>> from scipy import stats
>>> pts = 1000
>>> np.random.seed(28041990)
>>> a = np.random.normal(0, 1, size=pts)
>>> b = np.random.normal(2, 1, size=pts)
>>> x = np.concatenate((a, b))
>>> k2, p = stats.normaltest(x)



### Scipy - statistical - Confidence interval and SE(standard error) of mean, var, std
#each result is of (center, (lower, upper))
mean_cntr, var_cntr, std_cntr = scipy.stats.bayes_mvs(data, alpha=0.9) #90% CI

#Calculates the standard error of the mean
s = sem(a, axis=0, ddof=1, nan_policy='propagate')


#Example 
from scipy import stats
data = [6, 9, 12, 7, 8, 8, 13]
mean, var, std = stats.bayes_mvs(data)
>>> mean
Mean(statistic=9.0, minmax=(7.1036502226125329, 10.896349777387467))
>>> var
Variance(statistic=10.0, minmax=(3.176724206..., 24.45910382...))
>>> std
Std_dev(statistic=2.9724954732045084, minmax=(1.7823367265645143, 4.9456146050146295))


##Alternate - How to calculate CI (Confidence interval)

#CI of p of Binom
#python - CI of binom  is beta distribution 
>>> statsmodels.stats.proportion.proportion_confint(2,3, method='beta')
(0.094299324050246075, 0.99159624134038737)
 
#CI of mean of normal dist 
import statsmodels.stats.api as sms

sms.DescrStatsW(a).tconfint_mean()  #t-dist because variance is unknown 
#OR
import numpy as np, scipy.stats as st
st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a)) #sem is SE of mean 

#The underlying assumptions for both are that the sample (array a) (size is small)
#was drawn independently from a normal distribution 
#with unknown standard deviation (T-Test).

#Basically it calculates SE of Mean and then from t distribution find 

import numpy as np
import scipy as sp
import scipy.stats

def mean_confidence_interval(data, confidence=0.95):
    a = 1.0*np.array(data)
    n = len(a)
    m, se = np.mean(a), scipy.stats.sem(a)
    h = se * sp.stats.t.ppf((1+confidence)/2., n-1)
    return m, m-h, m+h
    
#for large size   
#For large sample size n, the sample mean is normally distributed, (central limit theorem)
#and one can calculate its confidence interval using st.norm.interval()
stats.norm.interval(0.95, loc=mu, scale=sigma)

#The 95% confidence interval for the mean of N draws from a normal distribution with mean mu and std deviation sigma is
stats.norm.interval(0.95, loc=mu, scale=sigma/sqrt(N))





### Scipy - statistical - descriptive statistics - mean, variance etc
#use prefix scipy.stats.
describe(a)             Compute mean, var etc
gmean(a)                Compute the geometric mean
hmean(a)                Calculates the harmonic mean
kurtosis(a)             Computes  kurtosis
mode(a)                 Returns an array of the modal (most common) value in the passed arra y.
moment(a,moment)        Calculates the nth moment about the mean for a sample.
skew(a)                 Computes the skewness of a data se t.
kstat(data, n)          Return the nth k-statistic (1<=n<=4 so far ).
zscore(a)               Calculates the z score of each value in the sample, relative to the sample mean and standard deviatio n.
iqr(x)                  Compute the interquartile range of the data along the specified axis.

#The z-scores, standardized by mean and standard deviation of input array a.
a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,
        0.1954,  0.6307,  0.6599,  0.1065,  0.0508])
>>> from scipy import stats
>>> stats.zscore(a,axis=0, ddof=0)
array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,
        0.6748, -1.1488, -1.3324])

#scipy.stats.describe(a, axis=0, ddof=1, bias=True, nan_policy='propagate')
>>> from scipy import stats
>>> a = np.arange(10)
>>> stats.describe(a)
DescribeResult(nobs=10, minmax=(0, 9), mean=4.5, variance=9.1666666666666661,
               skewness=0.0, kurtosis=-1.2242424242424244)



### Scipy - statistical - Descriptive statistics of 'a' 
#only inside limits = None or (lower limit, upper limit)
#Values in the input array less than the lower limit or greater than the upper limit will be ignored
tmean(a,limits=None, inclusive=(True, True), axis=None)
tvar(a, limits=None, inclusive=(True, True), axis=0, ddof=1)
tmin(a, lowerlimit=None, axis=0, inclusive=True, nan_policy='propagate') #Values in the input array less than the given limit will be ignored. 
tmax(a, lowerlimit=None, axis=0, inclusive=True, nan_policy='propagate')
tstd(a, limits=None, inclusive=(True, True), axis=0, ddof=1)
tsem(a, limits=None, inclusive=(True, True), axis=0, ddof=1)




### Scipy - statistical - Plotting - plot-tests
ppcc_max(x[, brack, dist])                  Calculate the shape parameter that maximizes the PPCC 
ppcc_plot(x, a, b[, dist, plot, N])         Calculate and optionally plot probability plot correlation coefficient. 
probplot(x[, sparams, dist, fit, plot, rvalue]) Calculate quantiles for a probability plot, and optionally show the plot. 
boxcox_normplot(x, la, lb[, plot, N])       Compute parameters for a Box-Cox normality plot, optionally show it. 


#scipy.stats.ppcc_max(x, brack=(0.0, 1.0), dist='tukeylambda')[source]
#The probability plot correlation coefficient (PPCC) plot can be used 
#to determine the optimal shape parameter for a one-parameter family 
#of distributions. 
#ppcc_max returns the shape parameter that would maximize 
#the probability plot correlation coefficient for the given data to a one-parameter family of distributions.

dist : str or stats.distributions instance, optional
    Distribution or distribution function name
    The default is 'tukeylambda'
Returns:
    shape_value : float
 

from scipy import stats
x = stats.tukeylambda.rvs(-0.7, loc=2, scale=0.5, size=10000,
                         random_state=1234567) + 1e4
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111)
res = stats.ppcc_plot(x, -5, 5, plot=ax)
#The line should coincide with the highest point in the ppcc_plot.
max = stats.ppcc_max(x)
ax.vlines(max, 0, 1, colors='r', label='Expected shape value')
plt.show()














































###Matplotlib - Python 2D plotting library 

import numpy as np
import matplotlib.pyplot as plt

# Compute the x and y coordinates for points on sine and cosine curves
x = np.arange(0, 3 * np.pi, 0.1)
y_sin = np.sin(x)
y_cos = np.cos(x)

# Plot the points using matplotlib
plt.plot(x, y_sin)
plt.plot(x, y_cos)
plt.xlabel('x axis label')
plt.ylabel('y axis label')
plt.title('Sine and Cosine')
plt.legend(['Sine', 'Cosine'])
plt.show()


###Matplotlib - Universe and Artist
#There are three layers to the matplotlib API. 
    1.The matplotlib.backend_bases.FigureCanvas is the area 
    onto which the figure is drawn, 
    2.The matplotlib.backend_bases.Renderer is the object 
    which knows how to draw on the FigureCanvas, 
    3.The matplotlib.artist.Artist is the object that knows 
    how to use a renderer to paint onto the canvas. 

The FigureCanvas and Renderer handle all the details of talking 
to user interface toolkits like wxPython or drawing languages like PostScript®
 
The Artist handles all the high level constructs 
like representing and laying out the figure, text, and lines. 

The typical user will spend 95% of their time working with the Artists.

There are two types of Artists: primitives and containers. 
#check class hierarchy https://matplotlib.org/api/artist_api.html

The primitives represent the standard graphical objects 
eg  Line2D, Rectangle, Text, AxesImage, etc.(subclass of Artist)
and the containers are places to put primitives (Axis, Axes and Figure)(subclass of Artist) 

The standard use is to create a Figure instance, 
use the Figure to create one or more Axes or Subplot instances, 
and use the Axes instance helper methods to create the primitives. 

#create a Figure instance
import matplotlib.pyplot as plt
fig = plt.figure()  #has fig.axes : list of axes in Figure

ax = fig.add_subplot(2,1,1) # two rows, one column, first Axe is current and returned

Axes is the plotting area into which most of the objects go, 
and the Axes has many special helper methods 
(plot(), text(), hist(), imshow()) to create the most common graphics primitives 
(Line2D, Text, Rectangle, Image, respectively). 

These helper methods will take  data (e.g., numpy arrays and strings) 
and create primitive Artist instances as needed (e.g., Line2D), 
add them to the relevant containers, 
and draw them when requested. 

The Subplot, ( is  a subclass of Axes) 
that lives on a regular rows by columns grid of Subplot instances. 

If you want to create an Axes at an arbitrary location, 
use the add_axes() method which takes [left, bottom, width, height] 
values in 0-1 relative figure coordinates:

fig2 = plt.figure()
ax2 = fig2.add_axes([0.15, 0.1, 0.7, 0.3])

import numpy as np
t = np.arange(0.0, 1.0, 0.01)
s = np.sin(2*np.pi*t)
line, = ax.plot(t, s, color='blue', lw=2)

when you call ax.plot, it creates a Line2D instance 
and adds it to the Axes.lines list. 
>>> ax.lines[0]
<matplotlib.lines.Line2D instance at 0x19a95710>

>>> line
<matplotlib.lines.Line2D instance at 0x19a95710>

If you make subsequent calls to ax.plot 
(and the hold state is 'on' which is the default) 
then additional lines will be added to the list. 
You can remove lines later simply by calling the list methods; 

del ax.lines[0]
ax.lines.remove(line)  # one or the other, not both!

The Axes also has helper methods to configure 
and decorate the x-axis and y-axis tick, tick labels and axis labels:
xtext = ax.set_xlabel('my xdata') # returns a Text instance
ytext = ax.set_ylabel('my ydata')

When you call ax.set_xlabel, 
it passes the information on the Text instance of the XAxis. 

Each Axes instance contains an XAxis and a YAxis instance, 
which handle the layout and drawing of the ticks, tick labels and axis labels.

import numpy as np
import matplotlib.pyplot as plt

fig = plt.figure()
fig.subplots_adjust(top=0.8)
ax1 = fig.add_subplot(211)
ax1.set_ylabel('volts')
ax1.set_title('a sine wave')

t = np.arange(0.0, 1.0, 0.01)
s = np.sin(2*np.pi*t)
line, = ax1.plot(t, s, color='blue', lw=2)

# Fixing random state for reproducibility
np.random.seed(19680801)

ax2 = fig.add_axes([0.15, 0.1, 0.7, 0.3])
n, bins, patches = ax2.hist(np.random.randn(1000), 50,facecolor='yellow', edgecolor='yellow')
ax2.set_xlabel('time (s)')

plt.show()

##Customizing your objects
Every element in the figure is represented by a matplotlib Artist, 
and each has an extensive list of properties to configure its appearance. 

The figure itself contains a Rectangle exactly the size of the figure, 
which you can use to set the background color and transparency of the figures. 

Likewise, each Axes bounding box 
(the standard white box with black edges in the typical matplotlib plot) 
has a Rectangle instance that determines the color, transparency, and other properties of the Axes. 

These instances are stored as member variables 
Figure.patch and Axes.patch 
('Patch' is a name inherited from MATLAB, and is a 2D 'patch' of color on the figure,
 e.g., rectangles, circles and polygons). 
 
#Every matplotlib Artist has the following properties
alpha       The transparency - a scalar from 0-1 
animated    A boolean that is used to facilitate animated drawing 
axes        The axes that the Artist lives in, possibly None 
clip_box    The bounding box that clips the Artist 
clip_on     Whether clipping is enabled 
clip_path   The path the artist is clipped to 
contains    A picking function to test whether the artist contains the pick point 
figure      The figure instance the artist lives in, possibly None 
label       A text label (e.g., for auto-labeling) 
picker      A python object that controls object picking 
transform   The transformation 
visible     A boolean whether the artist should be drawn 
zorder      A number which determines the drawing order 
rasterized  Boolean; Turns vectors into rastergraphics: (for compression & eps transparency) 

#Each of the properties is accessed with an old-fashioned setter or getter 
#For example, to multiply the current alpha by a half:
a = o.get_alpha()
o.set_alpha(0.5*a)

#If you want to set a number of properties at once, 
#you can also use the set method with keyword arguments. 
o.set(alpha=0.5, zorder=2)


#To inspect the Artist properties is to use the matplotlib.artist.getp() function 
#This works for classes derived from Artist as well, e.g., Figure and Rectangle. 

>>> matplotlib.artist.getp(fig.patch)
    alpha = 1.0
    animated = False
    antialiased or aa = True
    axes = None
    clip_box = None
    clip_on = False
    clip_path = None
    contains = None
    edgecolor or ec = w
    facecolor or fc = 0.75
    figure = Figure(8.125x6.125)
    fill = 1
    hatch = None
    height = 1
    label =
    linewidth or lw = 1.0
    picker = None
    transform = <Affine object at 0x134cca84>
    verts = ((0, 0), (0, 1), (1, 1), (1, 0))
    visible = True
    width = 1
    window_extent = <Bbox object at 0x134acbcc>
    x = 0
    y = 0
    zorder = 1
    



###Matplotlib -  plt./ax.plot(x,y,fmt,...)
##check http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot
plot(x, y)        # plot x and y using default line style and color
plot(x, y, 'bo')  # plot x and y using blue circle markers
plot(y)           # plot y using x as index array 0..N-1
plot(y, 'r+')     # ditto, but with red plusses
#If x and/or y is 2D, 
#then the corresponding columns will be plotted.

#arbitrary number can be specified
plot(x1, y1, 'g^', x2, y2, 'g- ')

#plot() returns Line2D instance which can be modified
line1, line2 = plot(x1, y1, x2, y2)

# Can use multiple plot commands on same graph
plot([1,2,3], [1,2,3], 'go-', label='line 1', linewidth=2)
plot([1,2,3], [1,4,9], 'rs',  label='line 2')
axis([0, 4, 0, 10])  #Set the axis
legend()


#fmt string
#For every x, y pair of arguments, there is an optional third argument
#which is the format string that indicates the color and line type of the plot.
#The default format string is 'b-', which is a solid blue line.


#character       description 
'-'             solid line style 
'--'            dashed line style 
'-.'            dash-dot line style 
':'             dotted line style 
'.'             point marker 
','             pixel marker 
'o'             circle marker 
'v'             triangle_down marker 
'^'             triangle_up marker 
'<'             triangle_left marker 
'>'             triangle_right marker 
'1'             tri_down marker 
'2'             tri_up marker 
'3'             tri_left marker 
'4'             tri_right marker 
's'             square marker 
'p'             pentagon marker 
'*'             star marker 
'h'             hexagon1 marker 
'H'             hexagon2 marker 
'+'             plus marker 
'x'             x marker 
'D'             diamond marker 
'd'             thin_diamond marker 
'|'             vline marker 
'_'             hline marker 

#character   color 
'b'         blue 
'g'         green 
'r'         red 
'c'         cyan 
'm'         magenta 
'y'         yellow 
'k'         black 
'w'         white
 
#Any Line2D property can be given
plot(x, y, color='green', linestyle='dashed', marker='o', markerfacecolor='blue', markersize=12).

## few Line2D properties that can be passsed to  plt.plot or ax.plot or other plot functions 
alpha                   float (0.0 transparent through 1.0 opaque) 
animated                [True | False] 
antialiased or aa       [True | False] 
clip_box                a matplotlib.transforms.Bbox instance 
clip_on                 [True | False] 
color or c              any matplotlib color 
dash_capstyle           ['butt' | 'round' | 'projecting'] 
dash_joinstyle          ['miter' | 'round' | 'bevel'] 
drawstyle               ['default' | 'steps' | 'steps-pre' | 'steps-mid' | 'steps-post'] 
fillstyle               ['full' | 'left' | 'right' | 'bottom' | 'top' | 'none'] 
label                   string or anything printable with '%s' conversion. 
linestyle or ls         ['solid' | 'dashed', 'dashdot', 'dotted' | (offset, on-off-dash-seq) | '-' | '--' | '-.' | ':' | 'None' | ' ' | ''] 
linewidth or lw         float value in points 
marker                  A valid marker style 
markeredgecolor or mec  any matplotlib color 
markeredgewidth or mew  float value in points 
markerfacecolor or mfc  any matplotlib color 
markersize or ms        float 
solid_capstyle          ['butt' | 'round' | 'projecting'] 
solid_joinstyle         ['miter' | 'round' | 'bevel'] 
visible                 [True | False] 
zorder                  any number 


#Example
import matplotlib.pyplot as plt
plt.plot([1,2,3,4])                #y values , x is taken as 0,1,2,3
plt.ylabel('some numbers')
plt.show()

# to plot the above with red circles, use 'ro'

import matplotlib.pyplot as plt
plt.plot([1,2,3,4], [1,4,9,16], 'ro')
plt.axis([0, 6, 0, 20])
plt.show()

#With multiple x, y pairs in same plot
import numpy as np
import matplotlib.pyplot as plt

# evenly sampled time at 200ms intervals
t = np.arange(0., 5., 0.2)

# red dashes, blue squares and green triangles
plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
plt.show()



###Matplotlib -  Controlling line properties- linewidth, dash style, antialiased, et c;
# http://matplotlib.org/api/lines_api.html#matplotlib.lines.Line2D

#Option-1 Use keyword args:
plt.plot(x, y, linewidth=2.0)

#Option-2: Use the setter methods of Line2D
line1, line2 = plot(x1, y1, x2, y2 ).
line1.set_antialiased(False) # turn off antialising

#Option-3: Use the setp() command

lines = plt.plot(x1, y1, x2, y2)
# use keyword args
plt.setp(lines, color='r', linewidth=2.0)
# or MATLAB style string value pairs
plt.setp(lines, 'color', 'r', 'linewidth', 2.0)

#To get a list of settable line properties, call the setp() function with a line or lines as argument
lines = plt.plot([1, 2, 3])
plt.setp(lines) 
#output lists of properties
agg_filter: unknown
alpha: float (0.0 transparent through 1.0 opaque)
animated: [True | False]
antialiased or aa: [True | False]
axes: an :class:`~matplotlib.axes.Axes` instance
clip_box: a :class:`matplotlib.transforms.Bbox` instance
clip_on: [True | False]
clip_path: [ (:class:`~matplotlib.path.Path`, :class:`~matotlib.transforms.Transform`) | :class:`~matplotlib.patchesatch` | None ]
color or c: any matplotlib color
contains: a callable function
dash_capstyle: ['butt' | 'round' | 'projecting']
dash_joinstyle: ['miter' | 'round' | 'bevel']
dashes: sequence of on/off ink in points
drawstyle: ['default' | 'steps' | 'steps-pre' | 'steps-mid'| 'steps-post']
figure: a :class:`matplotlib.figure.Figure` instance
fillstyle: ['full' | 'left' | 'right' | 'bottom' | 'top' |'none']
gid: an id string
label: string or anything printable with '%s' conversion.
linestyle or ls: ['solid' | 'dashed', 'dashdot', 'dotted'|(offset, on-off-dash-seq) | '-' | '--' | '-.'|':' | 'None' | ' ' | '']
linewidth or lw: float value in points
marker: :mod:`A valid marker style <matplotlib.markers>`
markeredgecolor or mec: any matplotlib color
markeredgewidth or mew: float value in points
markerfacecolor or mfc: any matplotlib color
markerfacecoloralt or mfcalt: any matplotlib color
markersize or ms: float
markevery: [None | int | length-2 tuple of int | slice | list/array of int | float | length-2 tuple of float]
path_effects: unknown
picker: float distance in points or callable pick function `fn(artist, event)
pickradius: float distance in points
rasterized: [True | False | None]
sketch_params: unknown
snap: unknown
solid_capstyle: ['butt' | 'round' |  'projecting']
solid_joinstyle: ['miter' | 'round' | 'bevel']
transform: a :class:`matplotlib.transforms.Transform` instance
url: a url string
visible: [True | False]
xdata: 1D array
ydata: 1D array
zorder: any number




##Few plt. methods, Note below  operates on the current axis.
#Note below relevant methods withour arg gives current value eg xlim() gives current xlimit 
plt.xlabel('x axis label', fontsize=14, color='red')
plt.ylabel('y axis label')
plt.title('Sine and Cosine')
plt.legend(['Sine', 'Cosine'])
plt.axis([0, 4, 0, 10]) #Set the axis, [xmin, xmax, ymin, ymax] or 'off' or 'equal' or 'scaled' or 'tight' etc 
plt.text(60, .025, r'$\mu=100,\ \sigma=15$')  #Any text  or  '$any LaTex code$'
plt.grid(True)
plt.ylim(-2,2)
plt.xlim(-2,2)
plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),arrowprops=dict(facecolor='black', shrink=0.05),) #xytext=location of text, xy=location for annotation
plt.yscale('linear') #log, symlog, logit
plt.xscale('linear')

##Few Axes methods  , many are available on plt. as well 
#Note below relevant methods withour arg or get_*() gives current value 
#eg xlim() gives current xlimit , get_xlabel() gives current xlabel 
ax.set_xlabel('x axis label')
ax.set_ylabel('y axis label')
ax.set_title('Simple plot')
ax.legend(['Sine', 'Cosine'])

ax.axis([0, 4, 0, 10]) #Set the axis, [xmin, xmax, ymin, ymax] or 'off' or 'equal' or 'scaled' or 'tight' etc 
ax.set_axis_off() #Turn off the axis. 
ax.set_axis_on()  # Turn on the axis. 

ax.text(60, .025, r'$\mu=100,\ \sigma=15$')  #Any text 
ax.annotate('local max', xy=(2, 1), xytext=(3, 1.5),arrowprops=dict(facecolor='black', shrink=0.05),) #xytext=location of text, xy=location for annotation
ax.arrow(x, y, dx, dy, **kwargs)    #Add an arrow to the axes.
ax.grid(b=True|False, color='r', linestyle='-', linewidth=2)
ax.set_label(s)       #Set the label to s for auto legend.
 
ax.set_ylim(-2,2)
ax.set_xlim(-2,2)
ax.set_yscale('linear') #log, symlog, logit
ax.set_xscale('linear')
ax.set_visible(b)     #Set the artist's visibility.
ax.set_zorder(level)  #Set the zorder for the artist. Artists with lower zorder values are drawn first.

ax.set_xticks(ticks, minor=False)     #Set the x ticks with list of ticks
ax.set_xticklabels(labels, fontdict=None, minor=False, **kwargs)#Set the x-tick labels with list of string labels.
ax.set_yticks(ticks, minor=False)#Set the y ticks with list of ticks
ax.set_yticklabels(labels, fontdict=None, minor=False, **kwargs)#Set the y-tick labels with list of strings labels.

ax.xaxis_date(tz=None)  #Sets up x-axis ticks and labels that treat the x data as dates.
ax.yaxis_date(tz=None)  #Sets up y-axis ticks and labels that treat the y data as dates.
ax.minorticks_off()     #Remove minor ticks from the axes.
ax.minorticks_on()      #Remove minor ticks from the axes.
new_ax = ax.twinx()     #Create a twin Axes sharing the xaxis
new_ax = ax.twiny()     #Create a twin Axes sharing the yaxis







###Matplotlib -  Specifying matplotlib.colors
#For the basic built-in colors, use a single letter
•b: blue,g: green,r: red,c: cyan,m: magenta,y: yellow,k: black w: white
#Gray shades can be given as a string encoding a float in the 0-1 range, e.g.:
color = '0.75'
#can specify the color using an html hex string, as in:
color = '#eeefff'
#or you can pass an R , G , B tuple, 
#where each of R , G , B are in the range [0,1].
color = (0.5,0.5,0.5)
#Or use legal html names for colors, like 'red, 'burlywood and 'chartreuse'
color = 'burlywood'




###Matplotlib -  Working with multiple figures and axes
#Plots may contain many Figure, 
#each figure is one display window
#each Figure may contain many Axes
#Figure contains set/get of figure related attributes 
#eg get_figheight(),get_figwidth()
#Axes contains plot() and set/get of xlabel, ylabel etc


##Option-1 
#note subplots(..) returns (figure,axes)
#where axes is numpy.ndarray, hence access like axes[0,0], axes[0,1],... for complex subplots 
#for simple , can destructure immediately
figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=True,...  )
ax1.plot(x,y,fmt,.....)  
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)
#then show 
plt.show()

##Reference pyplot.subplots(..)
matplotlib.pyplot.subplots(nrows=1, ncols=1, sharex=False, sharey=False,   
        squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw)
        
#subplots args= numrows, numcols, 
#fignum where fignum ranges from 1 to numrows*numcol s.
# , in args is optional if numrows*numcols<10

#Can take addition args as below
#axisbg :The background color of the subplot, which can be any valid color specificer.
#polar:A boolean flag indicating whether the subplot plot should be a polar projection. Defaults to False.

#All Figure creation commands can be passed  eg
f, axs = plt.subplots(2,2,figsize=(15,15))  #axs is tuple of all ax es

#Returns instance of Figure and number(nrows*ncols) of AxesSubplot
fig1, (ax1, ax2) = plt.subplots(nrows=2,ncols=1)
dir(fig1)  # check all the attributes
dir(ax1)    #check all the attributes

#Example
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Just a figure and one subplot
f, ax = plt.subplots()
ax.plot(x, y)
ax.set_title('Simple plot')

# Two subplots, unpack the output array immediately
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)

# Four polar axes
plt.subplots(2, 2, subplot_kw=dict(polar=True))

# Share a X axis with each column of subplots
plt.subplots(2, 2, sharex='col')

# Share a Y axis with each row of subplots
plt.subplots(2, 2, sharey='row')

# Share a X and Y axis with all subplots
plt.subplots(2, 2, sharex='all', sharey='all')
# same as
plt.subplots(2, 2, sharex=True, sharey=True)



##Option-2 
fig = plt.figure(figsize=None, dpi=None, facecolor=None, edgecolor=None, linewidth=0. 0,
        frameon=None, subplotpars=None, tight_layout=None)
#Example       
fig = plt.figure(figsize=(15,15))        #create a figure 
ax1 = fig.add_subplot(211)               ## nrows,ncols, which_Axes_tomake_current ie 1  
ax1.plot(x,y,fmt,.....)  
ax2 = fig.add_subplot(212)
ax2.plot(x,y,fmt,.....) 
#then show 
plt.show()

#Few figure methods 
fig.legend(handles=(line1, line2, line3), labels=('label1', 'label2', 'label3'), loc='upper right') #loc can be (x,y) or predefined string , linen are matplotlib line instance
fig.text(x, y, s, *args, **kwargs) #Add text to figure.
fig.savefig(fname, **kwargs) #Save the current figure
fig.sca(a)          #Set the current axes to be a and return a
fig.set_dpi(val)            #Set the dots-per-inch of the figure, val is float 
fig.set_edgecolor(color)    #any matplotlib color 
fig.set_facecolor(color)    #any matplotlib color 
fig.set_figheight(val, forward=False) #val is float 
fig.set_figwidth(val, forward=False)  #val is float 
fig.set_size_inches(w, h=None, forward=True)
fig.subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None) #returns axes as ndarray
#Add an axes at position rect [left, bottom, width, height] 
#where all quantities are in fractions of figure width and height.
#projection :['aitoff' | 'hammer' | 'lambert' | 'mollweide' | 'polar' | 'rectilinear'], optional The projection type of the axes.
#polar : boolean, optionalIf True, equivalent to projection='polar'.
#also takes keyword arguments for Axes, get properties via plt.setp(ax) 
fig.add_axes(rect,projection, polar,**kwargs) 
#1st arg = Either a 3-digit integer or three separate integers 
#If the three integers are I, J, and K, 
#the subplot is the Ith plot on a grid with J rows and K columns.
fig.add_subplot(i,j,k, projection, polar,**kwargs)





##Option-3 
plt.figure(1)                # the first figure
plt.subplot(211)             # nrows,ncols, which_Axes_tomake_current ie 1                             
plt.plot(x,y,fmt,.....)      # all plot/scatter/box/hist etc goes subplot 1 
plt.subplot(212)             # nrows,ncols, which_Axes_tomake_current ie 2
plt.plot(x,y,fmt,.....)      # all plot/scatter/box/hist etc goes subplot 2

plt.figure(2)                # a second figure, sets to this figure, all plot commands go here
plt.plot(x,y,fmt,.....)      # creates a subplot(111) by default

plt.figure(1)                # figure 1 current; subplot(212) still current
plt.subplot(211)             # make subplot(211) in figure1 current
plt.title('Easy as 1, 2, 3') # subplot 211 title
#then show 
plt.show()

#All plotting commands apply to the current axes.
#The function gca() returns the current axes (a matplotlib.axes.Axes instance ),
#and gcf() returns the current figure (matplotlib.figure.Figure instance ).
#clear the current figure with clf() and the current axes with cla()
ax = plt.gca()  #Get the current Axes instance on the current figure 
fig = plt.gcf()  #Get a reference to the current figure.
ax.cla() #clear 
fig.clf() # clear 



##Example
import numpy as np
import matplotlib.pyplot as plt

def f(t):    
    return np.exp(-t) * np.cos(2*np.pi*t)

t1 = np.arange(0.0, 5.0, 0.1)
t2 = np.arange(0.0, 5.0, 0.02)

plt.figure(1)          # created by defau lt
plt.subplot(211)
plt.plot(t1, f(t1), 'bo', t2, f(t2), 'k')

plt.subplot(212)
plt.plot(t2, np.cos(2*np.pi*t2), 'r--')
plt.show()


##Another example of using explicit Figure and Axes
import matplotlib.pyplot as plt
import numpy as np

def main():    
    x = np.linspace(0, 6 * np.pi, 100)
        
    fig1, (ax1, ax2) = plt.subplots(nrows=2)    
    plot(x, np.sin(x), ax1)    
    plot(x, np.random.random(100), ax2)
        
    fig2 = plt.figure()    
    plot(x, np.cos(x))
        
    plt.show()

def plot(x, y, ax=None):    
    if ax is None:        
        ax = plt.gca()  #returns current axes    
        line, = ax.plot(x, y, 'go')    
        ax.set_ylabel('Yabba dabba do!')    
        return line

#Attach a subplot to a Figure
def subplot(data, fig=None, index=111):    
    if fig is None:        
        fig = plt.figure()    
        ax = fig.add_subplot(index)    
        ax.plot(data)

# add an axes instance to another figure by append()
import matplotlib.pyplot as plt

fig1, ax = plt.subplots()
ax.plot(range(10))

fig2 = plt.figure()
fig2.axes.append(ax)

plt.show()





###Matplotlib -  Images
import numpy as np
from scipy.misc import imread, imresize
import matplotlib.pyplot as plt

img = imread('data/cat.jpg')
img_tinted = img * [1, 0.95, 0.9]

# Show the original image
plt.subplot(1, 2, 1)
plt.imshow(img)

# Show the tinted image
plt.subplot(1, 2, 2)

# A slight gotcha with imshow is that it might give strange results
# if presented with data that is not uint8. To work around this, we
# explicitly cast the image to uint8 before displaying it.
plt.imshow(np.uint8(img_tinted))
plt.show()






###Matplotlib - Plotting commands summary 
#http://matplotlib.org/api/pyplot_summary.html

#All these functions take any Line2D properties eg  ls='solid', ...
#http://matplotlib.org/api/lines_api.html#matplotlib.lines.Line2D


#use with prefix matplotlib.pyplot.

acorr(x)                    Plot the autocorrelation of  x.
xcorr(x,y)                  Plot the cross correlation between x and y.


bar(left, height)           Make a bar plot with x=left array and y=height array
barbs(...)                  Plot a 2-D field of barbs.
barh(bottom, width)         Make a horizontal bar plot with bottom and width array
boxplot(x)                  Make a box pl ot

cohere(x,y)                 Plot the coherence between x and  y
contour(x,y,z,N)            Plot contours, with egde
contourf(x,y,z,N)           Plot contours without edges

stem(x, y, linefmt, markerfmt, basefmt=)        plots vertical lines (using linefmt) at each x location from the baseline to y, and places a marker there using markerfmt. A horizontal line at 0 is is plotted using basefmt
errorbar(x, y, yerr, xerr, fmt)                 Plot an errorbar grap h.
eventplot(positions, orientation='horizontal')  Plot identical parallel lines at specific positio ns

fill(x,y,fmt)               Plot filled polygons

hexbin(x,y,C)               Make a hexagonal binning plot,  C specifies values at the coordinate (x[i],y[i] ).
hist(x, bins=10)            Plot a histogra m.
hist2d(x, y, bins=10)       Make a 2D histogram plo t.
hlines(y, xmin, xmax)       Plot horizontal lines at each y from xmin to xma x.
vlines(x, ymin,ymax)        Plot vertical lines at each x from ymin to ym ax


loglog(x,y)                 Make a plot with log scaling on both the x and y axi s.
semilogx(x,y)               Make a plot with log scaling on x
semilogy(x,y)               Make a plot with log scaling on  y

pcolor(x2d,y2d,Color1D)     Create a pseudocolor plot of a 2-D arra y.
pcolormesh(x2d,y2d,Color1D) Plot a quadrilateral mesh of 2D array

magnitude_spectrum(x,Fs=2, Fc=0)  Plot the magnitude spectrum. Fs= sampling frequency, Fc=center frequen cy
angle_spectrum(x,Fs=2, Fc=0)      Plot the angle spectru m.
phase_spectrum(x,Fs=2, Fc=0)      Plot the phase spectru m.
psd(x,Fs=2, Fc=0)                 Plot the power spectral densit y.
specgram(x, NFFT=256, Fs=2, Fc=0) Plot a spectrogra m.
csd(x,y)                          Plot the cross-spectral densit y.

pie(x)                          Plot a pie char t.
plot(x,y,fmt)                   Plot lines and/or markers to the Axe s.
plot_date(x,y,fmt)              Plot with data with dates in either x,y or bo th
spy(Z)                          Plot the sparsity pattern on a 2-D array,Z.
polar(theta, r)                 Make a polar plo t.
quiver(x,y,C)                   Plot a 2-D field of arrow s.

step(x,y)                       Make a step plo t.
streamplot(x,y,u,v)             Draws streamlines of a vector flow. x,y=1d array u, v : 2d arrays = x and y-velocities. Number of rows should match length of y, and the number of columns should match x.
stackplot(x,y)                  Draws a stacked area plot. x : 1d array of dimension N y : 2d array of dimension MxN
scatter(x,y)                    Make a scatter plot of x vs y, where x and y are sequence like objects of the same lengt hs


violinplot(dataset)             Make a violin plot for each column of datas  et 





###Matplotlib - example -violinplot 

import random
import numpy as np
import matplotlib.pyplot as plt

# fake data
fs = 10  # fontsize
pos = [1, 2, 4, 5, 7, 8]
data = [np.random.normal(0, std, size=100) for std in pos]

fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(6, 6))

axes[0, 0].violinplot(data, pos, points=20, widths=0.3,
                      showmeans=True, showextrema=True, showmedians=True)
axes[0, 0].set_title('Custom violinplot 1', fontsize=fs)

axes[0, 1].violinplot(data, pos, points=40, widths=0.5,
                      showmeans=True, showextrema=True, showmedians=True,
                      bw_method='silverman')
axes[0, 1].set_title('Custom violinplot 2', fontsize=fs)

axes[0, 2].violinplot(data, pos, points=60, widths=0.7, showmeans=True,
                      showextrema=True, showmedians=True, bw_method=0.5)
axes[0, 2].set_title('Custom violinplot 3', fontsize=fs)

axes[1, 0].violinplot(data, pos, points=80, vert=False, widths=0.7,
                      showmeans=True, showextrema=True, showmedians=True)
axes[1, 0].set_title('Custom violinplot 4', fontsize=fs)

axes[1, 1].violinplot(data, pos, points=100, vert=False, widths=0.9,
                      showmeans=True, showextrema=True, showmedians=True,
                      bw_method='silverman')
axes[1, 1].set_title('Custom violinplot 5', fontsize=fs)

axes[1, 2].violinplot(data, pos, points=200, vert=False, widths=1.1,
                      showmeans=True, showextrema=True, showmedians=True,
                      bw_method=0.5)
axes[1, 2].set_title('Custom violinplot 6', fontsize=fs)

for ax in axes.flatten():
    ax.set_yticklabels([])

fig.suptitle("Violin Plotting Examples")
fig.subplots_adjust(hspace=0.4)
plt.show()




###Matplotlib - Various matplotlib.pyplot. methods 
#all impact current Axes 

##Handling of scale, lim, label, ticks etc
matplotlib.pyplot.annotate(s, xy=(x,y), xytext=(x,y), arrowprops=dict(arrowstyle="->"))
    Create an annotation: a piece of text referring to a data point.
matplotlib.pyplot.arrow(x, y, dx, dy)
    Add an arrow to the axes
matplotlib.pyplot.autoscale(enable=True, axis='both', tight=None)
    Autoscale the axis view to the data (toggle).
matplotlib.pyplot.axis(newLimit_or_string)
    Convenience method to get or set axis properties.
    Get current one  as
    [xmin, xmax, ymin, ymax] = axis() 
    and set a new by 
    axis([xmin, xmax, ymin, ymax])
    string could be 'off', 'equal','scaled', 'tight', 'image','auto'
matplotlib.pyplot.grid(b=None, which='major', axis='both', **kwargs)
    Turn the axes grids on or off.
matplotlib.pyplot.setp(handle, keyword_property)
    Set a property on an artist object. For example 
    line, = plot([1,2,3]);setp(line, linestyle='--')
matplotlib.pyplot.suptitle('this is the figure title', fontsize=12)
    Add a centered title to the figure.
matplotlib.pyplot.text(x, y, s, fontdict=None, withdash=False, **kwargs)
    Add text to the axes.
matplotlib.pyplot.tick_params(axis='both', **kwargs)
    Change the appearance of ticks and tick labels.
matplotlib.pyplot.tight_layout(pad=1.08, h_pad=None, w_pad=None, rect=None)
    Automatically adjust subplot parameters to give specified padding.
matplotlib.pyplot.title(s, fontdict, loc )
    Set a title of the current axes.
    loc : {'center, 'left, 'right}, str, optional
matplotlib.pyplot.xlabel(s, *args, **kwargs)
matplotlib.pyplot.ylabel(s, *args, **kwargs)
    Set the x or y axis label of the current axis.
matplotlib.pyplot.xlim(  (xmin, xmax) )
matplotlib.pyplot.ylim( (ymin, ymax) )
    Get or set the x or y limits of the current axes.
matplotlib.pyplot.xscale(scale, **kwargs)
matplotlib.pyplot.yscale(scale, **kwargs)
    Set the scaling of the x or y-axis.
    scale could be 'linear', 'log(addl keywords basex/basey), 
    'logit', 
    'symlog'(addl keywords basex/basey, linthreshx/linthreshy)
matplotlib.pyplot.xticks(positions_array, labels_array)
matplotlib.pyplot.yticks(positions_array, labels_array)
    Get or set the x or y-limits of the current tick locations and labels.Example:
    xticks( arange(3), ('x', 'y', 'z'))
    xticks( arange(12), calendar.month_name[1:13], rotation=17 )
matplotlib.pyplot.legend()
    Places a legend on the axes. Use like below 
    line, = ax.plot([1, 2, 3], label='Inline label')
    # Overwrite the label by calling the method.
    line.set_label('Label via method')
    ax.legend()
matplotlib.pyplot.locator_params(axis='both', tight=None, **kwargs)
    Control behavior of tick locators.
matplotlib.pyplot.minorticks_off()
    Remove minor ticks from the current plot.
matplotlib.pyplot.minorticks_on()
    Display minor ticks on the current plot.
matplotlib.pyplot.margins(xmargin, ymargin)
    Set or retrieve autoscaling margins.

##Creation of lines 
matplotlib.pyplot.axhline(y=0, xmin=0, xmax=1, hold=None, **kwargs)
    Add a horizontal line across the axis, 
    xmin=0=leftoftheaxis, xmax=1=rightoftheaxis
matplotlib.pyplot.axvline(x=0, ymin=0, ymax=1, hold=None, **kwargs)
    Add a vertical line across the axes.
matplotlib.pyplot.axhspan(ymin, ymax, xmin=0, xmax=1, hold=None, **kwargs)
    Add a horizontal span (rectangle) across the axis.
matplotlib.pyplot.axvspan(xmin, xmax, ymin=0, ymax=1, hold=None, **kwargs)
    Add a vertical span (rectangle) across the axes

##figure and axes setting getting 
matplotlib.pyplot.cla()
    Clear the current axes.
matplotlib.pyplot.clf()
    Clear the current figure.
matplotlib.pyplot.gca()
    Get the current Axes instance on the current figure 
matplotlib.pyplot.gcf()
    Get a reference to the current figure.
matplotlib.pyplot.sca(ax)
    Set the current Axes instance to ax. The current Figure is updated to the parent of ax.
matplotlib.pyplot.sci(im)
    Set the current image. This image will be the target of colormap commands like jet(), hot() or clim()). The current image is an attribute of the current axes.
matplotlib.pyplot.close(figure_number)
    Close a figure window. to close current one close(), to close all, close('all')
matplotlib.pyplot.delaxes(*args)
    Remove an axes from the current figure. If ax doesnt exist, an error will be raised.
    delaxes(): delete the current axes
matplotlib.pyplot.hold(b=None)
    Set the hold state. If b is None (default), toggle the hold state, else set the hold state to boolean value b:
    When hold is True, subsequent plot commands will be added to the current axes. When hold is False, the current axes and figure will be cleared on the next plot command.

##Interactive 
matplotlib.pyplot.waitforbuttonpress(timeout=-1)
    wait for a button press
matplotlib.pyplot.ginput(n=1, timeout=30, show_clicks=True, mouse_add=1, mouse_pop=3, mouse_stop=2)
    This will wait for n clicks from the user and return a list of the coordinates of each click.
matplotlib.pyplot.pause(interval)
    Pause for interval seconds.
matplotlib.pyplot.ioff()
    Turn interactive mode off.
matplotlib.pyplot.ion()
    Turn interactive mode on.
matplotlib.pyplot.ishold()
    Return the hold status of the current axes.
matplotlib.pyplot.isinteractive()
    Return status of interactive mode.

#Figure test, legend etc 
matplotlib.pyplot.figlegend( handles=(Line2D_instance, Patch_instance,...), labels=('label1', 'label2',...),   loc='upper right' )
    Place a legend in the figure.
matplotlib.pyplot.figtext(x, y, s, fontdict=None, **kwargs)
    Add text to figure.

##Image related 
matplotlib.pyplot.imread(file_name_or_url_python_file_obj)
    Read an image from a file into an array.
    format is deduced from the filename
    Return value is a numpy.array. For grayscale images, the return array is MxN. For RGB images, the return value is MxNx3. For RGBA images the return value is MxNx4.
matplotlib.pyplot.imsave(file_name_or_python_file_obj, arr)
    Save an array as in image file.
    output format is deduced from the extension of the filename.
    arr:An MxN (luminance), MxNx3 (RGB) or MxNx4 (RGBA) array.
matplotlib.pyplot.imshow(X, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, shape=None, filternorm=1, filterrad=4.0, imlim=None, resample=None, url=None, hold=None, data=None, **kwargs)
    Display an image on the axes.

##misc 
matplotlib.pyplot.matshow(A, fignum=None, **kw)
    Display an array as a matrix in a new figure window.
matplotlib.pyplot.rgrids(radii, labels=None, angle=22.5, **kwargs)
    Get or set the radial gridlines on a polar plot.
matplotlib.pyplot.thetagrids(angles, labels=None, fmt='%d', frac = 1.1)
    Get or set the theta locations of the gridlines in a polar plot.

##fill between 
matplotlib.pyplot.fill_between(x, y1, y2=0, where=None, interpolate=False, step=None, hold=None, data=None, **kwargs)
    Make filled polygons between two curves
matplotlib.pyplot.fill_betweenx(y, x1, x2=0, where=None, step=None, hold=None, data=None, **kwargs)
    Make filled polygons between two horizontal curves.

##Example 
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(0.0, 2, 0.01)
y1 = np.sin(2*np.pi*x)
y2 = 1.2*np.sin(4*np.pi*x)

fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True)

ax1.fill_between(x, 0, y1)
ax1.set_ylabel('between y1 and 0')

ax2.fill_between(x, y1, 1)
ax2.set_ylabel('between y1 and 1')

ax3.fill_between(x, y1, y2)
ax3.set_ylabel('between y1 and y2')
ax3.set_xlabel('x')

# now fill between y1 and y2 where a logical condition is met.  Note
# this is different than calling
#   fill_between(x[where], y1[where],y2[where]
# because of edge effects over multiple contiguous regions.
fig, (ax, ax1) = plt.subplots(2, 1, sharex=True)
ax.plot(x, y1, x, y2, color='black')
ax.fill_between(x, y1, y2, where=y2 >= y1, facecolor='green', interpolate=True)
ax.fill_between(x, y1, y2, where=y2 <= y1, facecolor='red', interpolate=True)
ax.set_title('fill between where')
           

###Matplotlib -  Event handling
matplotlib.pyplot.connect(s, func)
    Connect event with strings to func.
    Return value is a connection id that can be used with disconnect()
    The signature of func is:
        def func(matplotlib.backend_bases.Event)
    Event string are
    •'button_press_event'
    •'button_release_event'
    •'draw_event'
    •'key_press_event'
    •'key_release_event'
    •'motion_notify_event'
    •'pick_event'
    •'resize_event'
    •'scroll_event'
    •'figure_enter_event',
    •'figure_leave_event',
    •'axes_enter_event',
    •'axes_leave_event'
    •'close_event'
    For the location events (button and key press/release), 
    if the mouse is over the axes,
    the variable event.inaxes will be set to the Axes the event occurs is over,
    the variables event.xdata and event.ydata will be defined with the mouse location in data coords.

#Example 
def on_press(event):    
    print('you pressed', event.button, event.xdata, event.ydata)

cid = plt.connect('button_press_event', on_press)
plt.disconnect(cid)




###Matplotlib - Figure : collection of Axes 
#use below to get figure instance 
matplotlib.pyplot.figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True, FigureClass=<class 'matplotlib.figure.Figure'>, **kwargs)
#https://matplotlib.org/api/figure_api.html#matplotlib.figure.Figure
    figsize= w,h tuple in inches                            

##instance Methods 
fig.add_axes(rect,projection, polar,**kwargs) 
    Add an axes at position rect [left, bottom, width, height] 
    where all quantities are in fractions of figure width and height.
    projection :['aitoff' | 'hammer' | 'lambert' | 'mollweide' | 'polar' | 'rectilinear'], optional The projection type of the axes.
    polar : boolean, optionalIf True, equivalent to projection='polar'.
    also takes keyword arguments for Axes, get properties via plt.setp(ax) 
    Returns current Axes
        fig.add_axes(rect, label='axes1')
        fig.add_axes(rect, label='axes2')
fig.add_subplot(i,j,k, projection, polar,**kwargs)
    1st arg = Either a 3-digit integer or three separate integers 
    If the three integers are I, J, and K, 
    the subplot is the Ith plot on a grid with J rows and K columns.
    For other arg check add_axes()
    Returns current Axes
        fig.add_subplot(111)
autofmt_xdate(bottom=0.2, rotation=30, ha='right')
    Date ticklabels often overlap, so it is useful to rotate them and right align them
    axes
        Read-only: list of axes in Figure
clear()
    Clear the figure – synonym for clf().
clf(keep_observers=False)
    Clear the figure.
delaxes(a)
    remove a from the figure and update the current axes
figimage(X, xo=0, yo=0, alpha=None, norm=None, cmap=None, vmin=None, vmax=None, origin=None, resize=False, **kwargs)
    Adds a non-resampled image to the figure.
    X must be a float array:
        •If X is MxN, assume luminance (grayscale)
        •If X is MxNx3, assume RGB
        •If X is MxNx4, assume RGBA
    #Example 
    import numpy as np
    import matplotlib
    import matplotlib.cm as cm
    import matplotlib.pyplot as plt
    fig = plt.figure()
    Z = np.arange(10000.0)
    Z.shape = 100, 100
    Z[:, 50:] = 1.
    im1 = plt.figimage(Z, xo=50, yo=0, origin='lower')
    im2 = plt.figimage(Z, xo=100, yo=100, alpha=.8, origin='lower')
    plt.show()

gca(**kwargs)
    Get the current axes, creating one if necessary
get_dpi()
    Return the dpi as a float
get_edgecolor()
    Get the edge color of the Figure rectangle
get_facecolor()
    Get the face color of the Figure rectangle
get_figheight()
    Return the figheight as a float
get_figwidth()
    Return the figwidth as a float
get_frameon()
    get the boolean indicating frameon
get_size_inches()
    Returns the current size of the figure in inches (1in == 2.54cm) as an numpy array.
ginput(n=1, timeout=30, show_clicks=True, mouse_add=1, mouse_pop=3, mouse_stop=2)
    Blocking call to interact with the figure.
    This will wait for n clicks from the user and return a list of the coordinates of each click.
legend(handles, labels, *args, **kwargs)
    Place a legend in the figure. Labels are a sequence of strings, handles is a sequence of Line2D or Patch instances, and loc can be a string or an integer specifying the legend location
    legend( (line1, line2, line3),
            ('label1', 'label2', 'label3'),
            'upper right')
savefig(fname, dpi=None, facecolor='w', edgecolor='w', orientation='portrait', papertype=None, format=None,
        transparent=False, bbox_inches=None, pad_inches=0.1,
        frameon=None)
    Save the current figure.
sca(a)
    Set the current axes to be a and return a
set_edgecolor(color)
    Set the edge color of the Figure rectangle
    ACCEPTS: any matplotlib color 
set_facecolor(color)
    Set the face color of the Figure rectangle
set_figheight(val, forward=False)
    Set the height of the figure in inches
    ACCEPTS: float
set_figwidth(val, forward=False)
    Set the width of the figure in inches
    ACCEPTS: float
set_frameon(b)
    Set whether the figure frame (background) is displayed or invisible
    ACCEPTS: boolean
set_size_inches(w, h=None, forward=True)
    Set the figure size in inches (1in == 2.54cm)
    fig.set_size_inches(w,h)  # OR
    fig.set_size_inches((w,h))
set_tight_layout(tight)
    Set whether tight_layout() is used upon drawing. 
    If None, the rcParams['figure.autolayout] value will be set.
subplots_adjust(left=None, bottom=None, right=None, top=None,wspace=None, hspace=None)
    Update the SubplotParams with kwargs (defaulting to rc when None) 
    and update the subplot locations
suptitle(t, **kwargs)
    Add a centered title to the figure.
    kwargs are matplotlib.text.Text properties
    fig.suptitle('this is the figure title', fontsize=12)
text(x, y, s, fontdict=None, **kwargs)
    Add text to figure.
tight_layout(renderer=None, pad=1.08, h_pad=None, w_pad=None, rect=None)
    Adjust subplot parameters to give specified padding.


     

###Matplotlib - Axes 
#https://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes
#The Axes contains most of the figure elements: Axis, Tick, Line2D, Text, Polygon, etc., 
#and sets the coordinate system.

#https://matplotlib.org/api/artist_api.html
Artist 
    _AxisBase
        Axis
            PolarAxis
            GeoAxis 
                MollweideAxes
                AitoffAxes
                HammerAxes
                LambertAxes
    Axis
        YAxis
        XAxis
    Table 
    QuiverKey 
    Line2D
    Legend
    Figure 
    Patch 
        Polygon 
            FancyArrow 
        FancyBboxPatch 
        PathPatch 
        Shadow
        Spine
        Wedge
        YAArrow
        Ellipse 
            Arc 
            Circle 
        Arrow 
            Rectangle
                Cell 
        RegularPolygon 
            CirclePolygon 
        FancyArrowPatch 
            ConnectorPatch 
    AnnotationBbox 
    Text 
        Annotation 
        ClabelText
        TextWithDash
    OffsetBox
        DrawingArea
        PackerBase 
            HPacker
            VPacker
        OffsetImage
        PaddedBox
        TextArea
        AnchoredOffsetBox
            Anchoredtext
        AuxTransformBox 
    Tick 
        YTick
        XTick 
    _ImageBase
        BboxImage
        FigureImage
        AxesImage   
            PcolorImage
            NonUniformImage
    Collection 
        TinMesh
        _CollectionWithStats
            PolyCOllection
                Barbs
                BrokenBarhCollection
                Quiver
            CircleCOllection
            PatchCollection
            RegularPolyCollection
                StarPolygonCollection
                AsteriskPolygonCollection
        EllipseCollection
        LineCOllection
        PatchCollection
        QuadMesh
        

#Creation 
#use 
fig,Axes_ndarray = matplotlib.pyplot.subplots(nrows=1, ncols=1,...)
#or 
ax = fig.add_axes(rect) 
#or 
ax = fig.add_subplot(i,j,k)

##Instance methods
#Basic
Axes.plot           Plot lines and/or markers to the Axes. 
Axes.errorbar       Plot an errorbar graph. 
Axes.scatter        Make a scatter plot of x vs y 
Axes.plot_date      A plot with data that contains dates. 
Axes.step           Make a step plot. 
Axes.loglog         Make a plot with log scaling on both the x and y axis. 
Axes.semilogx       Make a plot with log scaling on the x axis. 
Axes.semilogy       Make a plot with log scaling on the y axis. 
Axes.fill_between   Make filled polygons between two curves. 
Axes.fill_betweenx  Make filled polygons between two horizontal curves. 
Axes.bar            Make a bar plot. 
Axes.barh           Make a horizontal bar plot. 
Axes.stem           Create a stem plot. 
Axes.eventplot      Plot identical parallel lines at specific positions. 
Axes.pie            Plot a pie chart. 
Axes.stackplot      Draws a stacked area plot. 
Axes.broken_barh    Plot horizontal bars. 
Axes.vlines         Plot vertical lines. 
Axes.hlines         Plot horizontal lines at each y from xmin to xmax. 
Axes.fill           Plot filled polygons. 


#Spans
Axes.axhline        Add a horizontal line across the axis. 
Axes.axhspan        Add a horizontal span (rectangle) across the axis. 
Axes.axvline        Add a vertical line across the axes. 
Axes.axvspan        Add a vertical span (rectangle) across the axes. 

#Spectral
Axes.acorr          Plot the autocorrelation of x. 
Axes.angle_spectrum Plot the angle spectrum. 
Axes.cohere         Plot the coherence between x and y. 
Axes.csd            Plot the cross-spectral density. 
Axes.magnitude_spectrum Plot the magnitude spectrum. 
Axes.phase_spectrum Plot the phase spectrum. 
Axes.psd            Plot the power spectral density. 
Axes.specgram       Plot a spectrogram. 
Axes.xcorr          Plot the cross correlation between x and y. 

#Statistics
Axes.boxplot        Make a box and whisker plot. 
Axes.violinplot     Make a violin plot. 
Axes.violin         Drawing function for violin plots. 
Axes.bxp            Drawing function for box and whisker plots. 

#Binned
Axes.hexbin         Make a hexagonal binning plot. 
Axes.hist           Plot a histogram. 
Axes.hist2d         Make a 2D histogram plot. 

#Contours
Axes.clabel         Label a contour plot. 
Axes.contour        Plot contours. 
Axes.contourf       Plot contours. 

#Array
Axes.imshow         Display an image on the axes. 
Axes.matshow        Plot a matrix or array as an image. 
Axes.pcolor         Create a pseudocolor plot of a 2-D array. 
Axes.pcolorfast     pseudocolor plot of a 2-D array 
Axes.pcolormesh     Plot a quadrilateral mesh. 
Axes.spy            Plot the sparsity pattern on a 2-D array. 

#Unstructured Triangles
Axes.tripcolor      Create a pseudocolor plot of an unstructured triangular grid. 
Axes.triplot        Draw a unstructured triangular grid as lines and/or markers. 
Axes.tricontour     Draw contours on an unstructured triangular grid. 
Axes.tricontourf    Draw contours on an unstructured triangular grid. 

#Text and Annotations
Axes.annotate       Annotate the point xy with text s. 
Axes.text           Add text to the axes. 
Axes.table          Add a table to the current axes. 
Axes.arrow          Add an arrow to the axes. 

#Fields
Axes.barbs          Plot a 2-D field of barbs. 
Axes.quiver         Plot a 2-D field of arrows. 
Axes.quiverkey      Add a key to a quiver plot. 
Axes.streamplot     Draws streamlines of a vector flow. 

#Clearing
Axes.cla            Clear the current axes. 
Axes.clear          clear the axes 

#Appearance
Axes.axis           Set axis properties. 
Axes.set_axis_off   turn off the axis 
Axes.set_axis_on    turn on the axis 
Axes.set_frame_on   Set whether the axes rectangle patch is drawn 
Axes.get_frame_on   Get whether the axes rectangle patch is drawn 
Axes.set_axisbelow  Set whether the axis ticks and gridlines are above or below most 
Axes.get_axisbelow  Get whether axis below is true or not 
Axes.grid           Turn the axes grids on or off. 
Axes.get_axis_bgcolor 
Axes.get_facecolor  
Axes.get_fc  
Axes.set_facecolor  
Axes.set_fc  
Axes.set_axis_bgcolor 

#Property cycle
Axes.set_prop_cycle     Set the property cycle for any future plot commands on this Axes. 
Axes.set_color_cycle    Set the color cycle for any future plot commands on this Axes. 

#Axis / limits
Axes.get_yaxis          Return the YAxis instance 
Axes.get_xaxis          Return the XAxis instance 

#Axis Limits and direction
Axes.invert_xaxis       Invert the x-axis. 
Axes.invert_yaxis       Invert the y-axis. 
Axes.xaxis_inverted     Returns True if the x-axis is inverted. 
Axes.yaxis_inverted     Returns True if the y-axis is inverted. 
Axes.set_xlim           Set the data limits for the x-axis 
Axes.set_ylim           Set the data limits for the y-axis 
Axes.get_ylim           Get the y-axis range 
Axes.get_xlim           Get the x-axis range 
Axes.update_datalim     Update the data lim bbox with seq of xy tups or equiv. 
Axes.update_datalim_bounds Update the datalim to include the given 
Axes.update_datalim_numerix 
Axes.set_ybound         Set the lower and upper numerical bounds of the y-axis. 
Axes.set_xbound         Set the lower and upper numerical bounds of the x-axis. 
Axes.get_ybound         Return y-axis numerical bounds in the form of 
Axes.get_xbound         Returns the x-axis numerical bounds 


#Axis Labels, title, and legend
Axes.get_xlabel         Get the xlabel text string. 
Axes.get_ylabel         Get the ylabel text string. 
Axes.set_xlabel         Set the label for the xaxis. 
Axes.set_ylabel         Set the label for the yaxis 
Axes.set_title          Set a title for the axes. 
Axes.get_title          Get an axes title. 
Axes.legend             Places a legend on the axes. 
Axes.get_legend         Return the legend.Legend instance, or None if no legend is defined 
Axes.get_legend_handles_labels Return handles and labels for legend 

#Axis scales
Axes.set_xscale         Set the x-axis scale 
Axes.get_xscale         Return the xaxis scale string: linear, log, logit, symlog 
Axes.get_yscale         Return the yaxis scale string: linear, log, logit, symlog 
Axes.set_yscale         Set the y-axis scale 

#Autoscaling and margins
Axes.use_sticky_edges   When autoscaling, whether to obey all Artist.sticky_edges. 
Axes.margins            Set or retrieve autoscaling margins. 
Axes.set_xmargin        Set padding of X data limits prior to autoscaling. 
Axes.set_ymargin        Set padding of Y data limits prior to autoscaling. 
Axes.relim              Recompute the data limits based on current artists. 
Axes.autoscale          Autoscale the axis view to the data (toggle). 
Axes.autoscale_view     Autoscale the view limits using the data limits. 
Axes.get_autoscale_on   Get whether autoscaling is applied for both axes on plot commands 
Axes.set_autoscale_on   Set whether autoscaling is applied on plot commands 
Axes.get_autoscalex_on  Get whether autoscaling for the x-axis is applied on plot commands 
Axes.set_autoscalex_on  Set whether autoscaling for the x-axis is applied on plot commands 
Axes.get_autoscaley_on  Get whether autoscaling for the y-axis is applied on plot commands 
Axes.set_autoscaley_on  Set whether autoscaling for the y-axis is applied on plot commands 

#Aspect ratio
Axes.apply_aspect       Use _aspect() and _adjustable() to modify the axes box or the view limits. 
Axes.get_aspect  
Axes.set_aspect aspect 
Axes.get_adjustable  
Axes.set_adjustable     ACCEPTS: [ 'box | 'datalim | 'box-forced] 

#Ticks and tick labels
Axes.xaxis_date         Sets up x-axis ticks and labels that treat the x data as dates. 
Axes.yaxis_date         Sets up y-axis ticks and labels that treat the y data as dates. 
Axes.get_xmajorticklabels Get the xtick labels as a list of Text instances. 
Axes.get_xminorticklabels Get the x minor tick labels as a list of matplotlib.text.Text instances. 
Axes.get_xticklabels    Get the x tick labels as a list of Text instances. 
Axes.get_xticklines     Get the xtick lines as a list of Line2D instances 
Axes.get_xticks         Return the x ticks as a list of locations 
Axes.get_ymajorticklabels Get the major y tick labels as a list of Text instances. 
Axes.get_yminorticklabels Get the minor y tick labels as a list of Text instances. 
Axes.get_yticklabels    Get the x tick labels as a list of Text instances. 
Axes.get_yticklines     Get the ytick lines as a list of Line2D instances 
Axes.get_yticks         Return the y ticks as a list of locations 
Axes.minorticks_off     Remove minor ticks from the axes. 
Axes.minorticks_on      Add autoscaling minor ticks to the axes. 
Axes.set_xticklabels    Set the xtick labels with list of strings labels 
Axes.set_xticks         Set the x ticks with list of ticks 
Axes.set_yticklabels    Set the y tick labels with list of strings labels 
Axes.set_yticks         Set the y ticks with list of ticks 
Axes.get_xgridlines     Get the x grid lines as a list of Line2D instances 
Axes.get_ygridlines     Get the y grid lines as a list of Line2D instances 
Axes.ticklabel_format   Change the ScalarFormatter used by default for linear axes. 
Axes.tick_params        Change the appearance of ticks and tick labels. 
Axes.locator_params     Control behavior of tick locators. 

#Units
Axes.convert_xunits     For artists in an axes, if the xaxis has units support, 
Axes.convert_yunits     For artists in an axes, if the yaxis has units support, 
Axes.have_units         Return True if units are set on the x or y axes 

#Adding Artists
Axes.add_artist         Add any Artist to the axes. 
Axes.add_collection     Add a Collection instance to the axes. 
Axes.add_container      Add a Container instance to the axes. 
Axes.add_image          Add a AxesImage to the axes. 
Axes.add_line           Add a Line2D to the list of plot 
Axes.add_patch          Add a Patch p to the list of axes patches; the clipbox will be set to the Axes clipping box. 
Axes.add_table          Add a Table instance to the 

#Twinning
Axes.twinx Create a twin Axes sharing the xaxis 
Axes.twiny Create a twin Axes sharing the yaxis 
Axes.get_shared_x_axes  Return a copy of the shared axes Grouper object for x axes 
Axes.get_shared_y_axes  Return a copy of the shared axes Grouper object for y axes 

#Axes Position
Axes.get_anchor  
Axes.set_anchor anchor 
Axes.get_axes_locator   return axes_locator 
Axes.set_axes_locator   set axes_locator 
Axes.reset_position     Make the original position the active position 
Axes.get_position       Return the a copy of the axes rectangle as a Bbox 
Axes.set_position       Set the axes position 

#Async/Event based
Axes.stale              If the artist is 'stale and needs to be re-drawn for the output to match the internal state of the artist. 
Axes.pchanged           Fire an event when property changed, calling all of the registered callbacks. 
Axes.add_callback       Adds a callback function that will be called whenever one of the Artist's properties changes. 
Axes.remove_callback    Remove a callback based on its id. 

#Interactive
Axes.can_pan            Return True if this axes supports any pan/zoom button functionality. 
Axes.can_zoom           Return True if this axes supports the zoom box button functionality. 
Axes.get_navigate       Get whether the axes responds to navigation commands 
Axes.set_navigate       Set whether the axes responds to navigation toolbar commands 
Axes.get_navigate_mode  Get the navigation toolbar button status: 'PAN, 'ZOOM, or None 
Axes.set_navigate_mode  Set the navigation toolbar button status; 
Axes.start_pan          Called when a pan operation has started. 
Axes.drag_pan           Called when the mouse moves during a pan operation. 
Axes.end_pan            Called when a pan operation completes (when the mouse button 
Axes.format_coord       Return a format string formatting the x, y coord 
Axes.format_cursor_data Return cursor data string formatted. 
Axes.format_xdata       Return x string formatted. 
Axes.format_ydata       Return y string formatted. 
Axes.hitlist            List the children of the artist which contain the mouse event event. 
Axes.mouseover  
Axes.in_axes            Return True if the given mouseevent (in display coords) 
Axes.pick               Trigger pick event 
Axes.pickable           Return True if Artist is pickable. 
Axes.get_picker         Return the picker object used by this artist 
Axes.set_picker         Set the epsilon for picking used by this artist 
Axes.set_contains       Replace the contains test used by this artist. 
Axes.get_contains       Return the _contains test used by the artist, or None for default. 
Axes.contains           Test whether the mouse event occured in the axes. 
Axes.contains_point     Returns True if the point (tuple of x,y) is inside the axes (the area defined by the its patch). 
Axes.get_cursor_data    Get the cursor data for a given event. 
Axes.get_cursor_props   Return the cursor propertiess as a (linewidth, color) 
Axes.set_cursor_props   Set the cursor property as 

#Children
Axes.get_children       return a list of child artists 
Axes.get_images         return a list of Axes images contained by the Axes 
Axes.get_lines          Return a list of lines contained by the Axes 
Axes.findobj            Find artist objects. 

#Drawing
Axes.draw               Draw everything (plot lines, axes, labels) 
Axes.draw_artist        This method can only be used after an initial draw which caches the renderer. 
Axes.redraw_in_frame    This method can only be used after an initial draw which caches the renderer. 
Axes.get_renderer_cache  
Axes.get_rasterization_zorder Get zorder value below which artists will be rasterized 
Axes.set_rasterization_zorder Set zorder value below which artists will be rasterized. 
Axes.get_window_extent        get the axes bounding box in display space; args and 
Axes.get_tightbbox            Return the tight bounding box of the axes. 

#Bulk property manipulation
Axes.set                A property batch setter. 
Axes.update             Update the properties of this Artist from the dictionary prop. 
Axes.properties         return a dictionary mapping property name -> value for all Artist props 
Axes.update_from        Copy properties from other to self. 

#General Artist Properties
Axes.set_alpha          Set the alpha value used for blending - not supported on all backends. 
Axes.set_animated       Set the artists animation state. 
Axes.set_clip_box       Set the artists clip Bbox. 
Axes.set_clip_on        Set whether artist uses clipping. 
Axes.set_clip_path      Set the artists clip path, which may be: 
Axes.set_gid            Sets the (group) id for the artist 
Axes.set_label          Set the label to s for auto legend. 
Axes.set_url            Sets the url for the artist 
Axes.set_visible        Set the artists visiblity. 
Axes.set_zorder         Set the zorder for the artist. 
Axes.set_rasterized     Force rasterized (bitmap) drawing in vector backend output. 
Axes.set_sketch_params  Sets the sketch parameters. 
Axes.set_agg_filter     set agg_filter fuction. 
Axes.set_snap           Sets the snap setting which may be: 
Axes.set_transform      Set the Transform instance used by this artist. 
Axes.set_path_effects   set path_effects, which should be a list of instances of 
Axes.get_agg_filter     return filter function to be used for agg filter 
Axes.get_sketch_params  Returns the sketch parameters for the artist. 
Axes.get_alpha          Return the alpha value used for blending - not supported on all 
Axes.get_animated       Return the artists animated state 
Axes.get_clip_box       Return artist clipbox 
Axes.get_clip_on        Return whether artist uses clipping 
Axes.get_clip_path      Return artist clip path 
Axes.get_gid            Returns the group id 
Axes.get_label          Get the label used for this artist in the legend. 
Axes.get_url            Returns the url 
Axes.get_visible        Return the artists visiblity 
Axes.get_zorder         Return the Artist's zorder. 
Axes.get_rasterized     return True if the artist is to be rasterized 
Axes.get_transform      Return the Transform instance used by this artist. 
Axes.get_snap           Returns the snap setting which may be: 
Axes.get_path_effects  
Axes.axes               The Axes instance the artist resides in, or None. 
Axes.get_axes           Return the Axes instance the artist resides in, or None. 
Axes.set_axes           Set the Axes instance in which the artist resides, if any. 
Axes.set_figure         Set the class:Axes figure 
Axes.get_figure         Return the Figure instance the artist belongs to. 

#Artist Methods
Axes.is_figure_set      Returns True if the artist is assigned to a Figure. 
Axes.remove             Remove the artist from the figure if possible. 
Axes.is_transform_set   Returns True if Artist has a transform explicitly set. 

#Projection
#Methods used by Axis that must be overridden for non-rectilinear Axes.
Axes.name  
Axes.get_xaxis_transform    Get the transformation used for drawing x-axis labels, ticks and gridlines. 
Axes.get_yaxis_transform    Get the transformation used for drawing y-axis labels, ticks and gridlines. 
Axes.get_data_ratio         Returns the aspect ratio of the raw data. 
Axes.get_data_ratio_log     Returns the aspect ratio of the raw data in log scale. 
Axes.get_xaxis_text1_transform Get the transformation used for drawing x-axis labels, which will add the given amount of padding (in points) between the axes and the label. 
Axes.get_xaxis_text2_transform Get the transformation used for drawing the secondary x-axis labels, which will add the given amount of padding (in points) between the axes and the label. 
Axes.get_yaxis_text1_transform Get the transformation used for drawing y-axis labels, which will add the given amount of padding (in points) between the axes and the label. 
Axes.get_yaxis_text2_transform Get the transformation used for drawing the secondary y-axis labels, which will add the given amount of padding (in points) between the axes and the label. 

#Other
Axes.zorder  
Axes.aname  
Axes.get_default_bbox_extra_artists  
Axes.get_transformed_clip_path_and_affine Return the clip path with the non-affine part of its transformation applied, and the remaining affine part of its transformation. 
Axes.has_data               Return True if any artists have been added to axes. 
Axes.hold 
Axes.ishold 
 


###Matplotlib - Colourmap - Setting a range of colors for various items 

#How to set color map for an image, pcolor, scatter, etc,
imshow(X, cmap=cm.hot)
#or
imshow(X);pyplot.set_cmap('hot')
#Or
pyplot.hot()

##List of colormap
 #base colormaps 
 autumn, bone, cool, copper, flag, gray, hot,
 hsv,inferno,jet,magma,pink,plasma,prism,spring,summer,viridis,winter 
 #Scientific colormaps
 gist_earth,gist_heat,gist_ncar,gist_rainbow,gist_stern        
 #Other 
 BrBG,PiYG ,PRGn ,PuOr,RdBu,RdGy ,RdYlBu,RdYlGn,Spectral  
 Blues,BuGn,BuPu,GnBu ,Greens,Greys,Oranges,OrRd,PuBu,PuBuGn ,PuRd,Purples,RdPu,Reds,YlGn,YlGnBu,YlOrBr,YlOrRd    
 Accent,Dark2,Paired,Pastel1,Pastel2,Set1,Set2,Set3
 afmhot,brg,bwr,coolwarm,CMRmap,cubehelix,gnuplot,gnuplot2,ocean,rainbow,seismic nipy_spectral,terrain


 
 
###Matplotlib - Working with Text

#text() can be used to add text in an arbitrary location
#xlabel(), ylabel() and title() are used to add text in the indicated locations


import numpy as np
import matplotlib.pyplot as plt

mu, sigma = 100, 15
x = mu + sigma * np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='g', alpha=0.75)


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title('Histogram of IQ')
plt.text(60, .025, r'$\mu=100,\ \sigma=15$') #returns matplotlib.text.Text
plt.axis([40, 160, 0, 0.03])
plt.grid(True)
plt.show()


#customize the properties by passing keyword arguments into the text functions or using setp():
t = plt.xlabel('my data', fontsize=14, color='red')
plt.setp(t)

#Using mathematical expressions in text - use TeX
# check http://matplotlib.org/users/mathtext.html for Tex syntax
# Must be under r'$....$' and each special character is prefixed by \
plt.title(r'$\sigma_i=15$')
plt.title(r'$\alpha > \beta$')


## Annotating text
#annotate() helps to make annotations 
#the location being annotated represented by the argument  xy
#the location of the text xytext

import numpy as np
import matplotlib.pyplot as plt

ax = plt.subplot(111)

t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)
line, = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5), 
    arrowprops=dict(facecolor='black', shrink=0.05))

plt.ylim(-2,2)
plt.show()



###Matplotlib - Logarithmic and other nonlinear axis  
#use plt.xscale('log'), plt.yscale('log')


import numpy as np
import matplotlib.pyplot as plt

# make up some data in the interval ]0, 1[
y = np.random.normal(loc=0.5, scale=0.4, size=1000)
y = y[(y > 0) & (y < 1)]
y.sort()
x = np.arange(len(y))

# plot with various axes scales
plt.figure(1)

# linear
plt.subplot(221)
plt.plot(x, y)
plt.yscale('linear')
plt.title('linear')
plt.grid(True)


# log
plt.subplot(222)
plt.plot(x, y)
plt.yscale('log')
plt.title('log')
plt.grid(True)


# symmetric log
plt.subplot(223)
plt.plot(x, y - y.mean())
plt.yscale('symlog', linthreshy=0.05)
plt.title('symlog')
plt.grid(True)

# logit
plt.subplot(224)
plt.plot(x, y)
plt.yscale('logit')
plt.title('logit')
plt.grid(True)

plt.show()




###Matplotlib - Customizing plots with style sheets

#There are a number of pre-defined styles provided by matplotlib.
#For example, theres a pre-defined style called 'ggplot'

import matplotlib.pyplot as plt
plt.style.use('ggplot')


#To list all available styles, use:
print(plt.style.available)



##Defining your own style

#mpl_configdir is obtained from matplotlib.get_configdir(),
# default is ~/.config/matplotlib
#mpl_configdir/stylelib/presentation.mplstyle
axes.titlesize : 24
axes.labelsize : 20
lines.linewidth : 3
lines.markersize : 10
xtick.labelsize : 16
ytick.labelsize : 16

#Use
import matplotlib.pyplot as plt
plt.style.use('presentation')

#Composing styles
import matplotlib.pyplot as plt 
plt.style.use(['dark_background', 'presentation']) #two style sheet

#Temporary styling
import numpy as np
import matplotlib.pyplot as plt
with plt.style.context(('dark_background')):    
plt.plot(np.sin(np.linspace(0, 2 * np.pi)), 'r-o')

plt.show()




###Matplotlib - The matplotlibrc file
#matplotlib uses matplotlibrc configuration files to customize 
#all kinds of properties, which we call rc settings or rc parameters 
#http://matplotlib.org/users/customizing.html

#Dynamic rc settings
#All of the rc settings are stored in a dictionary-like variable called 
#matplotlib.rcParams, which is global to the matplotlib package. 

#rcParams can be modified directly, for example:
import matplotlib as mpl
mpl.rcParams['lines.linewidth'] = 2
mpl.rcParams['lines.color'] = 'r'

#Or using rc()
import matplotlib as mpl
mpl.rc('lines', linewidth=2, color='r')
# matplotlib.rcdefaults() command will restore the standard matplotlib default settings.




###Matplotlib - Customizing Location of Subplot Using GridSpec

#subplot2grid 
matplotlib.pyplot.subplot2grid(shape, loc, rowspan=1, colspan=1, **kwargs)
    shape = (numrows,numcols), activated_ax= (ithrow, jthcol)
#unlike matplotlibs subplot, the index starts from 0 in gridspec
ax = plt.subplot2grid((2,2),(0, 0))  # same as , ax = plt.subplot(2,2,1)


#To create a subplot that spans multiple cells,
ax2 = plt.subplot2grid((3,3), (1, 0), colspan=2) #3x3 plots with 2nd row, 1st col is covering 2 cols
ax3 = plt.subplot2grid((3,3), (1, 2), rowspan=2)


#Example to create 6x6 subplots
import numpy as np
import matplotlib.pyplot as p lt
fig1= plt.figure(1)
ax1 = plt.subplot2grid((3,3), (0,0), colspan=3)
ax2 = plt.subplot2grid((3,3), (1,0), colspan=2)
ax3 = plt.subplot2grid((3,3), (1, 2), rowspan=2)
ax4 = plt.subplot2grid((3,3), (2, 0))
ax5 = plt.subplot2grid((3,3), (2, 1))
fig2= plt.figure(2)
ax11 = plt.subplot2grid((3,3), (0,0), colspan=3)
ax21 = plt.subplot2grid((3,3), (1,0), colspan=2)
ax31 = plt.subplot2grid((3,3), (1, 2), rowspan=2)
ax41 = plt.subplot2grid((3,3), (2, 0))
ax51 = plt.subplot2grid((3,3), (2, 1))
#Now use ax1.plot() to direct commands to ax1, etc
x = np.linspace(-5,5,20)
y = np.sin(x)
ax1.plot(x,y,"r-")
ax11.plot(x,y,"b-")
plt.tight_layout()
plt.show()



###Matplotlib -  Saving to a file
#output format is deduced from the extension of the filename
matplotlib.pyplot.savefig(file_name)



###Matplotlib - Example of   a stacked bar plot with errorbars
import numpy as np
import matplotlib.pyplot as plt


N = 5
menMeans = (20, 35, 30, 35, 27)
womenMeans = (25, 32, 34, 20, 25)
menStd = (2, 3, 4, 1, 2)
womenStd = (3, 5, 2, 3, 3)
ind = np.arange(N)    # the x locations for the groups
width = 0.35       # the width of the bars: can also be len(x) sequence

p1 = plt.bar(ind, menMeans, width, color='r', yerr=menStd)
p2 = plt.bar(ind, womenMeans, width, color='y',             
        bottom=menMeans, yerr=womenStd)

plt.ylabel('Scores')
plt.title('Scores by group and gender')
plt.xticks(ind + width/2., ('G1', 'G2', 'G3', 'G4', 'G5'))
plt.yticks(np.arange(0, 81, 10))
plt.legend((p1[0], p2[0]), ('Men', 'Women'))

plt.show()



###Matplotlib - Example of Boxplot

import numpy as np
import matplotlib.pyplot as plt

# fake data
np.random.seed(937)
data = np.random.lognormal(size=(37, 4), mean=1.5, sigma=1.75)
labels = list('ABCD')
fs = 10  # fontsize

# demonstrate how to toggle the display of different elements:
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(6, 6))
axes[0, 0].boxplot(data, labels=labels)
axes[0, 0].set_title('Default', fontsize=fs)

axes[0, 1].boxplot(data, labels=labels, showmeans=True)
axes[0, 1].set_title('showmeans=True', fontsize=fs)

axes[0, 2].boxplot(data, labels=labels, showmeans=True, meanline=True)
axes[0, 2].set_title('showmeans=True,\nmeanline=True', fontsize=fs)

axes[1, 0].boxplot(data, labels=labels, showbox=False, showcaps=False)
axes[1, 0].set_title('Tufte Style \n(showbox=False,\nshowcaps=False)', fontsize=fs)

axes[1, 1].boxplot(data, labels=labels, notch=True, bootstrap=10000)
axes[1, 1].set_title('notch=True,\nbootstrap=10000', fontsize=fs)

axes[1, 2].boxplot(data, labels=labels, showfliers=False)
axes[1, 2].set_title('showfliers=False', fontsize=fs)

for ax in axes.flatten():    
    ax.set_yscale('log')    
    ax.set_yticklabels([])

fig.subplots_adjust(hspace=0.4)
plt.show()


# demonstrate how to customize the display different elements:
boxprops = dict(linestyle='--', linewidth=3, color='darkgoldenrod')
flierprops = dict(marker='o', markerfacecolor='green', markersize=12,                  
linestyle='none')
medianprops = dict(linestyle='-.', linewidth=2.5, color='firebrick')
meanpointprops = dict(marker='D', markeredgecolor='black',                      
markerfacecolor='firebrick')
meanlineprops = dict(linestyle='--', linewidth=2.5, color='purple')

fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(6, 6))
axes[0, 0].boxplot(data, boxprops=boxprops)
axes[0, 0].set_title('Custom boxprops', fontsize=fs)

axes[0, 1].boxplot(data, flierprops=flierprops, medianprops=medianprops)
axes[0, 1].set_title('Custom medianprops\nand flierprops', fontsize=fs)

axes[0, 2].boxplot(data, whis='range')
axes[0, 2].set_title('whis="range"', fontsize=fs)

axes[1, 0].boxplot(data, meanprops=meanpointprops, meanline=False,                   
showmeans=True)
axes[1, 0].set_title('Custom mean\nas point', fontsize=fs)

axes[1, 1].boxplot(data, meanprops=meanlineprops, meanline=True, showmeans=True)
axes[1, 1].set_title('Custom mean\nas line', fontsize=fs)

axes[1, 2].boxplot(data, whis=[15, 85])
axes[1, 2].set_title('whis=[15, 85]\n#percentiles', fontsize=fs)

for ax in axes.flatten():    
    ax.set_yscale('log')    
    ax.set_yticklabels([])

fig.suptitle("I never said they'd be pretty")
fig.subplots_adjust(hspace=0.4)
plt.show()



###Matplotlib - Example of scatter


import numpy as np
import matplotlib.pyplot as plt


N = 50
x = np.random.rand(N)
y = np.random.rand(N)
colors = np.random.rand(N)
area = np.pi * (15 * np.random.rand(N))**2  # 0 to 15 point radiuses

plt.scatter(x, y, s=area, c=colors, alpha=0.5)
plt.show()



###Matplotlib - Example of Histogram

import numpy as np
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt


# example data
mu = 100  # mean of distribution
sigma = 15  # standard deviation of distribution
x = mu + sigma * np.random.randn(10000)

num_bins = 50
# the histogram of the data
n, bins, patches = plt.hist(x, num_bins, normed=1, facecolor='green', alpha=0.5)
# add a 'best fit' line
y = mlab.normpdf(bins, mu, sigma)
plt.plot(bins, y, 'r--')
plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'Histogram of IQ: $\mu=100$, $\sigma=15$')

# Tweak spacing to prevent clipping of ylabel
plt.subplots_adjust(left=0.15)
plt.show()


###Matplotlib - Example of coutour
import matplotlib
import numpy as np
import matplotlib.cm as cm
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

matplotlib.rcParams['xtick.direction'] = 'out'
matplotlib.rcParams['ytick.direction'] = 'out'

delta = 0.025
x = np.arange(-3.0, 3.0, delta)
y = np.arange(-2.0, 2.0, delta)
X, Y = np.meshgrid(x, y)
Z1 = mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)
Z2 = mlab.bivariate_normal(X, Y, 1.5, 0.5, 1, 1)
# difference of Gaussians
Z = 10.0 * (Z2 - Z1)


# Create a simple contour plot with labels using default colors.  The
# inline argument to clabel will control whether the labels are draw
# over the line segments of the contour, removing the lines beneath
# the label
plt.figure()
CS = plt.contour(X, Y, Z)
plt.clabel(CS, inline=1, fontsize=10)
plt.title('Simplest default with labels')

# And you can manually specify the colors of the contour
plt.figure()
CS = plt.contour(X, Y, Z, 6,                 
    linewidths=np.arange(.5, 4, .5),                 
    colors=('r', 'green', 'blue', (1, 1, 0), '#afeeee', '0.5')                 
    )
plt.clabel(CS, fontsize=9, inline=1)
plt.title('Crazy lines')
 

###Matplotlib - Animation example

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation


def data_gen(t=0):    
    cnt = 0    
    while cnt < 1000:        
        cnt += 1        
        t += 0.1        
        yield t, np.sin(2*np.pi*t) * np.exp(-t/10.)


def init():    
    ax.set_ylim(-1.1, 1.1)    
    ax.set_xlim(0, 10)    
    del xdata[:]    
    del ydata[:]    
    line.set_data(xdata, ydata)    
    return line,

fig, ax = plt.subplots()
line, = ax.plot([], [], lw=2)
ax.grid()
xdata, ydata = [], []


def run(data):    
    # update the data    
    t, y = data    
    xdata.append(t)    
    ydata.append(y)    
    xmin, xmax = ax.get_xlim()
        
    if t >= xmax:        
    ax.set_xlim(xmin, 2*xmax)        
    ax.figure.canvas.draw()    
    line.set_data(xdata, ydata)
        
    return line,

ani = animation.FuncAnimation(fig, run, data_gen, blit=False, interval=10,                              
repeat=False, init_func=init)
plt.show()


















###Pandas - Introduction 

#functionality

1.DataFrame object for data manipulation with integrated indexing
2.Tools for reading and writing data between in-memory data structures and different file formats
3.Data alignment and integrated handling of missing data
4.Reshaping and pivoting of data sets
5.Label-based slicing, fancy indexing, and subsetting of large data sets
6.Data structure column insertion and deletion
7.Group by engine allowing split-apply-combine operations on data sets
8.Data set merging and joining
9.Hierarchical axis indexing to work with high-dimensional data in a lower-dimensional data structure
10.Time series-functionality: date range generation and frequency conversion, moving window statistics, moving window linear regressions, date shifting and lagging


###Pandas - io with Quick Example
import numpy as np
import pandas as pd

data="""cruiseid  year  station  month  day  date        lat        lon         depth_w  taxon                        count        
AA8704    1987  1        04     13   13-APR-87   35.85      -75.48      18       Centropages_typicus          75343        
AA8704    1987  1        04     13   13-APR-87   35.85      -75.48      18       Gastropoda                     0        
AA8704    1987  1        04     13   13-APR-87   35.85      -75.48      18       Calanus_finmarchicus         2340        
AA8704    1987  1        07     13   13-JUL-87   35.85      -75.48      18       Acartia_spp.                 5616        
AA8704    1987  1        07     13   13-JUL-87   35.85      -75.48      18       Metridia_lucens              468        
AA8704    1987  1        08     13   13-AUG-87   35.85      -75.48      18       Evadne_spp.                        0        
AA8704    1987  1        08     13   13-AUG-87   35.85      -75.48      18       Salpa                              0        
AA8704    1987  1        08     13   13-AUG-87   35.85      -75.48      18       Oithona_spp.                 468
"""
datafile = open('data.txt','w')
datafile.write(data)
datafile.close()

#read - header row is zeroth row, index col is "Datetime"
#and created from parse_dates = column 1(year), 3(month), and 4(day) (0 based)
#whch is then joined with " " passed thru function given in date_parser

#usecols= list of column index or column names which would be only be read 
#sep is regex \s+ , equivalent to delim_whitespace=True arg 

#Note filename can be 'https://download.bls.gov/pub/time.series/cu/cu.item',sep='\t'
#or 's3://pandas-test/tips.csv' etc 

#Any error in file structure would result into "NotImplementedError: file structure not yet supported"
df = pd.read_csv('data.txt',index_col='Datetime', header=0, 
    parse_dates={"Datetime" : [1,3,4]}, skipinitialspace=True,   
    sep=r'\s+', skiprows=0, usecols=list(range(11)),squeeze=False,
    date_parser=lambda x: pd.to_datetime(x, format='%Y %m %d'))  

>>> df.index.month
array([4, 4, 4, 7, 7, 8, 8, 8], dtype=int32)
>>> df.dtypes
cruiseid     object
station       int64
date         object
lat         float64
lon         float64
depth_w       int64
taxon        object
count         int64
dtype: object
>>> df.taxon   #access a column 
Datetime
1987-04-13     Centropages_typicus
1987-04-13              Gastropoda
1987-04-13    Calanus_finmarchicus
1987-07-13            Acartia_spp.
1987-07-13         Metridia_lucens
1987-08-13             Evadne_spp.
1987-08-13                   Salpa
1987-08-13            Oithona_spp.
Name: taxon, dtype: object
>>> df.iloc[0:2,0:3]      #index based 
           cruiseid  station       date
Datetime
1987-04-13   AA8704        1  13-APR-87
1987-04-13   AA8704        1  13-APR-87
#label based 
>>> df.loc[pd.to_datetime('1987-04-13',format="%Y %m %d"),:]
>>> df.loc['1987-04-13',:]
>>> df['station'][0:3]  #access a column and then rows 
Datetime
1987-04-13    1
1987-04-13    1
1987-04-13    1
Name: station, dtype: int64
>>> df[['station','lat']][0:3] #multiple columns 
            station    lat
Datetime
1987-04-13        1  35.85
1987-04-13        1  35.85
1987-04-13        1  35.85
>>> df[df.taxon == 'Calanus_finmarchicus']
>>> df[((df.taxon == 'Calanus_finmarchicus') | (df.taxon == 'Gastropoda')) & (df.index.month == 4)]
>>> df[df.taxon.isin(['Calanus_finmarchicus', 'Gastropoda']) & (df.index.month == 4)]


#creation of new columns , note columns should be unique 
#hence first time assignment is creation, then only update 
>>> df.newStation = df['station'].astype(str)
__main__:1: UserWarning: Pandas doesn't allow columns to be created via a new at
tribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#at
tribute-access
>>> df['newStation'] = df['station'].astype(str) #conversion 
#any arithmatic operations , np.methods() etc 
>>> df['newStation2'] = 2 * df['station']         
>>> df.dtypes
cruiseid        object
station          int64
date            object
lat            float64
lon            float64
depth_w          int64
taxon           object
count            int64
newStation      object
newStation2      int64
dtype: object
>>> del df['newStation2']

  
    
##Other few imp Args to pd_read_csv
squeeze : boolean, default False,
    If the parsed data only contains one column then return a Series
prefix : str, default None
    Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...
dtype : Type name or dict of column -> type, default None
    Data type for data or columns. 
    E.g. {'a': np.float64, 'b': np.int32} 
    Use str or object to preserve and not interpret dtype. 
    If converters are specified, they will be applied 
    INSTEAD of dtype conversion.
converters : dict, default None
    Dict of functions for converting values in certain columns. 
    Keys can either be integers or column labels
true_values : list, default None
    Values to consider as True
false_values : list, default None
    Values to consider as False
skiprows : list-like or integer or callable, default None
    Number of lines to skip (int) at the start of the file.
    If callable, the callable function will be evaluated against the row indices,
    returning True if the row should be skipped and False otherwise. 
    An example of a valid callable argument would be lambda x: x in [0, 2].
skipfooter : int, default 0
    Number of lines at bottom of file to skip 
nrows : int, default None
    Number of rows of file to read. 
    Useful for reading pieces of large files
na_values : scalar, str, list-like, or dict, default None
    Additional strings to recognize as NA/NaN. 
    If dict passed, specific per-column NA values. 
    By default the following values are interpreted as NaN: 
    '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan', 'null'.
keep_default_na : bool, default True
    If na_values are specified and keep_default_na is False 
    the default NaN values are overridden, otherwise they're appended to.
na_filter : boolean, default True
    Detect missing value markers (empty strings and the value of na_values). 
    In data without any NAs, passing na_filter=False can improve the performance of reading a large file
verbose : boolean, default False
    Indicate number of NA values placed in non-numeric columns
skip_blank_lines : boolean, default True
    If True, skip over blank lines rather than interpreting as NaN values
keep_date_col : boolean, default False
    If True and parse_dates specifies combining multiple columns 
    then keep the original columns.
compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'
    For on-the-fly decompression of on-disk data. 
thousands : str, default None
    Thousands separator
decimal : str, default '.'
    Character to recognize as decimal point (e.g. use ',' for European data).
float_precision : string, default None
    Specifies which converter the C engine should use for floating-point values.
    The options are None for the ordinary converter, 
    high for the high-precision converter, 
    and round_trip for the round-trip converter.
lineterminator : str (length 1), default None
    Character to break file into lines. Only valid with C parser.
quotechar : str (length 1), optional
    The character used to denote the start and end of a quoted item. 
    Quoted items can include the delimiter and it will be ignored.
quoting : int or csv.QUOTE_* instance, default 0
    Control field quoting behavior per csv.
    QUOTE_* constants. 
    Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) 
    or QUOTE_NONE (3).
doublequote : boolean, default True
    When quotechar is specified and quoting is not QUOTE_NONE, 
    indicate whether or not to interpret two consecutive quotechar elements 
    INSIDE a field as a single quotechar element.
escapechar : str (length 1), default None
    One-character string used to escape delimiter when quoting is QUOTE_NONE.
comment : str, default None
    Indicates remainder of line should not be parsed. 
    If found at the beginning of a line, the line will be ignored altogether. 
    This parameter must be a single character
    For example, if comment='#', parsing '#emptyna,b,cn1,2,3' with header=0 
    will result in 'a,b,c' being treated as the header.
encoding : str, default None
    Encoding to use for UTF when reading/writing (ex. 'utf-8'). 
error_bad_lines : boolean, default True
    Lines with too many fields (e.g. a csv line with too many commas) 
    will by default cause an exception to be raised, 
    and no DataFrame will be returned. 
    If False, then these 'bad lines' will dropped from the DataFrame 
    that is returned.
warn_bad_lines : boolean, default True
    If error_bad_lines is False, and warn_bad_lines is True, 
    a warning for each 'bad line' will be output.
memory_map : boolean, default False
    If a filepath is provided for filepath_or_buffer, 
    map the file object directly onto memory and access the data directly 
    from there. Using this option can improve performance 
    because there is no longer any I/O overhead

##Conversion functions can be used 
pandas.to_numeric(arg, errors='raise', downcast=None)[source]
    Convert argument to a numeric type.
    arg : list, tuple, 1-d array, or Series
pandas.to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False, utc=None, box=True, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix')[source]
    Convert argument to datetime.
    arg : integer, float, string, datetime, list, tuple, 1-d array, Series
    #common format 
    %w  Weekday as a decimal number, where 0 is Sunday and 6 is Saturday. 0, 1, …, 6   
    %d  Day of the month as a zero-padded decimal number. 01, 02, …, 31 
    %b  Month as locale's abbreviated name. Jan, Feb, …, Dec (en_US);
    %B  Month as locale's full name. January, February, …, December (en_US);
    %m  Month as a zero-padded decimal number. 01, 02, …, 12   
    %y  Year without century as a zero-padded decimal number. 00, 01, …, 99   
    %Y  Year with century as a decimal number. 1970, 1988, 2001, 2013   
    %H  Hour (24-hour clock) as a zero-padded decimal number. 00, 01, …, 23   
    %I  Hour (12-hour clock) as a zero-padded decimal number. 01, 02, …, 12   
    %p  Locale's equivalent of either AM or PM. AM, PM (en_US);
    %M  Minute as a zero-padded decimal number. 00, 01, …, 59   
    %S  Second as a zero-padded decimal number. 00, 01, …, 59 
    %f  Microsecond as a decimal number, zero-padded on the left. 000000, 000001, …, 999999 
    %z  UTC offset in the form +HHMM or -HHMM (empty string if the the object is naive). (empty), +0000, -0400, +1030 
    %Z  Time zone name (empty string if the object is naive). (empty), UTC, EST, CST   
    %j  Day of the year as a zero-padded decimal number. 001, 002, …, 366   
    %U  Week number of the year (Sunday as the first day of the week) as a zero padded decimal number. All days in a new year preceding the first Sunday are considered to be in week 0. 00, 01, …, 53 
    %W  Week number of the year (Monday as the first day of the week) as a decimal number. All days in a new year preceding the first Monday are considered to be in week 0. 00, 01, …, 53 
pandas.to_timedelta(arg, unit='ns', box=True, errors='raise')[source]
    Convert argument to timedelta
    arg : string, timedelta, list, tuple, 1-d array, or Series
    unit : unit of the arg (D,h,m,s,ms,us,ns) denote the unit, which is an integer/float number
 

##Other quick IO 
Format Type     Data Description        Reader              Writer
text            CSV                     read_csv            to_csv 
text            JSON                    read_json           to_json 
text            HTML                    read_html           to_html 
text            Local clipboard         read_clipboard      to_clipboard 
binary          MS Excel                read_excel          to_excel 
binary          HDF5 Format             read_hdf            to_hdf 
binary          Feather Format          read_feather        to_feather 
binary          Parquet Format          read_parquet        to_parquet 
binary          Msgpack                 read_msgpack        to_msgpack 
binary          Stata                   read_stata          to_stata 
binary          SAS                     read_sas   
binary          Python Pickle Format    read_pickle         to_pickle 
SQL             SQL                     read_sql            to_sql 
SQL             Google Big Query        read_gbq            to_gbq 

##methods 
#CSV 
DataFrame.to_csv(path_or_buf=None, sep=', ', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression=None, quoting=None, quotechar='"', line_terminator='\n', chunksize=None, tupleize_cols=None, date_format=None, doublequote=True, escapechar=None, decimal='.')


#JSON 
pandas.read_json(path_or_buf=None, orient=None, typ='frame', dtype=True, convert_axes=True, convert_dates=True, keep_default_dates=True, numpy=False, precise_float=False, date_unit=None, encoding=None, lines=False, chunksize=None, compression='infer')
DataFrame.to_json(path_or_buf=None, orient=None, date_format=None, double_precision=10, force_ascii=True, date_unit='ms', default_handler=None, lines=False, compression=None)

#Clipboard
pandas.read_clipboard(sep='\\s+', **kwargs)
DataFrame.to_clipboard(excel=True, sep=None, **kwargs)

#EXCEl: xlrd is required for pandas.read_excel 
pandas.read_excel(io, sheet_name=0, header=0, skiprows=None, skip_footer=0, index_col=None, names=None, usecols=None, parse_dates=False, date_parser=None, na_values=None, thousands=None, convert_float=True, converters=None, dtype=None, true_values=None, false_values=None, engine=None, squeeze=False, **kwds)
    df = pd.read_excel("file_name", 'Sheet1') #needs xlrd
    #for example setting correct index from one column, if datetime, use to_datetime
    df.index = pd.to_datetime(df.Date)
DataFrame.to_excel(excel_writer, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep='inf', verbose=True, freeze_panes=None)

#SQL 
pandas.read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None)
    sql : string or SQLAlchemy Selectable (select or text object)
          can be database table name or sql query
    con : SQLAlchemy connectable(engine/connection) or database string URI
        or DBAPI2 connection (fallback mode) 
        Using SQLAlchemy makes it possible to use any DB supported by that library. 
        If a DBAPI2 object, only sqlite3 is supported.
        #Example 
        from sqlalchemy import create_engine
        engine = create_engine('sqlite:///:memory:')
        engine = create_engine('postgresql://scott:tiger@localhost:5432/mydatabase')
        #Engine URL format 
        dialect+driver://username:password@host:port/
        # default
        engine = create_engine('mysql://scott:tiger@localhost/foo')
        # mysql-python
        engine = create_engine('mysql+mysqldb://scott:tiger@localhost/foo')
        # MySQL-connector-python
        engine = create_engine('mysql+mysqlconnector://scott:tiger@localhost/foo')
        # OurSQL
        engine = create_engine('mysql+oursql://scott:tiger@localhost/foo')
        # sqlite://<nohostname>/<path>
        # where <path> is relative:
        engine = create_engine('sqlite:///foo.db')
        #Unix/Mac - 4 initial slashes in total
        engine = create_engine('sqlite:////absolute/path/to/foo.db')
        #Windows
        engine = create_engine('sqlite:///C:\\path\\to\\foo.db')
        #Windows alternative using raw string
        engine = create_engine(r'sqlite:///C:\path\to\foo.db')
        #To use a SQLite :memory: database, specify an empty URL:
        engine = create_engine('sqlite://')
        #quick sqlalchemy 
        connection = engine.connect()
        result = connection.execute("select username from users")
        for row in result:
            print("username:", row['username'])
        result.close()
        connection.close()
 
DataFrame.to_sql(name, con, flavor=None, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None)
    Write records stored in a DataFrame to a SQL database.
    name : string
        Name of SQL table
    con : SQLAlchemy engine or DBAPI2 connection (legacy mode)
    if_exists : {'fail', 'replace', 'append'}, default 'fail'
        •fail: If table exists, do nothing.
        •replace: If table exists, drop it, recreate it, and insert data.
        •append: If table exists, insert data. Create if does not exist.

#HTML - requires lxml, BeautifulSoup4 and html5lib,
#read_html returns a list of DataFrame objects, 
#even if there is only a single table contained in the HTML content
pandas.read_html(io, match='.+', flavor=None, header=None, index_col=None, skiprows=None, attrs=None, parse_dates=False, tupleize_cols=None, thousands=', ', encoding=None, decimal='.', converters=None, na_values=None, keep_default_na=True)
    df_list = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')
    io : str or file-like
        A URL, a file-like object, or a raw string containing HTML. 
        Note that lxml only accepts the http, ftp and file url protocols. 
        If you have a URL that starts with 'https' you might try removing the 's'.
    match : str or compiled regular expression, optional
        The set of tables containing text matching this regex or string will be returned.
        Defaults to '.+' (match any non-empty string). 
        The default value will return all tables contained on a page. 
    attrs : dict or None, optional
        This is a dictionary of attributes that you can pass to use to identify the table in the HTML
        For example,
        attrs = {'id': 'table'}
DataFrame.to_html(buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, bold_rows=True, classes=None, escape=True, max_rows=None, max_cols=None, show_dimensions=False, notebook=False, decimal='.', border=None)
    classes : str or list or tuple, default None
        CSS class(es) to apply to the resulting html table
    escape : boolean, default True
        Convert the characters <, >, and & to HTML-safe sequences.=
    max_rows : int, optional
        Maximum number of rows to show before truncating. If None, show all.
    max_cols : int, optional
        Maximum number of columns to show before truncating. If None, show all.

##To and from dict 
classmethod DataFrame.from_dict(data, orient='columns', dtype=None)[source]
    Construct DataFrame from dict of array-like or dicts
    Note dict can be passed to directly to pd.DataFrame constructor
    Parameters:
    data : dict
        {field : array-like} or {field : dict}
    orient : {'columns', 'index'}, default 'columns'
        The 'orientation' of the data. 
        If the keys of the passed dict should be the columns of the resulting DataFrame, 
        pass 'columns' (default). 
        Otherwise if the keys should be rows, pass 'index'.
    dtype : dtype, default None
        Data type to force, otherwise infer
    
DataFrame.to_dict(orient='dict', into=<class 'dict'>)[source]
    Convert DataFrame to dictionary.
    Parameters:
    orient : str {'dict', 'list', 'series', 'split', 'records', 'index'}
        Determines the type of the values of the dictionary.
        •dict (default) : dict like {column -> {index -> value}}
        •list : dict like {column -> [values]}
        •series : dict like {column -> Series(values)}
        •split : dict like {index -> [index], columns -> [columns], data -> [values]}
        •records : list like [{column -> value}, ... , {column -> value}]
        •index : dict like {index -> {column -> value}}
        Abbreviations are allowed. s indicates series and sp indicates split.
    
#Example 
#MultiIndex Dict 
data = {('a', 'b'): {('A', 'B'): 1, ('A', 'C'): 2},
              ('a', 'a'): {('A', 'C'): 3, ('A', 'B'): 4},
              ('a', 'c'): {('A', 'B'): 5, ('A', 'C'): 6},
              ('b', 'a'): {('A', 'C'): 7, ('A', 'B'): 8},
              ('b', 'b'): {('A', 'D'): 9, ('A', 'B'): 10}}
>>> pd.DataFrame(data)
       a              b      
       a    b    c    a     b
A B  4.0  1.0  5.0  8.0  10.0
  C  3.0  2.0  6.0  7.0   NaN
  D  NaN  NaN  NaN  NaN   9.0
  
>>> pd.DataFrame.from_dict(data)
       a              b
       a    b    c    a     b
A B  4.0  1.0  5.0  8.0  10.0
  C  3.0  2.0  6.0  7.0   NaN
  D  NaN  NaN  NaN  NaN   9.0
>>> pd.DataFrame.from_dict(data, orient='index')
      A
      B    C    D
a a   4  3.0  NaN
  b   1  2.0  NaN
  c   5  6.0  NaN
b a   8  7.0  NaN
  b  10  NaN  9.0
#Simple index dict 
df = pd.DataFrame( {'col1': [1, 2], 'col2': [0.5, 0.75]}, index=['a', 'b'])
>>> df
   col1  col2
a     1   0.1
b     2   0.2
>>> df.to_dict()
{'col1': {'a': 1, 'b': 2}, 'col2': {'a': 0.5, 'b': 0.75}}

>>> df.to_dict('lists')
{'col1': [1, 2], 'col2': [0.5, 0.75]}

>>> df.to_dict('series')
{'col1': a    1
b    2
Name: col1, dtype: int64, 'col2': a    0.50
b    0.75
Name: col2, dtype: float64}

>>> df.to_dict('split')
{'columns': ['col1', 'col2'], 'data': [[1.0, 0.5], [2.0, 0.75]], 'index': ['a','b']}

>>> df.to_dict('records')
[{'col1': 1.0, 'col2': 0.5}, {'col1': 2.0, 'col2': 0.75}]

>>> pd.DataFrame.from_dict(df.to_dict('records'))
   col1  col2
0   1.0  0.50
1   2.0  0.75

>>> df.to_dict('index')
{'a': {'col1': 1.0, 'col2': 0.5}, 'b': {'col1': 2.0, 'col2': 0.75}}

>>> pd.DataFrame.from_dict(df.to_dict('index')) #orient='columns'
        a     b
col1  1.0  2.00
col2  0.5  0.75
>>> pd.DataFrame.from_dict(df.to_dict('index'), orient='index')
   col1  col2
a   1.0  0.50
b   2.0  0.75

>>> from collections import OrderedDict, defaultdict
>>> df.to_dict(into=OrderedDict)
OrderedDict([('col1', OrderedDict([('a', 1), ('b', 2)])),
           ('col2', OrderedDict([('a', 0.5), ('b', 0.75)]))])

   
Series.tolist()
    Return a list of the values.
    These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)

    
Series.values, DataFrame.values , DataFrame.as_matrix()
    Convert to ndArray 
    
#Example DF to list 
#Use .values to get a numpy.array and then .tolist() to get a list.
import pandas as pd
df = pd.DataFrame({'a':[1,3,5,7,4,5,6,4,7,8,9],
                   'b':[3,5,6,2,4,6,7,8,7,8,9]})

>>> df['a'].values.tolist()
[1, 3, 5, 7, 4, 5, 6, 4, 7, 8, 9]

>>> df.values
array([[1, 3],
       [3, 5],
       [5, 6],
       [7, 2],
       [4, 4],
       [5, 6],
       [6, 7],
       [4, 8],
       [7, 7],
       [8, 8],
       [9, 9]], dtype=int64)
       
>>> df.values.tolist()
[[1, 3], [3, 5], [5, 6], [7, 2], [4, 4], [5, 6], [6, 7], [4, 8], [7, 7], [8, 8],
 [9, 9]]
 
>>> df.as_matrix()
array([[1, 3],
       [3, 5],
       [5, 6],
       [7, 2],
       [4, 4],
       [5, 6],
       [6, 7],
       [4, 8],
       [7, 7],
       [8, 8],
       [9, 9]], dtype=int64)
 
DataFrame.to_records(index=True, convert_datetime64=True)
    Convert DataFrame to record array. Index will be put in the 'index' field of the record array if requested
    Parameters:
    index : boolean, default True
        Include index in resulting record array, stored in 'index' field
    convert_datetime64 : boolean, default True
        Whether to convert the index to datetime.datetime if it is a DatetimeIndex
 
classmethod DataFrame.from_records(data, index=None, exclude=None, columns=None, coerce_float=False, nrows=None)[source]
    Convert structured or record ndarray to DataFrame
    Parameters:
    data : ndarray (structured dtype), list of tuples, dict, or DataFrame
    index : string, list of fields, array-like
        Field of array to use as the index, 
        alternately a specific set of input labels to use
    exclude : sequence, default None
        Columns or fields to exclude
    columns : sequence, default None
        Column names to use. 
        If the passed data do not have names associated with them, 
        this argument provides names for the columns. 
        Otherwise this argument indicates the order of the columns in the result 
        (any names not found in the data will become all-NA columns)
    coerce_float : boolean, default False
        Attempt to convert values of non-string, non-numeric objects 
        (like decimal.Decimal) to floating point, useful for SQL result sets

#Example  
data = np.zeros((2,), dtype=[('A', 'i4'),('B', 'f4'),('C', 'a10')])
data[:] = [(1,2.,'Hello'), (2,3.,"World")]
#or     
data = np.array([(1,  2., b'Hello'), (2,  3., b'World')],
      dtype=[('A', '<i4'), ('B', '<f4'), ('C', 'S10')])

>>> x = pd.DataFrame.from_records(data, index='C')
          A    B
C               
b'Hello'  1  2.0
b'World'  2  3.0

>>> x.to_records()
rec.array([(b'Hello', 1, 2.), (b'World', 2, 3.)],
          dtype=[('C', 'O'), ('A', '<i4'), ('B', '<f4')])
          
          
          
classmethod DataFrame.from_items(items, columns=None, orient='columns')[source]
    Convert (key, value) pairs to DataFrame. 
    The keys will be the axis index (usually the columns, but depends on the specified orientation). The values should be arrays or Series.
    Parameters:
    items : sequence of (key, value) pairs
        Values should be arrays or Series.
    columns : sequence of column labels, optional
        Must be passed if orient='index'.
    orient : {'columns', 'index'}, default 'columns'
        The 'orientation' of the data. 
        If the keys of the input correspond to column labels,           
          
#Example 

>>> pd.DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6])])
Out[54]: 
   A  B
0  1  4
1  2  5
2  3  6

>>> pd.DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6])],
                        orient='index', columns=['one', 'two', 'three'])
   one  two  three
A    1    2      3
B    4    5      6

         
          
          
          
          

###Pandas - Quick Intro 

import numpy as np
import pandas as pd


##DF is composed of list of Series with one column is designated by Index 
#Creation 
df1 = pd.DataFrame(np.random.randn(6,4),index=list('abcdef'),columns=list('ABCD'))
#datetime based = 'M', 'D', 'Y','YS','Q','W','H','T'(min),'S'(sec),'ms','us'
dft = pd.DataFrame(np.random.randn(100000,1),
            columns=['A'],
            index=pd.date_range('20130101',periods=100000,freq='T'))
ts = pd.Series(np.random.randn(100000), index=pd.date_range('20130101',periods=100000,freq='T'))
#special index based access only for DatetimeIndex 
>>> dft.head()
                            A
2013-01-01 00:00:00  2.375359
2013-01-01 00:01:00  0.663875
2013-01-01 00:02:00 -0.534566
2013-01-01 00:03:00  0.172524
2013-01-01 00:04:00  0.502204

from datetime import datetime
#For specific exact index for Only DF , use .loc 
dft['2013-01-01 00:00:00'] #ERROR 
dft[datetime(2013, 1, 1)] #equivalent to exact  #ERROR 
#use below 
dft.loc['2013-01-01 00:00:00']
dft.loc[datetime(2013, 1, 1)] 
#note for Series, below works 
ts['2013-01-01 00:00:00']
ts[datetime(2013, 1, 1)]
ts.loc[datetime(2013, 1, 1)]
#for both DF and Series- any partial date string or slice of exact index works 
dft['2013-01-01 00:00:00':'2013-01-01 00:04:00']
dft['2013-1-1']   #from 2013-01-01 00:00:00 till upto 2013-01-01 23:59:00
dft['2013']     #Year based , from 2013-01-01 00:00:00 till upto 2013-03-11 10:39:00
dft['2013-1':'2013-2']   #slice, end inclusive 
dft['2013-1':'2013-2-28'] # stop time that includes all of the times on the last day
dft['2013-1':'2013-2-28 00:00:00'] #exact stop time     
dft[datetime(2013, 1, 1):datetime(2013,2,28)] #exact start and stop time 
dft[datetime(2013, 1, 1, 10, 12, 0):datetime(2013, 2, 28, 10, 12, 0)] #exact start and stop time 
#Note the difference, first one is Series, 2nd one is DF 
>>> dft.loc[datetime(2013, 1, 1)]
A    2.375359
Name: 2013-01-01 00:00:00, dtype: float64
>>> dft.loc[[datetime(2013, 1, 1)]]
                   A
2013-01-01  2.375359

#Note 
ts[0] #first item , scalar 
#but 
dft[0] #error as for DF, [] includes column label 
dft['A'] #OK 
#but for slicing , works as it is row slicing 
ts[0:5]
dft[0:5]


##If a column is of strings , check string methods 
s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])
dir(s.str)
s.str.lower()

##if column is of datetime , check datetime methods 
s1 = pd.Series(pd.date_range('20130101 09:10:12', periods=4))
dir(s1.dt)
s1.dt.hour

##if column is of type categorical, check category method 
s = pd.Series(["a","b","c","a"], dtype="category")
dir(s.cat)
s.cat.categories
s.cat.rename_categories([1,2,3])

##Column can be plotted 
s1 = pd.Series(pd.date_range('20130101 09:10:12', periods=4))
dir(s1.plot)
s1.plot.line()

##check column(ie Series) methods 
s = pd.Series(pd.RangeIndex(stop=10))#start,stop,step
dir(s)
#for example conversion 
s.astype(str)
s.sum()
#np.methods() can be applied 
np.log(s)
#or arithmatic 
s * 2 
#or logical 
s == 2 

##Check DF methods 
dir(dft)
dft.describe()

##Accessing 
df[column]
    column can be 
        Single column label eg ['City']
        Array of column lables eg ['City', 'City']
        A boolean eg df[df.City == 'Chicago']
        A callable, fn(df):returns_any_from_above eg. df[lambda d: d.City == 'Chicago']
    Can be used for update dfi['C'] = dfi['A']
df.column_name
    Only for accessing or update 
    Not for creation of new column, use df['column_name'] style 
    For index column, access always like df.index 
df[row_slice]
    row_slice can be 
        start:stop:step, stop exclusive 
        where start,stop are row index 
    there is no way to get row based on index label
    But for DatetimeIndex, it's possible to access based on datetime index 
df.loc[row,column] 
    label based(both row and column)
    row,column takes 
        A single label, e.g. 5 or 'a', 
        A list or array of labels ['a', 'b', 'c']
        A slice object with labels 'a':'f' (end inclusive)
        A boolean array eg df.A > 0 
        :  means all 
        A Callable fn(df):returns_any_from_above eg. lambda d: d.A > 0 
        For DatetimeIndex, row can be as given in above example 
    Can be used for update eg dfi.loc[:,'C'] = dfi.loc[:,'A']
df.loc[row]
    equivalent to df.loc[row,:]
df.iloc[row,column] 
    index based(both row, column)
    row,column takes 
        An integer e.g. 5
        A list or array of integers [4, 3, 0]
        A slice object with ints 1:7:1 , end exclusive 
        A boolean array
        :  means all 
        A Callable fn(df):returns_any_from_above eg. lambda d: d.A > 0 
    Can be used for update eg dfi.loc[:,'C'] = dfi.loc[:,'A']    
df.iloc[row]
    equivalent to df.iloc[row,:]
df.ix[row,column] 
    For each of row,column ,at first tries label based like df.loc[] 
    if Fails, then tries index based like df.iloc[]
    If the index or column does not have label,then behaves like df.iloc[] always 
    For DatetimeIndex, row can be as given in above example 
df.ix[row]
    equivalent to df.ix[row,:]
    For DatetimeIndex, row can be as given in above example

#reading csv 
ver=pd.read_csv("data/ver.csv")  
#columns 
#index,year,day of the year,time,atmospheric pressure (mBar),rainfall (mm),wind speed (m/s),wind direction (degrees),surface temperature (C),relative humidity (%),wind_max (m/s),Tdew (C),wind_chill (C),uncalibrated solar flux (Kw/m2), calibrated solar flux (Kw/m2),battery (V),not used


#display few rows
pd.set_option('display.max_columns', 80)
ver.head(3)

#Determine the number of rows and columns in the dataset
ver.shape

#Find the number of rows in the dataset
len(ver)

#Get the names of the columns
ver.columns

#Get the first five rows of a column by name
ver['atmospheric pressure (mBar)'][:5]

#Create categorical ranges for numerical data. 14 is number of bins
atmospheric_pressure = pd.cut(ver['atmospheric pressure (mBar)'], 14)
atmospheric_pressure[:5]

#Look at the frequencies in the ranges created above
pd.value_counts(atmospheric_pressure)

#first six columns of the first row
#ix like .loc[row,column] , ie label based at first if not then iloc[row,column], index based
ver.ix[0,0:6]

#Order the data by specified column
ver['atmospheric pressure (mBar)'].sort_values()[:5]

#Sort by a column and that obtain a cross-section of that data , multiple can be given
sorteddata = ver.sort_values(by=['atmospheric pressure (mBar)', 'day of the year'])
sorteddata.ix[:,0:6].head(3)  

#Obtain the first three rows and first three columns of the sorted data
sorteddata.iloc[0:3,0:3]

#Obtain value counts of specific column
ver['atmospheric pressure (mBar)'].value_counts()

#to obtain the datatype for every colu mn
zip(ver.columns, [type(x) for x in ver.ix[0,:]])
#OR
ver.dtypes

#Get the unique values for a column by name.
ver['year'].unique()

#Get a count of the unique values of a column
len(ver['year'].unique())

#Index into a column and get the first four rows
ver.ix[0:3,'atmospheric pressure (mBar)']

#Obtain True/False  values
ver.ix[0:3,'atmospheric pressure (mBar)'] == 1025



#Return descriptive statistics of the dataset- mean, std etc is calculated for each colu mn
ver.describe()
#output 
atmospheric pressure (mBar)
count                 45000.000000
mean                   1015.946356
std                      12.309651
min                     977.000000
25%                    1010.000000
50%                    1016.000000
75%                    1025.000000
max                    1034.000000


#Crosstab(frequency) of the data by specified columns (of one column vs another)
pd.crosstab(ver['atmospheric pressure (mBar)'],ver['rainfall (mm)'])

#Return a subset of the data
atmsubset = ver[(ver['atmospheric pressure (mBar)'] > 1010 ) & (ver['atmospheric pressure (mBar)'] < 1016)]
atmsubset.head(5)


#Look at the shape of the dataset
atmsubset.shape

#Query the data
qry1 = ver.query('year == 2015')   #here column name must not contain any space etc
qry1.head(10)

#if column names contains space , use below , required for even attribute access

originals = ver.columns[:]
ver.columns = [c.replace(' ', '_') for c in ver.columns]
ver.columns = [c.replace('(', '') for c in ver.columns]
ver.columns = [c.replace(')', '') for c in ver.columns]
qry1 = ver.query('atmospheric_pressure_mBar == 1016')

#Look at the shape of the data
qry1.shape

#Group data and obtain the mean,
#group by col1 and then col2 and find mean of all other columns
grouped1 = ver.groupby(['atmospheric_pressure_mBar','rainfall_mm']).mean()
grouped1

#Check a boolean condition
(ver.ix[:,'atmospheric_pressure_mBar'] > 1016).any()

#Get descriptive statistics for a specified column
ver.atmospheric_pressure_mBar.describe()

#Group data and obtain the mean/median/etc  values of all other colum ns
grpagg = ver.groupby('atmospheric_pressure_mBar').aggregate(np.median)
grpagg

#Group data and get the sums of all other colum ns
grpsum = ver.groupby('atmospheric_pressure_mBar').aggregate(np.sum)
grpsum

# add a new column which is string version of one colu mn
ver['applicant_race_name_1'] = pd.Series(np.array(map(str, ver['atmospheric_pressure_mBar'])), index=ver.index)
ver['applicant_race_name_1'][0]  #'1025'


#Return boolean values for a specified criteria
criterion = ver['applicant_race_name_1'].map(lambda x: x.endswith('5'))
>>> criterion.value_counts()  #single variable- frequency
False    40005
True      4995

#Melt data  - creating a generic form internally, which you can cast to specific shape
#each row is converted into id_vars versus other columns
melt = pd.melt(ver, id_vars = 'atmospheric pressure (mBar)')
#Obtain the first five rows of melted data
melt.iloc[0:5,:]   
#output - called 'stacked' or 'record' format:
        atmospheric pressure (mBar) variable  value
0                         1025      index       101.0
1                         1025      index       101.0
2                         1025      index       101.0
3                         1025      index       101.0
4                         1025      index       101.0
#Select 
melt[melt['variable'] == 'rainfall (mm)']

#check unique 
#subset : Only consider certain columns for identifying duplicates, by default use all of the columns
melt.drop_duplicates(subset=['variable', 'value'], keep='first') #keep : {'first', 'last', False}, default 'first'

#note melt can be on multiple id_vars and can be DF instance method
#and 'variable' can be renamed 
ver.melt(id_vars=['atmospheric pressure (mBar)', 'rainfall (mm)'], var_name='melted_variable')



#Create a pivot - pivot(index=None, columns=None, values=None)
#'columns' columns name would new DF's column header 
#'values' column Name  to use for populating new frame's values under above column name 
#'index' Column name to use to make new frame's index
#Note index has to be unique and date has to be in melted form 
ver_new = ver.drop_duplicates(subset=['atmospheric pressure (mBar)'])
melt1 = pd.melt(ver_new, id_vars = 'atmospheric pressure (mBar)')
pm = melt1.pivot(index='atmospheric pressure (mBar)', columns='variable', values='value')
>>> pm.index
Int64Index([ 977,  978,  979,  980,  981,  982,  983,  984,  985,  986,  987,
             988,  989,  990,  991,  992,  993,  994,  995,  996,  997,  998,
             999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009,
            1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020,
            1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031,
            1032, 1033, 1034],
           dtype='int64', name='atmospheric pressure (mBar)')
>>> pm.columns
Index([' calibrated solar flux (Kw/m2)', 'Tdew (C)', 'battery (V)',
       'day of the year', 'index', 'not used', 'rainfall (mm)',
       'relative humidity (%)', 'surface temperature (C)', 'time',
       'uncalibrated solar flux (Kw/m2)', 'wind direction (degrees)',
       'wind speed (m/s)', 'wind_chill (C)', 'wind_max (m/s)', 'year'],
      dtype='object', name='variable')






## Using Pandas for Analyzing Data - Visualization

#Plot counts of a specified column using Pand as
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns  #install it, conda install seaborn

ver.atmospheric_pressure_mBar.value_counts().plot(kind='barh')
plt.show()
#or use 
ver.plot(x=None, y='atmospheric_pressure_mBar', kind='line', 
    ax=None, subplots=False, sharex=None, sharey=False, layout=None, 
    figsize=None, use_index=True, title=None, grid=None, legend=True, 
    style=None, logx=False, logy=False, loglog=False, xticks=None, yticks=None, 
    xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, table=False, 
    yerr=None, xerr=None, secondary_y=False, sort_columns=False, **kwds)
#x : label string or position, default None means index 
#y : label string or position, default None, means each column 
#kind:
#line : line plot (default)
#'bar' : vertical bar plot
#'barh' : horizontal bar plot
#'hist' : histogram
#'box' : boxplot
#'kde' : Kernel Density Estimation plot
#'density' : same as 'kde'
#'area' : area plot
#'pie' : pie plot
#'scatter' : scatter plot
#'hexbin' : hexbin plot

#note 
df.plot(kind='line') 
#is equivalent to 
df.plot.line()
#and similarly for others 

#with subplots 
#multiple in one plot 
ver.plot(kind='line', y=['Open', 'Day Wise Variation ( points) '] )
#with subplots 
ver.plot(kind='line', y=['Open', 'Day Wise Variation ( points) '], subplots=True )

#Bar plot of median values
ver.groupby('atmospheric_pressure_mBar')['surface_temperature_C'].agg(np.median).plot(kind = 'bar')
plt.show()

#Box plot example - only for first 200 rows
g = sns.factorplot("atmospheric_pressure_mBar", "surface_temperature_C", "rainfall_mm" ,ver.loc[0:200], kind="box", palette="PRGn",aspect=2.25)
g.set(ylim=(0, 10))
plt.show()

#Bar plot example
g = sns.factorplot("atmospheric_pressure_mBar","rainfall_mm", data=ver.loc[0:200], hue="surface_temperature_C",size=3,aspect=2)
plt.show()



##scatter_matrix 
#https://pandas.pydata.org/pandas-docs/stable/visualization.html
from pandas.plotting import scatter_matrix
df = pd.DataFrame(np.random.randn(1000, 4), columns=['a', 'b', 'c', 'd'])
scatter_matrix(ver, alpha=0.2, figsize=(6, 6), diagonal='kde')
#only density plot 
df.a.plot.kde()

##lagplot -Lag plots are used to check if a data set or time series is random. 
#Random data should not exhibit any structure in the lag plot. 
#Non-random structure implies that the underlying data are not random.
from pandas.plotting import lag_plot
lag_plot(df.a)

##Autocorrelation plots are often used for checking randomness in time series
#If time series is random, such autocorrelations should be near zero for any and all time-lag separations
from pandas.plotting import autocorrelation_plot
autocorrelation_plot(df.a)

#Bootstrap plots are used to visually assess the uncertainty of a statistic, 
#such as mean, median, midrange, etc. 
#A random subset of a specified size is selected from a data set, 
#the statistic in question is computed for this subset 
#and the process is repeated a specified number of times. 
#Resulting plots and histograms are what constitutes the bootstrap plot.
from pandas.plotting import bootstrap_plot
bootstrap_plot(df.a, size=50, samples=500, color='grey')

#Time/Date example 
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))
ts = ts.cumsum()
ts.plot()

df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list('ABCD'))
df = df.cumsum()
plt.figure(); 
df.plot();

#with subplots 
df.plot(subplots=True, figsize=(6, 6));
#means 2 rows and 3 columns , each one is 6x6 
df.plot(subplots=True, layout=(2, 3), figsize=(6, 6), sharex=False);

#or maximum control 
fig, axes = plt.subplots(4, 4, figsize=(6, 6));
plt.subplots_adjust(wspace=0.5, hspace=0.5);
target1 = [axes[0][0], axes[1][1], axes[2][2], axes[3][3]]
target2 = [axes[3][0], axes[2][1], axes[1][2], axes[0][3]]
df.plot(subplots=True, ax=target1, legend=False, sharex=False, sharey=False);
(-df).plot(subplots=True, ax=target2, legend=False, sharex=False, sharey=False);
#or 
fig, axes = plt.subplots(nrows=2, ncols=2)
df['A'].plot(ax=axes[0,0]); axes[0,0].set_title('A');
df['B'].plot(ax=axes[0,1]); axes[0,1].set_title('B');
df['C'].plot(ax=axes[1,0]); axes[1,0].set_title('C');
df['D'].plot(ax=axes[1,1]); axes[1,1].set_title('D');



###Pandas - Quick intro - iris example 
iris = pd.read_csv('data/iris.csv')
>>> iris.head()
SepalLength  SepalWidth  PetalLength  PetalWidth         Name
0          5.1         3.5          1.4         0.2  Iris-setosa
1          4.9         3.0          1.4         0.2  Iris-setosa
2          4.7         3.2          1.3         0.2  Iris-setosa
3          4.6         3.1          1.5         0.2  Iris-setosa
4          5.0         3.6          1.4         0.2  Iris-setosa

>>> (iris.assign(sepal_ratio = iris['SepalWidth'] / iris['SepalLength']).head())
SepalLength  SepalWidth  PetalLength  PetalWidth         Name  sepal_ratio
0          5.1         3.5          1.4         0.2  Iris-setosa       0.6863
1          4.9         3.0          1.4         0.2  Iris-setosa       0.6122
2          4.7         3.2          1.3         0.2  Iris-setosa       0.6809
3          4.6         3.1          1.5         0.2  Iris-setosa       0.6739
4          5.0         3.6          1.4         0.2  Iris-setosa       0.7200


#Or use function of one argument which is the  DF
>>> iris.assign(sepal_ratio = lambda x: (x['SepalWidth'] /x['SepalLength'])).head()
SepalLength  SepalWidth  PetalLength  PetalWidth         Name  sepal_ratio
0          5.1         3.5          1.4         0.2  Iris-setosa       0.6863
1          4.9         3.0          1.4         0.2  Iris-setosa       0.6122
2          4.7         3.2          1.3         0.2  Iris-setosa       0.6809
3          4.6         3.1          1.5         0.2  Iris-setosa       0.6739
4          5.0         3.6          1.4         0.2  Iris-setosa       0.7200



#Example - limit the DataFrame with a Sepal Length greater than 5, calculate the ratio, and plot:

(iris.query('SepalLength > 5').assign( SepalRatio = lambda x: x.SepalWidth / x.SepalLength,            
                     PetalRatio = lambda x: x.PetalWidth / x.PetalLength)   
                .plot(kind='scatter', x='SepalRatio', y='PetalRatio'))
plt.show()





###Pandas - Reshaping and Pivot Tables- melt, wide_to_long, pivot, pivot_table, crosstab, cut, Categorical, factorise, get_dummies

##Reshaping by pivoting DataFrame objects

import pandas.util.testing as tm; 
tm.N = 3
dfo = tm.makeTimeDataFrame()
>>> dfo
                   A         B         C         D
2000-01-03  0.555400  1.064253  1.468849  1.024841
2000-01-04 -0.613935 -1.946462 -1.607492  0.741685
2000-01-05  1.697235  0.055215  0.619437  0.051020
>>> dfo['date'] = dfo.index

#Data is often stored in CSV files or databases in 'stacked' or 'record' format
#as given below 
>>> df = pd.melt(dfo, id_vars = 'date')
>>> df
         date variable     value
0  2000-01-03        A  0.555400
1  2000-01-04        A -0.613935
>>> df[df['variable'] == 'A']

#To undo 
>>> df.pivot(index='date', columns='variable', values='value')
variable           A         B         C         D
date
2000-01-03  0.555400  1.064253  1.468849  1.024841
2000-01-04 -0.613935 -1.946462 -1.607492  0.741685
2000-01-05  1.697235  0.055215  0.619437  0.051020

#create a new column 
df['value2'] = df['value'] * 2
>>> df
         date variable     value    value2
0  2000-01-03        A  0.555400  1.110799
1  2000-01-04        A -0.613935 -1.227871
2  2000-01-05        A  1.697235  3.394469
3  2000-01-03        B  1.064253  2.128506
4  2000-01-04        B -1.946462 -3.892923
#check the output of pivot 
pivoted = df.pivot('date', 'variable')
>>> pivoted
               value                                  value2            \
variable           A         B         C         D         A         B
date
2000-01-03  0.555400  1.064253  1.468849  1.024841  1.110799  2.128506
2000-01-04 -0.613935 -1.946462 -1.607492  0.741685 -1.227871 -3.892923
2000-01-05  1.697235  0.055215  0.619437  0.051020  3.394469  0.110429


variable           C         D
date
2000-01-03  2.937697  2.049682
2000-01-04 -3.214983  1.483370
2000-01-05  1.238875  0.102040
#can select one 
>>> pivoted['value2']
variable           A         B         C         D
date
2000-01-03  1.110799  2.128506  2.937697  2.049682
2000-01-04 -1.227871 -3.892923 -3.214983  1.483370
2000-01-05  3.394469  0.110429  1.238875  0.102040


##Pivot - also called Wide format 
DataFrame.pivot(index=None, columns=None, values=None)
    index : string or object, optional
        Column name to use to make new frame's index. 
        If None, uses existing index.
    columns : string or object
        Column name to use to make new frame's columns
    values : string or object, optional
        Column name to use for populating new frame's values. 
        If not specified, all remaining columns will be used 
        and the result will have hierarchically indexed columns
#Example 
df = pd.DataFrame({'foo': ['one','one','one','two','two','two'],
                       'bar': ['A', 'B', 'C', 'A', 'B', 'C'],
                       'baz': [1, 2, 3, 4, 5, 6]})
>>> df
  bar  baz  foo
0   A    1  one
1   B    2  one
2   C    3  one
3   A    4  two
4   B    5  two
5   C    6  two
#Make new DF : make 'foo' as index and DF's column names from 'bar'
#and columns values from baz 
>>> df.pivot(index='foo', columns='bar', values='baz')
bar  A  B  C
foo
one  1  2  3
two  4  5  6

##Unpivot - also called Long format 
pandas.melt(frame, id_vars=None, value_vars=None, 
            var_name=None, value_name='value', col_level=None)
    One or more columns are identifier variables (id_vars), 
    while all other columns, considered measured variables (value_vars), 
    are 'unpivoted' to the row axis, leaving  columns, 'variable' and 'value'.
    Parameters 
        id_vars : tuple, list, or ndarray, optional
            Column(s) to use as identifier variables.
        value_vars : tuple, list, or ndarray, optional
            Column(s) to unpivot. 
            If not specified, uses all columns that are not set as id_vars.
        var_name : scalar
            Name to use for the 'variable' column. 
            If None it uses frame.columns.name or 'variable'.
        value_name : scalar, default 'value'
            Name to use for the 'value' column.
        col_level : int or string, optional
            If columns are a MultiIndex then use this level to melt.
    
>>> df1 = df.pivot(index='foo', columns='bar', values='baz')   
#id_vars only accept string, hence create a new column 
>>> df1['foo'] = df1.index
>>> df1
bar  A  B  C  foo
foo
one  1  2  3  one
two  4  5  6  two
#make new DF:  'foo' is identifier variables (id_vars), 
#and 'A','B','C' are measured variables (value_vars)(here it is default)
>>> pd.melt(df1, id_vars='foo', value_vars=['A','B','C'], value_name='baz')
   foo bar  baz
0  one   A    1
1  two   A    4
2  one   B    2
3  two   B    5
4  one   C    3
5  two   C    6
##Example 
import pandas as pd
df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},
                    'B': {0: 1, 1: 3, 2: 5},
                    'C': {0: 2, 1: 4, 2: 6}})
>>> df
   A  B  C
0  a  1  2
1  b  3  4
2  c  5  6
#New DF with three column headers = (id_vars, variable, value)
#with values from = (value from 'id_vars', column names in 'value_vars', value from 'value_vars' )
>>> pd.melt(df, id_vars=['A'], value_vars=['B'])
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5
>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5
3  a        C      2
4  b        C      4
5  c        C      6

##A spreadsheet-style pivot table
DataFrame.pivot_table(values=None, index=None, columns=None, aggfunc='mean', 
        fill_value=None, margins=False, dropna=True, margins_name='All')
    values : column to aggregate
        Must be numeric 
    index : column, Grouper, array, or list of the previous
        Keys to group by on the pivot table index
    columns : column, Grouper, array, or list of the previous
        Keys to group by on the pivot table column.
    aggfunc : function or list of functions, default numpy.mean
    fill_value : scalar, default None
        Value to replace missing values with
    margins : boolean, default False
        Add all row / columns (e.g. for subtotal / grand totals)
    dropna : boolean, default True
        Do not include columns whose entries are all NaN
    margins_name : string, default 'All'
        Name of the row / column that will contain the totals when margins is True.
#Example 

>>> df = pd.DataFrame({"A": ["foo", "foo", "foo", "foo", "foo",
                            "bar", "bar", "bar", "bar"],
                        "B": ["one", "one", "one", "two", "two",
                            "one", "one", "two", "two"],
                        "C": ["small", "large", "large", "small",
                            "small", "large", "small", "small",
                            "large"],
                        "D": [1, 2, 2, 3, 3, 4, 5, 6, 7]})
>>> df
     A    B      C  D
0  foo  one  small  1
1  foo  one  large  2
2  foo  one  large  2
3  foo  two  small  3
4  foo  two  small  3
5  bar  one  large  4
6  bar  one  small  5
7  bar  two  small  6
8  bar  two  large  7

>>> table = pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'], aggfunc=[np.sum, np.max])
>>> table 
          sum        amax
C       large small large small
A   B
bar one   4.0   5.0   4.0   5.0
    two   7.0   6.0   7.0   6.0
foo one   4.0   1.0   2.0   1.0
    two   NaN   6.0   NaN   3.0

##Wide to long - Less flexible but more user-friendly than melt.
pandas.wide_to_long(df, stubnames, i, j, sep='', suffix='\\d+')[source]
    With stubnames ['A', 'B'], this function expects to find one 
    or more group of columns with format Asuffix1, Asuffix2,..., Bsuffix1, Bsuffix2,... 
    You specify what you want to call this suffix in the resulting long format 
    with j (for example j='year')
    Each row of these wide variables are assumed to be uniquely identified by i 
    (can be a single column name or a list of column names)
    All remaining variables in the data frame are left intact.
    Returns A DataFrame that contains 
    each stub name as a variable, with new index (i, j)
    sep : str, default ''
        A character indicating the separation of the variable names in the wide format, 
        to be stripped from the names in the long format
    suffix : str, default '\d+'
        A regular expression capturing the wanted suffixes. 
        '\d+' captures numeric suffixes.

#Example 
import pandas as pd
import numpy as np
np.random.seed(123)
# format is { column_name : { row_number1:value1, row_number2: value2,... }}
#or {column_name: [ value1,value2,..] }
df = pd.DataFrame({"A1970" : {0 : "a", 1 : "b", 2 : "c"},
                   "A1980" : {0 : "d", 1 : "e", 2 : "f"},
                   "B1970" : {0 : 2.5, 1 : 1.2, 2 : .7},
                   "B1980" : {0 : 3.2, 1 : 1.3, 2 : .1},
                   "X"     : dict(zip(range(3), np.random.randn(3)))
                  })
df["id"] = df.index
>>> df
  A1970 A1980  B1970  B1980         X  id
0     a     d    2.5    3.2 -1.085631   0
1     b     e    1.2    1.3  0.997345   1
2     c     f    0.7    0.1  0.282978   2
#Strip number(year) after 'A' and 'B and make level 2 index as year 
#level1 index is 'id' 
#Values of A*, B* => make as columns(A, B) values
>>> pd.wide_to_long(df, ["A", "B"], i="id", j="year")
                X  A    B
id year
0  1970 -1.085631  a  2.5
1  1970  0.997345  b  1.2
2  1970  0.282978  c  0.7
0  1980 -1.085631  d  3.2
1  1980  0.997345  e  1.3
2  1980  0.282978  f  0.1

#With multuple id columns
df = pd.DataFrame({
    'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],
    'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],
    'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],
    'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]
})
>>> df
   birth  famid  ht1  ht2
0      1      1  2.8  3.4
1      2      1  2.9  3.8
2      3      1  2.2  2.9

#Strip number(age) after 'ht' and make level 3 index as age 
#level1 index is 'famid', 'birth' 
#Values of ht* => make as columns('ht') values
>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age')
>>> l
                  ht
famid birth age
1     1     1    2.8
            2    3.4
      2     1    2.9
            2    3.8

#Undoing 
#reset_index() would convert MultiIndex to columns 
>>> l.reset_index()
    famid  birth age   ht
0       1      1   1  2.8
1       1      1   2  3.4
#set_index() would convert columns to MultiIndex 
>>> l.reset_index().set_index(['famid', 'birth', 'age'])
                  ht
famid birth age
1     1     1    2.8
            2    3.4
      2     1    2.9

#Unstack would take out inner Level of MultiIndex ie age 
#make that (age's values) as column's inner Level, 
#hence converting original column as MultiIndex Column 
>>> w = l.unstack()
>>> w
              ht
age            1    2
famid birth
1     1      2.8  3.4
      2      2.9  3.8
      3      2.2  2.9
>>> w.columns
MultiIndex(levels=[['ht'], ['1', '2']],
           labels=[[0, 0], [0, 1]],
           names=[None, 'age'])
>>> pd.Index(w.columns)  #flatten MultiIndex into list of tuples 
Index([('ht', '1'), ('ht', '2')], dtype='object')
>>> w.columns = pd.Index(w.columns).str.join('') #join is applied on each element ie Tuple 
>>> w
             ht1  ht2
famid birth
1     1      2.8  3.4
      2      2.9  3.8
      3      2.2  2.9
>>> w.reset_index()
   famid  birth  ht1  ht2
0      1      1  2.8  3.4
1      1      2  2.9  3.8
2      1      3  2.2  2.9
#Axis =0 means columnswise 
>>> df.mean(axis=0)
birth    2.000000
famid    2.000000
ht1      2.244444
ht2      3.122222
dtype: float64
#Axis =1 means rowwise 
>>> df.mean(axis=1)
0    2.050
1    2.425
2    2.275
3    2.050
4    2.150
5    2.325
6    2.375
7    2.675
8    2.750
dtype: float64

##Reshaping by stacking and unstacking
DataFrame.unstack(level=-1, fill_value=None)
DataFrame.stack(level=-1, dropna=True)
    level : int, string, or list of these, default -1 (last level)
        Level(s) of index to unstack, can pass level name
    fill_value : replace NaN with this value if the unstack produces missing values
    dropna : boolean, default True
        Whether to drop rows in the resulting Frame/Series with no valid values

#Meanings 
stack(column->row): Move last level(default) of MultiIndex column labels 
       into last level of row labels
unstack(row->column): Move last level(default) of MultiIndex row labels 
       into last level of column labels

#The stack function moves out a level in the DataFrame's columns 
#to produce either:
    •A Series, in the case of a simple column Index
    •A DataFrame, in the case of a MultiIndex in the columns

#Example 
tuples = list(zip(*[['bar', 'bar', 'baz', 'baz',
                    'foo', 'foo', 'qux', 'qux'],
                   ['one', 'two', 'one', 'two',
                    'one', 'two', 'one', 'two']]))
index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])
df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])
df2 = df[:4]
>>> df2
                     A         B
first second                    
bar   one     0.721555 -0.706771
      two    -1.039575  0.271860
baz   one    -0.424972  0.567020
      two     0.276232 -1.087401
stacked = df2.stack() #(column->row)
>>> stacked
first  second   
bar    one     A    0.721555
               B   -0.706771
       two     A   -1.039575
               B    0.271860
baz    one     A   -0.424972
               B    0.567020
       two     A    0.276232
               B   -1.087401
dtype: float64

>>> stacked.unstack() #(row->column)
                     A         B
first second                    
bar   one     0.721555 -0.706771
      two    -1.039575  0.271860
baz   one    -0.424972  0.567020
      two     0.276232 -1.087401

>>> stacked.unstack(1) #(row->column), level=1 (0-based)
second        one       two
first                      
bar   A  0.721555 -1.039575
      B -0.706771  0.271860
baz   A -0.424972  0.276232
      B  0.567020 -1.087401

>>> stacked.unstack(0) #(row->column), level=0 (0-based)
first          bar       baz
second                      
one    A  0.721555 -0.424972
       B -0.706771  0.567020
two    A -1.039575  0.276232
       B  0.271860 -1.087401

#If the indexes have names
>>> stacked.unstack('second')#(row->column), level=1 (0-based)
second        one       two
first                      
bar   A  0.721555 -1.039575
      B -0.706771  0.271860
baz   A -0.424972  0.276232
      B  0.567020 -1.087401

##Combining stack/unstack with stats and GroupBy
columns = pd.MultiIndex.from_tuples([('A', 'cat'), ('B', 'dog'),
                                     ('B', 'cat'), ('A', 'dog')],
                                    names=['exp', 'animal'])
index = pd.MultiIndex.from_product([('bar', 'baz', 'foo', 'qux'),
                                    ('one', 'two')],
                                   names=['first', 'second'])
df = pd.DataFrame(np.random.randn(8, 4), index=index, columns=columns)
>>> df
exp                  A         B                   A
animal             cat       dog       cat       dog
first second                                        
bar   one     0.895717  0.805244 -1.206412  2.565646
      two     1.431256  1.340309 -1.170299 -0.226169
baz   one     0.410835  0.813850  0.132003 -0.827317
      two    -0.076467 -1.187678  1.130127 -1.436737
foo   one    -1.413681  1.607920  1.024180  0.569605
      two     0.875906 -2.211372  0.974466 -2.006747
qux   one    -0.410001 -0.078638  0.545952 -1.219217
      two    -1.226825  0.769804 -1.281247 -0.727707
      
#(column->row), then mean rowwise (index (0), columns (1)), then (row->column)
#here axis=0 means columnwise , =1 mean rowwise
>>> df.stack().mean(1).unstack() 
animal             cat       dog
first second                    
bar   one    -0.155347  1.685445
      two     0.130479  0.557070
baz   one     0.271419 -0.006733
      two     0.526830 -1.312207
foo   one    -0.194750  1.088763
      two     0.925186 -2.109060
qux   one     0.067976 -0.648927
      two    -1.254036  0.021048

# same result, another way
#axis : int, default 0, axis=0 means columnwise , =1 mean rowwise
#level : int, level name, or sequence of such, default None
#If the axis is a MultiIndex (hierarchical), group by a particular level or levels
>>> df.groupby(level=1, axis=1).mean()
animal             cat       dog
first second                    
bar   one    -0.155347  1.685445
      two     0.130479  0.557070
baz   one     0.271419 -0.006733
      two     0.526830 -1.312207
foo   one    -0.194750  1.088763
      two     0.925186 -2.109060
qux   one     0.067976 -0.648927
      two    -1.254036  0.021048
#(column->row),
>>> df.stack().groupby(level=1).mean()
exp            A         B
second                    
one     0.071448  0.455513
two    -0.424186 -0.204486
#(row->column)
>>> df.mean().unstack(0)
exp            A         B
animal                    
cat     0.060843  0.018596
dog    -0.413580  0.232430



##Compute a simple cross-tabulation of two (or more) factors
pandas.crosstab(index, columns, values=None, rownames=None, colnames=None, 
        aggfunc=None, margins=False, margins_name='All', 
        dropna=True, normalize=False)
    index : array-like, Series, or list of arrays/Series
        Values to group by in the rows
    columns : array-like, Series, or list of arrays/Series
        Values to group by in the columns
    values : array-like, optional
        Array of values to aggregate according to the factors. 
        Requires aggfunc be specified.
    aggfunc : function, optional
        If specified, requires values be specified as well
    rownames : sequence, default None
        If passed, must match number of row arrays passed
    colnames : sequence, default None
        If passed, must match number of column arrays passed
    margins : boolean, default False
        Add row/column margins (subtotals)
    margins_name : string, default 'All'
        Name of the row / column that will contain the totals when margins is True.
    dropna : boolean, default True
        Do not include columns whose entries are all NaN
    normalize : boolean, {'all', 'index', 'columns'}, or {0,1}, default False
        Normalize by dividing all values by the sum of values.
        •If passed 'all' or True, will normalize over all values.
        •If passed 'index' will normalize over each row.
        •If passed 'columns' will normalize over each column.
        •If margins is True, will also normalize margin values.

#Example 
df = pd.DataFrame({'A': [1, 2, 2, 2, 2], 'B': [3, 3, 4, 4, 4],
                   'C': [1, 1, np.nan, 1, 1]})
>>> df
   A  B    C
0  1  3  1.0
1  2  3  1.0
2  2  4  NaN
3  2  4  1.0
4  2  4  1.0
>>> pd.crosstab(df.A, df.B)
B  3  4
A      
1  1  0
2  1  3
>>> pd.crosstab(df.A, df.B, values=df.C, aggfunc=np.sum, normalize=True,margins=True)
B       3    4   All
A                   
1    0.25  0.0  0.25
2    0.25  0.5  0.75
All  0.50  0.5  1.00

foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])
bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])
>>> pd.crosstab(foo, bar)
col_0  d  e  f
row_0         
a      1  0  0
b      0  1  0
c      0  0  0

##Represents a categorical variable in classic R / S-plus fashion
class pandas.Categorical(values, categories=None, ordered=None, 
            dtype=None, fastpath=False)
    Categoricals can only take on only a limited, 
    and usually fixed, number of possible values (categories). 
        values : list-like
            The values of the categorical. 
            If categories are given, values not in categories will be replaced with NaN.
        categories : Index-like (unique), optional
            The unique categories for this categorical. 
            If not given, the categories are assumed to be the unique values of values.
        ordered : boolean, (default False)
            Whether or not this categorical is treated as a ordered categorical. 
            If not given, the resulting categorical will not be ordered.
        dtype : CategoricalDtype
            An instance of CategoricalDtype to use for this categorical
#Example 
s = pd.Series(["a","b","c","a"], dtype="category")
>>> s
Out[2]: 
0    a
1    b
2    c
3    a
dtype: category
Categories (3, object): [a, b, c]
#or conversion 
df = pd.DataFrame({"A":["a","b","c","a"]})
>>> df["B"] = df["A"].astype('category')
   A  B
0  a  a
1  b  b
2  c  c
3  a  a
#or using cut 
df = pd.DataFrame({'value': np.random.randint(0, 100, 20)})
labels = [ "{0} - {1}".format(i, i + 9) for i in range(0, 100, 10) ]
df['group'] = pd.cut(df.value, range(0, 105, 10), right=False, labels=labels)
>>> df.head(10)
   value    group
0     65  60 - 69
1     49  40 - 49
2     56  50 - 59
3     43  40 - 49
4     43  40 - 49
5     91  90 - 99
6     32  30 - 39
7     87  80 - 89
8     36  30 - 39
9      8    0 - 9
#Or using Categorical constructor 
raw_cat = pd.Categorical(["a","b","c","a"], categories=["b","c","d"],ordered=False)
>>> s = pd.Series(raw_cat)
0    NaN
1      b
2      c
3    NaN
dtype: category
Categories (3, object): [b, c, d]

cat = pd.Categorical(["a", "c", "c", np.nan], categories=["b", "a", "c"])
df = pd.DataFrame({"cat":cat, "s":["a", "c", "c", np.nan]})
>>> df.describe()
       cat  s
count    3  3
unique   2  2
top      c  c
freq     2  2
>> df["cat"].describe()
count     3
unique    2
top       c
freq      2
Name: cat, dtype: object


##Encode input values as an enumerated type or categorical variable
pandas.factorize(values, sort=False, order=None, na_sentinel=-1, size_hint=None)
    values : ndarray (1-d)
        Sequence
    sort : boolean, default False
        Sort by values
    na_sentinel : int, default -1
        Value to mark 'not found'
    size_hint : hint to the hashtable sizer
    Returns:
    labels : the indexer to the original array
    uniques : ndarray (1-d) or Index
        the unique values. Index is returned when passed values is Index or Series
        note: an array of Periods will ignore sort as it returns an always sorted
#Example 
>>> x = pd.Series(['A', 'A', np.nan, 'B', 3.14, np.inf])
0       A
1       A
2     NaN
3       B
4    3.14
5     inf
dtype: object
labels, uniques = pd.factorize(x)
>>> labels
array([ 0,  0, -1,  1,  2,  3])  #index of uniques array 
>>> uniques
Index(['A', 'B', 3.14, inf], dtype='object')
>>> pd.factorize(x, sort=True)
(array([ 2,  2, -1,  3,  0,  1]),
 Index([3.14, inf, u'A', u'B'], dtype='object'))
>>> x.astype('category')
0       A
1       A
2     NaN
3       B
4    3.14
5     inf
dtype: category
Categories (4, object): [3.14, inf, A, B]


##pandas.cut and pandas.qcut 
pandas.cut(x, bins, right=True, labels=None, retbins=False, 
            precision=3, include_lowest=False)
    Return indices of half-open bins to which each value of x belongs.
    x : array-like
        Input array to be binned. It has to be 1-dimensional.
    bins : int, sequence of scalars, or IntervalIndex
        If bins is an int, it defines the number of equal-width bins in the range of x. 
        However, in this case, the range of x is extended by .1% on each side to include the min or max values of x. 
        If bins is a sequence it defines the bin edges allowing for non-uniform bin width. 
        No extension of the range of x is done in this case.
    right : bool, optional
        Indicates whether the bins include the rightmost edge or not. If right == True (the default), then the bins [1,2,3,4] indicate (1,2], (2,3], (3,4].
    labels : array or boolean, default None
        Used as labels for the resulting bins. Must be of the same length as the resulting bins. If False, return only integer indicators of the bins.
    retbins : bool, optional
        Whether to return the bins or not. Can be useful if bins is given as a scalar.
    precision : int, optional
        The precision at which to store and display the bins labels
    include_lowest : bool, optional
        Whether the first interval should be left-inclusive or not.
    Returns:
    out : Categorical or Series or array of integers if labels is False
        The return type (Categorical or Series) depends on the input: a Series of type category if input is a Series else Categorical. Bins are represented as categories when categorical data is returned.
    bins : ndarray of floats
        Returned only if retbins is True.
#Example 
ages = np.array([10, 15, 13, 12, 23, 25, 28, 59, 60])
>>> pd.cut(ages, bins=3)
[(9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (9.95, 26.667], (26.667, 43.333], (43.333, 60.0], (43.333, 60.0]]
Categories (3, interval[float64]): [(9.95, 26.667] < (26.667, 43.333] < (43.333, 60.0]]
>>> c = pd.cut(ages, bins=[0, 18, 35, 70])
[(0, 18], (0, 18], (0, 18], (0, 18], (18, 35], (18, 35], (18, 35], (35, 70], (35, 70]]
Categories (3, interval[int64]): [(0, 18] < (18, 35] < (35, 70]]


pandas.qcut(x, q, labels=None, retbins=False, precision=3, duplicates='raise')
    Quantile-based discretization function. 
    Discretize variable into equal-sized buckets based on rank or based on sample quantiles
    x : 1d ndarray or Series
    q : integer or array of quantiles
        Number of quantiles. 10 for deciles, 4 for quartiles, etc. 
        Alternately array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles
    labels : array or boolean, default None
        Used as labels for the resulting bins. Must be of the same length as the resulting bins. If False, return only integer indicators of the bins.
    retbins : bool, optional
        Whether to return the (bins, labels) or not. Can be useful if bins is given as a scalar.
    precision : int, optional
        The precision at which to store and display the bins labels
    duplicates : {default 'raise', 'drop'}, optional
        If bin edges are not unique, raise ValueError or drop non-uniques.
    Returns:
        out : Categorical or Series or array of integers if labels is False
            The return type (Categorical or Series) depends on the input: 
            a Series of type category if input is a Series else Categorical. 
            Bins are represented as categories when categorical data is returned.
        bins : ndarray of floats
            Returned only if retbins is True.
#Example  
>>> pd.qcut(range(5), 4)
[(-0.001, 1.0], (-0.001, 1.0], (1.0, 2.0], (2.0, 3.0], (3.0, 4.0]]
Categories (4, interval[float64]): [(-0.001, 1.0] < (1.0, 2.0] ...

>>> pd.qcut(range(5), 3, labels=["good", "medium", "bad"])
[good, good, medium, bad, bad]
Categories (3, object): [good < medium < bad]

>>> pd.qcut(range(5), 4, labels=False)
array([0, 0, 1, 2, 3])

#Difference between cut and qcut 
factors = np.random.randn(30)
>>> pd.qcut(factors, 5).value_counts()
[-2.578, -0.829]    6
(-0.829, -0.36]     6
(-0.36, 0.366]      6
(0.366, 0.868]      6
(0.868, 2.617]      6
>>> pd.cut(factors, 5).value_counts()
(-2.583, -1.539]    5
(-1.539, -0.5]      5
(-0.5, 0.539]       9
(0.539, 1.578]      9
(1.578, 2.617]      2


##Convert categorical variable into dummy/indicator variables
pandas.get_dummies(data, prefix=None, prefix_sep='_', 
            dummy_na=False, columns=None, sparse=False, drop_first=False)
    data : array-like, Series, or DataFrame
    prefix : string, list of strings, or dict of strings, default None
        String to append DataFrame column names 
        Pass a list with length equal to the number of columns 
        when calling get_dummies on a DataFrame. 
        Alternatively, prefix can be a dictionary mapping column names to prefixes.
    prefix_sep : string, default '_'
        If appending prefix, separator/delimiter to use. Or pass a list or dictionary as with prefix.
    dummy_na : bool, default False
        Add a column to indicate NaNs, if False NaNs are ignored.
    columns : list-like, default None
        Column names in the DataFrame to be encoded. 
        If columns is None then all the columns with object or category dtype will be converted.
    sparse : bool, default False
        Whether the dummy columns should be sparse or not. 
        Returns SparseDataFrame if data is a Series or if all columns are included. Otherwise returns a DataFrame with some SparseBlocks.
    drop_first : bool, default False
        Whether to get k-1 dummies out of k categorical levels 
        by removing the first level.
#Example 
df = pd.DataFrame({'key': list('bbacab'), 'data1': range(6)})
>>> pd.get_dummies(df['key'])
   a  b  c
0  0  1  0
1  0  1  0
2  1  0  0
3  0  0  1
4  1  0  0
5  0  1  0
>>> dummies = pd.get_dummies(df['key'], prefix='key')
   key_a  key_b  key_c
0      0      1      0
1      0      1      0
2      1      0      0
3      0      0      1
4      1      0      0
5      0      1      0
>>> df[['data1']].join(dummies)
   data1  key_a  key_b  key_c
0      0      0      1      0
1      1      0      1      0
2      2      1      0      0
3      3      0      0      1
4      4      1      0      0
5      5      0      1      0
#With cut 
>>> values = np.random.randn(10)
array([ 0.4082, -1.0481, -0.0257, -0.9884,  0.0941,  1.2627,  1.29  ,
        0.0824, -0.0558,  0.5366])
bins = [0, 0.2, 0.4, 0.6, 0.8, 1]
>>> pd.get_dummies(pd.cut(values, bins))
   (0.0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1.0]
0           0           0           1           0           0
1           0           0           0           0           0
2           0           0           0           0           0
3           0           0           0           0           0
4           1           0           0           0           0
5           0           0           0           0           0
6           0           0           0           0           0
7           1           0           0           0           0
8           0           0           0           0           0
9           0           0           1           0           0

#for DF : By default all categorical variables 
#(categorical in the statistical sense, those with object or categorical dtype) are encoded as dummy variables.
df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['c', 'c', 'b'],'C': [1, 2, 3]})
>>> pd.get_dummies(df)
   C  A_a  A_b  B_b  B_c
0  1    1    0    0    1
1  2    0    1    0    1
2  3    1    0    1    0
#control the columns that are encoded with the columns keyword.
>>> pd.get_dummies(df, columns=['A'])
   B  C  A_a  A_b
0  c  1    1    0
1  c  2    0    1
2  b  3    1    0










###Pandas - Remote data access - http://pandas.pydata.org/pandas-docs/stable/remote_data.html
#update datareader 
$ pip install pandas pandas_datareader --upgrade
#note 0.4.1 fixes Yahoo! finance, now available 0.5.0
#if still not working, check https://pypi.python.org/pypi/fix-yahoo-finance

from pandas_datareader import data, wb

#can access from
◦Yahoo! Finance
◦Google Finance
◦Enigma
◦Quandl
◦FRED
◦Fama/French
◦World Bank
◦OECD
◦Eurostat
◦EDGAR Index
◦TSP Fund Data
◦Nasdaq Trader Symbol Definitions

##Main Interface 
DataReader(name, data_source=None, start=None, end=None, 
            retry_count=3, pause=0.001, session=None, access_key=None)
    Imports data from a number of online sources.
    Currently supports Yahoo! Finance, Google Finance, St. Louis FED (FRED),
    Kenneth French's data library, and the SEC's EDGAR Index.
    Parameters
    ----------
    name : str or list of strs
        the name of the dataset. Some data sources (yahoo, google, fred) will
        accept a list of names.
    data_source: {str, None}
        the data source ("yahoo", "yahoo-actions", "yahoo-dividends",
        "google", "fred", "ff", or "edgar-index")
    start : {datetime, None}
        left boundary for range (defaults to 1/1/2010)
    end : {datetime, None}
        right boundary for range (defaults to today)
    retry_count : {int, 3}
        Number of times to retry query request.
    pause : {numeric, 0.001}
        Time, in seconds, to pause between consecutive queries of chunks. If
        single value given for symbol, represents the pause between retries.
    session : Session, default None
            requests.sessions.Session instance to be used

#Examples

# Data from Yahoo! Finance
gs = DataReader("GS", "yahoo")

# Corporate Actions (Dividend and Split Data)
# with ex-dates from Yahoo! Finance
gs = DataReader("GS", "yahoo-actions")

# Data from Google Finance
aapl = DataReader("AAPL", "google")

# Data from FRED
vix = DataReader("VIXCLS", "fred")

# Data from Fama/French
ff = DataReader("F-F_Research_Data_Factors", "famafrench")
ff = DataReader("F-F_Research_Data_Factors_weekly", "famafrench")
ff = DataReader("6_Portfolios_2x3", "famafrench")
ff = DataReader("F-F_ST_Reversal_Factor", "famafrench")

# Data from EDGAR index
ed = DataReader("full", "edgar-index")
ed2 = DataReader("daily", "edgar-index")


#Example Yahoo! Finance
import pandas_datareader.data as web
import datetime

start = datetime.datetime(2010, 1, 1)  #year, month, day 
end = datetime.datetime(2013, 1, 27)
f = web.DataReader("F", 'yahoo', start, end)  #'google' for google finance
>>> f.dtypes
Open         float64
High         float64
Low          float64
Close        float64
Adj Close    float64
Volume         int64
dtype: object
>>> f.head(3)
             Open   High    Low  Close  Adj Close     Volume
Date
2009-12-31  10.04  10.06   9.92  10.00   7.880220   31253700
2010-01-04  10.17  10.28  10.05  10.28   8.100866   60855800
2010-01-05  10.45  11.24  10.40  10.96   8.636724  215620200
>>> f.ix['2010-01-04']
Open         1.017000e+01
High         1.028000e+01
Low          1.005000e+01
Close        1.028000e+01
Adj Close    8.100866e+00
Volume       6.085580e+07
Name: 2010-01-04 00:00:00, dtype: float64


##Historical corporate actions (Dividends and Stock Splits) with ex-dates from Yahoo! Finance.

import pandas_datareader.data as web
import datetime

start = datetime.datetime(2010, 1, 1)
end = datetime.datetime(2015, 5, 9)
>>> web.DataReader('AAPL', 'yahoo-actions', start, end)
              value    action
Date                         
2015-05-07  0.52000  DIVIDEND
2015-02-05  0.47000  DIVIDEND
2014-11-06  0.47000  DIVIDEND
2014-08-07  0.47000  DIVIDEND
2014-06-09  0.00000     SPLIT
2014-05-08  0.47000  DIVIDEND
2014-02-06  0.43571  DIVIDEND
2013-11-06  0.43571  DIVIDEND
2013-08-08  0.43571  DIVIDEND
2013-05-09  0.43571  DIVIDEND
2013-02-07  0.37857  DIVIDEND
2012-11-07  0.37857  DIVIDEND
2012-08-09  0.37857  DIVIDEND



##The YahooQuotesReader class allows to get quotes data from Yahoo! Finance.
import pandas_datareader.data as web
amzn = web.get_quote_yahoo('AMZN')
>>> amzn
          PE change_pct     last  short_ratio    time
AMZN  195.83     +0.09%  1039.87         1.14  4:00pm

##The get_all_data method downloads and caches option data for all expiry months 
#and provides a formatted DataFrame with a hierarchical index, 
from pandas_datareader.data import Options
aapl = Options('aapl', 'yahoo')
data = aapl.get_all_data()
>>> data.iloc[0:5, 0:5]
                                              Last     Bid     Ask       Chg  \
Strike Expiry     Type Symbol                                                  
2.5    2017-08-18 call AAPL170818C00002500  147.70  147.45  148.05  0.000000   
                  put  AAPL170818P00002500    0.02    0.00    0.02  0.000000   
       2018-01-19 call AAPL180119C00002500  150.18  152.50  153.20 -2.130005   
5.0    2017-08-18 call AAPL170818C00005000  145.80  144.95  145.50  0.000000   
       2018-01-19 call AAPL180119C00005000  147.98  150.05  150.70  0.000000   

                                              PctChg  
Strike Expiry     Type Symbol                         
2.5    2017-08-18 call AAPL170818C00002500  0.000000  
                  put  AAPL170818P00002500  0.000000  
       2018-01-19 call AAPL180119C00002500 -1.390978  
5.0    2017-08-18 call AAPL170818C00005000  0.000000  
       2018-01-19 call AAPL180119C00005000  0.000000  

#Show the $100 strike puts at all expiry dates:
>>> data.loc[(100, slice(None), 'put'),:].iloc[0:5, 0:5]
                                            Last   Bid   Ask  Chg  PctChg
Strike Expiry     Type Symbol                                            
100    2017-07-28 put  AAPL170728P00100000  0.01  0.00  0.02    0       0
       2017-08-18 put  AAPL170818P00100000  0.01  0.00  0.01    0       0
       2017-08-25 put  AAPL170825P00100000  0.02  0.00  0.02    0       0
       2017-09-01 put  AAPL170901P00100000  0.04  0.00  0.03    0       0
       2017-09-15 put  AAPL170915P00100000  0.02  0.01  0.02    0       0

#Show the volume traded of $100 strike puts at all expiry dates:
>>> data.loc[(100, slice(None), 'put'),'Vol'].head()
Strike  Expiry      Type  Symbol             
100     2017-07-28  put   AAPL170728P00100000      1
        2017-08-18  put   AAPL170818P00100000    620
        2017-08-25  put   AAPL170825P00100000      2
        2017-09-01  put   AAPL170901P00100000      1
        2017-09-15  put   AAPL170915P00100000      3
Name: Vol, dtype: float64

##Google Finance

import pandas_datareader.data as web
import datetime
start = datetime.datetime(2010, 1, 1)
end = datetime.datetime(2013, 1, 27)
f = web.DataReader("F", 'google', start, end)
>>> f.ix['2010-01-04']

Open            10.17
High            10.28
Low             10.05
Close           10.28
Volume    60855796.00
Name: 2010-01-04 00:00:00, dtype: float64

##Google Finance Quotes
#The GoogleQuotesReader class allows to get quotes data from Google Finance.

import pandas_datareader.data as web
q = web.get_quote_google(['AMZN', 'GOOG'])
>>> q
      change_pct     last                time
AMZN        0.09  1039.87 2017-07-25 16:00:00
GOOG       -3.02   950.70 2017-07-25 16:00:00

##The Options class allows the download of options data from Google Finance.
##Available expiry dates can be accessed from the expiry_dates property.

from pandas_datareader.data import Options
goog = Options('goog', 'google')
data = goog.get_options_data(expiry=goog.expiry_dates[0])
>>> data.iloc[0:5, 0:5]
                                              Last     Bid     Ask    Chg  \
Strike Expiry     Type Symbol                                               
340    2018-01-19 call GOOG180119C00340000  578.00  611.20  615.50   0.00   
                  put  GOOG180119P00340000    0.05     NaN    0.05   0.00   
350    2018-01-19 call GOOG180119C00350000  604.50  601.30  604.20 -30.80   
                  put  GOOG180119P00350000    0.05    0.05    0.10  -0.05   
360    2018-01-19 call GOOG180119C00360000  612.40  591.30  595.50   0.00   

                                            PctChg  
Strike Expiry     Type Symbol                       
340    2018-01-19 call GOOG180119C00340000    0.00  
                  put  GOOG180119P00340000    0.00  
350    2018-01-19 call GOOG180119C00350000   -4.85  
                  put  GOOG180119P00350000  -50.00  
360    2018-01-19 call GOOG180119C00360000    0.00  

 
  
  
  

###Pandas - Analyzing Data with Pandas - Time Series


import matplotlib.pyplot as plt
import datetime
import pandas as pd
import pandas_datareader.data as web

#for NSE , check nsetools python 
#Read the data - check retry_count and internet connection, else ERROR:Unable to read URL
#year,month,day
yhoo = web.DataReader("F", "yahoo", datetime.datetime(2007, 1, 1), datetime.datetime(2012,1,1), retry_count=20)


#Plot stock price and volume

#4x4 gridspecs, specification for 0,0 cell , row spanning 3 rows, column spanning 4 columns 
top = plt.subplot2grid((4,4), (0, 0), rowspan=3, colspan=4)
top.plot(yhoo.index, yhoo["Close"])
plt.title('Yahoo Price from 2007 - 2012')

#4x4 gridspecs, specification for 3,0 cell , row spanning 1 rows, column spanning 4 columns 
bottom = plt.subplot2grid((4,4), (3,0), rowspan=1, colspan=4)
bottom.bar(yhoo.index, yhoo['Volume'])
plt.title('Yahoo Trading Volume')

plt.gcf().set_size_inches(15,8)

#Calculate moving averages
#arg : Series, DataFrame
#window : int,Size of the moving window. 
#This is the number of observations used for calculating the statistic.
mavg = yhoo['30_MA_Open'] = pd.rolling_mean(yhoo['Open'], 30)
#Return the last n=5(default) rows.
yhoo['30_MA_Open'].tail()

#Look at selected rows
yhoo[160:165]

#Index into a particular date
yhoo.ix['2010-01-04']

#Look at volume for the time period
yhoo.Volume.plot()

#More plots- complete DF 
yhoo.plot(subplots = True, figsize = (8, 8));
plt.legend(loc = 'best')
plt.show()

#Again plot rolling mean 
close_px = yhoo['Adj Close']
mavg = pd.rolling_mean(close_px, 30) #retruns Series 
#Or better yet
mavg = close_px.rolling(window=30,center=False).mean()#retruns Series 

#Moving average plot
yhoo.Close.plot(label='Yahoo')
mavg.plot(label='mavg')
plt.legend()
plt.gcf().set_size_inches(15,8)

#KDE plot
yhoo.Close.plot(kind='kde')











###Pandas -  Series reference 
#check http://pandas.pydata.org/pandas-docs/stable/dsintro.html

#Series is a one-dimensional labelled array capable of holding any data type
# the elements' labels are called index.

>>> s = pd.Series(data, index=index)

#data can  be
•a Python dict
•an ndarray
•a scalar value (like 5)

#Example
import pandas as pd
import numpy as np

#Creation-  From ndarray, default index = [0, ..., len(data) - 1].
s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])

>> s.index
Index([u'a', u'b', u'c', u'd', u'e'], dtype='object')

#Creation - From dict, index are ke ys
d = {'a' : 0., 'b' : 1., 'c' : 2.}
pd.Series(d)
a    0.0
b    1.0
c    2.0
dtype: float64
#selective values
pd.Series(d, index=['b', 'c', 'd', 'a'])
b    1.0
c    2.0
d    NaN  #NaN (not a number) is the standard missing data marker used in pandas
a    0.0
dtype: float64



#Creation - From scalar value  - index is mu st
pd.Series(5., index=['a', 'b', 'c', 'd', 'e'])
a    5.0
b    5.0
c    5.0
d    5.0
e    5.0
dtype: float64



## Series is ndarray-like and is a valid argument to most NumPy functions.
>> s[0]
Out[11]: -2.7827595933769937

>> s[:3]
Out[12 ]:
a   -2.7828
b    0.4264
c   -0.6505
dtype: float64

>>  s[s > s.median()]
Out[13 ]:
b    0.4264
d    1.1465
dtype: float64

>>  s[ [4, 3, 1] ]  #many indices
Out[14 ]:
e   -0.6631
d    1.1465
b    0.4264
dtype: float64

>>  np.exp(s)   #use of numpy
Out[15 ]:
a    0.0619
b    1.5318
c    0.5218
d    3.1472
e    0.5153
dtype: float64


## Series is dict-like - can get and set values by index label:
>>  s['a']
Out[16]: -2.7827595933769937

>>  s['e'] = 12.

>>  s
Out[18 ]:
a    -2.7828
b     0.4264
c    -0.6505
d     1.1465
e    12.0000
dtype: float64

>>  'e' in s
Out[19]: True

>>  'f' in s
Out[20]: False

>>> s['f']
KeyError: 'f'

>>  s.get('f')

>>  s.get('f', np.nan)
Out[22]: nan

#Can be heterogeneous
>>> s = pd.Series([1,2,3,4.0,"ok"], index=['a', 'b', 'c', 'd', 'e'])
>>> s
a     1
b     2
c     3
d     4
e    ok
dtype: object
>>> s['f'] = pd.Series([2.0,3.0,5], index=['1','2','3'])
>>> s
a                                            1
b                                            2
c                                            3
d                                            4
e                                           ok
f    1    2.0
2    3.0
3    5.0
dtype: float64
dtype: object

>>> s['f']['1']
2.0

## Vectorized operations and label alignment with Series
#all arithmatic operations are supported, all numpy method can take series
>>  s + s
Out[23 ]:
a    -5.5655
b     0.8529
c    -1.3010
d     2.2930
e    24.0000
dtype: float64

>>  s * 2
Out[24 ]:
a    -5.5655
b     0.8529
c    -1.3010
d     2.2930
e    24.0000
dtype: float64

>>  np.exp(s)
Out[25 ]:
a         0.0619
b         1.5318
c         0.5218
d         3.1472
e    162754.7914
dtype: float64


# operations  between Series automatically align the data based on label.
#operation between unaligned Series will have the union of the indexes involved

>>> s[1:] + s[:-1]
Out[26 ]:
a       NaN
b    0.8529
c   -1.3010
d    2.2930
e       NaN
dtype: float64

#Name attribute
>>> s = pd.Series(np.random.randn(5), name='something')
Out[28 ]:
0    0.5410
1   -1.1747
2    0.1287
3    0.0430
4   -0.4287
Name: something, dtype: float64

>>> s.name
Out[29]: 'something'

#rename
>>> s2 = s.rename("different")
>>> s2.name
'different'



##Series - Constructor
Series([data, index, dtype, name, copy, ...   ])

##Series - Attributes
Series.index            axis/elements  labels
Series.values           Return Series as ndarray or ndarray-li ke

Series.dtype            return the dtype object of the underlying data
Series.shape            return a tuple of the shape of the underlying data
Series.nbytes           return the number of bytes in the underlying data
Series.ndim             return the number of dimensions of the underlying data,
Series.size             return the number of elements in the underlying data

Series.T                return the transpose, which is by definition self

##Series - copy and null checking 
Series.copy([deep])                             Make a copy of this objects dat a.
Series.isnull()                                 Return a boolean same-sized object indicating if the values are null.
Series.notnull()                                Return a boolean same-sized object indicating if the values are not null.

##Series - Conversion to each element by a single arg function 
Series.astype(dtype[, copy, raise_on_error])    Cast object to input numpy.dty pe



##Series - Indexing, iteration(check details in DF section)
Series.get(key[, default])      Get item from object for given key (DataFrame column, Panel slice, etc. ).
Series.at                       Fast label-based scalar access or
Series.iat                      Fast integer location scalar accesso r.
Series.ix                       A primarily label-location based indexer, with integer position fallbac k.
Series.loc                      Purely label-location based indexer for selection by labe l.
Series.iloc                     Purely integer-location based indexing for selection by positio n.
Series.__iter__()               provide iteration over the values of the Seri es
Series.iteritems()              Lazily iterate over (index, value) tupl es


##Series - Binary operator functions
Series.add(other[, level, fill_value, axis])        Addition of series and other, element-wise (binary operator add ).
Series.sub(other[, level, fill_value, axis])        Subtraction of series and other, element-wise (binary operator sub ).
Series.mul(other[, level, fill_value, axis])        Multiplication of series and other, element-wise (binary operator mul ).
Series.div(other[, level, fill_value, axis])        Floating division of series and other, element-wise (binary operator truediv ).

Series.truediv(other[, level, fill_value, axis])    Floating division of series and other, element-wise (binary operator truediv ).
Series.floordiv(other[, level, fill_value, axis])   Integer division of series and other, element-wise (binary operator floordiv ).

Series.mod(other[, level, fill_value, axis])        Modulo of series and other, element-wise (binary operator mod ).
Series.pow(other[, level, fill_value, axis])        Exponential power of series and other, element-wise (binary operator pow ).

Series.radd(other[, level, fill_value, axis])       Addition of series and other, element-wise (binary operator radd ).
Series.rsub(other[, level, fill_value, axis])       Subtraction of series and other, element-wise (binary operator rsub ).
Series.rmul(other[, level, fill_value, axis])       Multiplication of series and other, element-wise (binary operator rmul ).
Series.rdiv(other[, level, fill_value, axis])       Floating division of series and other, element-wise (binary operator rtruediv ).
Series.rtruediv(other[, level, fill_value, axis])   Floating division of series and other, element-wise (binary operator rtruediv ).
Series.rfloordiv(other[, level, fill_value, ...])   Integer division of series and other, element-wise (binary operator rfloordiv ).
Series.rmod(other[, level, fill_value, axis])       Modulo of series and other, element-wise (binary operator rmod ).
Series.rpow(other[, level, fill_value, axis])       Exponential power of series and other, element-wise (binary operator rpow ).

Series.combine(other, func[, fill_value])           Perform elementwise binary operation on two Series using given functi on
Series.combine_first(other)                         Combine Series values, choosing the calling Series's values firs t.
Series.round([decimals])                            Round each value in a Series to the given number of decimal s.

Series.lt(other[, axis  ])
Series.gt(other[, axis  ])
Series.le(other[, axis  ])
Series.ge(other[, axis  ])
Series.ne(other[, axis  ])
Series.eq(other[, axis  ])


##Series - Function application, GroupBy & Window
Series.apply(func[, convert_dtype, args])           Invoke function on values of Serie s.
Series.map(arg[, na_action])                        Map values of Series using input (which can be a dict, Series, or function)
Series.groupby([by, axis, level, as_index, ...])    Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns.
#Below returns Rolling, Expanding, EWM class 
#which has many methods eg mean, sum etc , check DF example 
Series.rolling(window[, min_periods, freq, ...])    Provides rolling transformation s.
Series.expanding([min_periods, freq, ...])          Provides expanding transformation s.
Series.ewm([com, span, halflife, alpha, ...])       Provides exponential weighted functio ns

#Example- apply 
>>> series = pd.Series([20, 21, 12], index=['London','New York','Helsinki'])
>>> series.apply(lambda x: x**2)

#Example- map
>>> x
one   1
two   2
three 3

>>> y
1  foo
2  bar
3  baz

>>> x.map(y)
one   foo
two   bar
three baz




##Series - Computations / Descriptive Stats

Series.abs()                                    Return an object with absolute value taken–only applicable to objects that are all numeri c.
Series.all([axis, bool_only, skipna, level])    Return whether all elements are True over requested ax is
Series.any([axis, bool_only, skipna, level])    Return whether any element is True over requested ax is

Series.autocorr([lag])                          Lag-N autocorrelati on
Series.corr(other[, method, min_periods])       Compute correlation with other Series, excluding missing valu es
Series.cov(other[, min_periods])                Compute covariance with Series, excluding missing valu es

Series.between(left, right[, inclusive])        Return boolean Series equivalent to left <= series <= righ t.
Series.clip([lower, upper, axis])               Trim values at input threshold(s ).
Series.clip_lower(threshold[, axis])            Return copy of the input with values below given value(s) truncate d.
Series.clip_upper(threshold[, axis])            Return copy of input with values above given value(s) truncate d.

Series.count([level])                           Return number of non-NA/null observations in the Seri es
Series.cummax([axis, dtype, out, skipna])       Return cumulative cummax over requested axi s.
Series.cummin([axis, dtype, out, skipna])       Return cumulative cummin over requested axi s.
Series.cumprod([axis, dtype, out, skipna])      Return cumulative cumprod over requested axi s.
Series.cumsum([axis, dtype, out, skipna])       Return cumulative cumsum over requested axi s.

Series.describe([percentiles, include, exclude])Generate various summary statistics, excluding NaN value s.

Series.diff([periods])                          1st discrete difference of obje ct
Series.factorize([sort, na_sentinel])           Encode the object as an enumerated type or categorical variab le

Series.kurt([axis, skipna, level, numeric_only])Return unbiased kurtosis over requested axis using Fisher's definition of kurtosis (kurtosis of normal == 0.0 ).
Series.mad([axis, skipna, level])               Return the mean absolute deviation of the values for the requested ax is
Series.skew([axis, skipna, level, numeric_only])Return unbiased skew over requested ax is

Series.max([axis, skipna, level, numeric_only]) This method returns the maximum of the values in the objec t.
Series.min([axis, skipna, level, numeric_only]) This method returns the minimum of the values in the objec t.
Series.sum([axis, skipna, level, numeric_only]) Return the sum of the values for the requested ax is

Series.mean([axis, skipna, level, numeric_only])Return the mean of the values for the requested ax is
Series.median([axis, skipna, level, ...])       Return the median of the values for the requested ax is
Series.mode()                                   Returns the mode(s) of the datase t.
Series.std([axis, skipna, level, ddof, ...])    Return sample standard deviation over requested axi s.
Series.var([axis, skipna, level, ddof, ...])    Return unbiased variance over requested axi s.
Series.quantile([q, interpolation])             Return value at the given quantile, a la numpy.percentil e.

Series.nlargest(*args, **kwargs)                Return the largest n element s.
Series.nsmallest(*args, **kwargs)               Return the smallest n element s.

Series.pct_change([periods, fill_method, ...])  Percent change over given number of period s.
Series.prod([axis, skipna, level, numeric_only])Return the product of the values for the requested ax is
Series.rank([axis, method, numeric_only, ...])  Compute numerical data ranks (1 through n) along axi s.
Series.sem([axis, skipna, level, ddof, ...])    Return unbiased standard error of the mean over requested axi s.

Series.unique()                                 Return array of unique values in the objec t.
Series.nunique([dropna])                        Return number of unique elements in the objec t.
Series.is_unique                                Return boolean if values in the object are uniq ue
Series.value_counts([normalize, sort, ...])     Returns object containing counts of unique value s.







##Series - Reindexing / Selection / Label manipulation
Series.align(other[, join, axis, level, ...])   Align two object on their axes with t he
Series.drop(labels[, axis, level, inplace, ...])Return new object with labels in requested axis remove d.
Series.drop_duplicates(*args, **kwargs)         Return Series with duplicate values remov ed

Series.duplicated(*args, **kwargs)              Return boolean Series denoting duplicate valu es
Series.equals(other)                            Determines if two NDFrame objects contain the same element s.

Series.first(offset)                            Convenience method for subsetting initial periods of time series data based on a date offse t.
Series.last(offset)                             Convenience method for subsetting final periods of time series data based on a date offse t.

Series.head([n])                                Returns first n ro ws
Series.tail([n])                                Returns last n ro ws

Series.idxmax([axis, skipna])                   Index of first occurrence of maximum of value s.
Series.idxmin([axis, skipna])                   Index of first occurrence of minimum of value s.
Series.isin(values)                             Return a boolean Series showing whether each element in the Series is exactly contained in the passed sequence of value s.

Series.reindex([index])                         Conform Series to new index with optional filling logic, placing NA/NaN in locations having no value in the previous inde x.
Series.reindex_like(other[, method, copy, ...]) Return an object with matching indices to mysel f.

Series.rename([index])                          Alter axes input function or function s.
Series.reset_index([level, drop, name, inplace])Analogous to the pandas.DataFrame.reset_index() function, see docstring ther e.
Series.sample([n, frac, replace, weights, ...]) Returns a random sample of items from an axis of objec t.

Series.take(indices[, axis, convert, is_copy])  return Series corresponding to requested indic es
Series.truncate([before, after, axis, copy])    Truncates a sorted NDFrame before and/or after some particular date s.

Series.select(crit[, axis])                     Return data corresponding to axis labels matching criter ia
Series.where(cond[, other, inplace, axis, ...]) Return an object of same shape as self and whose corresponding entries are from self where cond is True and otherwise are from othe r.
Series.mask(cond[, other, inplace, axis, ...])  Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from othe r.


##Series - Missing data handling
Series.dropna([axis, inplace])                  Return Series without null values
Series.fillna([value, method, axis, ...])       Fill NA/NaN values using the specified method
Series.interpolate([method, axis, limit, ...])  Interpolate values according to different methods.


##Series- Reshaping, sorting
Series.argsort([axis, kind, order])             Overrides ndarray.argsor t.
Series.reorder_levels(order)                    Rearrange index levels using input orde r.
Series.sort_values([axis, ascending, ...])      Sort by the values along either ax is
Series.sort_index([axis, level, ascending, ...])Sort object by labels (along an axi s)
Series.sortlevel([level, ascending, ...])       Sort Series with MultiIndex by chosen leve l.
Series.swaplevel([i, j, copy])                  Swap levels i and j in a MultiInd ex
Series.unstack([level, fill_value])             Unstack, a.k. a.
Series.searchsorted(v[, side, sorter])          Find indices where elements should be inserted to maintain orde r.


##Series - Combining / joining / merging
Series.append(to_append[, verify_integrity])      Concatenate two or more Serie s.
Series.replace([to_replace, value, inplace, ...]) Replace values given in 'to_replace' with 'value
Series.update(other)                              Modify Series in place using non-NA values from passed Serie s.


#Example
>>> s1 = pd.Series([1, 2, 3])
>>> s2 = pd.Series([4, 5, 6])
>>> s3 = pd.Series([4, 5, 6], index=[3,4,5])
>>> s1.append(s2).as_matrix() #as_matrix converts to ndarray
array([1, 2, 3, 4, 5, 6], dtype=int64)



##Series- Time series-related
Series.asfreq(freq[, method, how, normalize])   Convert all TimeSeries inside to specified frequency using DateOffset object s.
Series.asof(where)                              Return last good (non-NaN) value in Series if value is NaN for requested dat e.
Series.shift([periods, freq, axis])             Shift index by desired number of periods with an optional time fr eq
Series.first_valid_index()                      Return label for first non-NA/null val ue
Series.last_valid_index()                       Return label for last non-NA/null val ue
Series.resample(rule[, how, axis, ...])         Convenience method for frequency conversion and resampling of regular time-series dat a.
Series.tz_convert(tz[, axis, level, copy])      Convert tz-aware axis to target time zon e.
Series.tz_localize(*args, **kwargs)             Localize tz-naive TimeSeries to target time zon e.




##Series - Serialization / IO / Conversion

Series.from_csv(path[, sep, parse_dates, ...])      Read CSV file (DISCOURAGED, please use pandas.read_csv() instead ).

Series.to_pickle(path)                              Pickle (serialize) object to input file pat h.
Series.to_csv(path[, index, sep, na_rep, ...])      Write Series to a comma-separated values (csv) fi le
Series.to_dict()                                    Convert Series to {label -> value} di ct
Series.to_frame([name])                             Convert Series to DataFra me
Series.to_xarray()                                  Return an xarray object from the pandas objec t.
Series.to_hdf(path_or_buf, key, **kwargs)           Activate the HDFStor e.

Series.to_sql(name, con[, flavor, schema, ...])     Write records stored in a DataFrame to a SQL databas e.
Series.to_msgpack([path_or_buf, encoding])          msgpack (serialize) object to input file pa th
Series.to_json([path_or_buf, orient, ...])          Convert the object to a JSON strin g.
Series.to_sparse([kind, fill_value])                Convert Series to SparseSeries
Series.to_dense()                                   Return dense representation of NDFrame (as opposed to spars e)
Series.to_string([buf, na_rep, ...])                Render a string representation of the Seri es
Series.to_clipboard([excel, sep])                   Attempt to write text representation of object to the system clipboard This can be pasted into Excel, for exampl e.


## Series - Sparse methods
SparseSeries.to_coo([row_levels, ...])              Create a scipy.sparse.coo_matrix from a SparseSeries with MultiInde x.
SparseSeries.from_coo(A[, dense_index])             Create a SparseSeries from a scipy.sparse.coo_matri x.



##Series - Plotting

Series.plot([kind, ax, figsize, ....])          Series plotting accessor and meth od
Series.hist([by, ax, grid, xlabelsize, ...])    Draw histogram of the input series using matplotl ib

Series.plot.area(**kwds)                        Area pl ot
Series.plot.bar(**kwds)                         Vertical bar pl ot
Series.plot.barh(**kwds)                        Horizontal bar pl ot
Series.plot.box(**kwds)                         Boxpl ot
Series.plot.density(**kwds)                     Kernel Density Estimate pl ot
Series.plot.hist([bins])                        Histogr am
Series.plot.kde(**kwds)                         Kernel Density Estimate pl ot
Series.plot.line(**kwds)                        Line pl ot
Series.plot.pie(**kwds)                         Pie cha rt















### Pandas - Handling datetime (one Column in DF  is datetime or Series is datetime)
# use .dt.xyz() methods  where xyz() would be applied to each element
import pandas as pd

df = pd.DataFrame({'my_dates':['2015-01-01','2015-01-02','2015-01-03'],'myvals':[1,2,3]})
df['my_dates'] = pd.to_datetime(df['my_dates'])
df['day_of_week'] = df['my_dates'].dt.dayofweek

days = {0:'Mon',1:'Tues',2:'Weds',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'}
df['day_of_week'] = df['day_of_week'].apply(lambda x: days[x])

#Output:    
my_dates  myvals day_of_week
0 2015-01-01       1       Thurs
1 2015-01-02       2         Fri
2 2015-01-01       3       Thurs



##Series - .dt.attributes
Series.dt.date              Returns numpy array of datetime.dat e.
Series.dt.time              Returns numpy array of datetime.tim e.
Series.dt.year              The year of the dateti me
Series.dt.month             The month as January=1, December= 12
Series.dt.day               The days of the dateti me
Series.dt.hour              The hours of the dateti me
Series.dt.minute            The minutes of the dateti me
Series.dt.second            The seconds of the dateti me
Series.dt.microsecond       The microseconds of the dateti me
Series.dt.nanosecond        The nanoseconds of the dateti me
Series.dt.week              The week ordinal of the ye ar
Series.dt.weekofyear        The week ordinal of the ye ar
Series.dt.dayofweek         The day of the week with Monday=0, Sunday =6
Series.dt.weekday           The day of the week with Monday=0, Sunday =6
Series.dt.weekday_name      The name of day in a week (ex: Frida y)
Series.dt.dayofyear         The ordinal day of the ye ar
Series.dt.quarter           The quarter of the da te
Series.dt.is_month_start    Logical indicating if first day of month (defined by frequenc y)
Series.dt.is_month_end      Logical indicating if last day of month (defined by frequenc y)
Series.dt.is_quarter_start  Logical indicating if first day of quarter (defined by frequenc y)
Series.dt.is_quarter_end    Logical indicating if last day of quarter (defined by frequenc y)
Series.dt.is_year_start     Logical indicating if first day of year (defined by frequenc y)
Series.dt.is_year_end       Logical indicating if last day of year (defined by frequenc y)
Series.dt.daysinmonth       The number of days in the mon th
Series.dt.days_in_month     The number of days in the mon th
Series.dt.tz
Series.dt.freq              get/set the frequncy of the Ind ex


##Series - .dt Datetime Methods
Series.dt.to_period(*args, **kwargs)    Cast to PeriodIndex at a particular frequen cy
Series.dt.to_pydatetime()
Series.dt.tz_localize(*args, **kwargs)  Localize tz-naive DatetimeIndex to given time zone (usi ng
Series.dt.tz_convert(*args, **kwargs)   Convert tz-aware DatetimeIndex from one time zone to another (usi ng
Series.dt.normalize(*args, **kwargs)    Return DatetimeIndex with times to midnigh t.
Series.dt.strftime(*args, **kwargs)     Return an array of formatted strings specified by date_format, which supports the same string format as the python standard librar y.
Series.dt.round(*args, **kwargs)        round the index to the specified fr eq
Series.dt.floor(*args, **kwargs)        floor the index to the specified fr eq
Series.dt.ceil(*args, **kwargs)         floor the index to the specified fr eq

##Series - .dt Timedelta Properties
Series.dt.days                          Number of days for each elemen t.
Series.dt.seconds                       Number of seconds (>= 0 and less than 1 day) for each elemen t.
Series.dt.microseconds                  Number of microseconds (>= 0 and less than 1 second) for each elemen t.
Series.dt.nanoseconds                   Number of nanoseconds (>= 0 and less than 1 microsecond) for each elemen t.
Series.dt.components                    Return a dataframe of the components (days, hours, minutes, seconds, milliseconds, microseconds, nanoseconds) of the Timedelta s.

##Series - .dt Timedelta Methods
Series.dt.to_pytimedelta  ()
Series.dt.total_seconds(*args, **kwargs)    Total duration of each element expressed in second s.





###Pandas - Handling String  (one Column in DF is String or  Series is String )
# use .str.xyz() methods where xyz() would be applied to each element
>>> calls = pd.Series(['a', 'b', 'c', 'd', 'e'])
>>> calls
0    a
1    b
2    c
3    d
4    e
dtype: object
>>> calls.str.upper()
0    A
1    B
2    C
3    D
4    E

>>> calls
0    a
1    b
2    c
3    d
4    e
dtype: object

##Series - .str Methods
Series.str.capitalize()                             Convert strings in the Series/Index to be capitalize d.
Series.str.cat([others, sep, na_rep])               Concatenate strings in the Series/Index with given separato r.
Series.str.center(width[, fillchar])                Filling left and right side of strings in the Series/Index with an additional characte r.
Series.str.contains(pat[, case, flags, na, ...])    Return boolean Series/array whether given pattern/regex is contained in each string in the Series/Inde x.

Series.str.count(pat[, flags])                      Count occurrences of pattern in each string of the Series/Inde x.
Series.str.decode(encoding[, errors])               Decode character string in the Series/Index using indicated encodin g.
Series.str.encode(encoding[, errors])               Encode character string in the Series/Index using indicated encodin g.

Series.str.extract(pat[, flags, expand])            For each subject string in the Series, extract groups from the first match of regular expression pa t.
Series.str.extractall(pat[, flags])                 For each subject string in the Series, extract groups from all matches of regular expression pa t.
Series.str.find(sub[, start, end])                  Return lowest indexes in each strings in the Series/Index where the substring is fully contained between [start:end ].
Series.str.findall(pat[, flags])                    Find all occurrences of pattern or regular expression in the Series/Inde x.
Series.str.get(i)                                   Extract element from lists, tuples, or strings in each element in the Series/Inde x.

Series.str.index(sub[, start, end])                 Return lowest indexes in each strings where the substring is fully contained between [start:end ].

Series.str.join(sep)                                Join lists contained as elements in the Series/Index with passed delimiter.
Series.str.len()                                    Compute length of each string in the Series/Inde x.
Series.str.ljust(width[, fillchar])                 Filling right side of strings in the Series/Index with an additional characte r.

Series.str.lower()                                  Convert strings in the Series/Index to lowercas e.
Series.str.upper()                                  Convert strings in the Series/Index to uppercas e.
Series.str.swapcase()                               Convert strings in the Series/Index to be swapcase d.

Series.str.lstrip([to_strip])                       Strip whitespace (including newlines) from each string in the Series/Index from left sid e.
Series.str.strip([to_strip])                        Strip whitespace (including newlines) from each string in the Series/Index from left and right side s.
Series.str.rstrip([to_strip])                       Strip whitespace (including newlines) from each string in the Series/Index from right sid e.

Series.str.match(pat[, case, flags, na, ...])       Deprecated: Find groups in each string in the Series/Index using passed regular expressio n.
Series.str.normalize(form)                          Return the Unicode normal form for the strings in the Series/Inde x.
Series.str.pad(width[, side, fillchar])             Pad strings in the Series/Index with an additional character to specified sid e.
Series.str.partition([pat, expand])                 Split the string at the first occurrence of sep, and return 3 elements containing the part before the separator, the separator itself, and the part after the separato r.
Series.str.repeat(repeats)                          Duplicate each string in the Series/Index by indicated number of time s.
Series.str.replace(pat, repl[, n, case, flags])     Replace occurrences of pattern/regex in the Series/Index with some other strin g.
Series.str.rfind(sub[, start, end])                 Return highest indexes in each strings in the Series/Index where the substring is fully contained between [start:end ].
Series.str.rindex(sub[, start, end])                Return highest indexes in each strings where the substring is fully contained between [start:end ].
Series.str.rjust(width[, fillchar])                 Filling left side of strings in the Series/Index with an additional characte r.
Series.str.rpartition([pat, expand])                Split the string at the last occurrence of sep, and return 3 elements containing the part before the separator, the separator itself, and the part after the separato r.
Series.str.slice([start, stop, step])               Slice substrings from each element in the Series/Ind ex
Series.str.slice_replace([start, stop, repl])       Replace a slice of each string in the Series/Index with another strin g.

Series.str.split(*args, **kwargs)                   Split each string (a la re.split) in the Series/Index by given pattern, propagating NA value s.
Series.str.rsplit([pat, n, expand])                 Split each string in the Series/Index by the given delimiter string, starting at the end of the string and working to the fron t.

Series.str.startswith(pat[, na])                    Return boolean Series/array indicating whether each string in the Series/Index starts with passed patter n.
Series.str.endswith(pat[, na])                      Return boolean Series indicating whether each string in the Series/Index ends with passed patter n.

Series.str.title()                                  Convert strings in the Series/Index to titlecas e.
Series.str.translate(table[, deletechars])          Map all characters in the string through the given mapping tabl e.
Series.str.wrap(width, **kwargs)                    Wrap long strings in the Series/Index to be formatted in paragraphs with length less than a given widt h.
Series.str.zfill(widt  h)

Series.str.isalnum()                                Check whether all characters in each string in the Series/Index are alphanumeri c.
Series.str.isalpha()                                Check whether all characters in each string in the Series/Index are alphabeti c.
Series.str.isdigit()                                Check whether all characters in each string in the Series/Index are digit s.
Series.str.isspace()                                Check whether all characters in each string in the Series/Index are whitespac e.
Series.str.islower()                                Check whether all characters in each string in the Series/Index are lowercas e.
Series.str.isupper()                                Check whether all characters in each string in the Series/Index are uppercas e.
Series.str.istitle()                                Check whether all characters in each string in the Series/Index are titlecas e.
Series.str.isnumeric()                              Check whether all characters in each string in the Series/Index are numeri c.
Series.str.isdecimal()                              Check whether all characters in each string in the Series/Index are decima l.
Series.str.get_dummies([sep])                       Split each string in the Series by sep and return a frame of dummy/indicator variable s.




###Pandas - Handling category (one Column in DF  is category or  Series is category)
# use .cat.xyz() methods  where xyz() would be applied to each eleme nt

#Creation
s = pd.Series(["a","b","c","a"], dtype="category")
#For  DF
df = pd.DataFrame({"A":["a","b","c","a"]})
df["B"] = df["A"].astype('category')


#Converting from Series to Category
s = pd.Series(["a","b","c","a"])
s_cat = s.astype("category", categories=["b","c","d"], ordered=False)
#Converting from Category to Series
s3 = s_cat.astype('string')
# Or
>>> np.asarray(s2)
array(['a', 'b', 'c', 'a'], dtype=object)


#Use .cat attribute
s = pd.Series(["a","b","c","a"], dtype="category")
>>> s.cat.categories
Index([u'a', u'b', u'c'], dtype='object')
>>> s.cat.ordered
False

#Renaming categories - by assigning new values to the Series.cat.categories
s = pd.Series(["a","b","c","a"], dtype="category")
s.cat.categories = ["Group %s" % g for g in s.cat.categories]

#Appending new categories
s = s.cat.add_categories([4])
>>> s.cat.categories
Index([u'Group a', u'Group b', u'Group c', 4], dtype='object')

#Removing categories
s = s.cat.remove_categories([4])

#Removing unused categories
s = pd.Series(pd.Categorical(["a","b","a"], categories=["a","b","c","d"]))
>>> s.cat.remove_unused_categories()
0    a
1    b
2    a
dtype: category
Categories (2, object): [a, b]

#Sorting and Order
#If categorical data is ordered (s.cat.ordered == True ),
#then the order of the categories has a meaning and certain operations are possibl e.
#If the categorical is unordered, .min()/.max() will raise a TypeError.

s = pd.Series(pd.Categorical(["a","b","c","a"], ordered=False))
s.sort_values(inplace=True)
# or
s = pd.Series(["a","b","c","a"]).astype('category', ordered=True)
>>> s.sort_values(inplace=True)
0    a
3    a
1    b
2    c
dtype: category
Categories (3, object): [a < b < c]

>>> s.min(), s.max()
('a', 'c')

# Or
>>> s.cat.as_ordered()
0    a
3    a
1    b
2    c
dtype: category
Categories (3, object): [a < b < c]

>>> s.cat.as_unordered()
0    a
3    a
1    b
2    c
dtype: category
Categories (3, object): [a, b, c]


##Categories - Comparisons
•comparing equality (== and !=) to a list-like object (list, Series, array, .. .)
 of the same length as the categorical data.
•all comparisons (==, !=, >, >=, <, and <=) of categorical data to another categorical
 Series, when ordered==True and the categories are the same.
•all comparisons of a categorical data to a scalar.

#All other comparisons will raise a TypeError.

cat = pd.Series([1,2,3]).astype("category", categories=[3,2,1], ordered=True)
cat_base = pd.Series([2,2,2]).astype("category", categories=[3,2,1], ordered=True)
cat_base2 = pd.Series([2,2,2]).astype("category", ordered=True)

>>> cat > cat_base
0     True
1    False
2    False
dtype: bool

>>> cat > 2
0     True
1    False
2    False
dtype: bool


>>> cat == cat_base
0    False
1     True
2    False
dtype: bool

>>> cat == np.array([1,2,3])
0    True
1    True
2    True
dtype: bool

>>> cat == 2

0    False
1     True
2    False
dtype: bool


##Series - .cat methods 
Series.cat.categories           The categories of this categorica l.
Series.cat.ordered              Gets the ordered attribu te

Series.cat.rename_categories(*args, **kwargs)       Renames categorie s.
Series.cat.reorder_categories(*args, **kwargs)      Reorders categories as specified in new_categorie s.
Series.cat.add_categories(*args, **kwargs)          Add new categorie s.
Series.cat.remove_categories(*args, **kwargs)       Removes the specified categorie s.
Series.cat.remove_unused_categories(*args, ...)     Removes categories which are not use d.
Series.cat.set_categories(*args, **kwargs)          Sets the categories to the specified new_categorie s.
Series.cat.as_ordered(*args, **kwargs)              Sets the Categorical to be order ed
Series.cat.as_unordered(*args, **kwargs)            Sets the Categorical to be unorder ed










###Pandas -DF - Construction 


pd.DataFrame([data, index, columns, dtype, copy ])

##construction From dict of Series
d = {'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']),    
     'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}    

df = pd.DataFrame(d)
>>> df   
   one  two
a  1.0  1.0
b  2.0  2.0
c  3.0  3.0
d  NaN  4.0           #non existing index in one Series is denoted with NaN

>>> df = pd.DataFrame(d, index=['d', 'b', 'a'])  #can give subset of Series index   
   one  two
d  NaN  4.0
b  2.0  2.0
a  1.0  1.0

df = pd.DataFrame(d, index=['d', 'b', 'a'], columns=['two', 'three']) #can provide columns name   
   two three
d  4.0   NaN
b  2.0   NaN
a  1.0   NaN


>>> df.index
Out[37]: Index([u'a', u'b', u'c', u'd'], dtype='object')

>>> df.columns
Out[38]: Index([u'one', u'two'], dtype='object')


##Constructions - From dict of ndarrays / lists 
#each must have same length , key=column label

# If no index is passed, the result will be RangeIndex(n), where n is the array length.
d = {'one' : [1., 2., 3., 4.], 'two' : [4., 3., 2., 1.]}    
>>> pd.DataFrame(d)
   one  two
0  1.0  4.0
1  2.0  3.0
2  3.0  2.0
3  4.0  1.0
>>> df['one']
0    1.0
1    2.0
2    3.0
3    4.0
Name: one, dtype: float64
>>> df['one'][0]
1.0

##Construction - From a list of dicts, dict containing column label

data2 = [{'a': 1, 'b': 2}, {'a': 5, 'b': 10, 'c': 20}]
>>> pd.DataFrame(data2)
   a   b     c
0  1   2   NaN
1  5  10  20.0

>>> pd.DataFrame(data2, index=['first', 'second'])
a   b     c
first   1   2   NaN
second  5  10  20.0

>>> pd.DataFrame(data2, columns=['a', 'b'])
   a   b
0  1   2
1  5  10


##Alternate Constructors
DataFrame.from_dict(data[, orient, dtype])      Construct DataFrame from dict of array-like or dicts 
DataFrame.from_items(items[, columns, orient])  Convert (key, value) pairs to DataFrame.
                                                key=column label                           
                                                If orient='index', the keys will be the row labels
DataFrame.from_records(data[, index, ...])      Convert structured or record ndarray to DataFrame 


#Example
#create two elements structured ndarray 
data = np.zeros((2,), dtype=[('A', 'i4'),('B', 'f4'),('C', 'a10')])
data[:] = [(1,2.,'Hello'), (2,3.,"World")]
>>> data
array([(1, 2.0, 'Hello'), (2, 3.0, 'World') ],      
dtype=[('A', '<i4'), ('B', '<f4'), ('C', 'S10')])

>>> pd.DataFrame.from_records(data, index='C')
Out[53 ]:       
       A    B            
C
Hello  1  2.0
World  2  3.0

>>> pd.DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6])])
   A  B
0  1  4
1  2  5
2  3  6
>>> pd.DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6])],                            
          orient='index', columns=['one', 'two', 'three'])
    one  two  three
A    1    2      3
B    4    5      6
#key becomes column 
sales = [{'account': 'Jones LLC', 'Jan': 150, 'Feb': 200, 'Mar': 140},
         {'account': 'Alpha Co',  'Jan': 200, 'Feb': 210, 'Mar': 215},
         {'account': 'Blue Inc',  'Jan': 50,  'Feb': 90,  'Mar': 95 }]
df = pd.DataFrame(sales)
>>> df
   Feb  Jan  Mar    account
0  200  150  140  Jones LLC
1  210  200  215   Alpha Co
2   90   50   95   Blue Inc
sales = {'account': ['Jones LLC', 'Alpha Co', 'Blue Inc'],
         'Jan': [150, 200, 50],
         'Feb': [200, 210, 90],
         'Mar': [140, 215, 95]}
df = pd.DataFrame.from_dict(sales)
>>> df
   Feb  Jan  Mar    account
0  200  150  140  Jones LLC
1  210  200  215   Alpha Co
2   90   50   95   Blue Inc



###Pandas -DF - Column addition, deletion
#Use df['new_column'] = .. style for addition 
#Use del df['old_column'] for deletion 

d = {'one' : [1., 2., 3., 4.],'two' : [4., 3., 2., 1.]}    
df = pd.DataFrame(d, index=['a','b','c','d'])
>>> df['one']             #accessing
a    1.0
b    2.0
c    3.0
d    NaN
Name: one, dtype: float64
>>> df.one
a    1.0
b    2.0
c    3.0
d    4.0
Name: one, dtype: float64

#creation of new column
df['three'] = df['one'] * df['two']
df['flag'] = df['one'] > 2
>>> df
   one  two  three   flag
a  1.0  1.0    1.0  False
b  2.0  2.0    4.0  False
c  3.0  3.0    9.0   True
d  NaN  4.0    NaN  False

#Deletion of column
del df['two']
#pop returns Series 
three = df.pop('three')
>>> df
   one   flag
a  1.0  False
b  2.0  False
c  3.0   True
d  NaN  False

#scalar value is boradcasted
df['foo'] = 'bar'
>>> df
   one   flag  foo
a  1.0  False  bar
b  2.0  False  bar
c  3.0   True  bar
d  NaN  False  bar


#When inserting a Series that does not have the same index as the DataFrame,
#those index would be 'NaN'
df['one_trunc'] = df['one'][:2]
>>> df
Out[66 ]:   
   one   flag  foo  one_trunc
a  1.0  False  bar        1.0
b  2.0  False  bar        2.0
c  3.0   True  bar        NaN
d  NaN  False  bar        NaN






###Pandas - Merge, join, concat, append
DataFrame.append(other[, ignore_index, ...]) Append rows of other to the end of this frame, returning a new object. 
DataFrame.assign(**kwargs) Assign new columns to a DataFrame, returning a new object (a copy) with all the original columns in addition to the new ones. 

pandas. concat(objs[, axis, join, join_axes, ...]) Concatenate pandas objects along a particular axis with optional set logic along the other axes. 
DataFrame.insert(loc, column, value[, ...]) Insert column into DataFrame at specified location. 

DataFrame.join(other[, on, how, lsuffix, ...]) Join columns with other DataFrame either on index or on a key column. 
DataFrame.merge(right[, how, on, left_on, ...]) Merge DataFrame objects by performing a database-style join operation by columns or indexes. 
DataFrame.update(other[, join, overwrite, ...]) Modify DataFrame in place using non-NA values from passed DataFrame. 

##DF-append 
DataFrame.append(other, ignore_index=False, verify_integrity=False)
    Append rows of other as vertical stacking 
    Parameters:
        other : DataFrame or Series/dict-like object, or list of these
            The data to append.
        ignore_index : boolean, default False
            If True, do not use the index labels.
        verify_integrity : boolean, default False
            If True, raise ValueError on creating index with duplicates.
        Returns:
            appended : DataFrame

>>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))
>>> df
   A  B
0  1  2
1  3  4
>>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))
>>> df.append(df2)
   A  B
0  1  2
1  3  4
0  5  6
1  7  8


##DF -  insert 
DataFrame.insert(loc, column, value, allow_duplicates=False)
    Insert column into DataFrame at specified location
        loc : int
            Insertion index. Must verify 0 <= loc <= len(columns)
        column : string, number, or hashable object
            label of the inserted column
        value : int, Series, or array-like
        allow_duplicates : bool, optional

#Example 
df.insert(1, 'bar', df['one'])
>>> df
   one  bar   flag  foo  one_trunc
a  1.0  1.0  False  bar        1.0
b  2.0  2.0  False  bar        2.0
c  3.0  3.0   True  bar        NaN
d  NaN  NaN  False  bar        NaN

##DF- assign 
DataFrame.assign(**kwargs)
    kwargs : keyword, value pairs
        column_header=fn(df):return_newSeries
        or column_header=Series, scalar, or array
    Returns:
    df : DataFrame
        A new DataFrame with the new columns in addition to all the existing columns
 
#All expressions are computed first, and then assigned.
#So you can't refer to another column being assigned in the same call to assign.

# Don't do this, bad reference to `C` 
df.assign(C = lambda x: x['A'] + x['B'], D = lambda x: x['A'] + x['C'])
# Instead, break it into two assigns
new_df = (df.assign(C = lambda x: x['A'] + x['B']).assign(D = lambda x: x['A'] + x['C']))



##DF  - Concat 
pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,       
        keys=None, levels=None, names=None, verify_integrity=False)
    Concatenate pandas objects along a particular axis 
    with optional set logic along the other axes.
        •objs: a sequence or mapping of Series, DataFrame, or Panel objects.    
        •axis: {0, 1}, default 0(index). 
            =0 means vertical(rowwise) stacking, =1 means horizontal(columnwise) stacking 
        •join: {'inner', 'outer'}, default 'outer'. 
            Outer for union and inner for intersection.
        keys: new columns name 
        ignore_index : boolean, default False
            If True, do not use the index values along the concatenation axis. 
            The resulting axis will be labeled 0, ..., n - 1.

#Example 
df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],  #keys are columns                    
    'B': ['B0', 'B1', 'B2', 'B3'],                    
    'C': ['C0', 'C1', 'C2', 'C3'],                    
    'D': ['D0', 'D1', 'D2', 'D3']},                    
    index=[0, 1, 2, 3])
df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],                   
    'B': ['B4', 'B5', 'B6', 'B7'],                   
    'C': ['C4', 'C5', 'C6', 'C7'],                    
    'D': ['D4', 'D5', 'D6', 'D7']},                    
    index=[4, 5, 6, 7])
df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],                    
    'B': ['B8', 'B9', 'B10', 'B11'],                     
    'C': ['C8', 'C9', 'C10', 'C11'],                        
    'D': ['D8', 'D9', 'D10', 'D11']},                    
    index=[8, 9, 10, 11])

#Note same index values in multiple DFs would be duplicated in result DF 
#To ignore index in input DF, use ignore_index=True 
frames = [df1, df2, df3]
result = pd.concat(frames)    #Using concat, default axis=0, index varying
>>> result
      A    B    C    D
0    A0   B0   C0   D0
1    A1   B1   C1   D1
2    A2   B2   C2   D2
3    A3   B3   C3   D3
4    A4   B4   C4   D4
5    A5   B5   C5   D5
6    A6   B6   C6   D6
7    A7   B7   C7   D7
8    A8   B8   C8   D8
9    A9   B9   C9   D9
10  A10  B10  C10  D10
11  A11  B11  C11  D11
>>> result = pd.concat(frames, axis=1) #column varying 
>>> result
      A    B    C    D    A    B    C    D    A    B    C    D
0    A0   B0   C0   D0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN
1    A1   B1   C1   D1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN
2    A2   B2   C2   D2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN
3    A3   B3   C3   D3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN
4   NaN  NaN  NaN  NaN   A4   B4   C4   D4  NaN  NaN  NaN  NaN
5   NaN  NaN  NaN  NaN   A5   B5   C5   D5  NaN  NaN  NaN  NaN
6   NaN  NaN  NaN  NaN   A6   B6   C6   D6  NaN  NaN  NaN  NaN
7   NaN  NaN  NaN  NaN   A7   B7   C7   D7  NaN  NaN  NaN  NaN
8   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   A8   B8   C8   D8
9   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   A9   B9   C9   D9
10  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  A10  B10  C10  D10
11  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  A11  B11  C11  D11

#Note append  is similar to axis=0 
result = df1.append(df2)
>>> result
    A   B   C   D
0  A0  B0  C0  D0
1  A1  B1  C1  D1
2  A2  B2  C2  D2
3  A3  B3  C3  D3
4  A4  B4  C4  D4
5  A5  B5  C5  D5
6  A6  B6  C6  D6
7  A7  B7  C7  D7

#the indexes must be disjoint or use ignore_index=True 
>>> result = df1.append([df1.copy()])
>>> result
    A   B   C   D
0  A0  B0  C0  D0
1  A1  B1  C1  D1
2  A2  B2  C2  D2
3  A3  B3  C3  D3
0  A0  B0  C0  D0
1  A1  B1  C1  D1
2  A2  B2  C2  D2
3  A3  B3  C3  D3

#To concatenate a mix of Series and DataFrames.
s1 = pd.Series(['X0', 'X1', 'X2', 'X3'], name='X')
result = pd.concat([df1, s1], axis=1) #horzontal stacking 
>>> result
    A   B   C   D   X
0  A0  B0  C0  D0  X0
1  A1  B1  C1  D1  X1
2  A2  B2  C2  D2  X2
3  A3  B3  C3  D3  X3



##DF - Database-style DataFrame merging
pandas.merge(left, right, how='inner', on=None, left_on=None, right_on=None,      
        left_index=False, right_index=False, sort=True,      
        suffixes=('_x', '_y'), copy=True, indicator=False)
DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, 
        left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)
    If joining columns on columns, ie on or (left_on,right_on), 
    the DataFrame indexes will be ignored. 
    Otherwise if joining indexes on indexes ie left_index,right_index 
    or indexes on a column(ie on*, *_index combination, Note, one's index values must match with other's column's value) 
    the index will be passed on.    
    left, right : DataFrame
    how : {'left', 'right', 'outer', 'inner'}, default 'inner'
        •left: use only keys from left frame, similar to a SQL left outer join; preserve key order
        •right: use only keys from right frame, similar to a SQL right outer join; preserve key order
        •outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically
        •inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys
    on: Columns (names) to join on. 
        Must be found in both the left and right DataFrame objects. 
        If not passed and left_index and right_index are False, 
        the intersection of the columns in the DataFrames will be inferred to be the join keys
    •left_on: Columns from the left DataFrame to use as keys. 
        Can either be column names or arrays with length equal to the length of the DataFrame
    •right_on: Columns from the right DataFrame to use as keys. 
        Can either be column names or arrays with length equal to the length of the DataFrame
    •left_index: If True, use the index (row labels) from the left DataFrame as its join key(s). 
        In the case of a DataFrame with a MultiIndex (hierarchical), 
        the number of levels must match the number of join keys from the right DataFrame
    right_index: Same usage as left_index for the right DataFrame
    sort : boolean, default False
        Sort the join keys lexicographically in the result DataFrame. 
    suffixes : 2-length sequence (tuple, list, ...)
        Suffix to apply to overlapping column names in the left and right side, 
    copy : boolean, default True
        If False, do not copy data unnecessarily
    indicator : boolean or string, default False
        If True, adds a column to output DataFrame called '_merge' with information 
        on the source of each row.eg 'left_only', 'right_only','both' 
    validate : string, default None
        If specified, checks if merge is of specified type.
        •'one_to_one' or '1:1': check if merge keys are unique in both left and right datasets.
        •'one_to_many' or '1:m': check if merge keys are unique in left dataset.
        •'many_to_one' or 'm:1': check if merge keys are unique in right dataset.
        •'many_to_many' or 'm:m': allowed, but does not result in checks.

#Type of joins
•one-to-one joins: for example when joining two DataFrame objects 
on their indexes (which must contain unique values)
•many-to-one joins: for example when joining an index (unique) to one or more                    
columns in a DataFrame
•many-to-many joins: joining columns on columns.

>>> A              >>> B
    lkey value        rkey value
0   foo  1            foo  5
1   bar  2            bar  6
2   baz  3            qux  7
3   foo  4            bar  8
#read above and split 
df = pd.read_clipboard()
A = df.iloc[:,[0,1]]
B = df.iloc[:, [2,3]]
B.columns= [ B.columns[0], 'value'] #rename 

#note for below index-column joining, one can use 'join' as well 
>>> pd.merge(A, B, right_index = True, left_on='value') #left column 'value' with right index
   value lkey  value_x rkey  value_y
0      1  foo        1  bar        6
1      2  bar        2  qux        7
2      3  baz        3  bar        8


>>> A.merge(B, left_index=True, right_index=True) #index joining 
  lkey  value_x rkey  value_y
0  foo        1  foo        5
1  bar        2  bar        6
2  baz        3  qux        7
3  foo        4  bar        8
>>> A.merge(B, left_index=True, right_index=True, how='outer')#does not matter 
  lkey  value_x rkey  value_y
0  foo        1  foo        5
1  bar        2  bar        6
2  baz        3  qux        7
3  foo        4  bar        8
>>> A.merge(B, left_on='lkey', right_on='rkey', how='outer') #column joining 
   lkey  value_x  rkey  value_y
0  foo   1        foo   5
1  foo   4        foo   5
2  bar   2        bar   6
3  bar   2        bar   8
4  baz   3        NaN   NaN
5  NaN   NaN      qux   7
>>> A.merge(B, left_on='lkey', right_on='rkey', how='inner')
  lkey  value_x rkey  value_y
0  foo        1  foo        5
1  foo        4  foo        5
2  bar        2  bar        6
3  bar        2  bar        8
>>> A.merge(B, left_on='lkey', right_on='rkey', how='left')
  lkey  value_x rkey  value_y
0  foo        1  foo      5.0
1  bar        2  bar      6.0
2  bar        2  bar      8.0
3  baz        3  NaN      NaN
4  foo        4  foo      5.0
>>> A.merge(B, left_on='lkey', right_on='rkey', how='right')
  lkey  value_x rkey  value_y
0  foo      1.0  foo        5
1  foo      4.0  foo        5
2  bar      2.0  bar        6
3  bar      2.0  bar        8
4  NaN      NaN  qux        7




#Example 
left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],                      
        'key2': ['K0', 'K1', 'K0', 'K1'],                     
        'A': ['A0', 'A1', 'A2', 'A3'],                     
        'B': ['B0', 'B1', 'B2', 'B3']})
right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],                      
        'key2': ['K0', 'K0', 'K0', 'K0'],                        
        'C': ['C0', 'C1', 'C2', 'C3'],                         
        'D': ['D0', 'D1', 'D2', 'D3']}) 
#multiple keys 
result = pd.merge(left, right, on=['key1', 'key2'])
>>> result
    A   B key1 key2   C   D
0  A0  B0   K0   K0  C0  D0
1  A2  B2   K1   K0  C1  D1
2  A2  B2   K1   K0  C2  D2

# With The merge indicator
pd.merge(left, right, on=['key1', 'key2'], indicator=True)





##DF - Joining on index 
DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)
    Join : this DF's index or column(if 'on' given) with 'other' index 
    'other' index values sould be similar to 'this' DF's index or column
    Efficiently Join multiple DataFrame objects by index at once by passing a list.
    Parameters:
        other : DataFrame, Series with name field set, or list of DataFrame
            Index should be similar to one of the columns in this one. 
            If a Series is passed, its name attribute must be set, 
            and that will be used as the column name in the resulting joined DataFrame
        on : column name, tuple/list of column names, or array-like
            Column(s) in the caller to join on the index in other, 
            otherwise joins index-on-index. 
            If multiples columns given, the passed DataFrame must have a MultiIndex. 
            Can pass an array as the join key if not already contained in the calling DataFrame. Like an Excel VLOOKUP operation
        how : {'left', 'right', 'outer', 'inner'}, default: 'left'
        How to handle the operation of the two objects.
            •left: use calling frame's index (or column if on is specified)
            •right: use other frame's index
            •outer: form union of calling frame's index (or column if on is specified) with other frame's index, and sort it lexicographically
            •inner: form intersection of calling frame's index (or column if on is specified) with other frame's index, preserving the order of the calling's one
        lsuffix : string
            Suffix to use from left frame's overlapping columns
        rsuffix : string
            Suffix to use from right frame's overlapping columns
        sort : boolean, default False
            Order result DataFrame lexicographically by the join key. 
            If False, the order of the join key depends on the join type (how keyword)
#Example 
left = pd.DataFrame({'A': ['A0', 'A1', 'A2'],                       
        'B': ['B0', 'B1', 'B2']},                     
        index=['K0', 'K1', 'K2'])


right = pd.DataFrame({'C': ['C0', 'C2', 'C3'],                      
    'D': ['D0', 'D2', 'D3']},                      
    index=['K0', 'K2', 'K3']) 

#index, index joining , default left (hence take only left's index)
#Note both index should be similar 
result = left.join(right)
>>> result
     A   B    C    D
K0  A0  B0   C0   D0
K1  A1  B1  NaN  NaN
K2  A2  B2   C2   D2

result = left.join(right, how='outer')
result = left.join(right, how='inner')

#Note, above can be implemented via merge as well 
result = pd.merge(left, right, left_index=True, right_index=True, how='outer')
result = pd.merge(left, right, left_index=True, right_index=True, how='inner');





###Pandas -DF - Indexing / Selection- isin, any,all, where, query, lookup
#Operation               Syntax                     Result
Select column           df[column_label]            Series of that col, note  int is not allowed
                        df.column_label 
                        df.loc[:, column_label]
                        df.iloc[:, column_index]
                        df.ix[:, column_label_or_column_index]
Select row by label     df.loc[row/index_label]     equivalent to .loc[index_label, :]
Select row by integer   df.iloc[row_int]            equivalent to .iloc[row_int, :]
Slice rows              df[start:end:step]          row index 
Select rows bool vector df[bool_vec]                DataFrame of same columns but rows wher bool_vec is True

#Note column_label, column_index, row_label, row_index can be 
    single element 
    list of elements 
    slice objects 
    boolean array 
    a fn(df):returns_any_of_above 

#copy this in clipboard   
   one   flag  foo  one_trunc
a  1.0  False  bar        1.0
b  2.0  False  bar        2.0
c  3.0   True  bar        NaN
d  NaN  False  bar        NaN

df = pd.read_clipboard();
>>> df.loc['b']  #class 'pandas.core.series.Series'
one              2
bar              2
flag         False
foo            bar
one_trunc        2
Name: b, dtype: object

>>> df.iloc[2]
one             3
bar             3
flag         True
foo           bar
one_trunc     NaN
Name: c, dtype: object

#.loc/.ix/[] operations can create new columns 
#when setting a non-existant key for that axis.

#In the Series case this is effectively an appending operation
se = pd.Series([1,2,3])
>>> se
0    1
1    2
2    3
dtype: int64

se[5] = 5.
>>> se
Out[110 ]:
0    1.0
1    2.0
2    3.0
5    5.0
dtype: float64


#A DataFrame can be enlarged on either axis via .loc
df = pd.DataFrame(np.arange(6).reshape(3,2), columns=['A','B'])
>>> df
   A  B
0  0  1
1  2  3
2  4  5

df.loc[:,'C'] = df.loc[:,'A']
>>> dfi
   A  B  C
0  0  1  0
1  2  3  2
2  4  5  4


#This is like an append operation on the DataFrame.
df.loc[3] = 5
>>> dfi
Out[116 ]:   
A  B  C
0  0  1  0
1  2  3  2
2  4  5  4
3  5  5  5

##DF - Boolean indexing - &, |, ~
#Using a boolean vector to index a Series works exactly as in a numpy ndarray:
s = pd.Series(range(-3, 4))
>>>  s
0   -3
1   -2
2   -1
3    0
4    1
5    2
6    3
dtype: int64

>>>  s[s > 0]
4    1
5    2
6    3
dtype: int64

>>>  s[(s < -1) | (s > 0.5)]
0   -3
1   -2
4    1
5    2
6    3
dtype: int64

>>>  s[~(s < 0)]
3    0
4    1
5    2
6    3
dtype: int64


#with  DF
>>>  df[df['A'] > 0]             
A         B         C         D    E   0
2000-01-04  7.000000  0.721555 -1.039575  0.271860  NaN NaN
2000-01-05  0.567020 -0.424972  0.276232 -1.087401  NaN NaN
2000-01-06  0.113648 -0.673690 -1.478427  0.524988  7.0 NaN
2000-01-07  0.577046  0.404705 -1.715002 -1.039268  NaN NaN


#List comprehensions and map method of Series can also be used
df2 = pd.DataFrame({'a' : ['one', 'one', 'two', 'three', 'two', 'one', 'six'],                     
        'b' : ['x', 'y', 'y', 'x', 'y', 'x', 'x'],                   
        'c' : np.random.randn(7)})   


# only want 'two' or 'three'
criterion = df2['a'].map(lambda x: x.startswith('t')) 
>>> df2[criterion]  
       a  b         c
2    two  y  0.109121
3  three  x  1.126203
4    two  y -0.977349

# equivalent but slower
>>>  df2[ [x.startswith('t') for x in df2['a']] ]   
a  b         c
2    two  y  0.109121
3  three  x  1.126203
4    two  y -0.977349

# Multiple criteria
>>>  df2[criterion & (df2['b'] == 'x')]  
a  b         c
3  three  x  1.126203


>>>  df2.loc[criterion & (df2['b'] == 'x'),'b':'c']
b         c
3  x  1.126203



##DF - Boolean indexing - Indexing with isin
DataFrame.isin(values)
    values : iterable, Series, DataFrame or dictionary
        The result will only be true at a location if all the labels match. 
        If values is a Series, that's the index. 
        If values is a dictionary, the keys must be the column names, which must match. 
        If values is a DataFrame, then both the index and column labels must match.
     

#Example 
s = pd.Series(np.arange(5), index=np.arange(5)[::-1], dtype='int64')
>>>  s
4    0
3    1
2    2
1    3
0    4
dtype: int64

>>>  s.isin([2, 4, 6])
4    False
3    False
2     True
1    False
0     True
dtype: bool

>>>  s[s.isin([2, 4, 6])]
2    2
0    4
dtype: int64


#The same method is available for Index objects
>>>  s[s.index.isin([2, 4, 6])]
4    0
2    2
dtype: int64

# compare it to the following
>>> s[[2, 4, 6]]
2    2.0
4    0.0
6    NaN
dtype: float64


#MultiIndex allows selecting a separate level to use in the membership check:
s_mi = pd.Series(np.arange(6),                    
index=pd.MultiIndex.from_product([[0, 1], ['a', 'b', 'c']]))    
>>> s_mi
0  a    0
   b    1
   c    2
1  a    3
   b    4
   c    5
dtype: int32

>>>  s_mi.iloc[s_mi.index.isin([(1, 'a'), (2, 'b'), (0, 'c')])]
0  c    2
1  a    3
dtype: int64

>>>  s_mi.iloc[s_mi.index.isin(['a', 'c', 'e'], level=1)]
0  a    0
   c    2
1  a    3
   c    5
dtype: int32


#DataFrame also has an isin method.
#When calling isin, pass a set of values as either an array or dict.
df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],                     
        'ids2': ['a', 'n', 'c', 'n']})    


>>> values = ['a', 'b', 1, 3]
>>>  df.isin(values)
ids   ids2   vals
0   True   True   True
1   True  False  False
2  False  False   True
3  False  False  False


#Oftentimes you'll want to match certain values with certain colums.
values = {'ids': ['a', 'b'], 'vals': [1, 3]}
>>>  df.isin(values)
Out[150 ]:     
ids   ids2   vals
0   True  False   True
1   True  False  False
2  False  False   True
3  False  False  False


#Combine DataFrame's isin with the any() and all() methods
#to quickly select subsets of your data that meet a given criteria.
DataFrame.all(axis=None, bool_only=None, skipna=None, level=None, **kwargs)
    Return whether all elements are True over requested axis
DataFrame.any(axis=None, bool_only=None, skipna=None, level=None, **kwargs)
    Return whether any element is True over requested axis
    Parameters:
        axis : {index (0), columns (1)}
        skipna : boolean, default True
            Exclude NA/null values. 
            If an entire row/column is NA, the result will be NA
        level : int or level name, default None
            If the axis is a MultiIndex (hierarchical), 
            count along a particular level, collapsing into a Series
        bool_only : boolean, default None
            Include only boolean columns. 
            If None, will attempt to use everything, 
            then use only boolean data. Not implemented for Series.

#To select a row where each column meets its own criterion:
values = {'ids': ['a', 'b'], 'ids2': ['a', 'c'], 'vals': [1, 3]}
row_mask = df.isin(values).all(1)
>>>  df[row_mask]
Out[153 ]:  
ids ids2  vals
0   a    a     1


##DF - Selection By Callable
#fn(this_df): returns_any_of_indexing_objects_like_list_boolean_etc 

df1 = pd.DataFrame(np.random.randn(6, 4),
                   index=list('abcdef'),
                   columns=list('ABCD'))


>>> df1
          A         B         C         D
a -0.023688  2.410179  1.450520  0.206053
b -0.251905 -2.213588  1.063327  1.266143
c  0.299368 -0.863838  0.408204 -1.048089
d -0.025747 -0.988387  0.094055  1.262731
e  1.289997  0.082423 -0.055758  0.536580
f -0.489682  0.369374 -0.034571 -2.484478

>>> df1.loc[lambda df: df.A > 0, :]  #where A 's element > 0 
          A         B         C         D
c  0.299368 -0.863838  0.408204 -1.048089
e  1.289997  0.082423 -0.055758  0.536580

>>> df1.loc[:, lambda df: ['A', 'B']]
          A         B
a -0.023688  2.410179
b -0.251905 -2.213588
c  0.299368 -0.863838
d -0.025747 -0.988387
e  1.289997  0.082423
f -0.489682  0.369374

>>> df1.iloc[:, lambda df: [0, 1]]
          A         B
a -0.023688  2.410179
b -0.251905 -2.213588
c  0.299368 -0.863838
d -0.025747 -0.988387
e  1.289997  0.082423
f -0.489682  0.369374

>>> df1[lambda df: df.columns[0]]
a   -0.023688
b   -0.251905
c    0.299368
d   -0.025747
e    1.289997
f   -0.489682
Name: A, dtype: float64


##DF -  .xs() , .iat[row_index,column_index], .at[row_label, column_label]
DataFrame.xs(key, axis=0, level=None, drop_level=True)
    Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. 
    Defaults to cross-section on the rows (axis=0).
    key: row or column label 
    
DataFrame.iat[rowIndex,columnIndex]
    Fast integer location scalar accessor.
    Similar to .iloc[i,j]
    
DataFrame.at[row_indexer,column_indexer]
    label based , scalar accessor 
    similar to .loc[row_label,col_label]

#Example 
>>> df
   A  B  C
a  4  5  2
b  4  0  9
c  9  7  3
>>> df.xs('a')
A    4
B    5
C    2
Name: a
>>> df.xs('C', axis=1)
a    2
b    9
c    3
Name: C


#Multiindex
>>> df
                    A  B  C  D
first second third
bar   one    1      4  1  8  9
      two    1      7  5  5  0
baz   one    1      6  6  8  0
      three  2      5  3  5  3
>>> df.xs(('baz', 'three'))
       A  B  C  D
third
2      5  3  5  3
>>> df.xs('one', level=1)
             A  B  C  D
first third
bar   1      4  1  8  9
baz   1      6  6  8  0
>>> df.xs(('baz', 2), level=[0, 'third'])
        A  B  C  D
second
three   5  3  5  3

#Another example 
df = pd.DataFrame(np.random.randn(6,4),
                      index=list(range(0,12,2)),
                      columns=list(range(0,8,2)))
>>> df
           0         2         4         6
0  -0.777754 -1.870957  0.579716 -0.908213
2   0.171079  1.588356  0.695900  2.054582
4  -0.452064 -0.856810  0.621094 -0.847803
6  -1.366536  0.662914  0.741095  1.351606
8  -0.471646  0.329032  0.555486 -0.160882
10  0.183088  0.478884 -0.438279  2.355064

>>> df.iloc[1, 1]  #index based 
1.588355597715046
>>> df1.iat[1, 1]   #index based 
1.588355597715046
>>> df1.iat[2, 2]   #label_based
1.588355597715046
>>> df.iloc[1]  #= .iloc[1,:]
0    0.171079
2    1.588356
4    0.695900
6    2.054582
Name: 2, dtype: float64
>>> df.xs(2)    #row label 
0    0.171079
2    1.588356
4    0.695900
6    2.054582
Name: 2, dtype: float64


##DF - where() Method and Masking
DataFrame.where(cond, other=nan, inplace=False, axis=None, 
        level=None, errors='raise', try_cast=False, raise_on_error=None)
    cond : boolean NDFrame, array-like, or callable
        Where cond is True, keep the original value. 
        Where False, replace with corresponding value from 'other'. 
        If cond is callable, it is computed on the NDFrame 
    other : scalar, NDFrame, or callable
        Entries where cond is False are replaced with corresponding value from other. 
        If other is callable, it is computed on the NDFrame 
        and should return scalar or NDFrame. 
    inplace : boolean, default False
        Whether to perform the operation in place on the data
    axis : alignment axis if needed, default None
    level : alignment level if needed, default None
        errors : str, {'raise', 'ignore'}, default 'raise'
        •raise : allow exceptions to be raised
        •ignore : suppress exceptions. On error return original object
    try_cast : boolean, default False
    try to cast the result back to the input type (if possible)
    
#To return only the selected rows
s = pd.Series(range(-3, 4))
>>> s
0   -3
1   -2
2   -1
3    0
4    1
5    2
6    3
dtype: int64
>>> s[s>0]
4    1
5    2
6    3
dtype: int64
>>> s.where(s>0)
0    NaN
1    NaN
2    NaN
3    NaN
4    1.0
5    2.0
6    3.0
dtype: float64
#with DF
>>> df[df < 0]
                A         B         C         D
2000-01-01 -1.282782       NaN -1.071357       NaN
2000-01-02       NaN       NaN       NaN -0.744471
2000-01-03       NaN       NaN -0.964980 -0.845696
2000-01-04 -1.340896       NaN -1.328865       NaN
2000-01-05 -1.717693       NaN       NaN       NaN
2000-01-06       NaN       NaN -1.197071 -1.066969
2000-01-07 -0.303421 -0.858447       NaN -0.028665
2000-01-08       NaN       NaN       NaN       NaN

#where takes an optional other argument for replacement of values
#where the condition is False, in the returned copy.
>>> df.where(df < 0, -df)  #do -df when where condition is false 
                A         B         C         D
2000-01-01 -1.282782 -0.781836 -1.071357 -0.441153
2000-01-02 -2.353925 -0.583787 -0.221471 -0.744471
2000-01-03 -0.758527 -1.729689 -0.964980 -0.845696
2000-01-04 -1.340896 -1.846883 -1.328865 -1.682706
2000-01-05 -1.717693 -0.888782 -0.228440 -0.901805
2000-01-06 -1.171216 -0.520260 -1.197071 -1.066969
2000-01-07 -0.303421 -0.858447 -0.306996 -0.028665
2000-01-08 -0.384316 -1.574159 -1.588931 -0.476720

#to set values based on some boolean criteria.
s2 = s.copy()
s2[s2 < 0] = 0
>>> s2
4    0
3    1
2    2
1    3
0    4
dtype: int64
#with DF 
df2 = df.copy()
df2[df2 < 0] = 0
>>> df2
A         B         C         D
2000-01-01  0.000000  0.781836  0.000000  0.441153
2000-01-02  2.353925  0.583787  0.221471  0.000000
2000-01-03  0.758527  1.729689  0.000000  0.000000
2000-01-04  0.000000  1.846883  0.000000  1.682706
2000-01-05  0.000000  0.888782  0.228440  0.901805
2000-01-06  1.171216  0.520260  0.000000  0.000000
2000-01-07  0.000000  0.000000  0.306996  0.000000
2000-01-08  0.384316  1.574159  1.588931  0.476720


#By default, where returns a modified copy of the data.
#There is an optional parameter inplace
df_orig = df.copy()
df_orig.where(df > 0, -df, inplace=True);
>>> df_orig
A         B         C         D
2000-01-01  1.282782  0.781836  1.071357  0.441153
2000-01-02  2.353925  0.583787  0.221471  0.744471
2000-01-03  0.758527  1.729689  0.964980  0.845696
2000-01-04  1.340896  1.846883  1.328865  1.682706
2000-01-05  1.717693  0.888782  0.228440  0.901805
2000-01-06  1.171216  0.520260  1.197071  1.066969
2000-01-07  0.303421  0.858447  0.306996  0.028665
2000-01-08  0.384316  1.574159  1.588931  0.476720


#Where can accept a callable as condition and other arguments.
df3 = pd.DataFrame({'A': [1, 2, 3],                      
        'B': [4, 5, 6],                      
        'C': [7, 8, 9]})

>>> df3.where(lambda x: x > 4, lambda x: x + 10) #x is this DF 
    A   B  C
0  11  14  7
1  12   5  8
2  13   6  9


#mask - mask is the inverse boolean operation of where.
DataFrame.mask(cond, other=nan, inplace=False, axis=None, 
        level=None, errors='raise', try_cast=False, raise_on_error=None)
>>> s.mask(s >= 0)
0   -3.0
1   -2.0
2   -1.0
3    NaN
4    NaN
5    NaN
6    NaN
dtype: float64

>>> df.mask(df >= 0)
A         B         C         D
2000-01-01 -1.282782       NaN -1.071357       NaN
2000-01-02       NaN       NaN       NaN -0.744471
2000-01-03       NaN       NaN -0.964980 -0.845696
2000-01-04 -1.340896       NaN -1.328865       NaN
2000-01-05 -1.717693       NaN       NaN       NaN
2000-01-06       NaN       NaN -1.197071 -1.066969
2000-01-07 -0.303421 -0.858447       NaN -0.028665
2000-01-08       NaN       NaN       NaN       NaN

##DF - eval  Method
DataFrame or pandas.eval(expr, parser='pandas', engine=None, 
    truediv=True, local_dict=None, global_dict=None, resolvers=(), 
    level=0, target=None, inplace=False)
    Evaluate expression involving this DF's columns 
    Only below operations are supported for pandas.eval
    for DataFrame.eval, column names involving below operations are supported 
        •Arithmetic operations except for the left shift (<<) and right shift (>>) operators, 
          e.g., df + 2 * pi / s ** 4 % 42 - the_golden_ratio
        •Comparison operations, including chained comparisons, e.g., 2 < df < df2
        •Boolean operations, e.g., df < df2 and df3 < df4 or not df_bool
        •list and tuple literals, e.g., [1, 2] or (1, 2)
        •Attribute access, e.g., df.a
        •Subscript expressions, e.g., df[0]
        •Simple variable evaluation, e.g., pd.eval('df') (this is not very useful)
        •Math functions, sin, cos, exp, log, expm1, log1p, sqrt, sinh, cosh, tanh, arcsin, arccos, arctan, arccosh, arcsinh, arctanh, abs and arctan2.
target : object, optional, default None
    This is the target object for assignment. 
    It is used when there is variable assignment in the expression. 
    If so, then target must support item assignment with string keys, 
    and if a copy is being returned, it must also support .copy().
inplace : bool, default False
    If target is provided, and the expression mutates target, 
    whether to modify target inplace. 
    Otherwise, return a copy of target with the mutation.
 
#expr involving columns name 
df = pd.DataFrame(np.random.randn(5, 2), columns=['a', 'b'])
>>> df.eval('a + b')
0   -0.246747
1    0.867786
2   -1.626063
3   -1.134978
4   -1.027798
dtype: float64

df = pd.DataFrame(dict(a=range(5), b=range(5, 10)))
df.eval('c = a + b', inplace=True)
df.eval('d = a + b + c', inplace=True)
df.eval('a = 1', inplace=True)
>>> df
   a  b   c   d
0  1  5   5  10
1  1  6   7  14
2  1  7   9  18
3  1  8  11  22
4  1  9  13  26
#multiline 
>>> df.eval("""
    c = a + b
    d = a + b + c
    a = 1""", inplace=False)
   a  b   c   d
0  1  5   6  12
1  1  6   7  14
2  1  7   8  16
3  1  8   9  18
4  1  9  10  20
#The equivalent in standard Python would be

df = pd.DataFrame(dict(a=range(5), b=range(5, 10)))
df['c'] = df.a + df.b
df['d'] = df.a + df.b + df.c
df['a'] = 1
>>> df
   a  b   c   d
0  1  5   5  10
1  1  6   7  14
2  1  7   9  18
3  1  8  11  22
4  1  9  13  26

##DF - query() Method
DataFrame.query(expr, inplace=False, **kwargs)
    Query the columns of a frame with a boolean expression.
    Uses the top-level pandas.eval() function to evaluate the passed query
    Parameters:
        expr : string
            The query string(involving colums name) to evaluate. 
            You can refer to variables in the environment by prefixing them with an '@' character like @a + b.
        inplace : bool
            Whether the query should modify the data in place or return a modified copy
        kwargs : dict
            See the documentation for pandas.eval() 
#example 
n = 10
df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))
>>> df
    a         b         c
0  0.287377  0.914479  0.010693
1  0.474521  0.259146  0.607235
2  0.317092  0.266463  0.712394
3  0.408662  0.705810  0.920000
4  0.740984  0.109157  0.456672
5  0.412527  0.455994  0.236508
6  0.769694  0.549898  0.530660
7  0.093826  0.022924  0.425014
8  0.029375  0.964501  0.680094
9  0.260754  0.755564  0.761476

# pure python
>>> df[(df.a < df.b) & (df.b < df.c)]
    a         b         c
3  0.408662  0.705810  0.920000
9  0.260754  0.755564  0.761476

# query
>>> df.query('(a < b) & (b < c)')
        a         b         c
3  0.408662  0.705810  0.920000
9  0.260754  0.755564  0.761476


#Do the same thing but fall back on a named index 
#if there is no column with the name a.
df = pd.DataFrame(np.random.randint(n / 2, size=(n, 2)), columns=list('bc'))
df.index.name = 'a'
>>> df
    b  c      
a
0  3  4
1  1  1
2  0  4
3  4  3
4  1  3
5  3  3
6  2  3
7  4  0
8  3  4
9  2  1

>>> df.query('a < b and b < c')
   b  c      
a
0  3  4


#If instead you don't want to or cannot name your index,
#you can use the name 'index' in your query expression:
#refer to the index as ilevel_0 as well
df = pd.DataFrame(np.random.randint(n, size=(n, 2)), columns=list('bc'))
>>> df
    b  c
0  8  8
1  8  1
2  3  4
3  8  4
4  8  9
5  3  5
6  9  4
7  0  8
8  9  2
9  3  9

>>> df.query('index < b < c')
   b  c
2  3  4
4  8  9


# If the name of your index overlaps with a column name, 
#the column name is given precedence.
df = pd.DataFrame({'a': np.random.randint(5, size=5)})
df.index.name = 'a'
>>> df.query('a > 2') # uses the column 'a', not the index
   a   
a
1  3
2  3
4  3

#You can still use the index in a query expressi on
#by using the special identifier 'index':
>>> df.query('index > 2')
    a   
a
3  1
4  3




#MultiIndex query() Syntax
n = 10
colors = np.random.choice(['red', 'green'], size=n)
foods = np.random.choice(['eggs', 'ham'], size=n)
>>> colors
array(['red', 'red', 'green', 'green', 'red', 'green', 'red', 'green',       
    'red', 'red' ],      
    dtype='|S5')

>>> foods
array(['ham', 'eggs', 'ham', 'eggs', 'ham', 'eggs', 'ham', 'eggs', 'ham',       
    'ham' ],      
    dtype='|S4')

index = pd.MultiIndex.from_arrays([colors, foods], names=['color', 'food'])
df = pd.DataFrame(np.random.randn(n, 2), index=index)
>>> df
                0         1
color food
red   ham   0.694028  0.177154      
      eggs  0.535700 -0.506675
green ham   0.421335 -1.289076      
      eggs -0.178069 -0.271841
red   ham   1.406993 -1.334905
green eggs -1.087664 -0.883833
red   ham  -1.554827 -0.118953
green eggs -1.460084 -0.020351
red   ham  -0.256125  0.358575      
      ham   1.112033 -0.200521

>>> df.query('color == "red"')
              0         1
color food
red   ham   0.694028  0.177154      
      eggs  0.535700 -0.506675      
      ham   1.406993 -1.334905      
      ham  -1.554827 -0.118953      
      ham  -0.256125  0.358575      
      ham   1.112033 -0.200521


#If the levels of the MultiIndex are unnamed,
#you can refer to them using special names:
#The convention is ilevel_0, which means 'index level 0' for the 0th level of the index.
df.index.names = [None, None]
>>> df
               0         1
red   ham   0.694028  0.177154      
      eggs  0.535700 -0.506675
green ham   0.421335 -1.289076      
      eggs -0.178069 -0.271841
red   ham   1.406993 -1.334905
green eggs -1.087664 -0.883833
red   ham  -1.554827 -0.118953
green eggs -1.460084 -0.020351
red   ham  -0.256125  0.358575      
      ham   1.112033 -0.200521

>>> df.query('ilevel_0 == "red"')
            0         1
red ham   0.694028  0.177154    
eggs  0.535700 -0.506675    
ham   1.406993 -1.334905    
ham  -1.554827 -0.118953    
ham  -0.256125  0.358575    
ham   1.112033 -0.200521


#The in and not in operators in query
# get all rows where columns "a" and "b" have overlapping values
>>>df = pd.DataFrame({'a': list('aabbccddeeff'), 'b': list('aaaabbbbcccc'),                     
    'c': np.random.randint(5, size=12),                    
    'd': np.random.randint(9, size=12)})
>>> df
    a  b  c  d
0   a  a  4  8
1   a  a  4  0
2   b  a  3  1
3   b  a  3  5
4   c  b  3  0
5   c  b  0  4
6   d  b  2  6
7   d  b  1  2
8   e  c  4  3
9   e  c  2  0
10  f  c  1  6
11  f  c  1  5

>>> df.query('a in b')
   a  b  c  d
0  a  a  4  8
1  a  a  4  0
2  b  a  3  1
3  b  a  3  5
4  c  b  3  0
5  c  b  0  4

# How you'd do it in pure Python
>>> df[df.a.isin(df.b)]
    a  b  c  d
0  a  a  4  8
1  a  a  4  0
2  b  a  3  1
3  b  a  3  5
4  c  b  3  0
5  c  b  0  4

>>> df.query('a not in b')
    a  b  c  d
6   d  b  2  6
7   d  b  1  2
8   e  c  4  3
9   e  c  2  0
10  f  c  1  6
11  f  c  1  5

# pure Python
>>> df[~df.a.isin(df.b)]
    a  b  c  d
6   d  b  2  6
7   d  b  1  2
8   e  c  4  3
9   e  c  2  0
10  f  c  1  6
11  f  c  1  5


#You can combine this with other expressions for very succinct queries:
# rows where cols a and b have overlapping values and col c's values are less than col d's
>>> df.query('a in b and c < d')
    a  b  c  d
0  a  a  4  8
3  b  a  3  5
5  c  b  0  4

# pure Python
>>> df[df.b.isin(df.a) & (df.c < df.d)]
    a  b  c  d
0   a  a  4  8
3   b  a  3  5
5   c  b  0  4
6   d  b  2  6
7   d  b  1  2
10  f  c  1  6
11  f  c  1  5


#In general, any operations that can be evaluated using numexpr will be.
#Special use of the == operator with list objects
#Comparing a list of values to a column using ==/!= works similarly to in/not in
>>> df.query('b == ["a", "b", "c"]')
    a  b  c  d
0   a  a  4  8
1   a  a  4  0
2   b  a  3  1
3   b  a  3  5
4   c  b  3  0
5   c  b  0  4
6   d  b  2  6
7   d  b  1  2
8   e  c  4  3
9   e  c  2  0
10  f  c  1  6
11  f  c  1  5

# pure Python
>>> df[df.b.isin(["a", "b", "c"])]
    a  b  c  d
0   a  a  4  8
1   a  a  4  0
2   b  a  3  1
3   b  a  3  5
4   c  b  3  0
5   c  b  0  4
6   d  b  2  6
7   d  b  1  2
8   e  c  4  3
9   e  c  2  0
10  f  c  1  6
11  f  c  1  5

>>> df.query('c == [1, 2]')
    a  b  c  d
6   d  b  2  6
7   d  b  1  2
9   e  c  2  0
10  f  c  1  6
11  f  c  1  5

>>> df.query('c != [1, 2]')
    a  b  c  d
0  a  a  4  8
1  a  a  4  0
2  b  a  3  1
3  b  a  3  5
4  c  b  3  0
5  c  b  0  4
8  e  c  4  3

# using in/not in
>>> df.query('[1, 2] in c')
    a  b  c  d
6   d  b  2  6
7   d  b  1  2
9   e  c  2  0
10  f  c  1  6
11  f  c  1  5

>>> df.query('[1, 2] not in c')
    a  b  c  d
0  a  a  4  8
1  a  a  4  0
2  b  a  3  1
3  b  a  3  5
4  c  b  3  0
5  c  b  0  4
8  e  c  4  3

# pure Python
>>> df[df.c.isin([1, 2])]
    a  b  c  d
6   d  b  2  6
7   d  b  1  2
9   e  c  2  0
10  f  c  1  6
11  f  c  1  5



#negate boolean expressions with the word not or the ~ operator.
df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))
df['bools'] = np.random.rand(len(df)) > 0.5
>>> df.query('~bools')
         a         b         c  bools
3  0.400348  0.495503  0.815711  False
4  0.126361  0.328858  0.356186  False
5  0.620423  0.750739  0.718272  False
8  0.900520  0.883878  0.452325  False

>>> df.query('not bools')
        a         b         c  bools
3  0.400348  0.495503  0.815711  False
4  0.126361  0.328858  0.356186  False
5  0.620423  0.750739  0.718272  False
8  0.900520  0.883878  0.452325  False

>>> df.query('not bools') == df[~df.bools]
    a     b     c bools
3  True  True  True  True
4  True  True  True  True
5  True  True  True  True
8  True  True  True  True

#complex query 
# short query syntax
shorter = df.query('a < b < c and (not bools) or bools > 2')
# equivalent in pure Python
longer = df[(df.a < df.b) & (df.b < df.c) & (~df.bools) | (df.bools > 2)]
>>> shorter
    a         b         c  bools
3  0.400348  0.495503  0.815711  False
4  0.126361  0.328858  0.356186  False

>>> longer
    a         b         c  bools
3  0.400348  0.495503  0.815711  False
4  0.126361  0.328858  0.356186  False

>>> shorter == longer
    a     b     c bools
3  True  True  True  True
4  True  True  True  True





##DF - Duplicate Data 
DataFrame.duplicated(subset=None, keep='first')
    returns a boolean vector whose length is the number of rows,
    and which indicates whether a row is duplicated.
DataFrame.drop_duplicates(subset=None, keep='first', inplace=False)
    Return DataFrame with duplicate rows removed, 
    Parameters:
    subset : column label or sequence of labels, optional
        Only consider certain columns for identifying duplicates, 
        by default use all of the columns
    keep : {'first', 'last', False}, default 'first'
        •first : Drop duplicates except for the first occurrence.
        •last : Drop duplicates except for the last occurrence.
        •False : Drop all duplicates.
    inplace : boolean, default False
        Whether to drop duplicates in place or to return a copy

#Example 
df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'two', 'two', 'three', 'four'],                        
            'b': ['x', 'y', 'x', 'y', 'x', 'x', 'x'],                        
            'c': np.random.randn(7)})  
>>> df2
       a  b         c
0    one  x  0.340520
1    one  y -0.252382
2    two  x -1.206215
3    two  y  1.233196
4    two  x  1.671173
5  three  x  0.120314
6   four  x  0.779461

>>> df2.duplicated('a')
0    False
1     True
2    False
3     True
4     True
5    False
6    False
dtype: bool

>>> df2.duplicated('a', keep='last')
0     True
1    False
2     True
3     True
4    False
5    False
6    False
dtype: bool

>>> df2.duplicated('a', keep=False)
0     True
1     True
2     True
3     True
4     True
5    False
6    False
dtype: bool

>>> df2.drop_duplicates('a')
       a  b         c
0    one  x  0.340520
2    two  x -1.206215
5  three  x  0.120314
6   four  x  0.779461

>>> df2.drop_duplicates('a', keep='last')
       a  b         c
1    one  y -0.252382
4    two  x  1.671173
5  three  x  0.120314
6   four  x  0.779461

>>> df2.drop_duplicates('a', keep=False)
       a  b         c
5  three  x  0.120314
6   four  x  0.779461


#can pass a list of columns to identify duplications.
>>> df2.duplicated(['a', 'b'])
0    False
1    False
2    False
3    False
4     True
5    False
6    False
dtype: bool

>>> df2.drop_duplicates(['a', 'b'])
       a  b         c
0    one  x  0.340520
1    one  y -0.252382
2    two  x -1.206215
3    two  y  1.233196
5  three  x  0.120314
6   four  x  0.779461


#To drop duplicates by index value,
#use Index.duplicated then perform slicing.
#Same options are available in keep parameter.

df3 = pd.DataFrame({'a': np.arange(6),                   
        'b': np.random.randn(6)},                  
        index=['a', 'a', 'b', 'c', 'b', 'a']) 

>>> df3
   a         b
a  0 -0.867696
a  1 -0.422603
b  2 -1.482245
c  3 -0.878999
b  4  0.511972
a  5  1.320469

>>> df3.index.duplicated()
array([False,  True, False, False,  True,  True], dtype=bool)

>>> df3[~df3.index.duplicated()]
   a         b
a  0 -0.867696
b  2 -1.482245
c  3 -0.878999

>>> df3[~df3.index.duplicated(keep='last')]
   a         b
c  3 -0.878999
b  4  0.511972
a  5  1.320469

>>> df3[~df3.index.duplicated(keep=False)]
   a         b
c  3 -0.878999



##DF - Dictionary-like get() method
#Each of Series, DataFrame, and Panel have a get method which can return a default value.
DataFrame.get(key, default=None)
    Get item from object for given key (DataFrame column, Series index label)
    Returns default value if not found.

s = pd.Series([1,2,3], index=['a','b','c'])
>>> s.get('a')               # equivalent to s['a']
1

>>> s.get('x', default=-1)
-1



##DF - The select() Method
DataFrame.select(crit, axis=0)
    Return data corresponding to axis labels matching criteria
    DEPRECATED: use df.loc[df.index.map(crit)] to select via labels
    Parameters:
        crit : function
            To be called on each index (label)(or column name if axis=1) 
            Should return True or False
        axis : int, 0=index, 1=column 
 

>>> df.select(lambda x: x == 'A', axis=1) #

              A
2000-01-01  0.888261
2000-01-02  1.203830
2000-01-03  0.202067
2000-01-04  0.051667
2000-01-05 -0.728628
2000-01-06  0.257538
2000-01-07  0.362494
2000-01-08 -1.335476



##DF - lookup() Method
DataFrame.lookup(row_labels, col_labels)
    Label-based 'fancy indexing' function for DataFrame. 
    Given equal-length arrays of row and column labels, 
    return an array of the values corresponding to each (row, col) pair.



dflookup = pd.DataFrame(np.random.rand(20,4), columns = ['A','B','C','D'])
>>> dflookup.lookup(list(range(0,10,2)), ['B','C','A','B','D'])
array([ 0.1473,  0.7212,  0.521 ,  0.6512,  0.2522])




##DF - reset_index() 
DataFrame.reset_index(level=None, drop=False, inplace=False, col_level=0, 
            col_fill='')
    For DataFrame with multi-level index, 
    return new DataFrame with labeling information in the columns 
    defaulting to 'level_0', 'level_1', etc. 
    Parameters:
    level : int, str, tuple, or list, default None
        Only remove the given levels from the index. Removes all levels by default
    drop : boolean, default False
        Do not try to insert index into dataframe columns. 
        This resets the index to the default integer index.
    inplace : boolean, default False
        Modify the DataFrame in place (do not create a new object)
    col_level : int or str, default 0
    If the columns have multiple levels, 
    determines which level the labels are inserted into. 
    By default it is inserted into the first level.
    col_fill : object, default ''
        If the columns have multiple levels, 
        determines how the other levels are named. 
        If None then the index name is repeated.
 

#Examples
>>> df = pd.DataFrame([('bird',    389.0),
                    ('bird',     24.0),
                    ('mammal',   80.5),
                    ('mammal', np.nan)],
                   index=['falcon', 'parrot', 'lion', 'monkey'],
                   columns=('class', 'max_speed'))
>>> df
         class  max_speed
falcon    bird      389.0
parrot    bird       24.0
lion    mammal       80.5
monkey  mammal        NaN
#When we reset the index, the old index is added as a column, 
#and a new sequential index is used:
>>> df.reset_index()
    index   class  max_speed
0  falcon    bird      389.0
1  parrot    bird       24.0
2    lion  mammal       80.5
3  monkey  mammal        NaN
#We can use the drop parameter to avoid the old index being added as a column:
>>> df.reset_index(drop=True)
    class  max_speed
0    bird      389.0
1    bird       24.0
2  mammal       80.5
3  mammal        NaN
#You can also use reset_index with MultiIndex.
>>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),
                                        ('bird', 'parrot'),
                                        ('mammal', 'lion'),
                                        ('mammal', 'monkey')],
                                    names=['class', 'name'])
>>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),
                                      ('species', 'type')])
>>> df = pd.DataFrame([(389.0, 'fly'),
                        ( 24.0, 'fly'),
                        ( 80.5, 'run'),
                        (np.nan, 'jump')],
                    index=index,
                    columns=columns)
>>> df
               speed species
                 max    type
class  name
bird   falcon  389.0     fly
       parrot   24.0     fly
mammal lion     80.5     run
       monkey    NaN    jump
#If the index has multiple levels, we can reset a subset of them:
>>> df.reset_index(level='class')
         class  speed species
                  max    type
name
falcon    bird  389.0     fly
parrot    bird   24.0     fly
lion    mammal   80.5     run
monkey  mammal    NaN    jump
#If we are not dropping the index, by default, it is placed in the top level. 
#We can place it in another level:
>>> df.reset_index(level='class', col_level=1)
                speed species
         class    max    type
name
falcon    bird  389.0     fly
parrot    bird   24.0     fly
lion    mammal   80.5     run
monkey  mammal    NaN    jump
#When the index is inserted under another level, 
#we can specify under which one with the parameter col_fill:
>>> df.reset_index(level='class', col_level=1, col_fill='species')
              species  speed species
                class    max    type
name
falcon           bird  389.0     fly
parrot           bird   24.0     fly
lion           mammal   80.5     run
monkey         mammal    NaN    jump
#If we specify a nonexistent level for col_fill, it is created:
>>> df.reset_index(level='class', col_level=1, col_fill='genus')
                genus  speed species
                class    max    type
name
falcon           bird  389.0     fly
parrot           bird   24.0     fly
lion           mammal   80.5     run
monkey         mammal    NaN    jump



##DF - reindex()
DataFrame.reindex(labels=None, index=None, columns=None, 
            axis=None, method=None, copy=True, level=None, fill_value=nan, limit=None, 
            tolerance=None)
    Conform DataFrame to new index with optional filling logic
    DataFrame.reindex supports two calling conventions
        •(index=index_labels, columns=column_labels, ...)
        •(labels, axis={'index', 'columns'}, ...)
    Parameters:
    labels : array-like, optional
        New labels / index to conform the axis specified by 'axis' to.
    index, columns : array-like, optional (should be specified using keywords)
        New labels / index to conform to. 
        Preferably an Index object to avoid duplicating data
    axis : int or str, optional
        Axis to target. Can be either the axis name ('index', 'columns') 
        or number (0, 1).
    method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}, optional
        method to use for filling holes in reindexed DataFrame. 
        this is only applicable to DataFrames/Series 
        with a monotonically increasing/decreasing index.
        •default: don't fill gaps
        •pad / ffill: propagate last valid observation forward to next valid
        •backfill / bfill: use next valid observation to fill gap
        •nearest: use nearest valid observations to fill gap
    copy : boolean, default True
        Return a new object, even if the passed indexes are the same
    level : int or name
        Broadcast across a level, matching Index values on the passed MultiIndex level
    fill_value : scalar, default np.NaN
        Value to use for missing values. Defaults to NaN, but can be any 'compatible' value
    limit : int, default None
        Maximum number of consecutive elements to forward or backward fill
    tolerance : optional
        Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance.
        Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index's type.

 
#Examples
>>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']
>>> df = pd.DataFrame({
        'http_status': [200,200,404,404,301],
        'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},
        index=index)
>>> df
           http_status  response_time
Firefox            200           0.04
Chrome             200           0.02
Safari             404           0.07
IE10               404           0.08
Konqueror          301           1.00
#Create a new index and reindex the dataframe. 
#By default values in the new index that do not have corresponding records 
#in the dataframe are assigned NaN.
>>> new_index= ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10','Chrome']
>>> df.reindex(new_index)
               http_status  response_time
Safari               404.0           0.07
Iceweasel              NaN            NaN
Comodo Dragon          NaN            NaN
IE10                 404.0           0.08
Chrome               200.0           0.02
#We can fill in the missing values by passing a value to the keyword fill_value. 
#Because the index is not monotonically increasing or decreasing, 
#we cannot use arguments to the keyword method to fill the NaN values.
>>> df.reindex(new_index, fill_value=0)
               http_status  response_time
Safari                 404           0.07
Iceweasel                0           0.00
Comodo Dragon            0           0.00
IE10                   404           0.08
Chrome                 200           0.02
>>> df.reindex(new_index, fill_value='missing')
              http_status response_time
Safari                404          0.07
Iceweasel         missing       missing
Comodo Dragon     missing       missing
IE10                  404          0.08
Chrome                200          0.02
#We can also reindex the columns.
>>> df.reindex(columns=['http_status', 'user_agent'])
           http_status  user_agent
Firefox            200         NaN
Chrome             200         NaN
Safari             404         NaN
IE10               404         NaN
Konqueror          301         NaN
#Or we can use 'axis-style' keyword arguments
>>> df.reindex(['http_status', 'user_agent'], axis="columns")
           http_status  user_agent
Firefox            200         NaN
Chrome             200         NaN
Safari             404         NaN
IE10               404         NaN
Konqueror          301         NaN
#create a dataframe with a monotonically increasing index 

>>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')
>>> df2 = pd.DataFrame({"prices": [100, 101, np.nan, 100, 89, 88]},
                    index=date_index)
>>> df2
            prices
2010-01-01     100
2010-01-02     101
2010-01-03     NaN
2010-01-04     100
2010-01-05      89
2010-01-06      88
>>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')
>>> df2.reindex(date_index2)
            prices
2009-12-29     NaN
2009-12-30     NaN
2009-12-31     NaN
2010-01-01     100
2010-01-02     101
2010-01-03     NaN
2010-01-04     100
2010-01-05      89
2010-01-06      88
2010-01-07     NaN
>>> df2.reindex(date_index2, method='bfill') #use next valid observation to fill gap
            prices
2009-12-29     100
2009-12-30     100
2009-12-31     100
2010-01-01     100  #this is valid obs 
2010-01-02     101
2010-01-03     NaN
2010-01-04     100
2010-01-05      89
2010-01-06      88
2010-01-07     NaN





###Pandas -DF -  Data alignment and arithmetic - elementwise 
#the resulting object will have the union of the column and row labels.
# randn(m,n) creates m rows and n columns random number

df = pd.DataFrame(np.random.randn(10, 4), columns=['A', 'B', 'C', 'D'])
df2 = pd.DataFrame(np.random.randn(7, 3), columns=['A', 'B', 'C'])
>> df + df2    #first 7x3 are added , remaining NAN     
       A       B       C   D
0 -1.9160 -0.9862 -2.4213 NaN
1  0.9651  1.6767  0.3298 NaN
2 -1.6622  2.1966 -1.9169 NaN
3 -0.1887  0.7653 -0.0010 NaN
4 -1.0760  0.3969 -1.1774 NaN
5  2.8104 -0.1792 -0.5705 NaN
6 -1.2272  0.1963  0.5312 NaN
7     NaN     NaN     NaN NaN
8     NaN     NaN     NaN NaN
9     NaN     NaN     NaN NaN


#DataFrame and Series, 
#the default behavior is to align the Series index on DF columns 
#and  broadcasting row-wise.
>>> df.iloc[0]
A    0.679512
B    1.036782
C    0.142549
D   -0.597687
Name: 0, dtype: float64
>>> df - df.iloc[0]  #remove  df.iloc[0] from each row 
     A       B       C       D
0  0.0000  0.0000  0.0000  0.0000
1  2.3859  1.3585  1.2234 -2.1065
2  2.1047  1.7004  1.3268 -0.6895
3  1.8741  2.7181  2.3819 -0.7597
4  2.1988  0.9662  0.8265  0.0932
5  4.9966  1.1967  1.3303 -0.2855
6  1.2632  0.5778  1.0712 -0.5254
7  3.4625  0.6322  1.0626 -0.4427
8  2.6802  3.1629  1.2977 -1.8177
9  1.3038  0.1957  3.5899 -0.8671


##But with time series data, the DataFrame index contains dates,
#the broadcasting will be column-wise
index = pd.date_range('1/1/2000', periods=8)  #default freq='D', 8 days or date_range(start_datetime,end_datetime)

#check http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases for offset aliases, 
#note for multiple, you can give '5H' ie every 5th hour 
#check https://github.com/pandas-dev/pandas/blob/master/doc/source/timeseries.rst#id11

df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=list('ABC'))
>>> df         
              A       B       C
2000-01-01  0.0627 -0.0284  0.4436
2000-01-02 -0.2688 -1.5776  1.8502
2000-01-03  0.6381 -0.5566 -0.0712
2000-01-04 -0.5114  0.1563 -1.0756
2000-01-05  1.6636 -0.4377 -0.0773
2000-01-06  0.0292  0.1790  1.7401
2000-01-07 -0.7290 -0.8980 -0.3142
2000-01-08 -0.0481 -0.8756  0.1691

>>> type(df['A'])
pandas.core.series.Series

#axis means
#axis : 0 means index, 1 means columns
#For Series input, axis to match Series index on
>>> df.sub(df['A'], axis=0)   # df - df['A'] is deprecated , axis=0, row varying-columnwise 
            A         B         C
2000-01-01  0.0  0.784846 -2.117680
2000-01-02  0.0 -0.128945 -1.516167
2000-01-03  0.0 -0.023183  1.285385
2000-01-04  0.0  0.504921 -0.062922
2000-01-05  0.0  0.474830 -0.672912
2000-01-06  0.0  1.131126 -0.302575
2000-01-07  0.0  0.095319 -1.910453
2000-01-08  0.0  1.067330 -1.884796

##Operations with scalars - scalers are broadcased
>>> df * 5 + 2              
              A       B        C
2000-01-01   2.3135  1.8579   4.2178
2000-01-02   0.6561 -5.8882  11.2510
2000-01-03   5.1903 -0.7830   1.6438
2000-01-04  -0.5571  2.7814  -3.3781
2000-01-05  10.3180 -0.1886   1.6135
2000-01-06   2.1460  2.8950  10.7003
2000-01-07  -1.6451 -2.4900   0.4289
2000-01-08   1.7596 -2.3780   2.8455

>>> 1 / df                
             A        B        C
2000-01-01  15.9483 -35.1931   2.2545
2000-01-02  -3.7205  -0.6339   0.5405
2000-01-03   1.5672  -1.7966 -14.0388
2000-01-04  -1.9553   6.3984  -0.9297
2000-01-05   0.6011  -2.2845 -12.9363
2000-01-06  34.2568   5.5863   0.5747
2000-01-07  -1.3717  -1.1136  -3.1826
2000-01-08 -20.8019  -1.1421   5.9134

>>> df ** 4             
                A           B           C
2000-01-01  1.5457e-05  6.5188e-07  3.8707e-02
2000-01-02  5.2191e-03  6.1948e+00  1.1718e+01
2000-01-03  1.6575e-01  9.5982e-02  2.5745e-05
2000-01-04  6.8412e-02  5.9663e-04  1.3386e+00
2000-01-05  7.6595e+00  3.6712e-02  3.5708e-05
2000-01-06  7.2612e-07  1.0268e-03  9.1678e+00
2000-01-07  2.8246e-01  6.5029e-01  9.7473e-03
2000-01-08  5.3406e-06  5.8781e-01  8.1783e-04


##DF - Boolean operators
df1 = pd.DataFrame({'a' : [1, 0, 1], 'b' : [0, 1, 1] }, dtype=bool) #key is column label
df2 = pd.DataFrame({'a' : [0, 1, 1], 'b' : [1, 1, 0] }, dtype=bool)

>>> df1 & df2  # ^(for xor) |(for or) ~(for not)     
    a      b
0  False  False
1  False  True
2  True  False
>>> df1[df1&df2]   #can be used for selecting 



##DF - Transposing - use .T
# only show the first 5 rows
>>> df[:5].T
        2000-01-01  2000-01-02  2000-01-03  2000-01-04  2000-01-05
A      0.0627     -0.2688      0.6381     -0.5114      1.6636
B     -0.0284     -1.5776     -0.5566      0.1563     -0.4377
C      0.4436      1.8502     -0.0712     -1.0756     -0.0773



##DF - DataFrame interoperability with NumPy functions 
#can use Elementwise NumPy ufuncs (log, exp, sqrt, ...) and almost all other functions
#each element is transformed 

>>> np.exp(df)               
             A       B       C
2000-01-01  1.0647  0.9720  1.5582
2000-01-02  0.7643  0.2065  6.3611
2000-01-03  1.8928  0.5732  0.9312
2000-01-04  0.5996  1.1692  0.3411
2000-01-05  5.2783  0.6455  0.9256
2000-01-06  1.0296  1.1960  5.6977
2000-01-07  0.4824  0.4074  0.7304
2000-01-08  0.9531  0.4166  1.1842

#The dot method on DataFrame implements matrix multiplication:
>>> df.T.dot(df)   
     A       B       C
A  4.0471 -0.0390  0.1783
B -0.0390  4.6207 -2.5806
C  0.1783 -2.5806  7.9431


# the dot method on Series implements dot product:
>>> s1 = pd.Series(np.arange(5,10))
>>> s1.dot(s1)
Out[100]: 255




###Pandas -DF -   Console display 
#use print(), df.info(), df.to_string() to get info on the data

baseball = pd.read_csv('data/baseball.csv')
>>> print(baseball)       
id     player  year  stint  ...   hbp   sh   sf  gidp
0   88641  womacto01  2006      2  ...   0.0  3.0  0.0   0.0
1   88643  schilcu01  2006      1  ...   0.0  0.0  0.0   0.0
..    ...        ...   ...    ...  ...   ...  ...  ...   ...
98  89533   aloumo01  2007      1  ...   2.0  0.0  3.0  13.0
99  89534  alomasa02  2007      1  ...   0.0  0.0  0.0   0.0

[100 rows x 23 columns]

>>> baseball.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 100 entries, 0 to 99
Data columns (total 23 columns):
id        100 non-null int64
player    100 non-null object
year      100 non-null int64
stint     100 non-null int64
team      100 non-null object
lg        100 non-null object
g         100 non-null int64

dtypes: float64(9), int64(11), object(3)
memory usage: 18.0+ KB


#Use .to_string()
>>> print(baseball.iloc[-20:, :12].to_string())       
id     player  year  stint team  lg    g   ab   r    h  X2b  X3b
80  89474  finlest01  2007      1  COL  NL   43   94   9   17    3    0
81  89480  embreal01  2007      1  OAK  AL    4    0   0    0    0    0
82  89481  edmonji01  2007      1  SLN  NL  117  365  39   92   15    2
83  89482  easleda01  2007      1  NYN  NL   76  193  24   54    6    0
84  89489  delgaca01  2007      1  NYN  NL  139  538  71  139   30    0
85  89493  cormirh01  2007      1  CIN  NL    6    0   0    0    0    0
86  89494  coninje01  2007      2  NYN  NL   21   41   2    8    2    0


#Or use set_option()
>>> pd.DataFrame(np.random.randn(3, 12))      
0         1         2         3         4         5         6   \
0  1.225021 -0.528620  0.448676  0.619107 -1.199110 -0.949097  2.1695   23
1 -1.753617  0.992384 -0.505601 -0.599848  0.133585  0.008836 -1.7677   10
2 -0.461585 -1.321106  1.745476  1.445100  0.991037 -0.860733 -0.8706   61
         
7         8         9         10          11
0  0.302230  0.919516  0.657436  0.262574 -0.8047  98
1  0.700112 -0.020773 -0.302481  0.347869  0.1791  23
2 -0.117845 -0.046266  2.095649 -0.524324 -0.6105  55

>>> pd.set_option('display.width', 40) # default is 80
>>> pd.DataFrame(np.random.randn(3, 12))        
0         1         2   \
0 -1.280951  1.472585 -1.0019   14
1  0.130529 -1.603771 -0.1288   30
2 -1.084566 -0.515272  1.3675   86
         
3         4         5   \
0  1.044770 -0.050668 -0.0132   89
1 -1.869301 -0.232977 -0.1398   01
2  0.963500  0.224105 -0.0200   51
 


 


###Pandas - DF - Selecting Random Samples 
DataFrame.sample(n=None, frac=None, replace=False, weights=None, 
            random_state=None, axis=None)
    Returns a random sample of items(n or frac(tion)) from an axis of object

#Example 
>>> s = pd.Series(np.random.randn(50))
>>> s.head()
0   -0.038497
1    1.820773
2   -0.972766
3   -1.598270
4   -1.095526
dtype: float64
>>> df = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))
>>> df.head()
          A         B         C         D
0  0.016443 -2.318952 -0.566372 -1.028078
1 -1.051921  0.438836  0.658280 -0.175797
2 -1.243569 -0.364626 -0.215065  0.057736
3  1.768216  0.404512 -0.385604 -1.457834
4  1.072446 -1.137172  0.314194 -0.046661

#3 random elements from the Series:
>>> s.sample(n=3)
27   -0.994689
55   -1.049016
67   -0.224565
dtype: float64

#And a random 10% of the DataFrame with replacement:
>>> df.sample(frac=0.1, replace=True)
           A         B         C         D
35  1.981780  0.142106  1.817165 -0.290805
49 -1.336199 -0.448634 -0.789640  0.217116
40  0.823173 -0.078816  1.009536  1.015108
15  1.421154 -0.055301 -1.922594 -0.019696
6  -0.148339  0.832938  1.787600 -1.383767



###Pandas - DF - Conversion and NA handling 
DataFrame.astype(dtype[, copy, errors])         Cast a pandas object to a specified dtype dtype. 
DataFrame.convert_objects([convert_dates, ...]) Deprecated. 
DataFrame.infer_objects()                       Attempt to infer better dtypes for object columns. 
DataFrame.copy([deep])                          Make a copy of this objects data. 

DataFrame.isna()        Return a boolean same-sized object indicating if the values are NA. 
DataFrame.notna()       Return a boolean same-sized object indicating if the values are not NA. 
pandas.isna(obj)        Detect missing values (NaN in numeric arrays, None/NaN in object arrays) 
pandas.isnull(obj)      Detect missing values (NaN in numeric arrays, None/NaN in object arrays) 
pandas.notna(obj)       Replacement for numpy.isfinite / -numpy.isnan which is suitable for use on object arrays. 
pandas.notnull(obj)     Replacement for numpy.isfinite / -numpy.isnan which is suitable for use on object arrays. 

DataFrame.dropna([axis, how, thresh, ...])      Return object with labels on given axis omitted where alternately any 
DataFrame.fillna([value, method, axis, ...])    Fill NA/NaN values using the specified method 
DataFrame.replace([to_replace, value, ...])     Replace values given in 'to_replace' with 'value'. 

DataFrame.astype(dtype, copy=True, errors='raise', **kwargs)
    Cast a pandas object to a specified dtype dtype.
    Parameters:
        dtype : data type, or dict of column name -> data type
        Use a numpy.dtype or Python type to cast entire pandas object to the same type. 
        Alternatively, use {col: dtype, ...}, where col is a column label 
    copy : bool, default True.
        Return a copy when copy=True 
    
#Example  
>>> ser = pd.Series([1, 2], dtype='int32')
>>> ser
0    1
1    2
dtype: int32
>>> ser.astype('int64')
0    1
1    2
dtype: int64


DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)
    Return object with labels on given axis omitted 
    where alternately any or all of the data are missing
    Parameters:
        axis : {0 or 'index', 1 or 'columns'}, or tuple/list thereof
            Pass tuple or list to drop on multiple axes
        how : {'any', 'all'}
            •any : if any NA values are present, drop that label
            •all : if all values are NA, drop that label
        thresh : int, default None
            int value : require that many non-NA values
        subset : array-like
            Labels along other axis to consider, 
            e.g. if you are dropping rows these would be a list of columns to include
        inplace : boolean, default False
            If True, do operation inplace and return None.
#Example 
>>> df = pd.DataFrame([[np.nan, 2, np.nan, 0], [3, 4, np.nan, 1],
                        [np.nan, np.nan, np.nan, 5]],
                    columns=list('ABCD'))
>>> df
     A    B   C  D
0  NaN  2.0 NaN  0
1  3.0  4.0 NaN  1
2  NaN  NaN NaN  5
#Drop the columns where all elements are nan:
>>> df.dropna(axis=1, how='all')
     A    B  D
0  NaN  2.0  0
1  3.0  4.0  1
2  NaN  NaN  5
#Drop the columns where any of the elements is nan
>>> df.dropna(axis=1, how='any')
   D
0  0
1  1
2  5
#Drop the rows where all of the elements are nan 
#(there is no row to drop, so df stays the same):
>>> df.dropna(axis=0, how='all')
     A    B   C  D
0  NaN  2.0 NaN  0
1  3.0  4.0 NaN  1
2  NaN  NaN NaN  5


DataFrame.fillna(value=None, method=None, axis=None, 
            inplace=False, limit=None, downcast=None, **kwargs)
    Fill NA/NaN values using the specified method
    Parameters:
        value : scalar, dict, Series, or DataFrame
            Value to use to fill holes (e.g. 0), 
            alternately a dict/Series/DataFrame of values 
            specifying which value to use for each index (for a Series) 
            or column (for a DataFrame). 
            (values not in the dict/Series/DataFrame will not be filled). 
            This value cannot be a list.
        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None
            Method to use for filling holes in reindexed Series 
            pad / ffill: propagate last valid observation forward to next valid 
            backfill / bfill: use NEXT valid observation to fill gap
        axis : {0 or 'index', 1 or 'columns'}
        inplace : boolean, default False
            If True, fill in place. 
            
#Example 
>>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],
                        [3, 4, np.nan, 1],
                        [np.nan, np.nan, np.nan, 5],
                        [np.nan, 3, np.nan, 4]],
                        columns=list('ABCD'))
>>> df
     A    B   C  D
0  NaN  2.0 NaN  0
1  3.0  4.0 NaN  1
2  NaN  NaN NaN  5
3  NaN  3.0 NaN  4
#Replace all NaN elements with 0s.
>>> df.fillna(0)
    A   B   C   D
0   0.0 2.0 0.0 0
1   3.0 4.0 0.0 1
2   0.0 0.0 0.0 5
3   0.0 3.0 0.0 4
#We can also propagate non-null values forward or backward.
>>> df.fillna(method='ffill') #forward
    A   B   C   D
0   NaN 2.0 NaN 0
1   3.0 4.0 NaN 1   #  |
2   3.0 4.0 NaN 5   #  v
3   3.0 3.0 NaN 4

#Replace all NaN elements in column 'A', 'B', 'C', and 'D', 
#with 0, 1, 2, and 3 respectively.
>>> values = {'A': 0, 'B': 1, 'C': 2, 'D': 3}
>>> df.fillna(value=values)
    A   B   C   D
0   0.0 2.0 2.0 0
1   3.0 4.0 2.0 1
2   0.0 1.0 2.0 5
3   0.0 3.0 2.0 4

#Only replace the first NaN element.
>>> df.fillna(value=values, limit=1)
    A   B   C   D
0   0.0 2.0 2.0 0
1   3.0 4.0 NaN 1
2   NaN 1.0 NaN 5
3   NaN 3.0 NaN 4


DataFrame.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad', axis=None)
    Replace values given in 'to_replace' with 'value'.
    Parameters:
        to_replace : str, regex, list, dict, Series, numeric, or None
            •str or regex:
                ◦str: string exactly matching to_replace will be replaced with value
                ◦regex: regexs matching to_replace will be replaced with value
            •list of str, regex, or numeric:
                ◦First, if to_replace and value are both lists, they must be the same length.
                ◦Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. 
                This doesn't matter much for value since there are only a few possible substitution regexes you can use.
                ◦str and regex rules apply as above.
            •dict:
                ◦Nested dictionaries, e.g., {'a': {'b': nan}}, are read as follows: 
                look in column 'a' for the value 'b' and replace it with nan. 
                You can nest regular expressions as well. 
                Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.
                ◦Keys map to column names and values map to substitution values. 
                You can treat this as a special case of passing two lists 
                except that you are specifying the column to search in.
            •None:
                ◦This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.
        value : scalar, dict, list, str, regex, default None
            Value to use to fill holes (e.g. 0), alternately a dict of values specifying which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.
        inplace : boolean, default False
            If True, in place. Note: this will modify any other views on this object (e.g. a column from a DataFrame). Returns the caller if this is True.
        limit : int, default None
            Maximum size gap to forward or backward fill
        regex : bool or same types as to_replace, default False
            Whether to interpret to_replace and/or value as regular expressions. 
            If this is True then to_replace must be a string. 
            Otherwise, to_replace must be None because this parameter will be interpreted as a regular expression or a list, dict, or array of regular expressions.
        method : string, optional, {'pad', 'ffill', 'bfill'}
            The method to use when for replacement, when to_replace is a list
        
#Example 
>>> data
   resp          A          B          C
0     1       poor       poor       good
1     2       good       poor       good
2     3  very_good  very_good  very_good
3     4       bad        poor       bad 
4     5   very_bad   very_bad   very_bad
5     6       poor       good   very_bad
6     7       good       good       good
7     8  very_good  very_good  very_good
8     9       bad        bad    very_bad
9    10   very_bad   very_bad   very_bad

#
data.replace({'very_bad': 1, 'bad': 2, 'poor': 3, 'good': 4, 'very_good': 5}, inplace=True)
#or 
>>> data.replace(['very_bad', 'bad', 'poor', 'good', 'very_good'], [1, 2, 3, 4, 5]) 
      resp  A  B  C
   0     1  3  3  4
   1     2  4  3  4
   2     3  5  5  5
   3     4  2  3  2
   4     5  1  1  1
   5     6  3  4  1
   6     7  4  4  4
   7     8  5  5  5
   8     9  2  2  1
   9    10  1  1  1
   

###Pandas - DF - sorting, filter, drop, idxmax, truncate 
DataFrame.sort_values(by[, axis, ascending, ...])   Sort by the values along either axis 
DataFrame.sort_index([axis, level, ...])            Sort object by labels (along an axis) 
DataFrame.nlargest(n, columns[, keep])              Get the rows of a DataFrame sorted by the n largest values of columns. 
DataFrame.nsmallest(n, columns[, keep])             Get the rows of a DataFrame sorted by the n smallest values of columns. 
DataFrame.swaplevel([i, j, axis])                   Swap levels i and j in a MultiIndex on a particular axis 
DataFrame.T                                         Transpose index and columns 
DataFrame.to_xarray()                               Return an xarray object from the pandas object. 
DataFrame.transpose(*args, **kwargs)                Transpose index and columns 

DataFrame.add_prefix(prefix)                        Concatenate prefix string with panel items names. 
DataFrame.add_suffix(suffix)                        Concatenate suffix string with panel items names. 
DataFrame.align(other[, join, axis, level, ...])    Align two objects on their axes with the 
DataFrame.drop([labels, axis, index, ...])          Return new object with labels in requested axis removed. 
DataFrame.equals(other)                             Determines if two NDFrame objects contain the same elements. 

DataFrame.idxmax([axis, skipna])                    Return index of first occurrence of maximum over requested axis. 
DataFrame.idxmin([axis, skipna])                    Return index of first occurrence of minimum over requested axis. 

DataFrame.filter([items, like, regex, axis])        Subset rows or columns of dataframe according to labels in the specified index. 
DataFrame.first(offset)                             Convenience method for subsetting initial periods of time series data based on a date offset. ts.first('10D') -> First 10 days
DataFrame.head([n])                                 Return the first n rows. 
DataFrame.last(offset)                              Convenience method for subsetting final periods of time series data based on a date offset. 
DataFrame.tail([n])                                 Return the last n rows. 
DataFrame.take(indices[, axis, convert, is_copy])   Return the elements in the given positional indices along an axis. 
DataFrame.truncate([before, after, axis, copy])     Truncates a sorted DataFrame/Series before and/or after some particular index value. 


DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, 
        kind='quicksort', na_position='last')
    Sort by the values along either axis
    Parameters:
    by : str or list of str
        Name or list of names which refer to the axis items.
    axis : {0 or 'index', 1 or 'columns'}, default 0=index =columnwise 
        Axis to direct sorting
    ascending : bool or list of bool, default True
        Sort ascending vs. descending. Specify list for multiple sort orders. 
        If this is a list of bools, must match the length of the by.
    inplace : bool, default False
        if True, perform operation in-place
    kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'
    na_position : {'first', 'last'}, default 'last'
        first puts NaNs at the beginning, last puts NaNs at the end
 
 
#Examples
>>> df = pd.DataFrame({
        'col1' : ['A', 'A', 'B', np.nan, 'D', 'C'],
        'col2' : [2, 1, 9, 8, 7, 4],
        'col3': [0, 1, 9, 4, 2, 3],
    })
>>> df
    col1 col2 col3
0   A    2    0
1   A    1    1
2   B    9    9
3   NaN  8    4
4   D    7    2
5   C    4    3
>>> df.sort_values(by=['col1']) #axis=0 ie columnwise 
    col1 col2 col3
0   A    2    0
1   A    1    1
2   B    9    9
5   C    4    3
4   D    7    2
3   NaN  8    4
#Sort by multiple columns
>>> df.sort_values(by=['col1', 'col2'])
    col1 col2 col3
1   A    1    1
0   A    2    0
2   B    9    9
5   C    4    3
4   D    7    2
3   NaN  8    4
>>> df.sort_values(by=['col1', 'col2'],ascending=[True,False])
  col1  col2  col3
0    A     2     0
1    A     1     1
2    B     9     9
5    C     4     3
4    D     7     2
3  NaN     8     4


DataFrame.nlargest(n, columns, keep='first')
    Get the rows of a DataFrame sorted by the n largest values of columns.
    Parameters:
    n : int
        Number of items to retrieve
    columns : list or str
        Column name or names to order by
    keep : {'first', 'last'}, default 'first'
        Where there are duplicate values: 
        - first : take the first occurrence. 
        - last : take the last occurrence.
 
#Examples
>>> df = DataFrame({'a': [1, 10, 8, 11, -1],
                    'b': list('abdce'),
                    'c': [1.0, 2.0, np.nan, 3.0, 4.0]})
>>> df.nlargest(3, 'a')
    a  b   c
3  11  c   3
1  10  b   2
2   8  d NaN




DataFrame.truncate(before=None, after=None, axis=None, copy=True)
    Truncates a sorted DataFrame/Series 
    before and/or after some particular index value. 
    If the axis contains only datetime values, 
    before/after parameters are converted to datetime values.
    Parameters:
        before : date, string, int
            Truncate all rows before this index value
        after : date, string, int
            Truncate all rows after this index value
        axis : {0 or 'index', 1 or 'columns'}
            •0 or 'index': apply truncation to rows
            •1 or 'columns': apply truncation to columns
        copy : boolean, default is True,
            return a copy of the truncated section
 
 
#Examples
>>> df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'],
                        'B': ['f', 'g', 'h', 'i', 'j'],
                        'C': ['k', 'l', 'm', 'n', 'o']},
                        index=[1, 2, 3, 4, 5])
>>> df.truncate(before=2, after=4) #from 2 to 4 inclusive 
   A  B  C
2  b  g  l
3  c  h  m
4  d  i  n
>>> df = pd.DataFrame({'A': [1, 2, 3, 4, 5],
                        'B': [6, 7, 8, 9, 10],
                        'C': [11, 12, 13, 14, 15]},
                        index=['a', 'b', 'c', 'd', 'e'])
>>> df.truncate(before='b', after='d')
   A  B   C
b  2  7  12
c  3  8  13
d  4  9  14

>>> dates = pd.date_range('2016-01-01', '2016-02-01', freq='s') #'s' = second 
>>> df = pd.DataFrame(index=dates, data={'A': 1})
#note the difference 
>>> df.truncate('2016-01-05', '2016-01-10').tail()
                     A
2016-01-09 23:59:56  1
2016-01-09 23:59:57  1
2016-01-09 23:59:58  1
2016-01-09 23:59:59  1
2016-01-10 00:00:00  1  
>>> df.loc['2016-01-05':'2016-01-10', :].tail()
                     A
2016-01-10 23:59:55  1
2016-01-10 23:59:56  1
2016-01-10 23:59:57  1
2016-01-10 23:59:58  1
2016-01-10 23:59:59  1




DataFrame.take(indices, axis=0, convert=None, is_copy=True, **kwargs)
    Return the elements in the given positional indices along an axis.
    similar to iloc indexing 
    Parameters:
    indices : array-like
        An array of ints indicating which positions to take.
    axis : int, default 0
        The axis on which to select elements. 
        '0' means that we are selecting rows, 
        '1' means that we are selecting columns, etc.
    convert : bool, default True
        For example, -1 would map to the len(axis) - 1. 
    is_copy : bool, default True
        Whether to return a copy of the original object or not.
 

 
#Examples
>>> df = pd.DataFrame([('falcon', 'bird',    389.0),
                       ('parrot', 'bird',     24.0),
                       ('lion',   'mammal',   80.5),
                       ('monkey', 'mammal', np.nan)],
                      columns=('name', 'class', 'max_speed'),
                      index=[0, 2, 3, 1])
>>> df
     name   class  max_speed
0  falcon    bird      389.0
2  parrot    bird       24.0
3    lion  mammal       80.5
1  monkey  mammal        NaN
#0th index, 3rd index 
>>> df.take([0, 3])
0  falcon    bird      389.0
1  monkey  mammal        NaN
#Take elements at indices 1 and 2 along the axis 1 (column selection).
>>> df.take([1, 2], axis=1)
    class  max_speed
0    bird      389.0
2    bird       24.0
3  mammal       80.5
1  mammal        NaN
We may take elements using negative integers for positive indices, starting from the end of the object, just like with Python lists.
>>> df.take([-1, -2])
     name   class  max_speed
1  monkey  mammal        NaN
3    lion  mammal       80.5



DataFrame.idxmax(axis=0, skipna=True)
    Return index of first occurrence of maximum over requested axis. 
    NA/null values are excluded.
    Parameters:
        axis : {0 or 'index', 1 or 'columns'}, default 0
            0 or 'index' for columnwise, 1 or 'columns' for row-wise
        skipna : boolean, default True
            Exclude NA/null values. 
            If an entire row/column is NA, the result will be NA.
     
#Example 
>>> df1
          A         B         C         D
a  0.659510 -0.067171  1.849775  0.232036
b  0.806557  0.327022  1.269242 -0.021473
c -1.063328  0.587773  1.571305 -0.618556
d  1.443707 -1.259372  0.258808  1.464349
e -1.529423 -2.167431 -0.846262 -2.248487
f  0.997932 -2.210722  0.415821 -1.012406
#result is pandas.core.series.Series
>>> df1.idxmax(axis=0) #columnwise ie in each column , index of max value 
A    d
B    c
C    a
D    d
dtype: object
>>> s['A']
'd'
>>> s.index
Index(['A', 'B', 'C', 'D'], dtype='object')
>>> s.index[0]
'A'
>>> df1.at[ s[s.index[0]], s.index[0]]
1.443707
>>> df1.idxmax(axis=1) #rowwise ie in each row, column name for max value 
a    C
b    C
c    C
d    D
e    C
f    A
dtype: object



DataFrame.filter(items=None, like=None, regex=None, axis=None)
    Note that this routine does not filter a dataframe on its contents. 
    The filter is applied to the labels of the index.
    Parameters:
        items : list-like
            List of info axis to restrict to (must not all be present)
        like : string
            Keep info axis where 'arg in col == True'
        regex : string (regular expression)
            Keep info axis with re.search(regex, col) == True
        axis : int or string axis name
            The axis to filter on. By default this is 'index' for Series, 
            'columns' for DataFrame
     
 
#Examples
>>> df
        one  two  three
mouse     1    2      3
rabbit    4    5      6
# select columns by name
>>> df.filter(items=['one', 'three'])
        one  three
mouse     1      3
rabbit    4      6
# select columns by regular expression
>>> df.filter(regex='e$', axis=1)
        one  three
mouse     1      3
rabbit    4      6
# select rows containing 'bbi'
>>> df.filter(like='bbi', axis=0)
        one  two  three
rabbit    4    5      6



DataFrame.drop(labels=None, axis=0, index=None, 
            columns=None, level=None, inplace=False, errors='raise')
    Return new object with labels in requested axis removed.
    Parameters:
        labels : single label or list-like
            Index or column labels to drop.
        axis : int or axis name
            Whether to drop labels from the index (0 / 'index') or columns (1 / 'columns').
        index, columns : single label or list-like
            Alternative to specifying axis 
            (labels, axis=1 is equivalent to columns=labels).
        level : int or level name, default None
        For MultiIndex
        inplace : bool, default False
            If True, do operation inplace and return None.
        errors : {'ignore', 'raise'}, default 'raise'
            If 'ignore', suppress error and existing labels are dropped.
 
#Examples
>>> df = pd.DataFrame(np.arange(12).reshape(3,4),
                      columns=['A', 'B', 'C', 'D'])
>>> df
   A  B   C   D
0  0  1   2   3
1  4  5   6   7
2  8  9  10  11
#Drop columns
>>> df.drop(['B', 'C'], axis=1)
   A   D
0  0   3
1  4   7
2  8  11
>>> df.drop(columns=['B', 'C'])
   A   D
0  0   3
1  4   7
2  8  11
#Drop a row by index
>>> df.drop([0, 1])
   A  B   C   D
2  8  9  10  11


###Pandas - DF - stats 
DataFrame.abs() Return an object with absolute value taken–only applicable to objects that are all numeric. 
DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis 
DataFrame.any([axis, bool_only, skipna, level]) Return whether any element is True over requested axis 
DataFrame.clip([lower, upper, axis, inplace])   Trim values at input threshold(s). 
DataFrame.clip_lower(threshold[, axis, inplace]) Return copy of the input with values below given value(s) truncated. 
DataFrame.clip_upper(threshold[, axis, inplace]) Return copy of input with values above given value(s) truncated. 
DataFrame.corr([method, min_periods])           Compute pairwise correlation of columns, excluding NA/null values 
DataFrame.corrwith(other[, axis, drop])         Compute pairwise correlation between rows or columns of two DataFrame objects. 
DataFrame.count([axis, level, numeric_only])    Return Series with number of non-NA/null observations over requested axis. 
DataFrame.cov([min_periods])                    Compute pairwise covariance of columns, excluding NA/null values 
DataFrame.cummax([axis, skipna])                Return cumulative max over requested axis. 
DataFrame.cummin([axis, skipna])                Return cumulative minimum over requested axis. 
DataFrame.cumprod([axis, skipna])               Return cumulative product over requested axis. 
DataFrame.cumsum([axis, skipna])                Return cumulative sum over requested axis. 
DataFrame.describe([percentiles, include, ...]) Generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values. 
DataFrame.diff([periods, axis])                 1st discrete difference of object 
DataFrame.eval(expr[, inplace])                 Evaluate an expression in the context of the calling DataFrame instance. 
DataFrame.kurt([axis, skipna, level, ...])      Return unbiased kurtosis over requested axis using Fisher's definition of kurtosis (kurtosis of normal == 0.0). 
DataFrame.mad([axis, skipna, level])            Return the mean absolute deviation of the values for the requested axis 
DataFrame.max([axis, skipna, level, ...])       This method returns the maximum of the values in the object. 
DataFrame.mean([axis, skipna, level, ...])      Return the mean of the values for the requested axis 
DataFrame.median([axis, skipna, level, ...])    Return the median of the values for the requested axis 
DataFrame.min([axis, skipna, level, ...])       This method returns the minimum of the values in the object. 
DataFrame.mode([axis, numeric_only])            Gets the mode(s) of each element along the axis selected. 
DataFrame.pct_change([periods, fill_method, ...]) Percent change over given number of periods. 
DataFrame.prod([axis, skipna, level, ...])      Return the product of the values for the requested axis 
DataFrame.quantile([q, axis, numeric_only, ...]) Return values at the given quantile over requested axis, a la numpy.percentile. 
DataFrame.rank([axis, method, numeric_only, ...]) Compute numerical data ranks (1 through n) along axis. 
DataFrame.round([decimals])                     Round a DataFrame to a variable number of decimal places. 
DataFrame.sem([axis, skipna, level, ddof, ...]) Return unbiased standard error of the mean over requested axis. 
DataFrame.skew([axis, skipna, level, ...])      Return unbiased skew over requested axis 
DataFrame.sum([axis, skipna, level, ...])       Return the sum of the values for the requested axis 
DataFrame.std([axis, skipna, level, ddof, ...]) Return sample standard deviation over requested axis. 
DataFrame.var([axis, skipna, level, ddof, ...]) Return unbiased variance over requested axis. 



DataFrame.all(axis=None, bool_only=None, skipna=None, level=None, **kwargs)
    Return whether all elements are True over requested axis
    Parameters:
        axis : {index (0) ie columnwise, columns (1)ie rowise}
        default 0
    skipna : boolean, default True
        Exclude NA/null values. 
        If an entire row/column is NA, the result will be NA
    level : int or level name, default None
        If the axis is a MultiIndex (hierarchical), 
        count along a particular level, collapsing into a Series
    bool_only : boolean, default None
        Include only boolean columns. 
        If None, will attempt to use everything, then use only boolean data. Not implemented for Series.
#Example 
import pandas as pd
df = pd.DataFrame()
df['x'] = [1,2,3]
df['y'] = [3,4,5]
print (df['x'] < df['y']).all() # '<' returns series 
print (df['x'] < df['y']).any() #

>>> df.all()
x    True
y    True
dtype: bool
>>> df.all(axis=0)
x    True
y    True
dtype: bool
>>> df.all(axis=1)
0    True
1    True
2    True
dtype: bool

DataFrame.corr(method='pearson', min_periods=1)
    Compute pairwise correlation of columns, excluding NA/null values
    Parameters:
    method : {'pearson', 'kendall', 'spearman'}
        •pearson : standard correlation coefficient
        •kendall : Kendall Tau correlation coefficient
        •spearman : Spearman rank correlation
    min_periods : int, optional
        Minimum number of observations required per pair of columns 
        to have a valid result. 
        Currently only available for pearson and spearman correlation
        
#Example 
>>> df
   x  y
0  1  3
1  2  4
2  3  5
>>> df.corr()
     x    y
x  1.0  1.0
y  1.0  1.0        
        
 
DataFrame.cumsum(axis=None, skipna=True, *args, **kwargs)
    Return cumulative sum over requested axis.
    Parameters:
        axis : {index (0), columns (1)}
    skipna : boolean, default True
        Exclude NA/null values. If an entire row/column is NA, the result will be NA
#Example  
>>>df:
  fruit    val1 val2
0 orange    15    3
1 apple     10   13
2 mango     5    5 

df['cum_sum'] = df.val1.cumsum()
df['cum_perc'] = 100*df.cum_sum/df.val1.sum()
>>> df 
   fruit    val1 val2   cum_sum    cum_perc
0 orange    15    3    15          50.00
1 apple     10   13    25          83.33
2 mango     5    5     30          100.00


DataFrame.describe(percentiles=None, include=None, exclude=None)
    Parameters:
    percentiles : list-like of numbers, optional
        The percentiles to include in the output. 
        All should fall between 0 and 1. 
        The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles.
    include : 'all', list-like of dtypes or None (default), optional
        A white list of data types to include in the result. Ignored for Series. 
        Here are the options:
        •'all' : All columns of the input will be included in the output.
        •A list-like of dtypes : Limits the results to the provided data types. 
            To limit the result to numeric types submit numpy.number. 
            To limit it instead to object columns submit the numpy.object data type. Strings can also be used in the style of select_dtypes (e.g. df.describe(include=['O'])). To select pandas categorical columns, use 'category'
        •None (default) : The result will include all numeric columns.
    exclude : list-like of dtypes or None (default), optional,
        A black list of data types to omit from the result. Ignored for Series. 

#Describing a numeric Series.
>>> s = pd.Series([1, 2, 3])
>>> s.describe()
count    3.0
mean     2.0
std      1.0
min      1.0
25%      1.5
50%      2.0
75%      2.5
max      3.0

#Describing a categorical Series.
>>> s = pd.Series(['a', 'a', 'b', 'c'])
>>> s.describe()
count     4
unique    3
top       a
freq      2
dtype: object

#Describing a timestamp Series.
>>> s = pd.Series([
        np.datetime64("2000-01-01"),
        np.datetime64("2010-01-01"),
        np.datetime64("2010-01-01")
        ])
>>> s.describe()
count                       3
unique                      2
top       2010-01-01 00:00:00
freq                        2
first     2000-01-01 00:00:00
last      2010-01-01 00:00:00
dtype: object

#Describing a DataFrame. By default only numeric fields are returned.
>>> df = pd.DataFrame({ 'object': ['a', 'b', 'c'],
                        'numeric': [1, 2, 3],
                        'categorical': pd.Categorical(['d','e','f'])
                    })
>>> df.describe()
       numeric
count      3.0
mean       2.0
std        1.0
min        1.0
25%        1.5
50%        2.0
75%        2.5
max        3.0

#Describing all columns of a DataFrame regardless of data type.
>>> df.describe(include='all')
        categorical  numeric object
count            3      3.0      3
unique           3      NaN      3
top              f      NaN      c
freq             1      NaN      1
mean           NaN      2.0    NaN
std            NaN      1.0    NaN
min            NaN      1.0    NaN
25%            NaN      1.5    NaN
50%            NaN      2.0    NaN
75%            NaN      2.5    NaN
max            NaN      3.0    NaN

#Describing a column from a DataFrame by accessing it as an attribute.
>>> df.numeric.describe()
count    3.0
mean     2.0
std      1.0
min      1.0
25%      1.5
50%      2.0
75%      2.5
max      3.0
Name: numeric, dtype: float64

#Including only numeric columns in a DataFrame description.
>>> df.describe(include=[np.number])
       numeric
count      3.0
mean       2.0
std        1.0
min        1.0
25%        1.5
50%        2.0
75%        2.5
max        3.0

#Including only string columns in a DataFrame description.
>>> df.describe(include=[np.object])
       object
count       3
unique      3
top         c
freq        1

#Including only categorical columns from a DataFrame description.
>>> df.describe(include=['category'])
       categorical
count            3
unique           3
top              f
freq             1

#Excluding numeric columns from a DataFrame description.
>>> df.describe(exclude=[np.number])
       categorical object
count            3      3
unique           3      3
top              f      c
freq             1      1




DataFrame.diff(periods=1, axis=0)
    1st discrete difference of object
    Parameters:
    periods : int, default 1
        Periods to shift for forming difference
    axis : {0 or 'index', 1 or 'columns'}, default 0
        Take difference over rows (0) or columns (1).
 
#Example  
import pandas as pd
import io

temp="""Customer Id,Purchase Date
4543,1/1/2015
4543,2/5/2015
4543,3/15/2015
2322,1/1/2015
2322,3/1/2015
2322,2/1/2015"""

data = pd.read_csv(io.StringIO(temp), parse_dates=['Purchase Date'])
data.sort_values(by=['Customer Id', 'Purchase Date'], inplace=True)
data['Purchase Difference'] = data.groupby(['Customer Id'])['Purchase Date'].diff()
>>> data
   Customer Id Purchase Date  Purchase Difference
3         2322    2015-01-01                  NaT
5         2322    2015-02-01              31 days
4         2322    2015-03-01              28 days
0         4543    2015-01-01                  NaT
1         4543    2015-02-05              35 days
2         4543    2015-03-15              38 days



DataFrame.var(axis=None, skipna=None, level=None, 
            ddof=1, numeric_only=None, **kwargs)
    Return unbiased variance over requested axis.
    Normalized by N-1 by default. This can be changed using the ddof argument
    Parameters:
    axis : {index (0) ie columnwise , columns (1)}
    skipna : boolean, default True
        Exclude NA/null values. If an entire row/column is NA, the result will be NA
    level : int or level name, default None
        If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series
    ddof : int, default 1
        degrees of freedom
    numeric_only : boolean, default None
        Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.
     

###Pandas - apply,map, applymap , aggregate, pipe, transform
Series.map              Map values of Series using input correspondence 
Series.apply            For applying more complex functions on a Series
DataFrame.apply         Apply a function row-/column-wise
DataFrame.applymap      Apply a function elementwise on a whole DataFrame
DataFrame.aggregate     Aggregate using callable, string, dict, or list of string/callables

Series.map(arg, na_action=None)
    Map values of Series using input correspondence 
    (which can be a dict, Series, or function)
    Parameters:
        arg : function, dict, or Series
        na_action : {None, 'ignore'}
            If 'ignore', propagate NA values, without passing them to the mapping function
     
#Map inputs to outputs (both of type Series)
>>> x = pd.Series([1,2,3], index=['one', 'two', 'three'])
>>> x
one      1
two      2
three    3
dtype: int64
>>> y = pd.Series(['foo', 'bar', 'baz'], index=[1,2,3])
>>> y
1    foo
2    bar
3    baz
>>> x.map(y) #x value is mapped to corresponding y's index's value 
one   foo
two   bar
three baz

#If arg is a dictionary, return a new Series 
#with values converted according to the dictionary's mapping:
>>> z = {1: 'A', 2: 'B', 3: 'C'}
>>> x.map(z)
one   A
two   B
three C

#Use function to convert each value to a new value 
#Use na_action to control whether NA values are affected by the mapping function.
>>> s = pd.Series([1, 2, 3, np.nan])
>>> s2 = s.map('this is a string {}'.format, na_action=None)
0    this is a string 1.0
1    this is a string 2.0
2    this is a string 3.0
3    this is a string nan
dtype: object
>>> s3 = s.map('this is a string {}'.format, na_action='ignore')
0    this is a string 1.0
1    this is a string 2.0
2    this is a string 3.0
3                     NaN
dtype: object




Series.apply(func, convert_dtype=True, args=(), **kwds)
    Invoke function on values of Series. 
    Can be ufunc (a NumPy function that applies to the entire Series) 
    or a Python function that only works on single values
    Parameters:
        func : function
        convert_dtype : boolean, default True
            Try to find better dtype for elementwise function results.
            If False, leave as dtype=object
        args : tuple
            Positional arguments to pass to function in addition to the value
            Additional keyword arguments will be passed as keywords to the function
 
#Example 
>>> import pandas as pd
>>> import numpy as np
>>> series = pd.Series([20, 21, 12], index=['London','New York','Helsinki'])
>>> series
London      20
New York    21
Helsinki    12
dtype: int64

>>> def square(x):
        return x**2
>>> series.apply(square)
London      400
New York    441
Helsinki    144
dtype: int64

#Define a custom function that needs additional positional arguments 

>>> def subtract_custom_value(x, custom_value):
        return x-custom_value
>>> series.apply(subtract_custom_value, args=(5,))
London      15
New York    16
Helsinki     7
dtype: int64

#Define a custom function that takes keyword arguments 
>>> def add_custom_values(x, **kwargs):
        for month in kwargs:
            x+=kwargs[month]
            return x
>>> series.apply(add_custom_values, june=30, july=20, august=25)
London      95
New York    96
Helsinki    87
dtype: int64

#Use a function from the Numpy library.
>>> series.apply(np.log)
London      2.995732
New York    3.044522
Helsinki    2.484907
dtype: float64



DataFrame.apply(func, axis=0, broadcast=False, raw=False, 
            reduce=None, args=(), **kwds)
    Applies function along input axis of DataFrame.
    In the current implementation apply calls func twice on the first column/row 
    to decide whether it can take a fast or slow code path. 
    This can lead to unexpected behavior if func has side-effects, 
    as they will take effect twice for the first column/row.
    func : function
        Function to apply to each column/row
    axis : {0 or 'index', 1 or 'columns'}, default 0
        •0 or 'index': apply function to each column
        •1 or 'columns': apply function to each row
    broadcast : boolean, default False
        For aggregation functions, return object of same size with values propagated
    raw : boolean, default False
        If False, convert each row or column into a Series. 
        If raw=True the passed function will receive ndarray objects instead. 
        If you are just applying a NumPy reduction function this will achieve much better performance
    reduce : boolean or None, default None
        Try to apply reduction procedures. 
        If the DataFrame is empty, apply will use reduce to determine 
        whether the result should be a Series or a DataFrame. 
        If reduce is None (the default), apply's return value will be 
        guessed by calling func an empty Series 
        (note: while guessing, exceptions raised by func will be ignored). 
        If reduce is True a Series will always be returned, 
        and if False a DataFrame will always be returned.
    args : tuple
        Positional arguments to pass to function in addition to the array/series
    kwds
        Additional keyword arguments will be passed as keywords to the function


#Examples
#note numpy.sqrt/sum would get each column one at a time 
df = pd.DataFrame(np.random.randn(3, 3))
>>> df.apply(numpy.sqrt) # returns DataFrame after sqrt over each column
>>> df.apply(numpy.sum, axis=0) # equiv to df.sum(0), columnwise , returns series
>>> df.apply(numpy.sum, axis=1) # equiv to df.sum(1), rowwise 



DataFrame.applymap(func)
    Apply a function to a DataFrame that is intended to operate elementwise, 
    i.e. like doing map(func, series) for each series in the DataFrame
    Parameters:
    func : function
        Python function, returns a single value from a single value
 
>>> df = pd.DataFrame(np.random.randn(3, 3))
>>> df
    0         1          2
0  -0.029638  1.081563   1.280300
1   0.647747  0.831136  -1.549481
2   0.513416 -0.884417   0.195343
>>> df = df.applymap(lambda x: '%.2f' % x)
>>> df
    0         1          2
0  -0.03      1.08       1.28
1   0.65      0.83      -1.55
2   0.51     -0.88       0.20



DataFrame.aggregate(func, axis=0, *args, **kwargs)
    Aggregate using callable, string, dict, or list of string/callables
    agg is an alias for aggregate. Use the alias.
    Parameters:
    func : callable, string, dictionary, or list of string/callables
        Function to use for aggregating the data. 
        *args, **kwargs are passed to the function
        If a function, must either work when passed a DataFrame 
        or when passed to DataFrame.apply(ie can take series as input eg numpy.methods)
        For a DataFrame, can pass a dict, if the keys are DataFrame column names.
        Accepted Combinations are:
            •string of function name , stringified of GroupBy instance methods
             eg (sum,count,prod,mean,median,var,std,sem,kurt,skew,min,max)
            •function eg numpy.methods 
             Numpy functions mean/median/prod/sum/std/var are applied with axis=0(columnwise)
             instead of default numpy action ie flattening the array 
            •list of functions 
            •dict of column names -> functions (or list of functions) or strings of function names
        
 
#Example 
>>> df = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],
                index=pd.date_range('1/1/2000', periods=10))
>>> df.iloc[3:7] = np.nan
>>> df
                   A         B         C
2000-01-01  0.661627 -0.381725 -1.348288
2000-01-02 -2.017526  1.044400  0.305807
2000-01-03 -0.840310 -0.197202 -0.875485
2000-01-04       NaN       NaN       NaN
2000-01-05       NaN       NaN       NaN
2000-01-06       NaN       NaN       NaN
2000-01-07       NaN       NaN       NaN
2000-01-08 -1.747777 -0.684690  0.549227
2000-01-09 -0.436775 -0.223555  0.755120
2000-01-10 -0.161184 -0.113025 -1.131874

#Aggregate these functions across all columns
>>> df.agg(['sum', 'min'])  #axis=0, ie columnwise 
            A         B         C
sum -4.541946 -0.555796 -1.745493
min -2.017526 -0.684690 -1.348288

#Different aggregations per column
>>> df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})
            A        B
max       NaN  1.04440
min -2.017526 -0.68469
sum -4.541946      NaN

>>> df.agg([np.sum, np.min])
             A         B         C
sum  -4.541946 -0.555796 -1.745493
amin -2.017526 -0.684690 -1.348288



DataFrame.transform(func, *args, **kwargs)
    Call function producing a like-indexed NDFrame 
    and return a NDFrame with the transformed values
    While aggregation must return a reduced version of the data, 
    transformation can return some transformed version of the full data to recombine. 
    For such a transformation, the output is the same shape as the input
    Parameters:
    func : callable, string, dictionary, or list of string/callables
        To apply to column, function takes each column as input arg and returns transformed column 
        args, kwargs are passed to the function 
        Accepted Combinations are:
        •string function name (Dataframe function which takes a column and returns a column)
        •function
        •list of functions
        •dict of column names -> functions (or list of functions)
 
#Example 
>>> df = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],
                index=pd.date_range('1/1/2000', periods=10))
df.iloc[3:7] = np.nan
>>> df.transform(lambda x: (x - x.mean()) / x.std())
                   A         B         C
2000-01-01  0.579457  1.236184  0.123424
2000-01-02  0.370357 -0.605875 -1.231325
2000-01-03  1.455756 -0.277446  0.288967
2000-01-04       NaN       NaN       NaN
2000-01-05       NaN       NaN       NaN
2000-01-06       NaN       NaN       NaN
2000-01-07       NaN       NaN       NaN
2000-01-08 -0.498658  1.274522  1.642524
2000-01-09 -0.540524 -1.012676 -0.828968
2000-01-10 -1.366388 -0.614710  0.005378

>>> df.transform('cumsum')
                   A         B         C
2000-01-01  0.661627 -0.381725 -1.348288
2000-01-02 -1.355900  0.662675 -1.042481
2000-01-03 -2.196209  0.465473 -1.917966
2000-01-04       NaN       NaN       NaN
2000-01-05       NaN       NaN       NaN
2000-01-06       NaN       NaN       NaN
2000-01-07       NaN       NaN       NaN
2000-01-08 -3.943987 -0.219217 -1.368739
2000-01-09 -4.380762 -0.442771 -0.613619
2000-01-10 -4.541946 -0.555796 -1.745493



DataFrame.pipe(func, *args, **kwargs)
    Apply func(self, *args, **kwargs)
    Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects. 
    func : function
        function to apply to the NDFrame. 
        args, and kwargs are passed into func. 
        Alternatively a (callable, data_keyword) tuple 
        where data_keyword is a string indicating the keyword of callable 
        that expects the NDFrame.
    args : iterable, optional
        positional arguments passed into func.
    kwargs : mapping, optional
        a dictionary of keyword arguments passed into func.
 

#Example 
#Example 
#Instead of writing
>>> f(g(h(df), arg1=a), arg2=b, arg3=c)
#You can write
>>> (df.pipe(h)
        .pipe(g, arg1=a)
        .pipe(f, arg2=b, arg3=c)
    )
If you have a function that takes the data as (say) the second argument, 
pass a tuple indicating which keyword expects the data. 
For example, suppose f takes its data as arg2:
>>> (df.pipe(h)
        .pipe(g, arg1=a)
        .pipe((f, 'arg2'), arg1=a, arg3=c)
    )

    
    

###Pandas - GroupBy 
##Group-by - split-apply-combine
•Splitting the data into groups based on some criteria
•Applying a function to each group independently
    Aggregation
    Transformation
    Filtration
•Combining the results into a data structure


##Understanding key 
# default is axis=0,columnwise
>>> grouped = obj.groupby(key)
>>> grouped = obj.groupby(key, axis=1)
>>> grouped = obj.groupby([key1, key2])

#key can be 
    •A Python function, to be called on the axis labels(axis=0, with Index, axis=1, with Columns)
    •A list or NumPy array of the same length as the selected axis
    •A dict or Series, providing a label -> group name mapping
    •For DataFrame objects, a string indicating a column to be used to group. 
     df.groupby('A') is just syntactic sugar for df.groupby(df['A']), 
    •For DataFrame objects, a string indicating an index level to be used to group.
    •A list of any of the above things
    •A Grouper object 


    
DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, **kwargs)
    Returns DataFrameGroupBy or SeriesGroupBy
    When aggregation function is applied on returned type,
    final return would be DataFrame or Series respectively 
    Parameters:
    by : mapping, function, str, or iterable
        Used to determine the groups for the groupby. 
        If by is a function, it's called on each value of the object's index. 
        If a dict or Series is passed, 
        the Series or dict VALUES will be used to determine the groups 
        (the Series' values are first aligned; see .align() method). 
        If an ndarray is passed, the values are used as-is determine the groups. 
        A str or list of strs may be passed to group by the columns in self
    axis : int, default 0(index, columnwise)
    level : int, level name, or sequence of such, default None
        If the axis is a MultiIndex (hierarchical), group by a particular level or levels
    as_index : boolean, default True
        For aggregated output, return object with group labels as the index. 
        Only relevant for DataFrame input. as_index=False is effectively "SQL-style' grouped output
    sort : boolean, default True
        Sort group keys. Get better performance by turning this off. 
    group_keys : boolean, default True
        When calling apply, add group keys to index to identify pieces
    squeeze : boolean, default False
        reduce the dimensionality of the return type if possible, 
        otherwise return a consistent type
 
#Example 
#DataFrame results
>>> data.groupby(func, axis=0).mean()
>>> data.groupby(['col1', 'col2'])['col3'].mean() #
#Example 
df = pd.read_excel("python/code/examples/data/sales_transactions.xlsx")
>>> df.groupby('order').mean()
        account  quantity  unit price  ext price
order
10001  383080.0      7.00   30.266667   192.0400
10005  412290.0     31.60   53.264000  1637.0980
10006  218895.0     14.25   65.672500   931.1225
>>> df.groupby('order')["ext price"].mean() #returntype is Series
order
10001     192.0400
10005    1637.0980
10006     931.1225
Name: ext price, dtype: float64
>>> df.groupby('order')[["ext price"]].mean() #return type ise Dataframe 
       ext price
order
10001   192.0400
10005  1637.0980
10006   931.1225
>>> g =df.groupby('order')
>>> g.groups
{10001: Int64Index([0, 1, 2], dtype='int64'), 10005: Int64Index
 dtype='int64'), 10006: Int64Index([8, 9, 10, 11], dtype='int64
>>> g.indices
{10001: array([0, 1, 2], dtype=int64), 10005: array([3, 4, 5, 6
), 10006: array([ 8,  9, 10, 11], dtype=int64)}
>>> g.ngroup()  #row_index vs group_index 
0     0
1     0
2     0
3     1
4     1
5     1
6     1
7     1
8     2
9     2
10    2
11    2
dtype: int64
>>> g.size() #group size
order
10001    3
10005    5
10006    4
dtype: int64



#Q= "What percentage of the order total does each sku represent?"
#First Approach - Merging(joining like SQL JOIN)
#Series with index =order 
>>> df.groupby('order')["ext price"].sum().rename("Order_Total")
order
10001     576.12
10005    8185.49
10006    3724.49
Name: Order_Total, dtype: float64

>>> df.groupby('order')["ext price"].sum().rename("Order_Total").reset_index()
   order  Order_Total
0  10001       576.12
1  10005      8185.49
2  10006      3724.49

#how to combine this data back with the original dataframe. 
order_total = df.groupby('order')["ext price"].sum().rename("Order_Total").reset_index()
df_1 = df.merge(order_total, on='order')
>>> df_1.head(3)
   account      name  order       sku  quantity  unit price  ext price  \
0   383080  Will LLC  10001  B1-20000         7       33.69     235.83
1   383080  Will LLC  10001  S1-27722        11       21.12     232.32
2   383080  Will LLC  10001  B1-86481         3       35.99     107.97

   Order_Total
0       576.12
1       576.12
2       576.12
df_1["Percent_of_Order"] = df_1["ext price"] / df_1["Order_Total"]


#Second Approach - Using Transform
>>> df.groupby('order')["ext price"]
<pandas.core.groupby.SeriesGroupBy object at 0x00000093AC67B978>
>>> df.groupby('order')["ext price"].transform('sum')
0      576.12
1      576.12
2      576.12
3     8185.49
4     8185.49
5     8185.49
6     8185.49
7     8185.49
8     3724.49
9     3724.49
10    3724.49
11    3724.49
Name: ext price, dtype: float64

df["Order_Total"] = df.groupby('order')["ext price"].transform('sum')
df["Percent_of_Order"] = df["ext price"] / df["Order_Total"]



##GroupBy Objects -Indexing, iteration
GroupBy.__iter__()                       Groupby iterator 
GroupBy.groups                          dict {group name -> group labels} 
GroupBy.indices                         dict {group name -> group indices} 
GroupBy.get_group(name[, obj])          Constructs NDFrame from group with provided name 

##GroupBy Objects -Function application
GroupBy.apply(func, *args, **kwargs)        Apply function func group-wise and combine the results together. 
GroupBy.aggregate(func, *args, **kwargs)  
GroupBy.transform(func, *args, **kwargs)  
GroupBy.pipe(func, *args, **kwargs)         Apply a function with arguments to this GroupBy object, 

##GroupBy Objects -Computations / Descriptive Stats
GroupBy.count()                 Compute count of group, excluding missing values 
GroupBy.cumcount([ascending])   Number each item in each group from 0 to the length of that group - 1. 
GroupBy.first(**kwargs)         Compute first of group values 
GroupBy.head([n])               Returns first n rows of each group. 
GroupBy.last(**kwargs)          Compute last of group values 
GroupBy.max(**kwargs)           Compute max of group values 
GroupBy.mean(*args, **kwargs)   Compute mean of groups, excluding missing values 
GroupBy.median(**kwargs)        Compute median of groups, excluding missing values 
GroupBy.min(**kwargs)           Compute min of group values 

GroupBy.ngroup([ascending])     Number each group from 0 to the number of groups - 1. 
GroupBy.nth(n[, dropna])        Take the nth row from each group if n is an int, or a subset of rows if n is a list of ints. 
GroupBy.ohlc()                  Compute sum of values, excluding missing values 
GroupBy.prod(**kwargs)          Compute prod of group values 
GroupBy.size()                  Compute group sizes 
GroupBy.sem([ddof])             Compute standard error of the mean of groups, excluding missing values 
GroupBy.std([ddof])             Compute standard deviation of groups, excluding missing values 
GroupBy.sum(**kwargs)           Compute sum of group values 
GroupBy.var([ddof])             Compute variance of groups, excluding missing values 
GroupBy.tail([n])               Returns last n rows of each group 

#For both SeriesGroupBy and DataFrameGroupBy objects
#in DataFrameGroupBy version usually permits the specification of an axis argument, 
#and often an argument indicating whether to restrict application to columns of a specific data type.

DataFrameGroupBy.agg(arg, *args, **kwargs)  Aggregate using callable, string, dict, or list of string/callables 
DataFrameGroupBy.all                        Return whether all elements are True over requested axis 
DataFrameGroupBy.any                        Return whether any element is True over requested axis 
DataFrameGroupBy.bfill([limit])             Backward fill the values 
DataFrameGroupBy.corr                       Compute pairwise correlation of columns, excluding NA/null values 
DataFrameGroupBy.count()                    Compute count of group, excluding missing values 
DataFrameGroupBy.cov                        Compute pairwise covariance of columns, excluding NA/null values 
DataFrameGroupBy.cummax([axis])             Cumulative max for each group 
DataFrameGroupBy.cummin([axis])             Cumulative min for each group 
DataFrameGroupBy.cumprod([axis])            Cumulative product for each group 
DataFrameGroupBy.cumsum([axis])             Cumulative sum for each group 

DataFrameGroupBy.describe(**kwargs)         Generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values. 
DataFrameGroupBy.diff                       1st discrete difference of object 
DataFrameGroupBy.ffill([limit])             Forward fill the values 
DataFrameGroupBy.fillna                     Fill NA/NaN values using the specified method 
DataFrameGroupBy.filter(func[, dropna])     Return a copy of a DataFrame excluding elements from groups that do not satisfy the boolean criterion specified by func. 

DataFrameGroupBy.idxmax                     Return index of first occurrence of maximum over requested axis. 
DataFrameGroupBy.idxmin                     Return index of first occurrence of minimum over requested axis. 
DataFrameGroupBy.mad                        Return the mean absolute deviation of the values for the requested axis 
DataFrameGroupBy.pct_change                 Percent change over given number of periods. 

DataFrameGroupBy.plot                       Class implementing the .plot attribute for groupby objects 
DataFrameGroupBy.hist                       Draw histogram of the DataFrame's series using matplotlib / pylab. 

DataFrameGroupBy.quantile                   Return values at the given quantile over requested axis, a la numpy.percentile. 
DataFrameGroupBy.rank                       Compute numerical data ranks (1 through n) along axis. 
DataFrameGroupBy.resample(rule, *args, **kwargs) Provide resampling when using a TimeGrouper 

DataFrameGroupBy.shift([periods, freq, axis])   Shift each group by periods observations 
DataFrameGroupBy.size()                         Compute group sizes 
DataFrameGroupBy.skew                           Return unbiased skew over requested axis 
DataFrameGroupBy.take                           Return the elements in the given positional indices along an axis. 
DataFrameGroupBy.tshift                         Shift the time index, using the index's frequency if available. 

##for SeriesGroupBy objects.
SeriesGroupBy.nlargest          Return the largest n elements. 
SeriesGroupBy.nsmallest         Return the smallest n elements. 
SeriesGroupBy.nunique([dropna]) Returns number of unique elements in the group 
SeriesGroupBy.unique            Return unique values in the object. 
SeriesGroupBy.value_counts([normalize, ...])  

##for DataFrameGroupBy objects.
DataFrameGroupBy.corrwith                   Compute pairwise correlation between rows or columns of two DataFrame objects. 
DataFrameGroupBy.boxplot(grouped[, ...])    Make box plots from DataFrameGroupBy data. 


##String acceptable for apply 
_common_apply_whitelist = frozenset([
    'last', 'first',
    'head', 'tail', 'median',
    'mean', 'sum', 'min', 'max',
    'cumcount', 'ngroup',
    'resample',
    'rank', 'quantile',
    'fillna',
    'mad',
    'any', 'all',
    'take',
    'idxmax', 'idxmin',
    'shift', 'tshift',
    'ffill', 'bfill',
    'pct_change', 'skew',
    'corr', 'cov', 'diff',
]) | _plotting_methods

_series_apply_whitelist = ((_common_apply_whitelist |
                            {'nlargest', 'nsmallest'}) -
                           {'boxplot'}) | frozenset(['dtype', 'unique'])

_dataframe_apply_whitelist = ((_common_apply_whitelist |
                              frozenset(['dtypes', 'corrwith'])) -
                              {'boxplot'})

##String acceptable for transforms                       
_cython_transforms = frozenset(['cumprod', 'cumsum', 'shift',
                                'cummin', 'cummax'])
                                
##String acceptable for aggregate                          
'aggregate': {
            'add': 'group_add',
            'prod': 'group_prod',
            'min': 'group_min',
            'max': 'group_max',
            'mean': 'group_mean',
            'median': {
                'name': 'group_median'
            },
            'var': 'group_var',
            'first': {
                'name': 'group_nth',
                'f': lambda func, a, b, c, d, e: func(a, b, c, d, 1, -1)
            },
            'last': 'group_last',
            'ohlc': 'group_ohlc',
        },
        
        
        
        
DataFrameGroupBy.agg(arg, *args, **kwargs)
    Aggregate using callable, string, dict, or list of string/callables
    Check further description from DataFrame.aggregate
    Parameters:
    func : callable, string, dictionary, or list of string/callables
        Accepted Combinations are:
        •string function name
        •function
        •list of functions
        •dict of column names -> functions (or list of functions)
 
#Examples
>>> df = pd.DataFrame({'A': [1, 1, 2, 2],
                    'B': [1, 2, 3, 4],
                    'C': np.random.randn(4)})
>>> df
   A  B         C
0  1  1  0.362838
1  1  2  0.227877
2  2  3  1.267767
3  2  4 -0.562860

#The aggregation is for each column.
>>> df.groupby('A').agg('min')
   B         C
A
1  1  0.227877
2  3 -0.562860
#Multiple aggregations
>>> df.groupby('A').agg(['min', 'max'])
    B             C
  min max       min       max
A
1   1   2  0.227877  0.362838
2   3   4 -0.562860  1.267767

#Select a column for aggregation
>>> df.groupby('A').B.agg(['min', 'max'])
   min  max
A
1    1    2
2    3    4

#Different aggregations per column
>>> df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})
    B             C
  min max       sum
A
1   1   2  0.590716
2   3   4  0.704907







GroupBy.apply(func, *args, **kwargs)
    Apply function func group-wise and combine the results together.
    The function passed to apply must take a dataframe as its first argument 
    and return a dataframe, a series or a scalar. 
    apply will then take care of combining the results back together into a single dataframe or series. 
    apply is therefore a highly flexible grouping method.
    Parameters:
        func : function
            A callable that takes a dataframe as its first argument, 
            and returns a dataframe, a series or a scalar. 
            In addition the callable may take positional and keyword arguments
        args, kwargs : tuple and dict
            Optional positional and keyword arguments to pass to func
 

#In the current implementation apply calls func twice on the first group to decide 
#whether it can take a fast or slow code path. 
#Examples
>>> df = pd.DataFrame({'A': 'a a b'.split(), 'B': [1,2,3], 'C': [4,6, 5]})
>>> g = df.groupby('A')

#Example 1: below the function passed to apply takes a dataframe as its argument 
#and returns a dataframe. 
#apply combines the result for each group together into a new dataframe:
>>> g.apply(lambda x: x / x.sum())
          B    C
0  0.333333  0.4
1  0.666667  0.6
2  1.000000  1.0

#Example 2: The function passed to apply takes a dataframe as its argument 
#and returns a series. 
#apply combines the result for each group together into a new dataframe:
>>> g.apply(lambda x: x.max() - x.min())
   B  C
A
a  1  2
b  0  0

#Example 3: The function passed to apply takes a dataframe as its argument 
#and returns a scalar. 
#apply combines the result for each group together into a series, 
#including setting the index as appropriate:
>>> g.apply(lambda x: x.C.max() - x.B.min())
A
a    5
b    2
dtype: int64



GroupBy.pipe(func, *args, **kwargs)
    Apply a function with arguments to this GroupBy object,
    Parameters:
    func : callable or tuple of (callable, string)
        Function to apply to this GroupBy object or, 
        alternatively, a (callable, data_keyword) tuple 
        where data_keyword is a string indicating the keyword of callable that expects the GroupBy object.
    args : iterable, optional
        positional arguments passed into func.
    kwargs : dict, optional
        a dictionary of keyword arguments passed into func.
 
#Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects. 
#Instead of writing
>>> f(g(h(df.groupby('group')), arg1=a), arg2=b, arg3=c)
#You can write
>>> (df
        .groupby('group')
        .pipe(f, arg1)
        .pipe(g, arg2)
        .pipe(h, arg3))


        
class pandas.Grouper(key=None, level=None, freq=None, axis=0, sort=False)
    Attributes
        ax  
        groups 
    Parameters:
    key : string, defaults to None
        groupby key, which selects the grouping column of the target
    level : name/number, defaults to None
        the level for the target index
    freq : string / frequency object, defaults to None
        This will groupby the specified frequency if the target selection 
        (via key or level) is a datetime-like object. 
    axis : number/name of the axis, defaults to 0
    sort : boolean, default to False
        whether to sort the resulting labels
    additional kwargs to control time-like groupers (when freq is passed)
        closed : closed end of interval; 'left' or 'right'
        label : interval boundary to use for labeling; 'left' or 'right'
        convention : {'start', 'end', 'e', 's'}
    If grouper is PeriodIndex
        base, loffset
 
#Examples
#Syntactic sugar for df.groupby('A')
>>> df.groupby(Grouper(key='A'))

#Specify a resample operation on the column 'date'
>>> df.groupby(Grouper(key='date', freq='60s'))

#Specify a resample operation on the level 'date' on the columns axis 
#with a frequency of 60s
>>> df.groupby(Grouper(level='date', freq='60s', axis=1))        
        
   
##GroupBy example 
>>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
                              'foo', 'bar', 'foo', 'foo'],
                       'B' : ['one', 'one', 'two', 'three',
                              'two', 'two', 'one', 'three'],
                       'C' : np.random.randn(8),
                       'D' : np.random.randn(8)})
    
>>> df
     A      B         C         D
0  foo    one  0.469112 -0.861849
1  bar    one -0.282863 -2.104569
2  foo    two -1.509059 -0.494929
3  bar  three -1.135632  1.071804
4  foo    two  1.212112  0.721555
5  bar    two -0.173215 -0.706771
6  foo    one  0.119209 -1.039575
7  foo  three -1.044236  0.271860
#These will split the DataFrame on its index (rows)
>>> grouped = df.groupby('A')
>>> grouped.groups
{'bar': Int64Index([1, 3, 5], dtype='int64'), 'foo': Int64Index([0, 2, 4, 6, 7],
 dtype='int64')}
 
>>> grouped.get_group('bar')
     A      B         C         D
1  bar    one  0.166434  0.608616
3  bar  three -1.651876 -1.739062
5  bar    two -2.653192 -1.804768

>>> for n,g in grouped:
        print(n,type(g),"\n",g)
bar <class 'pandas.core.frame.DataFrame'>
      A      B         C         D
1  bar    one  0.166434  0.608616
3  bar  three -1.651876 -1.739062
5  bar    two -2.653192 -1.804768
foo <class 'pandas.core.frame.DataFrame'>
      A      B         C         D
0  foo    one -1.720274  0.271589
2  foo    two -0.243745  0.061812
4  foo    two -0.625297 -0.359143
6  foo    one -0.725909 -1.169616
7  foo  three  0.626842  0.258832

#Selecting one column of groupby objects. use only []
>>> grouped['C']
<pandas.core.groupby.SeriesGroupBy object at 0x00000093AC6A4828>
>>> grouped['C'].groups
{'bar': Int64Index([1, 3, 5], dtype='int64'), 'foo': Int64Index([0, 2, 4, 6, 7],
 dtype='int64')}
>>> grouped['C'].describe() #index is grouped_data
     count      mean       std       min       25%       50%       75%  \
A
bar    3.0 -1.379545  1.429404 -2.653192 -2.152534 -1.651876 -0.742721
foo    5.0 -0.537677  0.849139 -1.720274 -0.725909 -0.625297 -0.243745
          max
A
bar  0.166434
foo  0.626842
>>> grouped['C'].describe().reset_index()
     A  count      mean       std       min       25%       50%       75%  \
0  bar    3.0 -1.379545  1.429404 -2.653192 -2.152534 -1.651876 -0.742721
1  foo    5.0 -0.537677  0.849139 -1.720274 -0.725909 -0.625297 -0.243745
        max
0  0.166434
1  0.626842
#Note the difference 
>>> grouped['C'].mean()  #Series
A
bar   -1.379545
foo   -0.537677
Name: C, dtype: float64
>>> grouped[ ['C'] ].mean()  #DF
            C
A
bar -1.379545
foo -0.537677
>>> grouped[ ['C', 'D'] ].mean()
            C         D
A
bar -1.379545 -0.978404
foo -0.537677 -0.187305
>>> grouped = df.groupby(['A', 'B'])
>>> grouped.groups
{('bar', 'three'): Int64Index([3], dtype='int64'), ('bar', 'two'): Int64Index([5
], dtype='int64'), ('bar', 'one'): Int64Index([1], dtype='int64'), ('foo', 'thre
e'): Int64Index([7], dtype='int64'), ('foo', 'one'): Int64Index([0, 6], dtype='i
nt64'), ('foo', 'two'): Int64Index([2, 4], dtype='int64')}

#We could also split by the columns(axis=1)
>>> def get_letter_type(letter):
            if letter.lower() in 'aeiou':
                return 'vowel'
            else:
                return 'consonant'
        
>>> g = df.groupby(get_letter_type, axis=1) #fn is called with Column labels as axis=1 
>>> g.groups
{'consonant': Index(['B', 'C', 'D'], dtype='object'), 'vowel': Index(['A'], dtyp
e='object')}
>>> g.indices
{'consonant': array([1, 2, 3], dtype=int64), 'vowel': array([0], dtype=int64)}
#with level 
>>> lst = [1, 2, 3, 1, 2, 3]
>>> s = pd.Series([1, 2, 3, 10, 20, 30], lst)
>>> s
1     1
2     2
3     3
1    10
2    20
3    30
dtype: int64
>>> grouped = s.groupby(level=0)  #0th level 
>>> grouped.groups
{1: Int64Index([1, 1], dtype='int64'), 2: Int64Index([2, 2], dtype='int64'), 3:
Int64Index([3, 3], dtype='int64')}
>>> grouped.first()

1    1
2    2
3    3
dtype: int64
>>> grouped.last()

1    10
2    20
3    30
dtype: int64
>>> grouped.sum()

1    11
2    22
3    33
dtype: int64

#GroupBy with MultiIndex
>>> arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],
              ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]
          
>>> index = pd.MultiIndex.from_arrays(arrays, names=['first', 'second'])
>>> s = pd.Series(np.random.randn(8), index=index)
>>> s
first  second
bar    one      -0.919854
       two      -0.042379
baz    one       1.247642
       two      -0.009920
foo    one       0.290213
       two       0.495767
qux    one       0.362949
       two       1.548106
dtype: float64
#We can then group by one of the levels in s.
>>> grouped = s.groupby(level=0)
>>> grouped.sum()
first
bar   -0.962232
baz    1.237723
foo    0.785980
qux    1.911055
dtype: float64
#If the MultiIndex has names specified, these can be passed 
>>> s.groupby(level='second').sum()
second
one    0.980950
two    1.991575
dtype: float64
#OR can use sum() directly 
>>> s.sum(level='second')
second
one    0.980950
two    1.991575
dtype: float64
#Grouping with multiple levels is supported.
>>> s
first  second  third
bar    doo     one     -1.131345
               two     -0.089329
baz    bee     one      0.337863
               two     -0.945867
foo    bop     one     -0.932132
               two      1.956030
qux    bop     one      0.017587
               two     -0.016692
dtype: float64
>>> s.groupby(level=['first', 'second']).sum()
first  second
bar    doo      -1.220674
baz    bee      -0.608004
foo    bop       1.023898
qux    bop       0.000895
dtype: float64

#Index level names may be supplied as keys.
>>> s.groupby(['first', 'second']).sum()
first  second
bar    doo      -1.220674
baz    bee      -0.608004
foo    bop       1.023898
qux    bop       0.000895
dtype: float64

##Grouping DataFrame with Index Levels and Columns
#A DataFrame may be grouped by a combination of columns and index levels 
>>> arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],
                    ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]
          
>>> index = pd.MultiIndex.from_arrays(arrays, names=['first', 'second'])
>>> df = pd.DataFrame({'A': [1, 1, 1, 1, 2, 2, 3, 3],
                             'B': np.arange(8)},
                            index=index)
          
>>> df
              A  B
first second      
bar   one     1  0
      two     1  1
baz   one     1  2
      two     1  3
foo   one     2  4
      two     2  5
qux   one     3  6
      two     3  7
#The following example groups df by the second index level and the A column.
>>> df.groupby([pd.Grouper(level=1), 'A']).sum()
          B
second A   
one    1  2
       2  4
       3  6
two    1  4
       2  5
       3  7
#Index levels may also be specified by name.
>>> df.groupby([pd.Grouper(level='second'), 'A']).sum()
          B
second A   
one    1  2
       2  4
       3  6
two    1  4
       2  5
       3  7
#Index level names may be specified as keys directly to groupby.
>>> df.groupby(['second', 'A']).sum()
          B
second A   
one    1  2
       2  4
       3  6
two    1  4
       2  5
       3  7

##Aggregation
>>> grouped = df.groupby('A')
>>> grouped.aggregate(np.sum)
            C         D
A                      
bar  0.392940  1.732707
foo -1.796421  2.824590
>>> grouped = df.groupby(['A', 'B'])
>>> grouped.aggregate(np.sum)
                  C         D
A   B                        
bar one    0.254161  1.511763
    three  0.215897 -0.990582
    two   -0.077118  1.211526
foo one   -0.983776  1.614581
    three -0.862495  0.024580
    two    0.049851  1.185429
#Groups becomes MultiIndex/Index, or change with as_index=False or use reset_index()
>>> grouped = df.groupby(['A', 'B'], as_index=False)
>>> grouped.aggregate(np.sum)
     A      B         C         D
0  bar    one  0.254161  1.511763
1  bar  three  0.215897 -0.990582
2  bar    two -0.077118  1.211526
3  foo    one -0.983776  1.614581
4  foo  three -0.862495  0.024580
5  foo    two  0.049851  1.185429
>>> df.groupby('A', as_index=False).sum()
     A         C         D
0  bar  0.392940  1.732707
1  foo -1.796421  2.824590
#with reset_index
>>> df.groupby(['A', 'B']).sum().reset_index()
     A      B         C         D
0  bar    one  0.254161  1.511763
1  bar  three  0.215897 -0.990582
2  bar    two -0.077118  1.211526
3  foo    one -0.983776  1.614581
4  foo  three -0.862495  0.024580
5  foo    two  0.049851  1.185429
#to compute the size of each group. 
>>> grouped.size()#Series
A    B    
bar  one      1
     three    1
     two      1
foo  one      2
     three    1
     two      2
dtype: int64
#Applying multiple functions at once
>>> grouped = df.groupby('A')
>>> grouped['C'].agg([np.sum, np.mean, np.std])
          sum      mean       std
A                                
bar  0.392940  0.130980  0.181231
foo -1.796421 -0.359284  0.912265
#or for whole DF 
>>> grouped.agg([np.sum, np.mean, np.std])
            C                             D                    
          sum      mean       std       sum      mean       std
A                                                              
bar  0.392940  0.130980  0.181231  1.732707  0.577569  1.366330
foo -1.796421 -0.359284  0.912265  2.824590  0.564918  0.884785
#With renaming the result 
>>> (grouped['C'].agg([np.sum, np.mean, np.std])
                       .rename(columns={'sum': 'foo',
                                        'mean': 'bar',
                                        'std': 'baz'})
          )
          foo       bar       baz
A                                
bar  0.392940  0.130980  0.181231
foo -1.796421 -0.359284  0.912265
>>> (grouped.agg([np.sum, np.mean, np.std])
                  .rename(columns={'sum': 'foo',
                                   'mean': 'bar',
                                   'std': 'baz'})
           )
            C                             D                    
          foo       bar       baz       foo       bar       baz
A                                                              
bar  0.392940  0.130980  0.181231  1.732707  0.577569  1.366330
foo -1.796421 -0.359284  0.912265  2.824590  0.564918  0.884785

#Applying different functions to DataFrame columns
>>> grouped.agg({'C' : np.sum, 'D' : lambda x: np.std(x, ddof=1)})      
            C         D
A                      
bar  0.392940  1.366330
foo -1.796421  0.884785

#The function names can also be strings. 
#function name : it must be either implemented on GroupBy 
#or available via dispatching:
>>> grouped.agg({'C' : 'sum', 'D' : 'std'})
            C         D
A                      
bar  0.392940  1.366330
foo -1.796421  0.884785

##Cython-optimized aggregation functions
# sum, mean, std, and sem, have optimized Cython implementations:
>>> df.groupby('A').sum()
            C         D
A                      
bar  0.392940  1.732707
foo -1.796421  2.824590
>>> df.groupby(['A', 'B']).mean()
                  C         D
A   B                        
bar one    0.254161  1.511763
    three  0.215897 -0.990582
    two   -0.077118  1.211526
foo one   -0.491888  0.807291
    three -0.862495  0.024580
    two    0.024925  0.592714
    
##Dispatching to instance methods
#to call an instance method(of GroupBy objects) on each data group. 
>>> grouped = df.groupby('A')
>>> grouped.agg(lambda x: x.std())#x is each datagroup's column(eg C,D) as Series 
            C         D
A                      
bar  0.181231  1.366330
foo  0.912265  0.884785
#OR can be used directly 
>>> grouped.std()
            C         D
A                      
bar  0.181231  1.366330
foo  0.912265  0.884785
>>> tsdf = pd.DataFrame(np.random.randn(1000, 3),
                              index=pd.date_range('1/1/2000', periods=1000),
                              columns=['A', 'B', 'C'])
          
>>> tsdf.iloc[::2] = np.nan
>>> grouped = tsdf.groupby(lambda x: x.year) #axis=0, hence x=Index
>>> grouped.fillna(method='pad') #on each data group, fillna() is called
                   A         B         C
2000-01-01       NaN       NaN       NaN
2000-01-02 -0.353501 -0.080957 -0.876864
2000-01-03 -0.353501 -0.080957 -0.876864
2000-01-04  0.050976  0.044273 -0.559849
2000-01-05  0.050976  0.044273 -0.559849
2000-01-06  0.030091  0.186460 -0.680149
2000-01-07  0.030091  0.186460 -0.680149
...              ...       ...       ...
2002-09-20  2.310215  0.157482 -0.064476
2002-09-21  2.310215  0.157482 -0.064476
2002-09-22  0.005011  0.053897 -1.026922
2002-09-23  0.005011  0.053897 -1.026922
2002-09-24 -0.456542 -1.849051  1.559856
2002-09-25 -0.456542 -1.849051  1.559856
2002-09-26  1.123162  0.354660  1.128135
[1000 rows x 3 columns]

#The nlargest and nsmallest methods work on Series style groupbys:
>>> s = pd.Series([9, 8, 7, 5, 19, 1, 4.2, 3.3])
>>> g = pd.Series(list('abababab'))
>>> gb = s.groupby(g) #when another series is passed, g's value is mapped to s' value(same index joined) and then based on g's value, it is grouped by 
>>> gb.nlargest(3)
a  4    19.0
   0     9.0
   2     7.0
b  1     8.0
   3     5.0
   7     3.3
dtype: float64
>>> gb.nsmallest(3)

a  6    4.2
   2    7.0
   0    9.0
b  5    1.0
   7    3.3
   3    5.0
dtype: float64


##Transformation
#The transform method returns an object that is of same size as the one being grouped. 
#Note aggegration returns reduced size (mostly scalar)
#The transform function must:
    •Return a result that is either the same size as the group chunk 
     or broadcastable to the size of the group chunk 
     (e.g., a scalar, grouped.transform(lambda x: x.iloc[-1])).
    •Operate column-by-column on the group chunk. 
     The transform is applied to the first group chunk using chunk.apply.
    •Not perform in-place operations on the group chunk. 
     Group chunks should be treated as immutable, 
    •(Optionally) operates on the entire group chunk. 
     If this is supported, a fast path is used starting from the second chunk.

#to standardize the data within each group:
>>> index = pd.date_range('10/1/1999', periods=1100)
>>> ts = pd.Series(np.random.normal(0.5, 2, 1100), index)
>>> ts = ts.rolling(window=100,min_periods=100).mean().dropna()
>>> ts.head()
2000-01-08    0.779333
2000-01-09    0.778852
2000-01-10    0.786476
2000-01-11    0.782797
2000-01-12    0.798110
Freq: D, dtype: float64
>>> ts.tail()
2002-09-30    0.660294
2002-10-01    0.631095
2002-10-02    0.673601
2002-10-03    0.709213
2002-10-04    0.719369
Freq: D, dtype: float64
##* transforming with zscore 
>>> key = lambda x: x.year
>>> zscore = lambda x: (x - x.mean()) / x.std()
>>> transformed = ts.groupby(key).transform(zscore)

#We would expect the result to now have mean 0 and standard deviation 1 
#within each group, Check 
# Original Data
>>> grouped = ts.groupby(key)
>>> grouped.mean()

2000    0.442441
2001    0.526246
2002    0.459365
dtype: float64
>>> grouped.std()

2000    0.131752
2001    0.210945
2002    0.128753
dtype: float64
# Transformed Data
>>> grouped_trans = transformed.groupby(key)
>>> grouped_trans.mean()

2000    1.168208e-15
2001    1.454544e-15
2002    1.726657e-15
dtype: float64
>>> grouped_trans.std()

2000    1.0
2001    1.0
2002    1.0
dtype: float64

#plot the original and transformed data sets.
>>> compare = pd.DataFrame({'Original': ts, 'Transformed': transformed})
>>> compare.plot()
Out[90]: <matplotlib.axes._subplots.AxesSubplot at 0x1238b3898>

#Transformation functions that have lower dimension outputs 
#are broadcast to match the shape of the input array.
>>> data_range = lambda x: x.max() - x.min()
>>> ts.groupby(key).transform(data_range)
2000-01-08    0.623893
2000-01-09    0.623893
2000-01-10    0.623893
2000-01-11    0.623893
2000-01-12    0.623893
2000-01-13    0.623893
2000-01-14    0.623893
                ...   
2002-09-28    0.558275
2002-09-29    0.558275
2002-09-30    0.558275
2002-10-01    0.558275
2002-10-02    0.558275
2002-10-03    0.558275
2002-10-04    0.558275
Freq: D, Length: 1001, dtype: float64

#OR the built-in methods can be could be used to produce the same outputs
>>> ts.groupby(key).transform('max') - ts.groupby(key).transform('min')
2000-01-08    0.623893
2000-01-09    0.623893
2000-01-10    0.623893
2000-01-11    0.623893
2000-01-12    0.623893
2000-01-13    0.623893
2000-01-14    0.623893
                ...   
2002-09-28    0.558275
2002-09-29    0.558275
2002-09-30    0.558275
2002-10-01    0.558275
2002-10-02    0.558275
2002-10-03    0.558275
2002-10-04    0.558275
Freq: D, Length: 1001, dtype: float64

##**Another common data transform is to replace missing data with the group mean.
>>> data_df
            A         B         C
0    1.539708 -1.166480  0.533026
1    1.302092 -0.505754       NaN
2   -0.371983  1.104803 -0.651520
3   -1.309622  1.118697 -1.161657
4   -1.924296  0.396437  0.812436
5    0.815643  0.367816 -0.469478
6   -0.030651  1.376106 -0.645129
..        ...       ...       ...
993  0.012359  0.554602 -1.976159
994  0.042312 -1.628835  1.013822
995 -0.093110  0.683847 -0.774753
996 -0.185043  1.438572       NaN
997 -0.394469 -0.642343  0.011374
998 -1.174126  1.857148       NaN
999  0.234564  0.517098  0.393534
[1000 rows x 3 columns]

>>> countries = np.array(['US', 'UK', 'GR', 'JP'])
>>> key = countries[np.random.randint(0, 4, 1000)] #start,end, size required 
>>> grouped = data_df.groupby(key) #key is of same size data_df Index, hence data_df index is mapped to key and then grouped 
# Non-NA count in each group
>>> grouped.count()
      A    B    C
GR  209  217  189
JP  240  255  217
UK  216  231  193
US  239  250  217
>>> f = lambda x: x.fillna(x.mean()) #x= each grouped data Series 
>>> transformed = grouped.transform(f) 

#We can verify that the group means have not changed in the transformed data 
#and that the transformed data contains no NAs.
>>> grouped_trans = transformed.groupby(key)
>>> grouped.mean() # original group means
           A         B         C
GR -0.098371 -0.015420  0.068053
JP  0.069025  0.023100 -0.077324
UK  0.034069 -0.052580 -0.116525
US  0.058664 -0.020399  0.028603
>>> grouped_trans.mean() # transformation did not change group means
           A         B         C
GR -0.098371 -0.015420  0.068053
JP  0.069025  0.023100 -0.077324
UK  0.034069 -0.052580 -0.116525
US  0.058664 -0.020399  0.028603
>>> grouped.count() # original has some missing data points
      A    B    C
GR  209  217  189
JP  240  255  217
UK  216  231  193
US  239  250  217
>>> grouped_trans.count() # counts after transformation
      A    B    C
GR  228  228  228
JP  267  267  267
UK  247  247  247
US  258  258  258
>>> grouped_trans.size() # Verify non-NA count equals group size
GR    228
JP    267
UK    247
US    258
dtype: int64

#Some functions when applied to a groupby object will automatically transform 
#the input, returning an object of the same shape as the original. 
#Passing as_index=False will not affect these transformation methods.
#For example: fillna, ffill, bfill, shift.
>>> grouped.ffill()
            A         B         C
0    1.539708 -1.166480  0.533026
1    1.302092 -0.505754  0.533026
2   -0.371983  1.104803 -0.651520
3   -1.309622  1.118697 -1.161657
4   -1.924296  0.396437  0.812436
5    0.815643  0.367816 -0.469478
6   -0.030651  1.376106 -0.645129
..        ...       ...       ...
993  0.012359  0.554602 -1.976159
994  0.042312 -1.628835  1.013822
995 -0.093110  0.683847 -0.774753
996 -0.185043  1.438572 -0.774753
997 -0.394469 -0.642343  0.011374
998 -1.174126  1.857148 -0.774753
999  0.234564  0.517098  0.393534
[1000 rows x 3 columns]


##Filtration
#The filter method returns a subset of the original object. 

#The argument of filter must be a function that, 
#applied to the group as a whole, returns True or False.

#Suppose we want to take only elements that belong to groups 
#with a group sum greater than 2.
>>> sf = pd.Series([1, 1, 2, 3, 3, 3])
>>> sf.groupby(sf).filter(lambda x: x.sum() > 2) #x= each grouped data Series 
3    3
4    3
5    3
dtype: int64

#filtering out elements that belong to groups with only a couple members.
>>> dff = pd.DataFrame({'A': np.arange(8), 'B': list('aabbbbcc')})
>>> dff.groupby('B').filter(lambda x: len(x) > 2)
   A  B
2  2  b
3  3  b
4  4  b
5  5  b

#Alternatively, instead of dropping the offending groups, 
#we can return a like-indexed objects 
#where the groups that do not pass the filter are filled with NaNs.
>>> dff.groupby('B').filter(lambda x: len(x) > 2, dropna=False)
     A    B
0  NaN  NaN
1  NaN  NaN
2  2.0    b
3  3.0    b
4  4.0    b
5  5.0    b
6  NaN  NaN
7  NaN  NaN

#For DataFrames with multiple columns, 
#filters should explicitly specify a column as the filter criterion.
>>> dff['C'] = np.arange(8)
>>> dff.groupby('B').filter(lambda x: len(x['C']) > 2) #x is now DF as groupedby result chunk is DF 
   A  B  C
2  2  b  2
3  3  b  3
4  4  b  4
5  5  b  5

#Some functions when applied to a groupby object will act as a filter 
#on the input, returning a reduced shape of the original 
#but with the index unchanged. 
#Passing as_index=False will not affect these transformation methods.
#For example: head, tail.
>>> dff.groupby('B').head(2)
   A  B  C
0  0  a  0
1  1  a  1
2  2  b  2
3  3  b  3
6  6  c  6
7  7  c  7


##Flexible apply
>>> df
     A      B         C         D
0  foo    one -0.575247  1.346061
1  bar    one  0.254161  1.511763
2  foo    two -1.143704  1.627081
3  bar  three  0.215897 -0.990582
4  foo    two  1.193555 -0.441652
5  bar    two -0.077118  1.211526
6  foo    one -0.408530  0.268520
7  foo  three -0.862495  0.024580
>>> grouped = df.groupby('A')

# could also just call .describe()
>>> grouped['C'].apply(lambda x: x.describe()) 
A         
bar  count    3.000000
     mean     0.130980
     std      0.181231
     min     -0.077118
     25%      0.069390
     50%      0.215897
     75%      0.235029
                ...   
foo  mean    -0.359284
     std      0.912265
     min     -1.143704
     25%     -0.862495
     50%     -0.575247
     75%     -0.408530
     max      1.193555
Name: C, Length: 16, dtype: float64

#The dimension of the returned result can also change:
>>> grouped = df.groupby('A')['C']
>>> def f(group):
              return pd.DataFrame({'original' : group,
                                   'demeaned' : group - group.mean()})
          
>>> grouped.apply(f)
   demeaned  original
0 -0.215962 -0.575247
1  0.123181  0.254161
2 -0.784420 -1.143704
3  0.084917  0.215897
4  1.552839  1.193555
5 -0.208098 -0.077118
6 -0.049245 -0.408530
7 -0.503211 -0.862495

#apply on a Series can operate on a returned value from the applied function, 
#that is itself a series, and possibly upcast the result to a DataFrame
>>> def f(x):
        return pd.Series([ x, x**2 ], index = ['x', 'x^2'])
          
>>> s
0     9.0
1     8.0
2     7.0
3     5.0
4    19.0
5     1.0
6     4.2
7     3.3
dtype: float64
>>> s.apply(f)
      x     x^2
0   9.0   81.00
1   8.0   64.00
2   7.0   49.00
3   5.0   25.00
4  19.0  361.00
5   1.0    1.00
6   4.2   17.64
7   3.3   10.89


##Automatic exclusion of 'nuisance' columns
>>> df

     A      B         C         D
0  foo    one -0.575247  1.346061
1  bar    one  0.254161  1.511763
2  foo    two -1.143704  1.627081
3  bar  three  0.215897 -0.990582
4  foo    two  1.193555 -0.441652
5  bar    two -0.077118  1.211526
6  foo    one -0.408530  0.268520
7  foo  three -0.862495  0.024580

#Suppose we wish to compute the standard deviation grouped by the A column. 
#but we don't care about the data in column B. 
#We refer to this as a 'nuisance' column. and automatically removed 
>>> df.groupby('A').std()
            C         D
A                      
bar  0.181231  1.366330
foo  0.912265  0.884785

##NA and NaT group handling
#If there are any NaN or NaT values in the grouping key, 
#these will be automatically excluded. 
#So there will never be an 'NA group' or 'NaT group'. 

##Grouping with ordered factors(Categorical)

>>> data = pd.Series(np.random.randn(100))
>>> factor = pd.qcut(data, [0, .25, .5, .75, 1.])
>>> data.groupby(factor).mean()
(-2.618, -0.684]    -1.331461
(-0.684, -0.0232]   -0.272816
(-0.0232, 0.541]     0.263607
(0.541, 2.369]       1.166038
dtype: float64

##Grouping with a Grouper specification
>>> import datetime
>>> df = pd.DataFrame({
                   'Branch' : 'A A A A A A A B'.split(),
                   'Buyer': 'Carl Mark Carl Carl Joe Joe Joe Carl'.split(),
                   'Quantity': [1,3,5,1,8,1,9,3],
                   'Date' : [
                       datetime.datetime(2013,1,1,13,0),
                       datetime.datetime(2013,1,1,13,5),
                       datetime.datetime(2013,10,1,20,0),
                       datetime.datetime(2013,10,2,10,0),
                       datetime.datetime(2013,10,1,20,0),
                       datetime.datetime(2013,10,2,10,0),
                       datetime.datetime(2013,12,2,12,0),
                       datetime.datetime(2013,12,2,14,0),
                       ]
                   })
          
>>> df
  Branch Buyer                Date  Quantity
0      A  Carl 2013-01-01 13:00:00         1
1      A  Mark 2013-01-01 13:05:00         3
2      A  Carl 2013-10-01 20:00:00         5
3      A  Carl 2013-10-02 10:00:00         1
4      A   Joe 2013-10-01 20:00:00         8
5      A   Joe 2013-10-02 10:00:00         1
6      A   Joe 2013-12-02 12:00:00         9
7      B  Carl 2013-12-02 14:00:00         3

#Groupby a specific column with the desired frequency. This is like resampling.
>>> df.groupby([pd.Grouper(freq='1M',key='Date'),'Buyer']).sum()
                  Quantity
Date       Buyer          
2013-01-31 Carl          1
           Mark          3
2013-10-31 Carl          6
           Joe           9
2013-12-31 Carl          3
           Joe           9
           
>>> df = df.set_index('Date')
>>> df['Date'] = df.index + pd.offsets.MonthEnd(2)
>>> df.groupby([pd.Grouper(freq='6M',key='Date'),'Buyer']).sum()

                  Quantity
Date       Buyer          
2013-02-28 Carl          1
           Mark          3
2014-02-28 Carl          9
           Joe          18
>>> df.groupby([pd.Grouper(freq='6M',level='Date'),'Buyer']).sum()

                  Quantity
Date       Buyer          
2013-01-31 Carl          1
           Mark          3
2014-01-31 Carl          9
           Joe          18
           
           
##Taking the first rows of each group
#call head and tail on a groupby:
>>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])
>>> df
   A  B
0  1  2
1  1  4
2  5  6
>>> g = df.groupby('A')
>>> g.head(1)
   A  B
0  1  2
2  5  6
>>> g.tail(1)
   A  B
1  1  4
2  5  6

#Taking the nth row of each group
>>> df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])
>>> g = df.groupby('A')
>>> g.nth(0)
     B
A     
1  NaN
5  6.0
>>> g.nth(-1)
     B
A     
1  4.0
5  6.0
>>> g.nth(1)
     B
A     
1  4.0
#If you want to select the nth not-null item, use the dropna kwarg. 
#For a DataFrame this should be either 'any' or 'all' j
#like you would pass to dropna:
# nth(0) is the same as g.first()
>>> g.nth(0, dropna='any')
     B
A     
1  4.0
5  6.0
>>> g.first()
     B
A     
1  4.0
5  6.0
# nth(-1) is the same as g.last()
>>> g.nth(-1, dropna='any')  # NaNs denote group exhausted when using dropna
     B
A     
1  4.0
5  6.0
>>> g.last()
     B
A     
1  4.0
5  6.0
>>> g.B.nth(0, dropna='all')
A
1    4.0
5    6.0
Name: B, dtype: float64

#with as_index=False 
>>> df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])
>>> g = df.groupby('A',as_index=False)
>>> g.nth(0)

   A    B
0  1  NaN
2  5  6.0
>>> g.nth(-1)

   A    B
1  1  4.0
2  5  6.0

#select multiple rows from each group by specifying multiple nth values as a list of ints.
>>> business_dates = pd.date_range(start='4/1/2014', end='6/30/2014', freq='B')
>>> df = pd.DataFrame(1, index=business_dates, columns=['a', 'b'])
# get the first, 4th, and last date index for each month
>>> df.groupby((df.index.year, df.index.month)).nth([0, 3, -1])
        a  b
2014 4  1  1
     4  1  1
     4  1  1
     5  1  1
     5  1  1
     5  1  1
     6  1  1
     6  1  1
     6  1  1
     
##Enumerate group items
#To see the order in which each row appears within its group, 
#use the cumcount method:
>>> dfg = pd.DataFrame(list('aaabba'), columns=['A'])
>>> dfg
   A
0  a
1  a
2  a
3  b
4  b
5  a
>>> dfg.groupby('A').cumcount()
0    0
1    1
2    2
3    0
4    1
5    3
dtype: int64
>>> dfg.groupby('A').cumcount(ascending=False)
0    3
1    2
2    1
3    1
4    0
5    0
dtype: int64

##Enumerate groups
#To see the ordering of the groups 
#(as opposed to the order of rows within a group given by cumcount) 
#you can use the ngroup method.

>>> dfg = pd.DataFrame(list('aaabba'), columns=['A'])
>>> dfg
   A
0  a
1  a
2  a
3  b
4  b
5  a
>>> dfg.groupby('A').ngroup()
0    0
1    0
2    0
3    1
4    1
5    0
dtype: int64
>>> dfg.groupby('A').ngroup(ascending=False)
0    1
1    1
2    1
3    0
4    0
5    1
dtype: int64


##Plotting
>>> np.random.seed(1234)
>>> df = pd.DataFrame(np.random.randn(50, 2))
>>> df['g'] = np.random.choice(['A', 'B'], size=50)
>>> df.loc[df['g'] == 'B', 1] += 3
We can easily visualize this with a boxplot:
>>> df.groupby('g').boxplot()

A         AxesSubplot(0.1,0.15;0.363636x0.75)
B    AxesSubplot(0.536364,0.15;0.363636x0.75)
dtype: object


##Piping function calls
#result of earlier is passed to pipe's 1st arg 
>>> import numpy as np
>>> n = 1000
>>> df = pd.DataFrame({'Store': np.random.choice(['Store_1', 'Store_2'], n),
                             'Product': np.random.choice(['Product_1', 'Product_2', 'Product_3'], n),
                             'Revenue': (np.random.random(n)*50+10).round(2),
                             'Quantity': np.random.randint(1, 10, size=n)})
          
>>> df.head(2)
     Product  Quantity  Revenue    Store
0  Product_1         6    30.35  Store_2
1  Product_3         2    35.69  Store_2
Now, to find prices per store/product, we can simply do:
>>> (df.groupby(['Store', 'Product'])
             .pipe(lambda grp: grp.Revenue.sum()/grp.Quantity.sum())
             .unstack().round(2))  #(row->column)
Product  Product_1  Product_2  Product_3
Store                                   
Store_1       6.93       6.82       7.15
Store_2       6.69       6.64       6.77

##Regrouping by factor
#Regroup columns of a DataFrame according to their sum, 
#and sum the aggregated ones.
>>> df = pd.DataFrame({'a':[1,0,0], 'b':[0,1,0], 'c':[1,0,0], 'd':[2,3,4]})
>>> df
   a  b  c  d
0  1  0  1  2
1  0  1  0  3
2  0  0  0  4
>>> df.groupby(df.sum(), axis=1).sum()
   1  9
0  2  2
1  1  3
2  0  4


##Groupby by Indexer to 'resample' data
>>> df = pd.DataFrame(np.random.randn(10,2))
>>> df
          0         1
0  0.431670  0.882143
1 -0.026213 -1.941880
2 -1.106825 -0.667835
3  0.210712 -0.530195
4 -0.295191 -0.172722
5  0.638454  1.807622
6  1.008900  0.672822
7  0.770658  1.533002
8  0.576321 -0.819781
9 -1.302052  1.599477

>>> df.index // 5
Int64Index([0, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype='int64')

>>> df.groupby(df.index // 5).std()
          0         1
0  0.595843  1.015451
1  0.931952  1.084644

##Returning a Series to propagate names
#Group DataFrame columns, compute a set of metrics and return a named Series. 
#The Series name is used as the name for the column index. 
>>> df = pd.DataFrame({
                   'a':  [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],
                   'b':  [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],
                   'c':  [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],
                   'd':  [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],
                   })
          
>>> def compute_metrics(x):
              result = {'b_sum': x['b'].sum(), 'c_mean': x['c'].mean()}
              return pd.Series(result, name='metrics')
          
>>> result = df.groupby('a').apply(compute_metrics)
>>> result
metrics  b_sum  c_mean
a                     
0          2.0     0.5
1          2.0     0.5
2          2.0     0.5
>>> result.stack()#(column->row)
a  metrics
0  b_sum      2.0
   c_mean     0.5
1  b_sum      2.0
   c_mean     0.5
2  b_sum      2.0
   c_mean     0.5
dtype: float64  
        

        
###Pandas - DF - plotting 
#DataFrame.plot is both a callable method 
#and a namespace attribute for specific plotting methods 
#of the form DataFrame.plot.<kind>.

DataFrame.plot([x, y, kind, ax, ....]) 
DataFrame.plot.area([x, y])             Area plot 
DataFrame.plot.bar([x, y])              Vertical bar plot 
DataFrame.plot.barh([x, y])             Horizontal bar plot 
DataFrame.plot.box([by])                Boxplot 
DataFrame.plot.density(**kwds)          Kernel Density Estimate plot 
DataFrame.plot.hexbin(x, y[, C, ...])   Hexbin plot 
DataFrame.plot.hist([by, bins])         Histogram 
DataFrame.plot.kde(**kwds)              Kernel Density Estimate plot 
DataFrame.plot.line([x, y])             Line plot 
DataFrame.plot.pie([y])                 Pie chart 
DataFrame.plot.scatter(x, y[, s, c])    Scatter plot 
DataFrame.boxplot([column, by, ax, ...]) Make a box plot from DataFrame column optionally grouped by some columns or 
DataFrame.hist(data[, column, by, grid, ...]) Draw histogram of the DataFrame's series using matplotlib / pylab. 


DataFrame.plot(x=None, y=None, kind='line', ax=None, subplots=False, 
            sharex=None, sharey=False, layout=None, figsize=None, 
            use_index=True, title=None, grid=None, legend=True, 
            style=None, logx=False, logy=False, loglog=False, 
            xticks=None, yticks=None, xlim=None, ylim=None, 
            rot=None, fontsize=None, colormap=None, table=False, 
            yerr=None, xerr=None, secondary_y=False, sort_columns=False, **kwds)
Parameters:
    x : label or position, default None
    y : label or position, default None
        Allows plotting of one column versus another
    kind : str
        •'line' : line plot (default)
        •'bar' : vertical bar plot
        •'barh' : horizontal bar plot
        •'hist' : histogram
        •'box' : boxplot
        •'kde' : Kernel Density Estimation plot
        •'density' : same as 'kde'
        •'area' : area plot
        •'pie' : pie plot
        •'scatter' : scatter plot
        •'hexbin' : hexbin plot
    ax : matplotlib axes object, default None
    subplots : boolean, default False
        Make separate subplots for each column
    sharex : boolean, default True if ax is None else False
        In case subplots=True, share x axis and set some x axis labels to invisible; 
    sharey : boolean, default False
        In case subplots=True, share y axis and set some y axis labels to invisible
    layout : tuple (optional)
        (rows, columns) for the layout of subplots
    figsize : a tuple (width, height) in inches
    use_index : boolean, default True
        Use index as ticks for x axis
    title : string or list
        Title to use for the plot. 
        If a string is passed, print the string at the top of the figure. 
        If a list is passed and subplots is True, print each item in the list above the corresponding subplot.
    grid : boolean, default None (matlab style default)
        Axis grid lines
    legend : False/True/'reverse'
        Place legend on axis subplots
    style : list or dict
        matplotlib line style per column
    logx : boolean, default False
        Use log scaling on x axis
    logy : boolean, default False
        Use log scaling on y axis
    loglog : boolean, default False
        Use log scaling on both x and y axes
    xticks : sequence
        Values to use for the xticks
    yticks : sequence
        Values to use for the yticks
    xlim : 2-tuple/list
    ylim : 2-tuple/list
    rot : int, default None
        Rotation for ticks (xticks for vertical, yticks for horizontal plots)
    fontsize : int, default None
        Font size for xticks and yticks
    colormap : str or matplotlib colormap object, default None
        Colormap to select colors from. 
        If string, load colormap with that name from matplotlib.
    colorbar : boolean, optional
        If True, plot colorbar (only relevant for 'scatter' and 'hexbin' plots)
    position : float
        Specify relative alignments for bar plot layout. 
        From 0 (left/bottom-end) to 1 (right/top-end). 
        Default is 0.5 (center)
    table : boolean, Series or DataFrame, default False
        If True, draw a table using the data in the DataFrame 
        and the data will be transposed to meet matplotlib's default layout. 
        If a Series or DataFrame is passed, use passed data to draw a table.
    yerr : DataFrame, Series, array-like, dict and str
    xerr : same types as yerr.
    stacked : boolean, default False in line and
        bar plots, and True in area plot. If True, create stacked plot.
    sort_columns : boolean, default False
        Sort column names to determine plot ordering
    secondary_y : boolean or sequence, default False
        Whether to plot on the secondary y-axis 
        If a list/tuple, which columns to plot on secondary y-axis
    mark_right : boolean, default True
        When using a secondary_y axis, automatically mark the column labels with '(right)' in the legend
    kwds : keywords
        Options to pass to matplotlib plotting method
        
        
#Example 
import matplotlib.pyplot as plt

#Basic Plotting: plot
#If the index consists of dates, it calls gcf().autofmt_xdate() to try to format the x-axis 
>>> ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))
>>> ts = ts.cumsum()
>>> ts.plot()

#to plot all of the columns with labels:
>>> df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list('ABCD'))
>>> df = df.cumsum()
>>> plt.figure(); df.plot();

#plot one column versus another using the x and y keywords in plot():
>>> df3 = pd.DataFrame(np.random.randn(1000, 2), columns=['B', 'C']).cumsum()
>>> df3['A'] = pd.Series(list(range(len(df))))
>>> df3.plot(x='A', y='B')

##Bar plots
For labeled, non-time series data, you may wish to produce a bar plot:
>>> plt.figure();
>>> df.iloc[5].plot.bar(); plt.axhline(0, color='k')

#Calling a DataFrame's plot.bar() method produces a multiple bar plot:
>>> df2 = pd.DataFrame(np.random.rand(10, 4), columns=['a', 'b', 'c', 'd'])
>>> df2.plot.bar();

#To produce a stacked bar plot, pass stacked=True:
>>> df2.plot.bar(stacked=True);

#To get horizontal bar plots, use the barh method:
>>> df2.plot.barh(stacked=True);

##Histograms
>>> df4 = pd.DataFrame({'a': np.random.randn(1000) + 1, 'b': np.random.randn(1000),
                        'c': np.random.randn(1000) - 1}, columns=['a', 'b', 'c'])
 
>>> plt.figure();
>>> df4.plot.hist(alpha=0.5)

#Histogram can be stacked by stacked=True. 
#Bin size can be changed by bins keyword.
>>> plt.figure();
>>> df4.plot.hist(stacked=True, bins=20)

#You can pass other keywords supported by matplotlib hist. 
#For example, horizontal and cumulative histogram can be drawn by 
#orientation='horizontal' and cumulative=True.
>>> plt.figure();
>>> df4['a'].plot.hist(orientation='horizontal', cumulative=True)

#The existing interface DataFrame.hist to plot histogram still can be used.
>>> plt.figure();
>>> df['A'].diff().hist()

#DataFrame.hist() plots the histograms of the columns on multiple subplots:
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> df.diff().hist(color='k', alpha=0.5, bins=50) #diff(axis=0) rowwise by default
array([[<matplotlib.axes._subplots.AxesSubplot object at 0x123e35e80>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x10191beb8>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x122cbeb00>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x12831d2e8>]], dtype=object)

#The by keyword can be specified to plot grouped histograms:
>>> data = pd.Series(np.random.randn(1000))
>>> data.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4))
array([[<matplotlib.axes._subplots.AxesSubplot object at 0x127d29f28>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x128ac4e10>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x128a30ac8>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x11cda6438>]], dtype=object)

        
##Box Plots
>>> df = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E'])
>>> df.plot.box()

#Boxplot can be colorized by passing color keyword. 
#You can pass a dict whose keys are boxes, whiskers, medians and caps. 
#If some keys are missing in the dict, default colors are used for the corresponding artists. 
#Also, boxplot has sym keyword to specify fliers style.
#When you pass other type of arguments via color keyword, 
#it will be directly passed to matplotlib for all the boxes, whiskers, medians and caps colorization.

The colors are applied to every boxes to be drawn. 
If you want more complicated colorization, 
you can get each drawn artists by passing return_type.
>>> color = dict(boxes='DarkGreen', whiskers='DarkOrange',
                 medians='DarkBlue', caps='Gray')
    
>>> df.plot.box(color=color, sym='r+')

#Also, you can pass other keywords supported by matplotlib boxplot. 
#For example, horizontal and custom-positioned boxplot can be drawn by vert=False and positions keywords.
>>> df.plot.box(vert=False, positions=[1, 4, 5, 6, 8])

#or use DataFrame.boxplot to plot boxplot 
>>> df = pd.DataFrame(np.random.rand(10,5))
>>> plt.figure();
>>> bp = df.boxplot()

#You can create a stratified boxplot using the by keyword argument to create groupings
>>> df = pd.DataFrame(np.random.rand(10,2), columns=['Col1', 'Col2'] )
>>> df['X'] = pd.Series(['A','A','A','A','A','B','B','B','B','B'])
>>> plt.figure();
>>> bp = df.boxplot(by='X')

#You can also pass a subset of columns to plot, as well as group by multiple columns:
>>> df = pd.DataFrame(np.random.rand(10,3), columns=['Col1', 'Col2', 'Col3'])
>>> df['X'] = pd.Series(['A','A','A','A','A','B','B','B','B','B'])
>>> df['Y'] = pd.Series(['A','B','A','B','A','B','A','B','A','B'])
>>> plt.figure();
>>> bp = df.boxplot(column=['Col1','Col2'], by=['X','Y'])

 
#In boxplot, the return type can be controlled by the return_type, keyword. 
#The valid choices are {"axes", "dict", "both", None}. 
#Faceting, created by DataFrame.boxplot with the by keyword, 
#will affect the output type as well:
#return_type=    Faceted         Output type 
None            No              axes 
None            Yes             2-D ndarray of axes 
'axes'          No              axes 
'axes'          Yes             Series of axes 
'dict'          No              dict of artists 
'dict'          Yes             Series of dicts of artists 
'both'          No              namedtuple 
'both'          Yes             Series of namedtuples 

#Groupby.boxplot always returns a Series of return_type.
>>> np.random.seed(1234)
>>> df_box = pd.DataFrame(np.random.randn(50, 2))
>>> df_box['g'] = np.random.choice(['A', 'B'], size=50)
>>> df_box.loc[df_box['g'] == 'B', 1] += 3
>>> bp = df_box.boxplot(by='g')

#Compare to:
>>> bp = df_box.groupby('g').boxplot()



##Area Plot
#When input data contains NaN, it will be automatically filled by 0. 
>>> df = pd.DataFrame(np.random.rand(10, 4), columns=['a', 'b', 'c', 'd'])
>>> df.plot.area();

#To produce an unstacked plot, pass stacked=False.
#Alpha value is set to 0.5 unless otherwise specified:
>>> df.plot.area(stacked=False);

##Scatter Plot
>>> df = pd.DataFrame(np.random.rand(50, 4), columns=['a', 'b', 'c', 'd'])
>>> df.plot.scatter(x='a', y='b');

#To plot multiple column groups in a single axes, repeat plot method specifying target ax. 
>>> ax = df.plot.scatter(x='a', y='b', color='DarkBlue', label='Group 1');
>>> df.plot.scatter(x='c', y='d', color='DarkGreen', label='Group 2', ax=ax);

#The keyword c may be given as the name of a column to provide colors for each point:
>>> df.plot.scatter(x='a', y='b', c='c', s=50);

#You can pass other keywords supported by matplotlib scatter. 
#Below example shows a bubble chart using a dataframe column values as bubble size.
>>> df.plot.scatter(x='a', y='b', s=df['c']*200);


##Hexagonal Bin Plot
#Hexbin plots can be a useful alternative to scatter plots if your data are too dense to plot each point individually.
>>> df = pd.DataFrame(np.random.randn(1000, 2), columns=['a', 'b'])
>>> df['b'] = df['b'] + np.arange(1000)
>>> df.plot.hexbin(x='a', y='b', gridsize=25)


#A useful keyword argument is gridsize; 
#it controls the number of hexagons in the x-direction, and defaults to 100. 
#A larger gridsize means more, smaller bins.

#By default, a histogram of the counts around each (x, y) point is computed. 
#You can specify alternative aggregations by passing values to the C and reduce_C_function arguments. 
#C specifies the value at each (x, y) point 
#and reduce_C_function is a function of one argument 
#that reduces all the values in a bin to a single number (e.g. mean, max, sum, std). 

#In this example the positions are given by columns a and b, 
#while the value is given by column z. 
#The bins are aggregated with numpy's max function.
>>> df = pd.DataFrame(np.random.randn(1000, 2), columns=['a', 'b'])
>>> df['b'] = df['b'] = df['b'] + np.arange(1000)
>>> df['z'] = np.random.uniform(0, 3, 1000)
>>> df.plot.hexbin(x='a', y='b', C='z', reduce_C_function=np.max,
                gridsize=25)
        



##Pie plot
#If your data includes any NaN, they will be automatically filled with 0. 
>>> series = pd.Series(3 * np.random.rand(4), index=['a', 'b', 'c', 'd'], name='series')
>>> series.plot.pie(figsize=(6, 6))
<matplotlib.axes._subplots.AxesSubplot at 0x12136d668>

#For pie plots it's best to use square figures, one's with an equal aspect ratio. 
#You can create the figure with equal width and height, 
#or force the aspect ratio to be equal after plotting by calling 
#ax.set_aspect('equal') 
#on the returned axes object.

#Note that pie plot with DataFrame requires that you either 
#specify a target column by the y argument or subplots=True. 
#When y is specified, pie plot of selected column will be drawn. 
#If subplots=True is specified, pie plots for each column are drawn as subplots. 
#A legend will be drawn in each pie plots by default; specify legend=False to hide it.
>>> df = pd.DataFrame(3 * np.random.rand(4, 2), index=['a', 'b', 'c', 'd'], columns=['x', 'y'])
>>> df.plot.pie(subplots=True, figsize=(8, 4))
array([<matplotlib.axes._subplots.AxesSubplot object at 0x128ac7a58>,
       <matplotlib.axes._subplots.AxesSubplot object at 0x124d78710>], dtype=object)

#You can use the labels and colors keywords to specify 
#the labels and colors of each wedge.

#If you want to hide wedge labels, specify labels=None. 
#If fontsize is specified, the value will be applied to wedge labels. 
#Also, other keywords supported by matplotlib.pyplot.pie() can be used.
>>> series.plot.pie(labels=['AA', 'BB', 'CC', 'DD'], colors=['r', 'g', 'b', 'c'],
                    autopct='%.2f', fontsize=20, figsize=(6, 6))
    
<matplotlib.axes._subplots.AxesSubplot at 0x1205d4320>

#If you pass values whose sum total is less than 1.0, matplotlib draws a semicircle.
>>> series = pd.Series([0.1] * 4, index=['a', 'b', 'c', 'd'], name='series2')
>>> series.plot.pie(figsize=(6, 6))



##Plotting with Missing Data
#or use .dropna()/.fillna() methods before plotting 
Plot Type       Default NaN Handling
Line            Leave gaps at NaNs 
Line (stacked)  Fill 0's 
Bar             Fill 0's 
Scatter         Drop NaNs 
Histogram       Drop NaNs (column-wise) 
Box             Drop NaNs (column-wise) 
Area            Fill 0's 
KDE             Drop NaNs (column-wise) 
Hexbin          Drop NaNs 
Pie             Fill 0's 


##Scatter Matrix Plot
>>> from pandas.plotting import scatter_matrix
>>> df = pd.DataFrame(np.random.randn(1000, 4), columns=['a', 'b', 'c', 'd'])
>>> scatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal='kde')
array([[<matplotlib.axes._subplots.AxesSubplot object at 0x11ff0a1d0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x1271cb5f8>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x127930668>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x120716668>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x128781e10>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x128781e48>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x11bb81f60>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x11fed5080>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x11fe885c0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x124d05cc0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x127812630>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x11fe446a0>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x12027c6a0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x120e640f0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x121472160>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x124524160>]], dtype=object)

##Density Plot
#Use Series.plot.kde() and DataFrame.plot.kde() methods.
>>> ser = pd.Series(np.random.randn(1000))
>>> ser.plot.kde()
<matplotlib.axes._subplots.AxesSubplot at 0x128c3a668>

##Andrews Curves
#Andrews curves allow one to plot multivariate data as a large number of curves 
#that are created using the attributes of samples as coefficients for Fourier series.
#By coloring these curves differently for each class it is possible to visualize data clustering. 

#Curves belonging to samples of the same class will usually be closer together 
#and form larger structures.

>>> from pandas.plotting import andrews_curves
>>> data = pd.read_csv('data/iris.data')
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> andrews_curves(data, 'Name')
<matplotlib.axes._subplots.AxesSubplot at 0x121b66be0>

##Parallel Coordinates
#Parallel coordinates is a plotting technique for plotting multivariate data. 
#It allows one to see clusters in data and to estimate other statistics visually. 
#Using parallel coordinates points are represented as connected line segments. 
#Each vertical line represents one attribute. 
#One set of connected line segments represents one data point. 
#Points that tend to cluster will appear closer together.
>>> from pandas.plotting import parallel_coordinates
>>> data = pd.read_csv('data/iris.data')
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> parallel_coordinates(data, 'Name')
<matplotlib.axes._subplots.AxesSubplot at 0x1278f6320>

##Lag Plot
#Lag plots are used to check if a data set or time series is random. 
#Random data should not exhibit any structure in the lag plot. 
#Non-random structure implies that the underlying data are not random.
>>> from pandas.plotting import lag_plot
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> data = pd.Series(0.1 * np.random.rand(1000) +
        0.9 * np.sin(np.linspace(-99 * np.pi, 99 * np.pi, num=1000)))
   .
>>> lag_plot(data)
<matplotlib.axes._subplots.AxesSubplot at 0x120425cf8>

##Autocorrelation Plot
#Autocorrelation plots are often used for checking randomness in time series. 
#This is done by computing autocorrelations for data values at varying time lags. 
#If time series is random, such autocorrelations should be near zero for any 
#and all time-lag separations. 
#If time series is non-random then one or more of the autocorrelations will be significantly non-zero. 
#The horizontal lines displayed in the plot correspond to 95% and 99% confidence bands. 
#The dashed line is 99% confidence band.
>>> from pandas.plotting import autocorrelation_plot
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> data = pd.Series(0.7 * np.random.rand(1000) +
    0.3 * np.sin(np.linspace(-9 * np.pi, 9 * np.pi, num=1000)))
   
>>> autocorrelation_plot(data)
<matplotlib.axes._subplots.AxesSubplot at 0x11fd94240>

##Bootstrap Plot
#Bootstrap plots are used to visually assess the uncertainty of a statistic, 
#such as mean, median, midrange, etc. 
#A random subset of a specified size is selected from a data set, 
#the statistic in question is computed for this subset 
#and the process is repeated a specified number of times. 
#Resulting plots and histograms are what constitutes the bootstrap plot.
>>> from pandas.plotting import bootstrap_plot
>>> data = pd.Series(np.random.rand(1000))
>>> bootstrap_plot(data, size=50, samples=500, color='grey')
<Figure size 640x480 with 6 Axes>

##RadViz
#RadViz is a way of visualizing multi-variate data. 
#It is based on a simple spring tension minimization algorithm. 
#Basically you set up a bunch of points in a plane. 
#In our case they are equally spaced on a unit circle. 
#Each point represents a single attribute. 
#You then pretend that each sample in the data set is attached to each of these points by a spring, 
#the stiffness of which is proportional to the numerical value of that attribute 
#(they are normalized to unit interval). 
#The point in the plane, where our sample settles to 
#(where the forces acting on our sample are at an equilibrium) 
#is where a dot representing our sample will be drawn. 
#Depending on which class that sample belongs it will be colored differently.

>>> from pandas.plotting import radviz
>>> data = pd.read_csv('data/iris.data')
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> radviz(data, 'Name')
<matplotlib.axes._subplots.AxesSubplot at 0x11ff13b38>


##Plot Formatting - General plot style arguments
#Most plotting methods have a set of keyword arguments 
#that control the layout and formatting of the returned plot:
>>> plt.figure(); ts.plot(style='k--', label='Series');

##Controlling the Legend
>>> df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list('ABCD'))
>>> df = df.cumsum()
>>> df.plot(legend=False)
<matplotlib.axes._subplots.AxesSubplot at 0x11ffe1f98>

##Scales
>>> ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))
>>> ts = np.exp(ts.cumsum())
>>> ts.plot(logy=True)
<matplotlib.axes._subplots.AxesSubplot at 0x1023e6a20>

##Plotting on a Secondary Y-axis
##To plot data on a secondary y-axis, use the secondary_y keyword:
>>> df.A.plot()
<matplotlib.axes._subplots.AxesSubplot at 0x1282add68>
>>> df.B.plot(secondary_y=True, style='g')
<matplotlib.axes._subplots.AxesSubplot at 0x12775eb00>

#To plot some columns in a DataFrame, 
#give the column names to the secondary_y keyword:
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> ax = df.plot(secondary_y=['A', 'B'])
>>> ax.set_ylabel('CD scale')
Text(0,0.5,'CD scale')
>>> ax.right_ax.set_ylabel('AB scale')
Text(0,0.5,'AB scale')

#Note that the columns plotted on the secondary y-axis 
#is automatically marked with '(right)' in the legend. 
#To turn off the automatic marking, use the mark_right=False keyword:
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> df.plot(secondary_y=['A', 'B'], mark_right=False)
<matplotlib.axes._subplots.AxesSubplot at 0x121b2cc50>

##Suppressing Tick Resolution Adjustment
#pandas includes automatic tick resolution adjustment for regular frequency time-series data. 
#For limited cases where pandas cannot infer the frequency information (e.g., in an externally created twinx), 
#you can choose to suppress this behavior for alignment purposes.
#Here is the default behavior, notice how the x-axis tick labelling is performed:
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> df.A.plot()
<matplotlib.axes._subplots.AxesSubplot at 0x121aa4908>

#Using the x_compat parameter, you can suppress this behavior:
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> df.A.plot(x_compat=True)
<matplotlib.axes._subplots.AxesSubplot at 0x128199550>

#If you have more than one plot that needs to be suppressed, 
#the use method in pandas.plotting.plot_params can be used in a with statement:
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> with pd.plotting.plot_params.use('x_compat', True):
        df.A.plot(color='r')
        df.B.plot(color='g')
        df.C.plot(color='b')
    

##Automatic Date Tick Adjustment
#TimedeltaIndex now uses the native matplotlib tick locator methods, 
#it is useful to call the automatic date tick adjustment from matplotlib 
#for figures whose ticklabels overlap.
#See the autofmt_xdate method and the matplotlib documentation for more.


##Subplots
#Each Series in a DataFrame can be plotted on a different axis with the subplots keyword:
>>> df.plot(subplots=True, figsize=(6, 6));

##Using Layout and Targeting Multiple Axes
#The layout of subplots can be specified by layout keyword. 
#It can accept (rows, columns). 
#The layout keyword can be used in hist and boxplot also. 
#If input is invalid, ValueError will be raised.
#The number of axes which can be contained by rows x columns specified by layout 
#must be larger than the number of required subplots. 
#If layout can contain more axes than required, blank axes are not drawn. 

#you can use -1 for one dimension to automatically calculate the number of rows or columns needed, 
#given the other.
>>> df.plot(subplots=True, layout=(2, 3), figsize=(6, 6), sharex=False);

#The above example is identical to using
>>> df.plot(subplots=True, layout=(2, -1), figsize=(6, 6), sharex=False);

#you can pass multiple axes created beforehand as list-like via ax keyword. 
#This allows to use more complicated layout. 
#The passed axes must be the same number as the subplots being drawn.
#When multiple axes are passed via ax keyword, 
#layout, sharex and sharey keywords don't affect to the output. 
#You should explicitly pass sharex=False and sharey=False, 
#otherwise you will see a warning.
>>> fig, axes = plt.subplots(4, 4, figsize=(6, 6));
>>> plt.subplots_adjust(wspace=0.5, hspace=0.5);
>>> target1 = [axes[0][0], axes[1][1], axes[2][2], axes[3][3]]
>>> target2 = [axes[3][0], axes[2][1], axes[1][2], axes[0][3]]
>>> df.plot(subplots=True, ax=target1, legend=False, sharex=False, sharey=False);
>>> (-df).plot(subplots=True, ax=target2, legend=False, sharex=False, sharey=False);

#Another option is passing an ax argument to Series.plot() 
#to plot on a particular axis:
>>> fig, axes = plt.subplots(nrows=2, ncols=2)
>>> df['A'].plot(ax=axes[0,0]); axes[0,0].set_title('A');
>>> df['B'].plot(ax=axes[0,1]); axes[0,1].set_title('B');
>>> df['C'].plot(ax=axes[1,0]); axes[1,0].set_title('C');
>>> df['D'].plot(ax=axes[1,1]); axes[1,1].set_title('D');

##Plotting With Error Bars
#Horizontal and vertical errorbars can be supplied to the xerr and yerr keyword arguments to plot(). 
#The error values can be specified using a variety of formats.
    •As a DataFrame or dict of errors with column names matching the columns attribute of the plotting DataFrame or matching the name attribute of the Series
    •As a str indicating which of the columns of plotting DataFrame contain the error values
    •As raw values (list, tuple, or np.ndarray). Must be the same length as the plotting DataFrame/Series

#Asymmetrical error bars are also supported, 
#however raw error values must be provided in this case. 
#For a M length Series, a Mx2 array should be provided indicating lower and upper (or left and right) errors. 
#For a MxN DataFrame, asymmetrical errors should be in a Mx2xN array.

#Here is an example of one way to easily plot group means with standard deviations from the raw data.
# Generate the data
>>> ix3 = pd.MultiIndex.from_arrays([['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], ['foo', 'foo', 'bar', 'bar', 'foo', 'foo', 'bar', 'bar']], names=['letter', 'word'])
>>> df3 = pd.DataFrame({'data1': [3, 2, 4, 3, 2, 4, 3, 2], 'data2': [6, 5, 7, 5, 4, 5, 6, 5]}, index=ix3)
# Group by index labels and take the means and standard deviations for each group
>>> gp3 = df3.groupby(level=('letter', 'word'))
>>> means = gp3.mean()
>>> errors = gp3.std()
>>> means
             data1  data2
letter word              
a      bar     3.5    6.0
       foo     2.5    5.5
b      bar     2.5    5.5
       foo     3.0    4.5
>>> errors
                data1     data2
letter word                    
a      bar   0.707107  1.414214
       foo   0.707107  0.707107
b      bar   0.707107  0.707107
       foo   1.414214  0.707107
# Plot
>>> fig, ax = plt.subplots()
>>> means.plot.bar(yerr=errors, ax=ax)
<matplotlib.axes._subplots.AxesSubplot at 0x1281a66d8>

##Plotting Tables
#DataFrame.plot() and Series.plot() with a table keyword. 
#The table keyword can accept bool, DataFrame or Series. 

#The simple way to draw a table is to specify table=True. 
#Data will be transposed to meet matplotlib's default layout.
>>> fig, ax = plt.subplots(1, 1)
>>> df = pd.DataFrame(np.random.rand(5, 3), columns=['a', 'b', 'c'])
>>> ax.get_xaxis().set_visible(False)   # Hide Ticks
>>> df.plot(table=True, ax=ax)
<matplotlib.axes._subplots.AxesSubplot at 0x127718a20>

#Also, you can pass different DataFrame or Series for table keyword. 
#The data will be drawn as displayed in print method (not transposed automatically). If required, it should be transposed manually as below example.
>>> fig, ax = plt.subplots(1, 1)
>>> ax.get_xaxis().set_visible(False)   # Hide Ticks
>>> df.plot(table=np.round(df.T, 2), ax=ax)
<matplotlib.axes._subplots.AxesSubplot at 0x1273f5dd8>

#there is a helper function pandas.plotting.table to create a table from DataFrame and Series, 
#and add it to an matplotlib.Axes. 
#This function can accept keywords which matplotlib table has.
>>> from pandas.plotting import table
>>> fig, ax = plt.subplots(1, 1)
>>> table(ax, np.round(df.describe(), 2),
          loc='upper right', colWidths=[0.2, 0.2, 0.2])
    
<matplotlib.table.Table at 0x1285ff940>
>>> df.plot(ax=ax, ylim=(0, 2), legend=None)
<matplotlib.axes._subplots.AxesSubplot at 0x1203a52e8>

##Colormaps
#DataFrame plotting supports the use of the colormap= argument, 
#which accepts either a Matplotlib colormap 
#or a string that is a name of a colormap registered with Matplotlib. 

#As matplotlib does not directly support colormaps for line-based plots, 
#the colors are selected based on an even spacing determined 
#by the number of columns in the DataFrame. 
#There is no consideration made for background color, 
#so some colormaps will produce lines that are not easily visible.

#To use the cubehelix colormap, we can simply pass 'cubehelix' to colormap=
>>> df = pd.DataFrame(np.random.randn(1000, 10), index=ts.index)
>>> df = df.cumsum()
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> df.plot(colormap='cubehelix')
<matplotlib.axes._subplots.AxesSubplot at 0x12157e8d0>

#or we can pass the colormap itself
>>> from matplotlib import cm
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> df.plot(colormap=cm.cubehelix)
<matplotlib.axes._subplots.AxesSubplot at 0x121a0bd30>

#Colormaps can also be used other plot types, like bar charts:
>>> dd = pd.DataFrame(np.random.randn(10, 10)).applymap(abs)
>>> dd = dd.cumsum()
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> dd.plot.bar(colormap='Greens')
<matplotlib.axes._subplots.AxesSubplot at 0x1210a3b00>

#Parallel coordinates charts:
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> parallel_coordinates(data, 'Name', colormap='gist_rainbow')
<matplotlib.axes._subplots.AxesSubplot at 0x1207a52b0>

#Andrews curves charts:
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> andrews_curves(data, 'Name', colormap='winter')
<matplotlib.axes._subplots.AxesSubplot at 0x120473ba8>

##Plotting directly with matplotlib
>>> price = pd.Series(np.random.randn(150).cumsum(),
                      index=pd.date_range('2000-1-1', periods=150, freq='B'))
   
>>> ma = price.rolling(20).mean()
>>> mstd = price.rolling(20).std()
>>> plt.figure()
<Figure size 640x480 with 0 Axes>
>>> plt.plot(price.index, price, 'k')
[<matplotlib.lines.Line2D at 0x12de71550>]
>>> plt.plot(ma.index, ma, 'b')
[<matplotlib.lines.Line2D at 0x12de48908>]
>>> plt.fill_between(mstd.index, ma-2*mstd, ma+2*mstd, color='b', alpha=0.2)
<matplotlib.collections.PolyCollection at 0x12df70898>












###Pandas -DF -  Index objects
# Not derived from Series 

# The pandas Index class and its subclasses is like  ordered multiset. 
#Duplicates are allowed.

    
class pandas.Index
    Base class for all Index subclass
    An Index instance can only contain hashable objects
    Parameters:
    data : array-like (1-dimensional)
    dtype : NumPy dtype (default: object)
    copy : bool
        Make a copy of input ndarray
    name : object
        Name to be stored in the index
    tupleize_cols : bool (default: True)
        When True, attempt to create a MultiIndex if possible
 
#Examples
>>> pd.Index([1, 2, 3])
Int64Index([1, 2, 3], dtype='int64')
>>> pd.Index(list('abc'))
Index(['a', 'b', 'c'], dtype='object')

##index - Attributes
Index.values                    return the underlying data as an ndarray 
Index.is_monotonic              alias for is_monotonic_increasing (deprecated) 
Index.is_monotonic_increasing   return if the index is monotonic increasing (only equal or 
Index.is_monotonic_decreasing   return if the index is monotonic decreasing (only equal or 
Index.is_unique  
Index.has_duplicates  
Index.dtype  
Index.inferred_type  
Index.is_all_dates  
Index.shape                     return a tuple of the shape of the underlying data 
Index.nbytes                    return the number of bytes in the underlying data 
Index.ndim                      return the number of dimensions of the underlying data, 
Index.size                      return the number of elements in the underlying data 
Index.empty  
Index.strides                   return the strides of the underlying data 
Index.itemsize                  return the size of the dtype of the item of the underlying data 
Index.base                      return the base object if the memory of the underlying data is 
Index.T                         return the transpose, which is by definition self 
Index.memory_usage([deep])      Memory usage of my values 

##index - Modifying and Computations
Index.all(*args, **kwargs)      Return whether all elements are True 
Index.any(*args, **kwargs)      Return whether any element is True 
Index.argmin([axis])            return a ndarray of the minimum argument indexer 
Index.argmax([axis])            return a ndarray of the maximum argument indexer 
Index.copy([name, deep, dtype]) Make a copy of this object. 
Index.delete(loc)               Make new Index with passed location(-s) deleted 
Index.drop(labels[, errors])    Make new Index with passed list of labels deleted 
Index.drop_duplicates([keep])   Return Index with duplicate values removed 
Index.duplicated([keep])        Return boolean np.ndarray denoting duplicate values 
Index.equals(other)             Determines if two Index objects contain the same elements. 
Index.factorize([sort, na_sentinel])    Encode the object as an enumerated type or categorical variable 
Index.identical(other)          Similar to equals, but check that other comparable attributes are 
Index.insert(loc, item)         Make new Index inserting new item at location. 

Index.min()                     The minimum value of the object 
Index.max()                     The maximum value of the object 
Index.reindex(target[, method, level, ...])     Create index with target's values (move/add/delete values as necessary) 
Index.repeat(repeats, *args, **kwargs)          Repeat elements of an Index. 
Index.where(cond[, other]) 
Index.take(indices[, axis, allow_fill, ...])    return a new Index of the values selected by the indices 
Index.putmask(mask, value)                      return a new Index of the values set with the mask 
Index.set_names(names[, level, inplace])        Set new names on index. 
Index.unique()                                  Return unique values in the object. 
Index.nunique([dropna])                         Return number of unique elements in the object. 
Index.value_counts([normalize, sort, ...])      Returns object containing counts of unique values. 

##index - Missing Values
Index.fillna([value, downcast]) Fill NA/NaN values with the specified value 
Index.dropna([how])             Return Index without NA/NaN values 
Index.isna()                    Detect missing values 
Index.notna()                   Inverse of isna 

##index - Conversion
Index.astype(dtype[, copy])     Create an Index with values cast to dtypes. 
Index.tolist()                  Return a list of the values. 
Index.to_datetime([dayfirst])   DEPRECATED: use pandas.to_datetime() instead. 
Index.to_series(**kwargs)       Create a Series with both index and values equal to the index keys 
Index.to_frame([index])         Create a DataFrame with a column containing the Index. 

##index - Sorting
Index.argsort(*args, **kwargs)                  Returns the indices that would sort the index and its underlying data. 
Index.sort_values([return_indexer, ascending])  Return sorted copy of Index 

##index - Time-specific operations
Index.shift([periods, freq])    Shift Index containing datetime objects by input number of periods and 

##index - Combining / joining / set operations
Index.append(other)                     Append a collection of Index options together 
Index.join(other[, how, level, ...])    this is an internal non-public method 
Index.intersection(other)               Form the intersection of two Index objects. 
Index.union(other)                      Form the union of two Index objects and sorts if possible. 
Index.difference(other)                 Return a new Index with elements from the index that are not in other. 
Index.symmetric_difference(other[, result_name]) Compute the symmetric difference of two Index objects. 

##index - Selecting
Index.get_indexer(target[, method, limit, ...]) Compute indexer and mask for new index given the current index. 
Index.get_indexer_non_unique(target)            Compute indexer and mask for new index given the current index. 
Index.get_level_values(level)                   Return an Index of values for requested level, equal to the length of the index. 
Index.get_loc(key[, method, tolerance])         Get integer location, slice or boolean mask for requested label. 
Index.get_value(series, key)                    Fast lookup of value from 1-dimensional ndarray. 
Index.isin(values[, level])                     Compute boolean array of whether each index value is found in the passed set of values. 
Index.slice_indexer([start, end, step, kind])   For an ordered Index, compute the slice indexer for input labels and 
Index.slice_locs([start, end, step, kind])      Compute slice locations for input labels. 

##index - Numeric Index (subclass of Index)
RangeIndex          Immutable Index implementing a monotonic integer range. 
Int64Index          Immutable ndarray implementing an ordered, sliceable set. 
UInt64Index         Immutable ndarray implementing an ordered, sliceable set. 
Float64Index        Immutable ndarray implementing an ordered, sliceable set. 

class pandas.RangeIndex
    This is the default index type used by DataFrame 
    and Series when no explicit index is provided by the user.
    Parameters:
    start : int (default: 0), or other RangeIndex instance.
        If int and 'stop' is not given, interpreted as 'stop' instead.
    stop : int (default: 0)
    step : int (default: 1)
    name : object, optional
        Name to be stored in the index
    copy : bool, default False
        Unused, accepted for homogeneity with other index types.
 
 
##index - CategoricalIndex (subclass of Index)
class pandas.CategoricalIndex
    Immutable Index implementing an ordered, sliceable set. 
    CategoricalIndex represents a sparsely populated Index 
    with an underlying Categorical.
    Parameters:
    data : array-like or Categorical, (1-dimensional)
    categories : optional, array-like
        categories for the CategoricalIndex
    ordered : boolean,
        designating if the categories are ordered
    copy : bool
        Make a copy of input ndarray
    name : object
        Name to be stored in the index
 
##index - Categorical Components
CategoricalIndex.codes  
CategoricalIndex.categories  
CategoricalIndex.ordered  
CategoricalIndex.rename_categories(*args, ...)      Renames categories. 
CategoricalIndex.reorder_categories(*args, ...)     Reorders categories as specified in new_categories. 
CategoricalIndex.add_categories(*args, **kwargs)    Add new categories. 
CategoricalIndex.remove_categories(*args, ...)      Removes the specified categories. 
CategoricalIndex.remove_unused_categories(...)      Removes categories which are not used. 
CategoricalIndex.set_categories(*args, **kwargs)    Sets the categories to the specified new_categories. 
CategoricalIndex.as_ordered(*args, **kwargs)        Sets the Categorical to be ordered 
CategoricalIndex.as_unordered(*args, **kwargs)      Sets the Categorical to be unordered 

##index - IntervalIndex (subclass of Index)
#IntervalIndex Immutable Index implementing an ordered, sliceable set. 
#A new IntervalIndex is typically constructed using interval_range():
>>> pd.interval_range(start=0, end=5)
IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]]
              closed='right', dtype='interval[int64]')
              
##index - IntervalIndex Components
IntervalIndex.from_arrays(left, right[, ...])       Construct an IntervalIndex from a a left and right array 
IntervalIndex.from_tuples(data[, closed, ...])      Construct an IntervalIndex from a list/array of tuples 
IntervalIndex.from_breaks(breaks[, closed, ...])    Construct an IntervalIndex from an array of splits 
IntervalIndex.from_intervals(data[, name, copy])    Construct an IntervalIndex from a 1d array of Interval objects 

##index - MultiIndex(subclass of Index)
MultiIndex          A multi-level, or hierarchical, index object for pandas objects 
IndexSlice          Create an object to more easily perform multi-index slicing 

class pandas.MultiIndex
    A multi-level, or hierarchical, index object for pandas objects
    Parameters:
    levels : sequence of arrays
        The unique labels for each level
    labels : sequence of arrays
        Integers for each level designating which label at each location
    sortorder : optional int
        Level of sortedness (must be lexicographically sorted by that level)
    names : optional sequence of objects
        Names for each of the index levels. (name is accepted for compat)
    copy : boolean, default False
        Copy the meta-data
    verify_integrity : boolean, default True
        Check that the levels/labels are consistent and valid
 
#Example 
>>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]
>>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))
MultiIndex(levels=[[1, 2], ['blue', 'red']],
       labels=[[0, 0, 1, 1], [1, 0, 1, 0]],
       names=['number', 'color'])
       
##index - MultiIndex Components
MultiIndex.from_arrays(arrays[, sortorder, ...]) Convert arrays to MultiIndex 
MultiIndex.from_tuples(tuples[, sortorder, ...]) Convert list of tuples to MultiIndex 
MultiIndex.from_product(iterables[, ...])       Make a MultiIndex from the cartesian product of multiple iterables 
MultiIndex.set_levels(levels[, level, ...])     Set new levels on MultiIndex. 
MultiIndex.set_labels(labels[, level, ...])     Set new labels on MultiIndex. 
MultiIndex.to_hierarchical(n_repeat[, n_shuffle]) Return a MultiIndex reshaped to conform to the shapes given by n_repeat and n_shuffle. 
MultiIndex.to_frame([index])                    Create a DataFrame with the levels of the MultiIndex as columns. 
MultiIndex.is_lexsorted()                       Return True if the labels are lexicographically sorted 
MultiIndex.droplevel([level])                   Return Index with requested level removed. 
MultiIndex.swaplevel([i, j])                    Swap level i with level j. 
MultiIndex.reorder_levels(order)                Rearrange levels using input order. 
MultiIndex.remove_unused_levels()               create a new MultiIndex from the current that removing 

##Index objects - operations 
#Index provides for lookups, data alignment, and reindexing.

index = pd.Index(['e', 'd', 'a', 'b'], name='something')
>>> index.name
'something'
>>> index
Index([u'e', u'd', u'a', u'b'], dtype='object')
>>> 'd' in index
True

#They default to returning a copy; or specify inplace=True to have the data change in place.
ind = pd.Index([1, 2, 3])
>>> ind.rename("apple")
Int64Index([1, 2, 3], dtype='int64', name=u'apple')

>>> ind
Int64Index([1, 2, 3], dtype='int64')


#set_names, set_levels, and set_labels also take an optional level argument
index = pd.MultiIndex.from_product([range(3), ['one', 'two']], names=['first', 'second'])
>>> index
MultiIndex(levels=[[0, 1, 2], [u'one', u'two']],      #label-names for level     
labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]],   #index of label-names in labels     
names=[u'first', u'second'])
# note how label-names are placed in 'labels'
>>> df= pd.DataFrame(np.random.random(6),index=index, columns=['A'])
>>> df
                     A
first second
0     one     0.481237
      two     0.093698
1     one     0.293074
      two     0.157239
2     one     0.903252
      two     0.630807
>>> index.levels[1]
Index([u'one', u'two'], dtype='object', name=u'second')

>>> index.set_levels(["a", "b"], level=1)
MultiIndex(levels=[[0, 1, 2], [u'a', u'b']],           
labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]],           
names=[u'first', u'second'])



##Index objects - operations - Set operations on Index objects
#+ can be replace by .union() or |, and - by .difference(). 
a = pd.Index(['c', 'b', 'a'])
b = pd.Index(['c', 'e', 'd'])
>>> a | b
Index([u'a', u'b', u'c', u'd', u'e'], dtype='object')

>>> a & b
Index([u'c'], dtype='object')

>>> a.difference(b)
Index([u'a', u'b'], dtype='object')




##Index objects- Index.fillna 
#fills missing values with specified scalar value.

idx1 = pd.Index([1, np.nan, 3, 4])
>>> idx1
Float64Index([1.0, nan, 3.0, 4.0], dtype='float64')

>>> idx1.fillna(2)
Float64Index([1.0, 2.0, 3.0, 4.0], dtype='float64')

idx2 = pd.DatetimeIndex([pd.Timestamp('2011-01-01'), pd.NaT, pd.Timestamp('2011-01-03')])
>>> idx2
DatetimeIndex(['2011-01-01', 'NaT', '2011-01-03'], dtype='datetime64[ns]', freq=None)

>>> idx2.fillna(pd.Timestamp('2011-01-02'))
DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], dtype='datetime64[ns]', freq=None)



##Index objects - Set an index - set_index()
#DataFrame has a set_index method which takes a column name (for a regular Index)
#or a list of column names (for a MultiIndex), 

>>> data
    a    b  c    d
0  bar  one  z  1.0
1  bar  two  y  2.0
2  foo  one  x  3.0
3  foo  two  w  4.0

>>> indexed1 = data.set_index('c')
>>> indexed1
    a    b    d               
c
z  bar  one  1.0
y  bar  two  2.0
x  foo  one  3.0
w  foo  two  4.0

indexed2 = data.set_index(['a', 'b'])
>>> indexed2
         c    d
a    b
bar one  z  1.0    
    two  y  2.0
foo one  x  3.0    
    two  w  4.0


#The append keyword option allow you to keep the existing index 
#and append the given columns to a MultiIndex:
frame = data.set_index('c', drop=False) #original column is not dropped 
frame = frame.set_index(['a', 'b'], append=True)

>>> frame
           c    d
c a    b
z bar one  z  1.0
y bar two  y  2.0
x foo one  x  3.0
w foo two  w  4.0

#to add the index in-place (without creating a new object):
data.set_index(['a', 'b'], inplace=True)
>>> data
         c    d
a    b
bar one  z  1.0    
    two  y  2.0
foo one  x  3.0    
    two  w  4.0



##Index objects - Reset the index - reset_index()
#transfers the index values into the DataFrame's columns
#and sets a simple integer index. This is the inverse operation to set_index


>>> data
         c    d
a    b
bar one  z  1.0    
    two  y  2.0
foo one  x  3.0    
    two  w  4.0

>>> data.reset_index()
    a    b  c    d
0  bar  one  z  1.0
1  bar  two  y  2.0
2  foo  one  x  3.0
3  foo  two  w  4.0


#can use the level keyword to remove only a portion of the index
>>> frame
           c    d
c a   b
z bar one  z  1.0
y bar two  y  2.0
x foo one  x  3.0
w foo two  w  4.0

>>> frame.reset_index(level=1)
        a  c    d
c b
z one  bar  z  1.0
y two  bar  y  2.0
x one  foo  x  3.0
w two  foo  w  4.0

#Adding an ad hoc index
data.index = index


##Index objects - chained indexing
#Returning a view versus a copy
#When setting values in a pandas object, care must be taken to avoid
#what is called chained indexing.


dfmi = pd.DataFrame([list('abcd'),                        
            list('efgh'),                         
            list('ijkl'),                         
            list('mnop')],                        
            columns=pd.MultiIndex.from_product([['one','two'],['first','second']]))


>>> dfmi
    one          two  
    first second first second
0     a      b     c      d
1     e      f     g      h
2     i      j     k      l
3     m      n     o      p


#Compare these two access methods:
#called chaining index 
#does not work with assignments and has other issues 
>>> dfmi['one']['second']  #two calls of  []
0    b
1    f
2    j
3    n
Name: second, dtype: object
#better and should be used, assignment works 
>>> dfmi.loc[:,('one','second')]  
0    b
1    f
2    j
3    n
Name: (one, second), dtype: object

#Example 
dfc = pd.DataFrame({'A':['aaa','bbb','ccc'],'B':[1,2,3]})
dfc.loc[0,'A'] = 11
>>> dfc
    A  B
0   11  1
1  bbb  2
2  ccc  3




##Index objects - Hierarchical indexing (MultiIndex)

# using tuples as atomic labels on an axis:
arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],             
      ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]   

tuples = list(zip(*arrays))
>>> tuples
[('bar', 'one'), 
('bar', 'two'), 
('baz', 'one'), 
('baz', 'two'), 
('foo', 'one'), 
('foo', 'two'), 
('qux', 'one'), 
('qux', 'two')]

>>> pd.Series(np.random.randn(8), index=tuples)
(bar, one)   -1.236269
(bar, two)    0.896171
(baz, one)   -0.487602
(baz, two)   -0.082240
(foo, one)   -2.182937
(foo, two)    0.380396
(qux, one)    0.084844
(qux, two)    0.432390
dtype: float64

#MultiIndex allows you to do grouping, selection, and reshaping operations
#The MultiIndex object is the hierarchical analogue of the standard Index object
#which typically stores the axis labels in pandas object s.
index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])
>>> index
MultiIndex(levels=[[u'bar', u'baz', u'foo', u'qux'], [u'one', u'two']], #label-names for levels 
labels=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]],         #index of label-names as labels 
names=[u'first', u'second'])

s = pd.Series(np.random.randn(8), index=index)
>>> s
            first  second
bar    one       0.469112       
       two      -0.282863
baz    one      -1.509059       
       two      -1.135632
foo    one       1.212112       
       two      -0.173215
qux    one       0.119209       
       two      -1.044236
dtype: float64

# OR
iterables = [['bar', 'baz', 'foo', 'qux'], ['one', 'two']]
>>> pd.MultiIndex.from_product(iterables, names=['first', 'second'])
MultiIndex(levels=[[u'bar', u'baz', u'foo', u'qux'], [u'one', u'two']],           
labels=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]],           
names=[u'first', u'second'])


# OR
arrays = [np.array(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux']),            
        np.array(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'])]  
s = pd.Series(np.random.randn(8), index=arrays)
>>> s
bar  one   -0.861849     
     two   -2.104569
baz  one   -0.494929     
     two    1.071804
foo  one    0.721555     
     two   -0.706771
qux  one   -1.039575     
     two    0.271860
dtype: float64

df = pd.DataFrame(np.random.randn(8, 4), index=arrays)
>>> df
            0         1         2         3
bar one -0.424972  0.567020  0.276232 -1.087401    
    two -0.673690  0.113648 -1.478427  0.524988
baz one  0.404705  0.577046 -1.715002 -1.039268    
    two -0.370647 -1.157892 -1.344312  0.844885
foo one  1.075770 -0.109050  1.643563 -1.469388    
    two  0.357021 -0.674600 -1.776904 -0.968914
qux one -1.294524  0.413738  0.276662 -0.472035    
    two -0.013960 -0.362543 -0.006154 -0.923061


#Reconstructing the level labels
#get_level_values(n) will return a vector of the labels 
#for each location at a particular level:
>>> index.get_level_values(0)
Index([u'bar', u'bar', u'baz', u'baz', u'foo', u'foo', u'qux', u'qux'], dtype='object', name=u'first')
>>> index.get_level_values('second')
Index([u'one', u'two', u'one', u'two', u'one', u'two', u'one', u'two'], dtype='object', name=u'second')



##MultiIndex - indexing 
df = pd.DataFrame(np.random.randn(3, 8), index=['A', 'B', 'C'], columns=index)
>>> df
first   bar                 baz                 foo                 qux  \
second  one       two       one       two       one       two       one
A       0.895717  0.805244 -1.206412  2.565646  1.431256  1.340309 -1.170299
B       0.410835  0.813850  0.132003 -0.827317 -0.076467 -1.187678  1.130127
C      -1.413681  1.607920  1.024180  0.569605  0.875906 -2.211372  0.974466

first
second       two
A      -0.226169
B      -1.436737
C      -2.006747

#[] - contains column specification , not row ones 
>>> df['bar']
second       one       two
A       0.895717  0.805244
B       0.410835  0.813850
C      -1.413681  1.607920

>>> df['bar', 'one']
A    0.895717
B    0.410835
C   -1.413681
Name: (bar, one), dtype: float64

>>> df['bar']['one']  #chaining [col][row]
A    0.895717
B    0.410835
C   -1.413681
Name: one, dtype: float64




# original multi-index
df.columns
MultiIndex(levels=[['bar', 'baz', 'foo', 'qux'], ['one', 'two']],
           labels=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]],
           names=['first', 'second'])

# sliced
>>> df[['foo','qux']].columns
MultiIndex(levels=[['bar', 'baz', 'foo', 'qux'], ['one', 'two']],
           labels=[[2, 2, 3, 3], [0, 1, 0, 1]],
           names=['first', 'second'])


#Getting value of Multiindex 
>>> df[['foo','qux']].columns.values
array([('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')], dtype=object)

# for a specific level
>>> df[['foo','qux']].columns.get_level_values(0)
Index(['foo', 'foo', 'qux', 'qux'], dtype='object', name='first')


#To reconstruct the multiindex with only the used levels
>>> df[['foo','qux']].columns.remove_unused_levels()
MultiIndex(levels=[['foo', 'qux'], ['one', 'two']],
           labels=[[0, 0, 1, 1], [0, 1, 0, 1]],
           names=['first', 'second'])




##Operations between differently-indexed objects having MultiIndex on the axes will work as you expect;
#data alignment will work the same as an Index of tuples:
s = pd.Series(np.random.randn(8), index=arrays)
>>> s + s[:-2]
bar  one   -1.723698
     two   -4.209138
baz  one   -0.989859
     two    2.143608
foo  one    1.443110
     two   -1.413542
qux  one         NaN
     two         NaN
dtype: float64

>>> s + s[::2]
bar  one   -1.723698
     two         NaN
baz  one   -0.989859
     two         NaN
foo  one    1.443110
     two         NaN
qux  one   -2.079150
     two         NaN
dtype: float64


#reindex can be called with another MultiIndex or even a list or array of tuples:
>>> s = pd.Series(np.random.randn(8), index=arrays)s.reindex(index[:3])
first  second
bar    one      -0.861849
       two      -2.104569
baz    one      -0.494929
dtype: float64

>>> s.reindex([('foo', 'two'), ('bar', 'one'), ('qux', 'one'), ('baz', 'one')])
foo  two   -0.706771
bar  one   -0.861849
qux  one   -1.039575
baz  one   -0.494929
dtype: float64



##MultiIndex - Advanced indexing with hierarchical index
df = df.T
>>> df
                     A         B         C
first second                              
bar   one     0.895717  0.410835 -1.413681
      two     0.805244  0.813850  1.607920
baz   one    -1.206412  0.132003  1.024180
      two     2.565646 -0.827317  0.569605
foo   one     1.431256 -0.076467  0.875906
      two     1.340309 -1.187678 -2.211372
qux   one    -1.170299  1.130127  0.974466
      two    -0.226169 -1.436737 -2.006747

#loc takes [list of row_label] for Multiindex  , Note that we can not give column label like simple Index
>>> df.loc['bar']
               A         B         C
second                              
one     0.895717  0.410835 -1.413681
two     0.805244  0.813850  1.607920

>>> df.loc['bar', 'two']
A    0.805244
B    0.813850
C    1.607920
Name: (bar, two), dtype: float64


>>> df.loc['baz':'foo']
                     A         B         C
first second                              
baz   one    -1.206412  0.132003  1.024180
      two     2.565646 -0.827317  0.569605
foo   one     1.431256 -0.076467  0.875906
      two     1.340309 -1.187678 -2.211372


#slice with a 'range' of values, by providing a slice of tuples.
>>> df.loc[('baz', 'two'):('qux', 'one')]
                     A         B         C
first second                              
baz   two     2.565646 -0.827317  0.569605
foo   one     1.431256 -0.076467  0.875906
      two     1.340309 -1.187678 -2.211372
qux   one    -1.170299  1.130127  0.974466


>>> df.loc[('baz', 'two'):'foo']
                     A         B         C
first second                              
baz   two     2.565646 -0.827317  0.569605
foo   one     1.431256 -0.076467  0.875906
      two     1.340309 -1.187678 -2.211372


#Passing a list of labels or tuples 
>>> df.loc[[('bar', 'two'), ('qux', 'one')]]
                     A         B         C
first second                              
bar   two     0.805244  0.813850  1.607920
qux   one    -1.170299  1.130127  0.974466



##MuktiIndex - Using slicers 
#slice(multiindex_labels) to select those labels 
#new way to slice multi-indexed objects - by providing multiple indexers.
#to select all the contents of that level - use None for that level 

#Use below 
df.loc[(slice('A1','A3'),.....), :]
#rather than this:
df.loc[(slice('A1','A3'),.....)]



def mklbl(prefix,n):
    return ["%s%s" % (prefix,i)  for i in range(n)]


miindex = pd.MultiIndex.from_product([mklbl('A',4),
    mklbl('B',2),
    mklbl('C',4),
    mklbl('D',2)])


micolumns = pd.MultiIndex.from_tuples([('a','foo'),('a','bar'),
    ('b','foo'),('b','bah')],
    names=['lvl0', 'lvl1'])

dfmi = pd.DataFrame(np.arange(len(miindex)*len(micolumns)).reshape((len(miindex),len(micolumns))),
        index=miindex,
        columns=micolumns).sort_index().sort_index(axis=1)
    

>>> dfmi
lvl0           a         b     
lvl1         bar  foo  bah  foo
A0 B0 C0 D0    1    0    3    2
         D1    5    4    7    6
      C1 D0    9    8   11   10
         D1   13   12   15   14
      C2 D0   17   16   19   18
         D1   21   20   23   22
      C3 D0   25   24   27   26
...          ...  ...  ...  ...
A3 B1 C0 D1  229  228  231  230
      C1 D0  233  232  235  234
         D1  237  236  239  238
      C2 D0  241  240  243  242
         D1  245  244  247  246
      C3 D0  249  248  251  250
         D1  253  252  255  254

[64 rows x 4 columns]


##Basic multi-index slicing using slices, lists, and labels.
#1st level - select A1, A3, 2nd level - select all, 2rd level - select C1, C3
>>> dfmi.loc[(slice('A1','A3'), slice(None), ['C1', 'C3']), :]
lvl0           a         b     
lvl1         bar  foo  bah  foo
A1 B0 C1 D0   73   72   75   74
         D1   77   76   79   78
      C3 D0   89   88   91   90
         D1   93   92   95   94
   B1 C1 D0  105  104  107  106
         D1  109  108  111  110
      C3 D0  121  120  123  122
...          ...  ...  ...  ...
A3 B0 C1 D1  205  204  207  206
      C3 D0  217  216  219  218
         D1  221  220  223  222
   B1 C1 D0  233  232  235  234
         D1  237  236  239  238
      C3 D0  249  248  251  250
         D1  253  252  255  254

[24 rows x 4 columns]


#use a pd.IndexSlice to have a more natural syntax using : 
#rather than using slice(None)
idx = pd.IndexSlice
>>> dfmi.loc[idx[:, :, ['C1', 'C3']], idx[:, 'foo']]
lvl0           a    b
lvl1         foo  foo
A0 B0 C1 D0    8   10
         D1   12   14
      C3 D0   24   26
         D1   28   30
   B1 C1 D0   40   42
         D1   44   46
      C3 D0   56   58
...          ...  ...
A3 B0 C1 D1  204  206
      C3 D0  216  218
         D1  220  222
   B1 C1 D0  232  234
         D1  236  238
      C3 D0  248  250
         D1  252  254

[32 rows x 2 columns]


#multiple axis 
>>> dfmi.loc['A1', (slice(None), 'foo')]
lvl0        a    b
lvl1      foo  foo
B0 C0 D0   64   66
      D1   68   70
   C1 D0   72   74
      D1   76   78
   C2 D0   80   82
      D1   84   86
   C3 D0   88   90
...       ...  ...
B1 C0 D1  100  102
   C1 D0  104  106
      D1  108  110
   C2 D0  112  114
      D1  116  118
   C3 D0  120  122
      D1  124  126

[16 rows x 2 columns]

>>> dfmi.loc[idx[:, :, ['C1', 'C3']], idx[:, 'foo']]
lvl0           a    b
lvl1         foo  foo
A0 B0 C1 D0    8   10
         D1   12   14
      C3 D0   24   26
         D1   28   30
   B1 C1 D0   40   42
         D1   44   46
      C3 D0   56   58
...          ...  ...
A3 B0 C1 D1  204  206
      C3 D0  216  218
         D1  220  222
   B1 C1 D0  232  234
         D1  236  238
      C3 D0  248  250
         D1  252  254

[32 rows x 2 columns]


##Using a boolean indexer 
mask = dfmi[('a', 'foo')] > 200
>>> dfmi.loc[idx[mask, :, ['C1', 'C3']], idx[:, 'foo']]
lvl0           a    b
lvl1         foo  foo
A3 B0 C1 D1  204  206
      C3 D0  216  218
         D1  220  222
   B1 C1 D0  232  234
         D1  236  238
      C3 D0  248  250
         D1  252  254


#specify the axis argument to .loc to interpret the passed slicers on a single axis.
#axis = 0, row, axis=1, column 

>>> dfmi.loc(axis=0)[:, :, ['C1', 'C3']]
lvl0           a         b     
lvl1         bar  foo  bah  foo
A0 B0 C1 D0    9    8   11   10
         D1   13   12   15   14
      C3 D0   25   24   27   26
         D1   29   28   31   30
   B1 C1 D0   41   40   43   42
         D1   45   44   47   46
      C3 D0   57   56   59   58
...          ...  ...  ...  ...
A3 B0 C1 D1  205  204  207  206
      C3 D0  217  216  219  218
         D1  221  220  223  222
   B1 C1 D0  233  232  235  234
         D1  237  236  239  238
      C3 D0  249  248  251  250
         D1  253  252  255  254

[32 rows x 4 columns]


#set the values using these methods
df2 = dfmi.copy()
>>> df2.loc(axis=0)[:, :, ['C1', 'C3']] = -10
>>> df2
lvl0           a         b     
lvl1         bar  foo  bah  foo
A0 B0 C0 D0    1    0    3    2
         D1    5    4    7    6
      C1 D0  -10  -10  -10  -10
         D1  -10  -10  -10  -10
      C2 D0   17   16   19   18
         D1   21   20   23   22
      C3 D0  -10  -10  -10  -10
...          ...  ...  ...  ...
A3 B1 C0 D1  229  228  231  230
      C1 D0  -10  -10  -10  -10
         D1  -10  -10  -10  -10
      C2 D0  241  240  243  242
         D1  245  244  247  246
      C3 D0  -10  -10  -10  -10
         D1  -10  -10  -10  -10

[64 rows x 4 columns]


#use a right-hand-side of an alignable object as well.
df2 = dfmi.copy()
>>> df2.loc[idx[:, :, ['C1', 'C3']], :] = df2 * 1000
>>> df2
lvl0              a               b        
lvl1            bar     foo     bah     foo
A0 B0 C0 D0       1       0       3       2
         D1       5       4       7       6
      C1 D0    9000    8000   11000   10000
         D1   13000   12000   15000   14000
      C2 D0      17      16      19      18
         D1      21      20      23      22
      C3 D0   25000   24000   27000   26000
...             ...     ...     ...     ...
A3 B1 C0 D1     229     228     231     230
      C1 D0  233000  232000  235000  234000
         D1  237000  236000  239000  238000
      C2 D0     241     240     243     242
         D1     245     244     247     246
      C3 D0  249000  248000  251000  250000
         D1  253000  252000  255000  254000

[64 rows x 4 columns]


##Cross-section
#The xs method of DataFrame additionally takes a level argument 
>>> df
                     A         B         C
first second                              
bar   one     0.895717  0.410835 -1.413681
      two     0.805244  0.813850  1.607920
baz   one    -1.206412  0.132003  1.024180
      two     2.565646 -0.827317  0.569605
foo   one     1.431256 -0.076467  0.875906
      two     1.340309 -1.187678 -2.211372
qux   one    -1.170299  1.130127  0.974466
      two    -0.226169 -1.436737 -2.006747

>>> df.xs('one', level='second')
              A         B         C
first                              
bar    0.895717  0.410835 -1.413681
baz   -1.206412  0.132003  1.024180
foo    1.431256 -0.076467  0.875906
qux   -1.170299  1.130127  0.974466



# using the slicers (new in 0.14.0)
>>> df.loc[(slice(None),'one'),:]
                     A         B         C
first second                              
bar   one     0.895717  0.410835 -1.413681
baz   one    -1.206412  0.132003  1.024180
foo   one     1.431256 -0.076467  0.875906
qux   one    -1.170299  1.130127  0.974466


#select on the columns with xs(), by providing the axis argument
df = df.T
>>> df.xs('one', level='second', axis=1)
first       bar       baz       foo       qux
A      0.895717 -1.206412  1.431256 -1.170299
B      0.410835  0.132003 -0.076467  1.130127
C     -1.413681  1.024180  0.875906  0.974466



# using the slicers (new in 0.14.0)
>>> df.loc[:,(slice(None),'one')]
first        bar       baz       foo       qux
second       one       one       one       one
A       0.895717 -1.206412  1.431256 -1.170299
B       0.410835  0.132003 -0.076467  1.130127
C      -1.413681  1.024180  0.875906  0.974466


#xs() also allows selection with multiple keys
>>> df.xs(('one', 'bar'), level=('second', 'first'), axis=1)
first        bar
second       one
A       0.895717
B       0.410835
C      -1.413681


# using the slicers (new in 0.14.0)
>>> df.loc[:,('bar','one')]
A    0.895717
B    0.410835
C   -1.413681
Name: (bar, one), dtype: float64


#pass drop_level=False to xs() to retain the level that was selected
>>> df.xs('one', level='second', axis=1, drop_level=False)
first        bar       baz       foo       qux
second       one       one       one       one
A       0.895717 -1.206412  1.431256 -1.170299
B       0.410835  0.132003 -0.076467  1.130127
C      -1.413681  1.024180  0.875906  0.974466


#versus the result with drop_level=True (the default value)
>>> df.xs('one', level='second', axis=1, drop_level=True)
first       bar       baz       foo       qux
A      0.895717 -1.206412  1.431256 -1.170299
B      0.410835  0.132003 -0.076467  1.130127
C     -1.413681  1.024180  0.875906  0.974466



##MultiIndex - Advanced reindexing and alignment
#The parameter level has been added to the reindex and align methods of pandas objects.
#This is useful to broadcast values across a level
midx = pd.MultiIndex(levels=[['zero', 'one'], ['x','y']],
                labels=[[1,1,0,0],[1,0,1,0]])
df = pd.DataFrame(np.random.randn(4,2), index=midx)
>>> df
              0         1
one  y  1.519970 -0.493662
     x  0.600178  0.274230
zero y  0.132885 -0.023688
     x  2.410179  1.450520

df2 = df.mean(level=0)
>>> df2
             0         1
zero  1.271532  0.713416
one   1.060074 -0.109716

>>> df2.reindex(df.index, level=0) #copied 
               0         1
one  y  1.060074 -0.109716
     x  1.060074 -0.109716
zero y  1.271532  0.713416
     x  1.271532  0.713416

# aligning
>>> df_aligned, df2_aligned = df.align(df2, level=0)
>>> df_aligned
               0         1
one  y  1.519970 -0.493662
     x  0.600178  0.274230
zero y  0.132885 -0.023688
     x  2.410179  1.450520

>>> df2_aligned
               0         1
one  y  1.060074 -0.109716
     x  1.060074 -0.109716
zero y  1.271532  0.713416
     x  1.271532  0.713416



##MultiIndex - Swapping levels with swaplevel()
#The swaplevel function can switch the order of two levels:
>>> df[:5]
               0         1
one  y  1.519970 -0.493662
     x  0.600178  0.274230
zero y  0.132885 -0.023688
     x  2.410179  1.450520

>>> df[:5].swaplevel(0, 1, axis=0)
               0         1
y one   1.519970 -0.493662
x one   0.600178  0.274230
y zero  0.132885 -0.023688
x zero  2.410179  1.450520


##MultiIndex - Reordering levels with reorder_levels()
#The reorder_levels function generalizes the swaplevel function, 
#allowing you to permute the hierarchical index levels in one step:
>>> df[:5].reorder_levels([1,0], axis=0)
               0         1
y one   1.519970 -0.493662
x one   0.600178  0.274230
y zero  0.132885 -0.023688
x zero  2.410179  1.450520


##MultiIndex - Sorting a MultiIndex
#As with any index, you can use sort_index.
import random; random.shuffle(tuples)
s = pd.Series(np.random.randn(8), index=pd.MultiIndex.from_tuples(tuples))
>>> s
qux  two    0.206053
     one   -0.251905
foo  one   -2.213588
bar  one    1.063327
baz  one    1.266143
bar  two    0.299368
foo  two   -0.863838
baz  two    0.408204
dtype: float64

>>> s.sort_index()
bar  one    1.063327
     two    0.299368
baz  one    1.266143
     two    0.408204
foo  one   -2.213588
     two   -0.863838
qux  one   -0.251905
     two    0.206053
dtype: float64

>>> s.sort_index(level=0)
bar  one    1.063327
     two    0.299368
baz  one    1.266143
     two    0.408204
foo  one   -2.213588
     two   -0.863838
qux  one   -0.251905
     two    0.206053
dtype: float64

>>> s.sort_index(level=1)
bar  one    1.063327
baz  one    1.266143
foo  one   -2.213588
qux  one   -0.251905
bar  two    0.299368
baz  two    0.408204
foo  two   -0.863838
qux  two    0.206053
dtype: float64


#pass a level name to sort_index if the MultiIndex levels are named.
s.index.set_names(['L1', 'L2'], inplace=True)
>>> s.sort_index(level='L1')
L1   L2 
bar  one    1.063327
     two    0.299368
baz  one    1.266143
     two    0.408204
foo  one   -2.213588
     two   -0.863838
qux  one   -0.251905
     two    0.206053
dtype: float64
>>> s.sort_index(level='L2')
L1   L2 
bar  one    1.063327
baz  one    1.266143
foo  one   -2.213588
qux  one   -0.251905
bar  two    0.299368
baz  two    0.408204
foo  two   -0.863838
qux  two    0.206053
dtype: float64





##Index - Take Methods
#pandas Index, Series, and DataFrame provide the take method 
#that retrieves elements along a given axis at the given indices. 
#The given indices must be either a list or an ndarray of integer index positions. 
#take will also accept negative integers as relative positions to the end of the object.


index = pd.Index(np.random.randint(0, 1000, 10))
>>> index
Int64Index([214, 502, 712, 567, 786, 175, 993, 133, 758, 329], dtype='int64')

>>> positions = [0, 9, 3]

>>> index[positions]
Int64Index([214, 329, 567], dtype='int64')

>>> index.take(positions)
Int64Index([214, 329, 567], dtype='int64')

ser = pd.Series(np.random.randn(10))

>>> ser.iloc[positions]
0   -0.179666
9    1.824375
3    0.392149
dtype: float64


##Index Objects -  CategoricalIndex
df = pd.DataFrame({'A': np.arange(6), 'B': list('aabbca')})   
df['B'] = df['B'].astype('category', categories=list('cab'))
>>> df
   A  B
0  0  a
1  1  a
2  2  b
3  3  b
4  4  c
5  5  a

>>> df.dtypes
A       int64
B    category
dtype: object

>>> df.B.cat.categories
Index([u'c', u'a', u'b'], dtype='object')


#Setting the index, will create a CategoricalIndex
>>> df2 = df.set_index('B')  #make B as index of df2
   A
B
a  0
a  1
b  2
b  3
c  4
a  5
>>> df2.index
CategoricalIndex([u'a', u'a', u'b', u'b', u'c', u'a'], categories=[u'c', u'a', u'b'], ordered=False, name=u'B', dtype='category')


#Indexing with __getitem__/.iloc/.loc/.ix works similarly to an Index with duplicates.
#The indexers MUST be in the category or the operation will raise.

>>> df2.loc['a']
   A   
B
a  0
a  1
a  5


#Sorting will order by the order of the categories
>>> df2.sort_index()
   A   
B
c  4
a  0
a  1
a  5
b  2
b  3


# Groupby operations on the index
>>> df2.groupby(level=0).sum()
   A   
B
c  4
a  6
b  5

>>> df2.groupby(level=0).sum().index
CategoricalIndex([u'c', u'a', u'b'], categories=[u'c', u'a', u'b'], ordered=False, name=u'B', dtype='category')


#Reindexing operations - index not in reindex would be dropped
>>> df2.reindex(['a','e'])
    A     
B
a  0.0
a  1.0
a  5.0
e  NaN

>>> df2.reindex(['a','e']).index
Index([u'a', u'a', u'a', u'e'], dtype='object', name=u'B')

>>> df2.reindex(pd.Categorical(['a','e'],categories=list('abcde')))
    A     
B
a  0.0
a  1.0
a  5.0
e  NaN

>>> df2.reindex(pd.Categorical(['a','e'],categories=list('abcde'))).index
CategoricalIndex([u'a', u'a', u'a', u'e'], categories=[u'a', u'b', u'c', u'd', u'e'], ordered=False, name=u'B', dtype='category')


# Reshaping and Comparison operations on a CategoricalIndex 
#must have the same categories or a TypeError will be raised. 


df3 = pd.DataFrame({'A' : np.arange(6),                    
        'B' : pd.Series(list('aabbca')).astype('category')})
df3 = df3.set_index('B')

>>> df3.index
CategoricalIndex([u'a', u'a', u'b', u'b', u'c', u'a'], categories=[u'a', u'b', u'c'], ordered=False, name=u'B', dtype='category')

>>> pd.concat([df2, df3]
TypeError: categories must match existing categories when appending



##Index Objects - Float64Index 
#for float values, (similarly Int64Index and RangeIndex for int values )in index


indexf = pd.Index([1.5, 2, 3, 4.5, 5])
>>> indexf
Float64Index([1.5, 2.0, 3.0, 4.5, 5.0], dtype='float64')

>>> sf = pd.Series(range(5), index=indexf)

>>> sf

1.5    0
2.0    1
3.0    2
4.5    3
5.0    4
dtype: int64


#Scalar selection for [],.ix,.loc will always be label based.
#An integer will match an equal float index (e.g. 3 is equivalent to 3.0)

>>> sf[3]
2

>>> sf[3.0]
2

>>> sf.ix[3]
2

>>> sf.ix[3.0]
2

>>> sf.loc[3]
2

>>> sf.loc[3.0]
2


#The only positional indexing is via iloc
>>> sf.iloc[3]
3


#Slicing is ALWAYS on the values of the index, for [],ix,loc
#and ALWAYS positional with iloc


>>> sf[2:4]
2.0    1
3.0    2
dtype: int64

>>> sf.ix[2:4]
2.0    1
3.0    2
dtype: int64

>>> sf.loc[2:4]
2.0    1
3.0    2
dtype: int64

>>> sf.iloc[2:4]
3.0    2
4.5    3
dtype: int64


#In float indexes, slicing using floats is allowed
>>> sf[2.1:4.6]
3.0    2
4.5    3
dtype: int64

>>> sf.loc[2.1:4.6]
3.0    2
4.5    3
dtype: int64


#In non-float indexes, slicing using floats will raise a TypeError

>>> pd.Series(range(5))[3.5]
TypeError: the label [3.5] is not a proper indexer for this index type (Int64Index)


# .ix with a float indexer on a non-float index, will be label based, and thus coerce the index.


s2 = pd.Series([1, 2, 3], index=list('abc'))
>>> s2

a    1
b    2
c    3
dtype: int64

>>> s2.ix[1.0] = 10
>>> s2

a       1
b       2
c       3
1.0    10
dtype: int64



##Index Objects - IntervalIndex

df = pd.DataFrame({'A': [1, 2, 3, 4]},
        index=pd.IntervalIndex.from_breaks([0, 1, 2, 3, 4]))

>>> df
        A
(0, 1]  1
(1, 2]  2
(2, 3]  3
(3, 4]  4


#Label based indexing via .loc along the edges of an interval works as
#selecting that particular interval.
>>> df.loc[2]
A    2
Name: (1, 2], dtype: int64

>>> df.loc[[2, 3]]
        A
(1, 2]  2
(2, 3]  3


#If you select a lable contained within an interval, 
#this will also select the interval.
>>> df.loc[2.5]
A    3
Name: (2, 3], dtype: int64

>>> df.loc[[2.5, 3.5]]
        A
(2, 3]  3
(3, 4]  4













###Pandas - DF - Time and  Date functionality
#Uses the NumPy datetime64 and timedelta64 
#Class          Remarks                             How to create
Timestamp       Represents a single timestamp       to_datetime, Timestamp 
DatetimeIndex   Sequence of Timestamp               to_datetime, date_range, bdate_range, DatetimeIndex 
Period          Represents a single time span       Period 
PeriodIndex     Sequence of Period                  period_range, PeriodIndex 
Timedelta       Represents difference in Timestamp  Timedelta
TimedeltaIndex  Sequence of Timedelta               timedelta_range, to_timedelta, TimedeltaIndex
Interval        bounded slice-like interval         Interval 
IntervalIndex   Sequence of Interval                interval_range, IntervalIndex 


##Timestamp is equivalent to datetime.datetime functionality 
#Sequence of Timestamp is DatetimeIndex 
>>> pd.Timestamp(datetime(2012, 5, 1))
Timestamp('2012-05-01 00:00:00')

>>> pd.Timestamp('2012-05-01')
Timestamp('2012-05-01 00:00:00')

>>> pd.Timestamp(2012, 5, 1)
Timestamp('2012-05-01 00:00:00')

dates = [pd.Timestamp('2012-05-01'), pd.Timestamp('2012-05-02'), pd.Timestamp('2012-05-03')]
ts = pd.Series(np.random.randn(3), dates)
>>> ts.index
DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None)
>>> pd.to_datetime(pd.Series(['Jul 31, 2009', '2010-01-10', None]))
0   2009-07-31
1   2010-01-10
2          NaT
dtype: datetime64[ns]
>>> pd.to_datetime(['2005/11/23', '2010.12.31'])
DatetimeIndex(['2005-11-23', '2010-12-31'], dtype='datetime64[ns]', freq=None)

>>> pd.date_range(datetime(2011, 1, 1), periods=1000, freq='M')



##Period is time span  
##Sequence of Period is PeriodIndex 
>>> pd.Period('2011-01')
Period('2011-01', 'M')

>>> pd.Period('2012-05', freq='D')
Period('2012-05-01', 'D')

periods = [pd.Period('2012-01'), pd.Period('2012-02'), pd.Period('2012-03')]
ts = pd.Series(np.random.randn(3), periods)
>>> ts.index
PeriodIndex(['2012-01', '2012-02', '2012-03'], dtype='period[M]', freq='M')
>>> pd.period_range(start='2017-01-01', end='2018-01-01', freq='M')
PeriodIndex(['2017-01', '2017-02', '2017-03', '2017-04', '2017-05',
             '2017-06', '2017-06', '2017-07', '2017-08', '2017-09',
             '2017-10', '2017-11', '2017-12', '2018-01'],
            dtype='period[M]', freq='M')


##Timedelta is a subclass of datetime.timedelta
#Timedelta are differences in times, can be used in arithmatic operations with Timestamp 
#DateOffsets (Day, Hour, Minute, Second, Milli, Micro, Nano) can be used in TimeDelta 
#Sequence of Timedelta is TimeDeltaIndex 
>>> pd.Timedelta('-1 days 2 min 3us')
imedelta('-2 days +23:57:59.999997')
>>> pd.Timedelta(days=1, seconds=1)
Timedelta('1 days 00:00:01')
>>> pd.Timedelta(1, unit='d')
Timedelta('1 days 00:00:00')
>>> pd.Timedelta(Second(2))
Timedelta('0 days 00:00:02')
>>> pd.to_timedelta(['1 days 06:05:01.00003', '15.5us', 'nan'])
TimedeltaIndex(['1 days 06:05:01.000030', '0 days 00:00:00.000015', NaT], 
>>> pd.timedelta_range(start='1 day', periods=4)
TimedeltaIndex(['1 days', '2 days', '3 days', '4 days'],
               dtype='timedelta64[ns]', freq='D')

               
##Interval is a bounded slice-like interval.
#Sequence of Interval is IntervalIndex 
iv = pd.Interval(left=0, right=5)
>>> iv
Interval(0, 5, closed='right')
>>> 2.5 in iv
True
year_2017 = pd.Interval(pd.Timestamp('2017-01-01'),
                        pd.Timestamp('2017-12-31'), closed='both')
>>> pd.Timestamp('2017-01-01 00:00') in year_2017
True

>>> pd.interval_range(start=0, end=5)
IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]]
              closed='right', dtype='interval[int64]')

>>> pd.interval_range(start=pd.Timestamp('2017-01-01'),
                      periods=3, freq='MS')
IntervalIndex([(2017-01-01, 2017-02-01], (2017-02-01, 2017-03-01],
               (2017-03-01, 2017-04-01]]
              closed='right', dtype='interval[datetime64[ns]]')
             
              
##index - Timestamp 
class pandas.Timestamp
    TimeStamp is the pandas equivalent of python's Datetime
    It's the type used for the entries that make up a DatetimeIndex, 
    and other timeseries oriented data structures in pandas.
    Most of DatetimeIndex.attributes/methods are available on this 
    If a Series is of datetime, access attributes via pandas.Series.dt.date 
    #option 1 for Creation 
    ts_input : datetime-like, str, int, float
        Value to be converted to Timestamp
    freq : str, DateOffset
        Offset which Timestamp will have
    tz : string, pytz.timezone, dateutil.tz.tzfile or None
        Time zone for time which Timestamp will have.
    unit : string
        numpy unit used for conversion, if ts_input is int or float
    #Option-2 for creation , like datetime.datetime 
    year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]
    
##Timestamp - Properties
Timestamp.asm8  
Timestamp.day  
Timestamp.dayofweek  
Timestamp.dayofyear  
Timestamp.days_in_month  
Timestamp.daysinmonth  
Timestamp.hour  
Timestamp.is_leap_year  
Timestamp.is_month_end  
Timestamp.is_month_start  
Timestamp.is_quarter_end  
Timestamp.is_quarter_start  
Timestamp.is_year_end  
Timestamp.is_year_start  
Timestamp.max  
Timestamp.microsecond  
Timestamp.min  
Timestamp.month  
Timestamp.nanosecond  
Timestamp.quarter  
Timestamp.resolution  
Timestamp.second  
Timestamp.tz Alias for tzinfo 
Timestamp.tzinfo  
Timestamp.value  
Timestamp.weekday_name  
Timestamp.weekofyear  
Timestamp.year  
##Timestamp - Methods
Timestamp.astimezone    Convert tz-aware Timestamp to another time zone. 
Timestamp.ceil          return a new Timestamp ceiled to this resolution 
Timestamp.combine  
Timestamp.ctime         Return ctime() style string. 
Timestamp.date          Return date object with same year, month and day. 
Timestamp.dst           Return self.tzinfo.dst(self). 
Timestamp.floor         return a new Timestamp floored to this resolution 
Timestamp.freq  
Timestamp.freqstr  
Timestamp.fromordinal   passed an ordinal, translate and convert to a ts 
Timestamp.fromtimestamp  
Timestamp.isocalendar   Return a 3-tuple containing ISO year, week number, and weekday. 
Timestamp.isoformat  
Timestamp.isoweekday    Return the day of the week represented by the date. 
Timestamp.normalize     Normalize Timestamp to midnight, preserving tz information. 
Timestamp.now           Return the current time in the local timezone. 
Timestamp.replace       implements datetime.replace, handles nanoseconds 
Timestamp.round         Round the Timestamp to the specified resolution 
Timestamp.strftime      format -> strftime() style string. 
Timestamp.strptime      string, format -> new datetime parsed from a string (like time.strptime()). 
Timestamp.time          Return time object with same time but with tzinfo=None. 
Timestamp.timestamp     Return POSIX timestamp as float. 
Timestamp.timetuple     Return time tuple, compatible with time.localtime(). 
Timestamp.timetz        Return time object with same time and tzinfo. 
Timestamp.to_datetime64     Returns a numpy.datetime64 object with 'ns' precision 
Timestamp.to_julian_date    Convert TimeStamp to a Julian Date. 
Timestamp.to_period         Return an period of which this timestamp is an observation. 
Timestamp.to_pydatetime     Convert a Timestamp object to a native Python datetime object. 
Timestamp.today             Return the current time in the local timezone. 
Timestamp.toordinal         Return proleptic Gregorian ordinal. 
Timestamp.tz_convert        Convert tz-aware Timestamp to another time zone. 
Timestamp.tz_localize       Convert naive Timestamp to local time zone, or remove timezone from tz-aware Timestamp. 
Timestamp.tzname            Return self.tzinfo.tzname(self). 
Timestamp.utcfromtimestamp  
Timestamp.utcnow  
Timestamp.utcoffset         Return self.tzinfo.utcoffset(self). 
Timestamp.utctimetuple      Return UTC time tuple, compatible with time.localtime(). 
Timestamp.weekday           Return the day of the week represented by the date. 


##index - Interval
class pandas.Interval
    Immutable object implementing an Interval, a bounded slice-like interval.
    Used for implementing IntervalIndex
    Parameters:
    left : value
        Left bound for interval.
    right : value
        Right bound for interval.
    closed : {'left', 'right', 'both', 'neither'}
        Whether the interval is closed on the left-side, right-side, both or neither. Defaults to 'right'.
 

##index - IntervalProperties
Interval.closed  
Interval.closed_left  
Interval.closed_right  
Interval.left  
Interval.mid  
Interval.open_left  
Interval.open_right  
Interval.right 
 
##index - Timedelta
class pandas.Timedelta
    Represents a duration, the difference between two dates or times.
    Timedelta is the pandas equivalent of python's datetime.timedelta 
    and is interchangable with it in most cases.
    Parameters:
    value : Timedelta, timedelta, np.timedelta64, string, or integer
    unit : string, [D,h,m,s,ms,us,ns]
        Denote the unit of the input, if input is an integer. Default 'ns'.
    days, seconds, microseconds,
    milliseconds, minutes, hours, weeks : numeric, optional
        Values for construction in compat with datetime.timedelta. 
        np ints and floats will be coereced to python ints and floats.
 
##index - Timedelta Properties
Timedelta.asm8 return a numpy timedelta64 array view of myself 
Timedelta.components Return a Components NamedTuple-like 
Timedelta.days              Number of Days 
Timedelta.freq  
Timedelta.max  
Timedelta.microseconds      Number of microseconds (>= 0 and less than 1 second). 
Timedelta.min  
Timedelta.nanoseconds       Number of nanoseconds (>= 0 and less than 1 microsecond). 
Timedelta.resolution        return a string representing the lowest resolution that we have 
Timedelta.seconds           Number of seconds (>= 0 and less than 1 day). 
Timedelta.value  
##index - Timedelta-Methods
Timedelta.ceil              return a new Timedelta ceiled to this resolution 
Timedelta.floor             return a new Timedelta floored to this resolution 
Timedelta.isoformat         Format Timedelta as ISO 8601 Duration like P[n]Y[n]M[n]DT[n]H[n]M[n]S, where the `[n]`s are replaced by the values. 
Timedelta.round             Round the Timedelta to the specified resolution 
Timedelta.to_pytimedelta    return an actual datetime.timedelta object 
Timedelta.to_timedelta64    Returns a numpy.timedelta64 object with 'ns' precision 
Timedelta.total_seconds     Total duration of timedelta in seconds (to ns precision) 



##index - Period
class pandas.Period
    Represents a period of time
    Parameters:
    value : Period or compat.string_types, default None
        The time period represented (e.g., '4Q2005')
    freq : str, default None
        One of pandas period strings or corresponding objects
    year : int, default None
    month : int, default 1
    quarter : int, default None
    day : int, default 1
    hour : int, default 0
    minute : int, default 0
    second : int, default 0 
##index - PeriodAttributes
Period.day  
Period.dayofweek  
Period.dayofyear  
Period.days_in_month  
Period.daysinmonth  
Period.end_time  
Period.freq  
Period.freqstr  
Period.hour  
Period.is_leap_year  
Period.minute  
Period.month  
Period.ordinal  
Period.quarter  
Period.qyear  
Period.second  
Period.start_time  
Period.week  
Period.weekday  
Period.weekofyear  
Period.year  
##index - PeriodMethods
Period.asfreq           Convert Period to desired frequency, either at the start or end of the 
Period.now  
Period.strftime         Returns the string representation of the Period, depending on the selected format. 
Period.to_timestamp     Return the Timestamp representation of the Period at the target            
              
              




##index - DatetimeIndex (subclass of Index)
class pandas.DatetimeIndex
    Immutable ndarray of datetime64 data, represented internally as int64, 
    and which can be boxed to Timestamp objects that are subclasses of datetime 
    and carry metadata such as frequency information.
    Parameters:
    data : array-like (1-dimensional), optional
        Optional datetime-like data to construct index with
    copy : bool
        Make a copy of input ndarray
    freq : string or pandas offset object, optional
        One of pandas date offset strings or corresponding objects
    start : starting value, datetime-like, optional
        If data is None, start is used as the start point in generating regular timestamp data.
    periods : int, optional, > 0
        Number of periods to generate, if generating index. 
        Takes precedence over end argument
    end : end time, datetime-like, optional
        If periods is none, generated index will extend to first conforming time on or just past end argument
    closed : string or None, default None
        Make the interval closed with respect to the given frequency to the 'left', 'right', or both sides (None)
    tz : pytz.timezone or dateutil.tz.tzfile
    ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'
        •'infer' will attempt to infer fall dst-transition hours based on order
        •bool-ndarray where True signifies a DST time, False signifies a non-DST time (note that this flag is only applicable for ambiguous times)
        •'NaT' will return NaT where there are ambiguous times
        •'raise' will raise an AmbiguousTimeError if there are ambiguous times
    infer_dst : boolean, default False
        Deprecated since version 0.15.0: Attempt to infer fall dst-transition hours based on order
    name : object
        Name to be stored in the index
        



 
##index - DatetimeIndex Components
DatetimeIndex.year          The year of the datetime 
DatetimeIndex.month         The month as January=1, December=12 
DatetimeIndex.day           The days of the datetime 
DatetimeIndex.hour          The hours of the datetime 
DatetimeIndex.minute        The minutes of the datetime 
DatetimeIndex.second        The seconds of the datetime 
DatetimeIndex.microsecond   The microseconds of the datetime 
DatetimeIndex.nanosecond    The nanoseconds of the datetime 
DatetimeIndex.date          Returns numpy array of python datetime.date objects (namely, the date part of Timestamps without timezone information). 
DatetimeIndex.time          Returns numpy array of datetime.time. 
DatetimeIndex.dayofyear     The ordinal day of the year 
DatetimeIndex.weekofyear    The week ordinal of the year 
DatetimeIndex.week          The week ordinal of the year 
DatetimeIndex.dayofweek     The day of the week with Monday=0, Sunday=6 
DatetimeIndex.weekday       The day of the week with Monday=0, Sunday=6 
DatetimeIndex.weekday_name  The name of day in a week (ex: Friday) 
DatetimeIndex.quarter       The quarter of the date 
DatetimeIndex.tz  
DatetimeIndex.freq          get/set the frequency of the Index 
DatetimeIndex.freqstr       Return the frequency object as a string if its set, otherwise None 

DatetimeIndex.is_month_start    Logical indicating if first day of month (defined by frequency) 
DatetimeIndex.is_month_end      Logical indicating if last day of month (defined by frequency) 
DatetimeIndex.is_quarter_start  Logical indicating if first day of quarter (defined by frequency) 
DatetimeIndex.is_quarter_end    Logical indicating if last day of quarter (defined by frequency) 
DatetimeIndex.is_year_start     Logical indicating if first day of year (defined by frequency) 
DatetimeIndex.is_year_end       Logical indicating if last day of year (defined by frequency) 
DatetimeIndex.is_leap_year      Logical indicating if the date belongs to a leap year 
DatetimeIndex.inferred_freq  

##index - DatetimeIndex Selecting
DatetimeIndex.indexer_at_time(time[, asof])     Select values at particular time of day (e.g. 
DatetimeIndex.indexer_between_time(...[, ...])  Select values between particular times of day (e.g., 9:00-9:30AM). 
Time-specific operations
DatetimeIndex.normalize()                   Return DatetimeIndex with times to midnight. 
DatetimeIndex.strftime(date_format)         Return an array of formatted strings specified by date_format, which supports the same string format as the python standard library. 
DatetimeIndex.snap([freq])                  Snap time stamps to nearest occurring frequency 
DatetimeIndex.tz_convert(tz)                Convert tz-aware DatetimeIndex from one time zone to another (using 
DatetimeIndex.tz_localize(tz[, ambiguous, ...]) Localize tz-naive DatetimeIndex to given time zone (using 
DatetimeIndex.round(freq, *args, **kwargs)  round the index to the specified freq 
DatetimeIndex.floor(freq)                   floor the index to the specified freq 
DatetimeIndex.ceil(freq)                    ceil the index to the specified freq 

##index - DatetimeIndex Conversion
DatetimeIndex.to_datetime([dayfirst])  
DatetimeIndex.to_period([freq])         Cast to PeriodIndex at a particular frequency 
DatetimeIndex.to_perioddelta(freq)      Calcuates TimedeltaIndex of difference between index values and index converted to PeriodIndex at specified freq. 
DatetimeIndex.to_pydatetime()           Return DatetimeIndex as object ndarray of datetime.datetime objects 
DatetimeIndex.to_series([keep_tz])      Create a Series with both index and values equal to the index keys 
DatetimeIndex.to_frame([index])         Create a DataFrame with a column containing the Index. 

##index - TimedeltaIndex (subclass of Index)
class pandas.TimedeltaIndex
    Immutable ndarray of timedelta64 data, represented internally as int64, 
    and which can be boxed to timedelta objects
    Parameters:
    data : array-like (1-dimensional), optional
        Optional timedelta-like data to construct index with
    unit: unit of the arg (D,h,m,s,ms,us,ns) denote the unit, optional
    freq: a frequency for the index, optional
    copy : bool
        Make a copy of input ndarray
    start : starting value, timedelta-like, optional
        If data is None, start is used as the start point in generating regular timedelta data.
    periods : int, optional, > 0
        Number of periods to generate, if generating index. 
        Takes precedence over end argument
    end : end time, timedelta-like, optional
        If periods is none, generated index will extend to first conforming time on or just past end argument
    closed : string or None, default None
        Make the interval closed with respect to the given frequency to the 'left', 'right', or both sides (None)
    name : object
        Name to be stored in the index
     
##index - TimedeltaIndex Components
#Note below operates on each element returning another Index/Series 
TimedeltaIndex.days         Number of days for each element. 
TimedeltaIndex.seconds      Number of seconds (>= 0 and less than 1 day) for each element. 
TimedeltaIndex.microseconds Number of microseconds (>= 0 and less than 1 second) for each element. 
TimedeltaIndex.nanoseconds  Number of nanoseconds (>= 0 and less than 1 microsecond) for each element. 
TimedeltaIndex.components   Return a dataframe of the components (days, hours, minutes, seconds, milliseconds, microseconds, nanoseconds) of the Timedeltas. 
TimedeltaIndex.inferred_freq  

##index - TimedeltaIndex Conversion
TimedeltaIndex.to_pytimedelta()     Return TimedeltaIndex as object ndarray of datetime.timedelta objects 
TimedeltaIndex.to_series(**kwargs)  Create a Series with both index and values equal to the index keys 
TimedeltaIndex.round(freq, *args, **kwargs) round the index to the specified freq 
TimedeltaIndex.floor(freq)          floor the index to the specified freq 
TimedeltaIndex.ceil(freq)           ceil the index to the specified freq 
TimedeltaIndex.to_frame([index])    Create a DataFrame with a column containing the Index. 

##index - PeriodIndex (subclass of Index)
class pandas.PeriodIndex
    Immutable ndarray holding ordinal values indicating regular periods in time 
    such as particular years, quarters, months, etc.
    Index keys are boxed to Period objects which carries the metadata 
    (eg, frequency information).
    Parameters:
    data : array-like (1-dimensional), optional
        Optional period-like data to construct index with
    copy : bool
        Make a copy of input ndarray
    freq : string or period object, optional
        One of pandas period strings or corresponding objects
    start : starting value, period-like, optional
        If data is None, used as the start point in generating regular period data.
    periods : int, optional, > 0
        Number of periods to generate, if generating index. 
        Takes precedence over end argument
    end : end value, period-like, optional
        If periods is none, generated index will extend to first conforming period on or just past end argument
    year : int, array, or Series, default None
    month : int, array, or Series, default None
    quarter : int, array, or Series, default None
    day : int, array, or Series, default None
    hour : int, array, or Series, default None
    minute : int, array, or Series, default None
    second : int, array, or Series, default None
    tz : object, default None
    Timezone for converting datetime64 data to Periods
    dtype : str or PeriodDtype, default None

 
##index - PeriodIndexAttributes
#Note below operates on each element returning another Index/Series 
PeriodIndex.day             The days of the period 
PeriodIndex.dayofweek       The day of the week with Monday=0, Sunday=6 
PeriodIndex.dayofyear       The ordinal day of the year 
PeriodIndex.days_in_month   The number of days in the month 
PeriodIndex.daysinmonth     The number of days in the month 
PeriodIndex.end_time  
PeriodIndex.freq  
PeriodIndex.freqstr         Return the frequency object as a string if its set, otherwise None 
PeriodIndex.hour            The hour of the period 
PeriodIndex.is_leap_year    Logical indicating if the date belongs to a leap year 
PeriodIndex.minute          The minute of the period 
PeriodIndex.month           The month as January=1, December=12 
PeriodIndex.quarter         The quarter of the date 
PeriodIndex.qyear  
PeriodIndex.second          The second of the period 
PeriodIndex.start_time  
PeriodIndex.week            The week ordinal of the year 
PeriodIndex.weekday         The day of the week with Monday=0, Sunday=6 
PeriodIndex.weekofyear      The week ordinal of the year 
PeriodIndex.year            The year of the period 

##index - PeriodIndexMethods
PeriodIndex.asfreq([freq, how])         Convert the PeriodIndex to the specified frequency freq. 
PeriodIndex.strftime(date_format)       Return an array of formatted strings specified by date_format, which supports the same string format as the python standard library. 
PeriodIndex.to_timestamp([freq, how])   Cast to DatetimeIndex 
PeriodIndex.tz_convert(tz)              Convert tz-aware DatetimeIndex from one time zone to another (using 
PeriodIndex.tz_localize(tz[, infer_dst]) Localize tz-naive DatetimeIndex to given time zone (using                 


##index - creation of Datetime Time/Period related Index 
pandas.to_datetime(arg[, errors, dayfirst, ...]) Convert argument to datetime. 
pandas.to_timedelta(arg[, unit, box, errors]) Convert argument to timedelta 
pandas.date_range([start, end, periods, freq, tz, ...]) Return a fixed frequency DatetimeIndex, with day (calendar) as the default 
pandas.bdate_range([start, end, periods, freq, tz, ...]) Return a fixed frequency DatetimeIndex, with business day as the default 
pandas.period_range([start, end, periods, freq, name]) Return a fixed frequency PeriodIndex, with day (calendar) as the default 
pandas.timedelta_range([start, end, periods, freq, ...]) Return a fixed frequency TimedeltaIndex, with day as the default 
pandas.infer_freq(index[, warn]) Infer the most likely frequency given the input index. 
pandas.interval_range([start, end, periods, freq, ...]) Return a fixed frequency IntervalIndex 


pandas.to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False, utc=None, box=True, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix')
    Convert argument to datetime.
    Returns:
        ret : datetime if parsing succeeded.
        Return type depends on input:
        •list-like: DatetimeIndex
        •Series: Series of datetime64 dtype
        •scalar: Timestamp
    Parameters:
    arg : integer, float, string, datetime, list, tuple, 1-d array, Series
    errors : {'ignore', 'raise', 'coerce'}, default 'raise'
        •If 'raise', then invalid parsing will raise an exception
        •If 'coerce', then invalid parsing will be set as NaT
        •If 'ignore', then invalid parsing will return the input
    dayfirst : boolean, default False
        Specify a date parse order if arg is str or its list-likes. 
        If True, parses dates with the day first, eg 10/11/12 is parsed as 2012-11-10. 
    yearfirst : boolean, default False
        Specify a date parse order if arg is str or its list-likes.
        •If True parses dates with the year first, eg 10/11/12 is parsed as 2010-11-12.
        •If both dayfirst and yearfirst are True, yearfirst is preceded (same as dateutil).
    utc : boolean, default None
        Return UTC DatetimeIndex if True 
    box : boolean, default True
        •If True returns a DatetimeIndex
        •If False returns ndarray of values.
    format : string, default None
        strftime to parse time, 
        eg '%d/%m/%Y', note that '%f' will parse all the way up to nanoseconds.
    exact : boolean, True by default
        •If True, require an exact format match.
        •If False, allow the format to match anywhere in the target string.
    unit : string, default 'ns'
        unit of the arg (D,s,ms,us,ns) denote the unit, 
        which is an integer or float number. 
        This will be based off the origin. 
        Example, with unit='ms' and origin='unix' (the default), 
        this would calculate the number of milliseconds to the unix epoch start.
    infer_datetime_format : boolean, default False
        If True and no format is given, attempt to infer the format of the datetime strings, 
        and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by ~5-10x.
    origin : scalar, default is 'unix'
        Define the reference date. The numeric values would be parsed as number of units (defined by unit) since this reference date.
        •If 'unix' (or POSIX) time; origin is set to 1970-01-01.
        •If 'julian', unit must be 'D', and origin is set to beginning of Julian Calendar. Julian day number 0 is assigned to the day starting at noon on January 1, 4713 BC.
        •If Timestamp convertible, origin is set to Timestamp identified by origin.
 
#common format 
    %w  Weekday as a decimal number, where 0 is Sunday and 6 is Saturday. 0, 1, …, 6   
    %d  Day of the month as a zero-padded decimal number. 01, 02, …, 31 
    %b  Month as locale's abbreviated name. Jan, Feb, …, Dec (en_US);
    %B  Month as locale's full name. January, February, …, December (en_US);
    %m  Month as a zero-padded decimal number. 01, 02, …, 12   
    %y  Year without century as a zero-padded decimal number. 00, 01, …, 99   
    %Y  Year with century as a decimal number. 1970, 1988, 2001, 2013   
    %H  Hour (24-hour clock) as a zero-padded decimal number. 00, 01, …, 23   
    %I  Hour (12-hour clock) as a zero-padded decimal number. 01, 02, …, 12   
    %p  Locale's equivalent of either AM or PM. AM, PM (en_US);
    %M  Minute as a zero-padded decimal number. 00, 01, …, 59   
    %S  Second as a zero-padded decimal number. 00, 01, …, 59 
    %f  Microsecond as a decimal number, zero-padded on the left. 000000, 000001, …, 999999 
    %z  UTC offset in the form +HHMM or -HHMM (empty string if the the object is naive). (empty), +0000, -0400, +1030 
    %Z  Time zone name (empty string if the object is naive). (empty), UTC, EST, CST   
    %j  Day of the year as a zero-padded decimal number. 001, 002, …, 366   
    %U  Week number of the year (Sunday as the first day of the week) as a zero padded decimal number. All days in a new year preceding the first Sunday are considered to be in week 0. 00, 01, …, 53 
    %W  Week number of the year (Monday as the first day of the week) as a decimal number. All days in a new year preceding the first Monday are considered to be in week 0. 00, 01, …, 53 

#Examples
#Converion of DF columns to datetime 
#The keys can be common abbreviations like 
#['year', 'month', 'day', 'minute', 'second', 'ms', 'us', 'ns']) 
#or plurals of the same
>>> df = pd.DataFrame({'year': [2015, 2016],
                       'month': [2, 3],
                       'day': [4, 5]})
>>> pd.to_datetime(df)
0   2015-02-04
1   2016-03-05
dtype: datetime64[ns]


>>> pd.to_datetime(1490195805, unit='s')
Timestamp('2017-03-22 15:16:45')
>>> pd.to_datetime(1490195805433502912, unit='ns')
Timestamp('2017-03-22 15:16:45.433502912')

#Using a non-unix epoch origin
>>> pd.to_datetime([1, 2, 3], unit='D',
                   origin=pd.Timestamp('1960-01-01'))
0    1960-01-02
1    1960-01-03
2    1960-01-04



pandas.to_timedelta(arg, unit='ns', box=True, errors='raise')
    Convert argument to timedelta
    Parameters:
    arg : string, timedelta, list, tuple, 1-d array, or Series
    unit : unit of the arg (D,h,m,s,ms,us,ns) denote the unit, which is an
    integer/float number
    box : boolean, default True
        •If True returns a Timedelta/TimedeltaIndex of the results
        •if False returns a np.timedelta64 or ndarray of values of dtype timedelta64[ns]
    errors : {'ignore', 'raise', 'coerce'}, default 'raise'
        •If 'raise', then invalid parsing will raise an exception
        •If 'coerce', then invalid parsing will be set as NaT
        •If 'ignore', then invalid parsing will return the input
     
#Examples
#Parsing a single string to a Timedelta:
>>> pd.to_timedelta('1 days 06:05:01.00003')
Timedelta('1 days 06:05:01.000030')
>>> pd.to_timedelta('15.5us')
Timedelta('0 days 00:00:00.000015')

#Parsing a list or array of strings:
>>> pd.to_timedelta(['1 days 06:05:01.00003', '15.5us', 'nan'])
TimedeltaIndex(['1 days 06:05:01.000030', '0 days 00:00:00.000015', NaT],
               dtype='timedelta64[ns]', freq=None)

#Converting numbers by specifying the unit keyword argument:
>>> pd.to_timedelta(np.arange(5), unit='s')
TimedeltaIndex(['00:00:00', '00:00:01', '00:00:02',
                '00:00:03', '00:00:04'],
               dtype='timedelta64[ns]', freq=None)
>>> pd.to_timedelta(np.arange(5), unit='d')
TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],
               dtype='timedelta64[ns]', freq=None)

   
   
pandas.period_range(start=None, end=None, periods=None, freq='D', name=None)
    Return a fixed frequency PeriodIndex, with day (calendar) 
    as the default frequency
    Parameters:
    start : string or period-like, default None
        Left bound for generating periods
    end : string or period-like, default None
        Right bound for generating periods
    periods : integer, default None
        Number of periods to generate
    freq : string or DateOffset, default 'D' (calendar daily)
        Frequency alias
    name : string, default None
        Name of the resulting PeriodIndex
#Of the three parameters: start, end, and periods, exactly two must be specified.
#Examples
>>> pd.period_range(start='2017-01-01', end='2018-01-01', freq='M')
PeriodIndex(['2017-01', '2017-02', '2017-03', '2017-04', '2017-05',
             '2017-06', '2017-06', '2017-07', '2017-08', '2017-09',
             '2017-10', '2017-11', '2017-12', '2018-01'],
            dtype='period[M]', freq='M')
            
#If start or end are Period objects, 
#they will be used as anchor endpoints for a PeriodIndex 
#with frequency matching that of the period_range constructor.
>>> pd.period_range(start=pd.Period('2017Q1', freq='Q'),
               end=pd.Period('2017Q2', freq='Q'), freq='M')
PeriodIndex(['2017-03', '2017-04', '2017-05', '2017-06'],
            dtype='period[M]', freq='M')
            
            
            
pandas.timedelta_range(start=None, end=None, periods=None, freq='D', name=None, closed=None)
    Return a fixed frequency TimedeltaIndex, with day as the default frequency
    Parameters:
    start : string or timedelta-like, default None
        Left bound for generating timedeltas
    end : string or timedelta-like, default None
        Right bound for generating timedeltas
    periods : integer, default None
        Number of periods to generate
    freq : string or DateOffset, default 'D' (calendar daily)
        Frequency strings can have multiples, e.g. '5H'
    name : string, default None
        Name of the resulting TimedeltaIndex
    closed : string, default None
        Make the interval closed with respect to the given frequency to the 'left', 'right', or both sides (None)
#Of the three parameters: start, end, and periods, exactly two must be specified.
#Examples
>>> pd.timedelta_range(start='1 day', periods=4)
TimedeltaIndex(['1 days', '2 days', '3 days', '4 days'],
               dtype='timedelta64[ns]', freq='D')

#The closed parameter specifies which endpoint is included. 
#The default behavior is to include both endpoints.
>>> pd.timedelta_range(start='1 day', periods=4, closed='right')
TimedeltaIndex(['2 days', '3 days', '4 days'],
               dtype='timedelta64[ns]', freq='D')
               
#The freq parameter specifies the frequency of the TimedeltaIndex. 
#Only fixed frequencies can be passed, 
#non-fixed frequencies such as 'M' (month end) will raise.
>>> pd.timedelta_range(start='1 day', end='2 days', freq='6H')
TimedeltaIndex(['1 days 00:00:00', '1 days 06:00:00', '1 days 12:00:00',
                '1 days 18:00:00', '2 days 00:00:00'],
               dtype='timedelta64[ns]', freq='6H')

               
               
               
pandas.interval_range(start=None, end=None, periods=None, freq=None, name=None, closed='right')
    Return a fixed frequency IntervalIndex
    Parameters:
    start : numeric or datetime-like, default None
        Left bound for generating intervals
    end : numeric or datetime-like, default None
        Right bound for generating intervals
    periods : integer, default None
        Number of periods to generate
    freq : numeric, string, or DateOffset, default None
        The length of each interval. 
        Must be consistent with the type of start and end, e.g. 2 for numeric, or '5H' for datetime-like. Default is 1 for numeric and 'D' (calendar daily) for datetime-like.
    name : string, default None
        Name of the resulting IntervalIndex
    closed : string, default 'right'
        options are: 'left', 'right', 'both', 'neither'
#Of the three parameters: start, end, and periods, exactly two must be specified.
#Examples
#Numeric start and end is supported.
>>> pd.interval_range(start=0, end=5)
IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]]
              closed='right', dtype='interval[int64]')
              
#Additionally, datetime-like input is also supported.
>>> pd.interval_range(start=pd.Timestamp('2017-01-01'),
                      end=pd.Timestamp('2017-01-04'))
IntervalIndex([(2017-01-01, 2017-01-02], (2017-01-02, 2017-01-03],
               (2017-01-03, 2017-01-04]]
              closed='right', dtype='interval[datetime64[ns]]')
              
#The freq parameter specifies the frequency between the left and right. 
#endpoints of the individual intervals within the IntervalIndex. 
#For numeric start and end, the frequency must also be numeric.
>>> pd.interval_range(start=0, periods=4, freq=1.5)
IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]]
              closed='right', dtype='interval[float64]')
              
#Similarly, for datetime-like start and end, 
#the frequency must be convertible to a DateOffset.
>>> pd.interval_range(start=pd.Timestamp('2017-01-01'),
                      periods=3, freq='MS')
IntervalIndex([(2017-01-01, 2017-02-01], (2017-02-01, 2017-03-01],
               (2017-03-01, 2017-04-01]]
              closed='right', dtype='interval[datetime64[ns]]')
              
#The closed parameter specifies which endpoints of the individual intervals within the IntervalIndex are closed.
>>> pd.interval_range(end=5, periods=4, closed='both')
IntervalIndex([[1, 2], [2, 3], [3, 4], [4, 5]]
              closed='both', dtype='interval[int64]')


              
              
pandas.date_range(start=None, end=None, periods=None, freq='D', tz=None, normalize=False, name=None, closed=None, **kwargs)[source]
    Return a fixed frequency DatetimeIndex, 
    with day (calendar) as the default frequency
    Of the three parameters: start, end, and periods, exactly two must be specified
    Parameters:
    start : string or datetime-like, default None
        Left bound for generating dates
    end : string or datetime-like, default None
        Right bound for generating dates
    periods : integer, default None
        Number of periods to generate
    freq : string or DateOffset, default 'D' (calendar daily)
        Frequency strings can have multiples, e.g. '5H'
    tz : string, default None
        Time zone name for returning localized DatetimeIndex, for example Asia/Hong_Kong
    normalize : bool, default False
        Normalize start/end dates to midnight before generating date range
    name : string, default None
        Name of the resulting DatetimeIndex
    closed : string, default None
        Make the interval closed with respect to the given frequency to the 'left', 'right', or both sides (None)
    Returns:
        rng : DatetimeIndex
        
##Example 
>>> pd.date_range(datetime(2011, 1, 1), periods=1000, freq='M')

        
        
##index - DatetimeIndex - freq string 
#check http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases for offset aliases, 
#note for multiple, you can give '5H' ie every 5th hour 
#check https://github.com/pandas-dev/pandas/blob/master/doc/source/timeseries.rst#id11

#eg '2Q'  denotes two quarterly, '30min' means 30 min

#Weekly can take day to denote which day of week eg 'W-MON' means Monday of week

#Q can take Month eg 'Q-Feb' means every 2nd month of Q

#B,D,H,Min,S,MS,U,N can be combined eg '30min20s' means 30 min 20 second
#eg '2H20min30s' , '2D12H20Min30s'

#Alias      Description
B           business day frequency 
C           custom business day frequency 
D           calendar day frequency 
W           weekly frequency 
M           month end frequency 
SM          semi-month end frequency (15th and end of month) 
BM          business month end frequency 
CBM         custom business month end frequency 
MS          month start frequency 
SMS         semi-month start frequency (1st and 15th) 
BMS         business month start frequency 
CBMS        custom business month start frequency 
Q           quarter end frequency 
BQ          business quarter end frequency 
QS          quarter start frequency 
BQS         business quarter start frequency 
A, Y        year end frequency 
BA, BY      business year end frequency 
AS, YS      year start frequency 
BAS, BYS    business year start frequency 
BH          business hour frequency 
H           hourly frequency 
T, min      minutely frequency 
S           secondly frequency 
L,ms        milliseconds 
U, us       microseconds 
N           nanoseconds 
##index - DatetimeIndex - freq stringAnchored Offsets
#For some frequencies you can specify an anchoring suffix:
#For those offsets that are anchored to the start or end of specific frequency 
#(MonthEnd, MonthBegin, WeekEnd, etc) 
#the following rules apply to rolling forward and backwards.
#When n is not 0, if the given date is not on an anchor point, 
#it snapped to the next(previous) anchor point, 
#and moved |n|-1 additional steps forwards or backwards.
#Alias              Description
W-SUN               weekly frequency (Sundays). Same as 'W' 
W-MON               weekly frequency (Mondays) 
W-TUE               weekly frequency (Tuesdays) 
W-WED               weekly frequency (Wednesdays) 
W-THU               weekly frequency (Thursdays) 
W-FRI               weekly frequency (Fridays) 
W-SAT               weekly frequency (Saturdays) 
(B)Q(S)-DEC         quarterly frequency, year ends in December. Same as 'Q' 
(B)Q(S)-JAN         quarterly frequency, year ends in January 
(B)Q(S)-FEB         quarterly frequency, year ends in February 
(B)Q(S)-MAR         quarterly frequency, year ends in March 
(B)Q(S)-APR         quarterly frequency, year ends in April 
(B)Q(S)-MAY         quarterly frequency, year ends in May 
(B)Q(S)-JUN         quarterly frequency, year ends in June 
(B)Q(S)-JUL         quarterly frequency, year ends in July 
(B)Q(S)-AUG         quarterly frequency, year ends in August 
(B)Q(S)-SEP         quarterly frequency, year ends in September 
(B)Q(S)-OCT         quarterly frequency, year ends in October 
(B)Q(S)-NOV         quarterly frequency, year ends in November 
(B)A(S)-DEC         annual frequency, anchored end of December. Same as 'A' 
(B)A(S)-JAN         annual frequency, anchored end of January 
(B)A(S)-FEB         annual frequency, anchored end of February 
(B)A(S)-MAR         annual frequency, anchored end of March 
(B)A(S)-APR         annual frequency, anchored end of April 
(B)A(S)-MAY         annual frequency, anchored end of May 
(B)A(S)-JUN         annual frequency, anchored end of June 
(B)A(S)-JUL         annual frequency, anchored end of July 
(B)A(S)-AUG         annual frequency, anchored end of August 
(B)A(S)-SEP         annual frequency, anchored end of September 
(B)A(S)-OCT         annual frequency, anchored end of October 
(B)A(S)-NOV         annual frequency, anchored end of November 
##Under the hood, these frequency strings are being translated into 
#an instance of 
pandas.DateOffset(years=0, months=0, days=0, leapdays=0, weeks=0, hours=0, 
            minutes=0, seconds=0, microseconds=0, 
            year=None, month=None, day=None, weekday=None, 
            yearday=None, nlyearday=None, hour=None, minute=None, 
            second=None, microsecond=None)
    #The key features of a DateOffset object are:
        •It can be added / subtracted to/from a datetime object to obtain a shifted date.
        •It can be multiplied by an integer (positive or negative) so that the increment will be applied multiple times.
        •It has pandas.DateOffset.rollforward and pandas.DateOffset.rollback methods for moving a date forward or backward to the next or previous "offset date".
    year, month, day, hour, minute, second, microsecond:
        Absolute information , dont use them for adding or substraction 
    years, months, weeks, days, hours, minutes, seconds, microseconds:
        Relative information, may be negative ,use them for adding or substraction 
    
#SubClass name          Description
#pandas.tseries.offsets.
DateOffset              Generic offset class, defaults to 1 calendar day 
BDay                    business day (weekday) 
CDay                    custom business day 
Week                    one week, optionally anchored on a day of the week 
WeekOfMonth             the x-th day of the y-th week of each month 
LastWeekOfMonth         the x-th day of the last week of each month 
MonthEnd                calendar month end 
MonthBegin              calendar month begin 
BMonthEnd               business month end 
BMonthBegin             business month begin 
CBMonthEnd              custom business month end 
CBMonthBegin            custom business month begin 
SemiMonthEnd            15th (or other day_of_month) and calendar month end 
SemiMonthBegin          15th (or other day_of_month) and calendar month begin 
QuarterEnd              calendar quarter end 
QuarterBegin            calendar quarter begin 
BQuarterEnd             business quarter end 
BQuarterBegin           business quarter begin 
FY5253Quarter           retail (aka 52-53 week) quarter 
YearEnd                 calendar year end 
YearBegin               calendar year begin 
BYearEnd                business year end 
BYearBegin              business year begin 
FY5253                  retail (aka 52-53 week) year 
BusinessHour            business hour 
CustomBusinessHour      custom business hour 
Hour                    one hour 
Minute                  one minute 
Second                  one second 
Milli                   one millisecond 
Micro                   one microsecond 
Nano                    one nanosecond 

#Example 
from datetime import datetime 
d = datetime(2008, 8, 18, 9, 0)  #year,month,date,hr,min,s,us
from pandas.tseries.offsets import *
d + DateOffset(months=4, days=5)
d - 5 * BDay() #using subclass 
d + BMonthEnd()

#apply(for construction), rollforward and rollback preserve time 
#(hour, minute, etc) information by default
offset = BMonthEnd()
offset.rollforward(d) #Timestamp('2008-08-29 09:00:00')
offset.rollback(d) #Timestamp('2008-07-31 09:00:00')

#To reset time, use normalize=True when creating the offset instance. 
#If normalize=True, the result is normalized after the function is applied.
day = Day()
day.apply(pd.Timestamp('2014-01-01 09:00')) #Timestamp('2014-01-02 09:00:00') #year,month,day,
day = Day(normalize=True)
day.apply(pd.Timestamp('2014-01-01 09:00')) #Timestamp('2014-01-02 00:00:00')
hour = Hour()
hour.apply(pd.Timestamp('2014-01-01 22:00'))
hour = Hour(normalize=True)
hour.apply(pd.Timestamp('2014-01-01 22:00'))
hour.apply(pd.Timestamp('2014-01-01 23:00'))

##Parametric Offsets
#Some of the offsets can be "parameterized" 
#when created to result in different behaviors. 
#For example, the Week offset for generating weekly data accepts a weekday parameter 
#which results in the generated dates always lying on a particular day of the week:
d
d + Week()
d + Week(weekday=4)
(d + Week(weekday=4)).weekday()
d - Week()
#The normalize option will be effective for addition and subtraction.
d + Week(normalize=True)
d - Week(normalize=True)
#Another example is parameterizing YearEnd with the specific ending month:
d + YearEnd()
d + YearEnd(month=6)

##Offsets can be used with either a Series or DatetimeIndex 
#to apply the offset to each element.
rng = pd.date_range('2012-01-01', '2012-01-03')
s = pd.Series(rng)
rng
rng + DateOffset(months=2)
s + DateOffset(months=2)
s - DateOffset(months=2)
#If the offset class maps directly to a Timedelta (Day, Hour, Minute, Second, Micro, Milli, Nano) 
#it can be used exactly like a Timedelta
s - Day(2)
td = s - pd.Series(pd.date_range('2011-12-29', '2011-12-31'))
td
td + Minute(15)

##infer frequency and to_offset 
pandas.infer_freq(index, warn=True)
    Infer the most likely frequency given the input index. 
    If the frequency is uncertain, a warning will be printed.
    Parameters:
    index : DatetimeIndex or TimedeltaIndex
        if passed a Series will use the values of the series (NOT THE INDEX)
    warn : boolean, default True
    Returns:
    freq : string or None
        None if no discernible frequency
 
>>> pd.infer_freq(pd.date_range(start='1/1/2018', periods=20, freq='D'))
'D'


pandas.tseries.frequencies.to_offset(freq)
    Return DateOffset object from string 
    or tuple representation or datetime.timedelta object
    Parameters:
    freq : str, tuple, datetime.timedelta, DateOffset or None
    Returns:
    delta : DateOffset
        None if freq is None
 
#Examples
>>> to_offset('5min')
<5 * Minutes>
>>> to_offset('1D1H')
<25 * Hours>
>>> to_offset(('W', 2))
<2 * Weeks: weekday=6>
>>> to_offset((2, 'B'))
<2 * BusinessDays>
>>> to_offset(datetime.timedelta(days=1))
<Day>
>>> to_offset(Hour())
<Hour>

#Example to change freq=None to freq='Something' 
def add_freq(idx, freq=None):
    """Add a frequency attribute to idx, through inference or directly.
    Returns a copy.  If `freq` is None, it is inferred.
    """
    idx = idx.copy()
    if freq is None:
        if idx.freq is None:
            freq = pd.infer_freq(idx)
        else:
            return idx
    idx.freq = pd.tseries.frequencies.to_offset(freq)
    if idx.freq is None:
        raise AttributeError('no discernible frequency found to `idx`.  Specify'
                     ' a frequency string with `freq`.')
    return idx

#An example:
idx=pd.to_datetime(['2003-01-02', '2003-01-03', '2003-01-06'])  # freq=None

>>> print(add_freq(idx))  # inferred
DatetimeIndex(['2003-01-02', '2003-01-03', '2003-01-06'], dtype='datetime64[ns]', freq='B')

>>> print(add_freq(idx, freq='D'))  # explicit





##Date and Time - Example 


# 72 hours starting with midnight Jan 1st, 2011
>>> rng = pd.date_range('1/1/2011', periods=72, freq='H')


#Index pandas objects with dates - Timeseries
ts = pd.Series(np.random.randn(len(rng)), index=rng)
>>> ts.head()

2011-01-01 00:00:00    0.469112
2011-01-01 01:00:00   -0.282863
2011-01-01 02:00:00   -1.509059
2011-01-01 03:00:00   -1.135632
2011-01-01 04:00:00    1.212112
Freq: H, dtype: float64



##Date and Time - Creation
#Single time (snapshot time)
>>> pd.Timestamp(datetime(2012, 5, 1))
Timestamp('2012-05-01 00:00:00')

>>> pd.Timestamp('2012-05-01')
Timestamp('2012-05-01 00:00:00')

#Single time span
>>> pd.Period('2011-01')
Period('2011-01', 'M')

>>> pd.Period('2012-05', freq='D')
Period('2012-05-01', 'D')

#Can be made index of Series ,  DF
dates = [pd.Timestamp('2012-05-01'), pd.Timestamp('2012-05-02'), pd.Timestamp('2012-05-03')]
ts = pd.Series(np.random.randn(3), dates)

>>> type(ts.index)
pandas.tseries.index.DatetimeIndex

>>> ts.index
DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None)

>>> ts
2012-05-01   -0.410001
2012-05-02   -0.078638
2012-05-03    0.545952
dtype: float64

periods = [pd.Period('2012-01'), pd.Period('2012-02'), pd.Period('2012-03')]
ts = pd.Series(np.random.randn(3), periods)
>>> type(ts.index)
pandas.tseries.period.PeriodIndex

>>> ts.index
PeriodIndex(['2012-01', '2012-02', '2012-03'], dtype='int64', freq='M')

>>> ts
2012-01   -1.219217
2012-02   -1.226825
2012-03    0.769804
Freq: M, dtype: float64

##Date and Time - Converting to Timestamps - to_datetime()

#To convert a Series or list-like object of date-like objects
#e.g. strings, epochs, or a mixture, use the to_datetime function.

#When passed a Series, this returns a Series (with the same index ),
#while a list-like is converted to a DatetimeIndex:


>>> pd.to_datetime(pd.Series(['Jul 31, 2009', '2010-01-10', None]))

0   2009-07-31
1   2010-01-10
2          NaT
dtype: datetime64[ns]

>>> pd.to_datetime(['2005/11/23', '2010.12.31'])
DatetimeIndex(['2005-11-23', '2010-12-31'], dtype='datetime64[ns]', freq=None)


# dates which start with the day first

>>> pd.to_datetime(['04-01-2012 10:00'], dayfirst=True)
DatetimeIndex(['2012-01-04 10:00:00'], dtype='datetime64[ns]', freq=None)

>>> pd.to_datetime(['14-01-2012', '01-14-2012'], dayfirst=True)
DatetimeIndex(['2012-01-14', '2012-01-14'], dtype='datetime64[ns]', freq=None)

#a single string to to_datetime, it returns single Timestamp.
>>> pd.to_datetime('2010/11/12')
Timestamp('2010-11-12 00:00:00')

>>> pd.Timestamp('2010/11/12')
Timestamp('2010-11-12 00:00:00')

#pass a DataFrame of integer or string columns to assemble into a Series of Timestamps.
df = pd.DataFrame({'year': [2015, 2016],                      
        'month': [2, 3],                     
        'day': [4, 5],                     
        'hour': [2, 3]})   


>>> pd.to_datetime(df)

0   2015-02-04 02:00:00
1   2016-03-05 03:00:00
dtype: datetime64[ns]


#pass only the columns that you need to assemble.
>>> pd.to_datetime(df[['year', 'month', 'day']])

0   2015-02-04
1   2016-03-05
dtype: datetime64[ns]


#Pass errors='coerce' to convert invalid data to NaT (not a time)
#Raise when unparseable, this is the default


>>> pd.to_datetime(['2009/07/31', 'asd'], errors='raise')
ValueError: Unknown string format

>>> pd.to_datetime(['2009/07/31', 'asd'], errors='ignore')
array(['2009/07/31', 'asd'], dtype=object)

>>> pd.to_datetime(['2009/07/31', 'asd'], errors='coerce')
DatetimeIndex(['2009-07-31', 'NaT'], dtype='datetime64[ns]', freq=None)



#Epoch Timestamps
>>> pd.to_datetime([1349720105, 1349806505, 1349892905, 1349979305, 1350065705], unit='s')   
DatetimeIndex(['2012-10-08 18:15:05', '2012-10-09 18:15:05',               
'2012-10-10 18:15:05', '2012-10-11 18:15:05',               
'2012-10-12 18:15:05'],              
dtype='datetime64[ns]', freq=None)

>>> pd.to_datetime([1349720105100, 1349720105200, 1349720105300,1349720105400, 1349720105500 ], unit='ms')   
DatetimeIndex(['2012-10-08 18:15:05.100000', '2012-10-08 18:15:05.200000',               
'2012-10-08 18:15:05.300000', '2012-10-08 18:15:05.400000',               
'2012-10-08 18:15:05.500000'],              
dtype='datetime64[ns]', freq=None)


>>> pd.to_datetime([1])
DatetimeIndex(['1970-01-01 00:00:00.000000001'], dtype='datetime64[ns]', freq=None)

>>> pd.to_datetime([1, 3.14], unit='s')
DatetimeIndex(['1970-01-01 00:00:01', '1970-01-01 00:00:03'], dtype='datetime64[ns]', freq=None)



##Date and Time -  Generating Ranges of Timestamps

#use either the DatetimeIndex or Index constructor

dates = [datetime(2012, 5, 1), datetime(2012, 5, 2), datetime(2012, 5, 3)]
# Note the frequency information
>>> index = pd.DatetimeIndex(dates)
>>> index
DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None)

# Automatically converted to DatetimeIndex
index = pd.Index(dates)
>>> index
DatetimeIndex(['2012-05-01', '2012-05-02', '2012-05-03'], dtype='datetime64[ns]', freq=None)


#OR use
index = pd.date_range('2000-1-1', periods=1000, freq='M')
index = pd.bdate_range('2012-1-1', periods=250)  #business d ay

#With defauly freq = 'D'
start = datetime(2011, 1, 1)
end = datetime(2012, 1, 1)
rng = pd.date_range(start, end)
rng = pd.bdate_range(start, end)


#DatetimeIndex usage
start = datetime(2011, 1, 1)
end = datetime(2012, 1, 1)
rng = pd.date_range(start, end, freq='B')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
>>> ts.index
DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-29',               
'2011-05-31', '2011-06-30', '2011-07-29', '2011-08-31',               
'2011-09-30', '2011-10-31', '2011-11-30', '2011-12-30'],              
dtype='datetime64[ns]', freq='BM')

>>> ts[:5].index
DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-29',               
'2011-05-31'],              
dtype='datetime64[ns]', freq='BM')

>>> ts[::2].index
DatetimeIndex(['2011-01-31', '2011-03-31', '2011-05-31', '2011-07-29',               
'2011-09-30', '2011-11-30'],              
dtype='datetime64[ns]', freq='2BM')



##Date and Time -  Indexing
#can pass in dates and strings that parse to dates as indexing parameters
>>> ts['1/31/2011']
-1.2812473076599531

>>> ts[datetime(2011, 12, 25):]
2011-12-30    0.687738
Freq: BM, dtype: float64

>>> ts['10/31/2011':'12/31/2011']
2011-10-31    0.149748
2011-11-30   -0.732339
2011-12-30    0.687738
Freq: BM, dtype: float64


#can pass in the year or year and month as strings:
>>> ts['2011']
2011-01-31   -1.281247
2011-02-28   -0.727707
2011-03-31   -0.121306
2011-04-29   -0.097883
2011-05-31    0.695775
2011-06-30    0.341734
2011-07-29    0.959726
2011-08-31   -1.110336
2011-09-30   -0.619976
2011-10-31    0.149748
2011-11-30   -0.732339
2011-12-30    0.687738
Freq: BM, dtype: float64

>>> ts['2011-6']
2011-06-30    0.341734
Freq: BM, dtype: float64


#Similar for  DF
dft = pd.DataFrame(randn(100000,1),           
    columns=['A'],                      
    index=pd.date_range('20130101',periods=100000,freq='T'))  

>>> dft
                        A
2013-01-01 00:00:00  0.176444
2013-01-01 00:01:00  0.403310

[100000 rows x 1 columns]

>>> dft['2013']

#Slicing  works
>>> dft['2013-1':'2013-2']

#This specifies a stop time that includes all of the times on the last day
>>> dft['2013-1':'2013-2-28']

#This specifies an exact stop time (and is not the same as the above)
>>> dft['2013-1':'2013-2-28 00:00:00']

#Another example - in pandas label slicing- end point are included
>>> dft['2013-1-15':'2013-1-15 12:30:00']

#Below is error
dft['2013-1-15 12:30:00']

#To select a single row, use .loc
>>> dft.loc['2013-1-15 12:30:00']
A    0.193284
Name: 2013-01-15 12:30:00, dtype: float64



#Indexing with python datetime  object
>>> dft[datetime(2013, 1, 1):datetime(2013,2,28)] #hr, m etc zero

#With no defaults.
>>> dft[datetime(2013, 1, 1, 10, 12, 0):datetime(2013, 2, 28, 10, 12, 0)]

#Truncating & Fancy Indexing - equivalent to slicing
>>> ts.truncate(before='10/31/2011', after='12/31/2011')
2011-10-31    0.149748
2011-11-30   -0.732339
2011-12-30    0.687738
Freq: BM, dtype: float64


>>> ts[[0, 2, 6]].index
DatetimeIndex(['2011-01-31', '2011-03-31', '2011-07-29'], dtype='datetime64[ns]', freq=None)





##Date and Time - Using offsets with Series / DatetimeIndex 
#- each element is offsetted

rng = pd.date_range('2012-01-01', '2012-01-03')
s = pd.Series(rng)

>>> rng
DatetimeIndex(['2012-01-01', '2012-01-02', '2012-01-03'], dtype='datetime64[ns]', freq='D')

>>> rng + DateOffset(months=2)
DatetimeIndex(['2012-03-01', '2012-03-02', '2012-03-03'], dtype='datetime64[ns]', freq='D')

>>> s + DateOffset(months=2)
0   2012-03-01
1   2012-03-02
2   2012-03-03
dtype: datetime64[ns]

>>> s - DateOffset(months=2)
0   2011-11-01
1   2011-11-02
2   2011-11-03
dtype: datetime64[ns]


#If the offset class maps directly to a Timedelta (Day, Hour, Minute, Second, Micro, Milli, Nano)
#it can be used exactly like a Timedelta
>>> s - Day(2)
0   2011-12-30
1   2011-12-31
2   2012-01-01
dtype: datetime64[ns]

>>> td = s - pd.Series(pd.date_range('2011-12-29', '2011-12-31'))
>>> td

0   3 days
1   3 days
2   3 days
dtype: timedelta64[ns]

>>> td + Minute(15)
0   3 days 00:15:00
1   3 days 00:15:00
2   3 days 00:15:00
dtype: timedelta64[ns]




##Date and Time - Combining Aliases

#Each can have one integer in front to denote how many
#eg '2Q'  denotes two quarterly, '30min' means 30 m in

#B,D,H,Min,S can be combined eg '30min20s' means 30 min 20 seco nd
#eg '2H20min30s' , '2D12H20Min30s'

>>> pd.date_range(start, periods=5, freq='B')

DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06',               
'2011-01-07'],              
dtype='datetime64[ns]', freq='B')

>>> pd.date_range(start, periods=5, freq=BDay())
DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06',               
'2011-01-07'],              
dtype='datetime64[ns]', freq='B')


>>> pd.date_range(start, periods=10, freq='2h20min')
DatetimeIndex(['2011-01-01 00:00:00', '2011-01-01 02:20:00',               
'2011-01-01 04:40:00', '2011-01-01 07:00:00',               
'2011-01-01 09:20:00', '2011-01-01 11:40:00',               
'2011-01-01 14:00:00', '2011-01-01 16:20:00',               
'2011-01-01 18:40:00', '2011-01-01 21:00:00'],              
dtype='datetime64[ns]', freq='140T')

>>> pd.date_range(start, periods=10, freq='1D10U')
DatetimeIndex([       '2011-01-01 00:00:00', '2011-01-02 00:00:00.000010',               
'2011-01-03 00:00:00.000020', '2011-01-04 00:00:00.000030',               
'2011-01-05 00:00:00.000040', '2011-01-06 00:00:00.000050',               
'2011-01-07 00:00:00.000060', '2011-01-08 00:00:00.000070',               
'2011-01-09 00:00:00.000080', '2011-01-10 00:00:00.000090'],              
dtype='datetime64[ns]', freq='86400000010U')

##Date and Time - Period

#A Period represents a span of time (e.g., a day, a month, a quarter, etc ).
>>> pd.Period('2012-1-1 19:00', freq='5H')
Period('2012-01-01 19:00', '5H')


#Adding and subtracting integers from periods shifts the period by its own frequency.
#Arithmetic is not allowed between Period with different freq (span).
p = pd.Period('2012', freq='A-DEC')

>>> p + 1
Period('2013', 'A-DEC')

>>> p - 3
Period('2009', 'A-DEC')

>>> p = pd.Period('2012-01', freq='2M')

>>> p + 2
Period('2012-05', '2M')

>>> p - 1
Period('2011-11', '2M')

>>> p == pd.Period('2012-01', freq='3M')
ERROR
IncompatibleFrequency: Input has different freq=3M from Period(freq=2M)


#If Period freq is  (D, H, T, S, L, U, N ),
#DateOffset and timedelta-like can be added if compatible
#Otherwise, ValueError will be raised.
p = pd.Period('2014-07-01 09:00', freq='H')
>>> p + Hour(2)
Period('2014-07-01 11:00', 'H')
>>> p + datetime.timedelta(minutes=120)
Period('2014-07-01 11:00', 'H')
>>> p + np.timedelta64(7200, 's')
Period('2014-07-01 11:00', 'H')

>>> p + Minute(5)
Traceback   
...
ValueError: Input has different freq from Period(freq=H)


#If Period has other freqs, only the same DateOffset can be added.
#Otherwise, ValueError will be raised.
>>> p = pd.Period('2014-07', freq='M')

>>> p + MonthEnd(3)
Period('2014-10', 'M')

>>> p + MonthBegin(3)
Traceback   
...
ValueError: Input has different freq from Period(freq=M)


#Taking the difference of Period instances with the same frequency
#will return the number of frequency units between them:
>>> pd.Period('2012', freq='A-DEC') - pd.Period('2002', freq='A-DEC')
10



##Date and Time - PeriodIndex and period_range

prng = pd.period_range('1/1/2011', '1/1/2012', freq='M')
>>> prng
PeriodIndex(['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06',             
'2011-07', '2011-08', '2011-09', '2011-10', '2011-11', '2011-12',             
'2012-01'],            
dtype='int64', freq='M')

>>> pd.PeriodIndex(['2011-1', '2011-2', '2011-3'], freq='M')
PeriodIndex(['2011-01', '2011-02', '2011-03'], dtype='int64', freq='M')
# OR
>>> pd.PeriodIndex(start='2014-01', freq='3M', periods=4)
PeriodIndex(['2014-01', '2014-04', '2014-07', '2014-10'], dtype='int64', freq='3M')

#PeriodIndex supports addition and subtraction with the same rule as Period.
idx = pd.period_range('2014-07-01 09:00', periods=5, freq='H')
>>> idx
PeriodIndex(['2014-07-01 09:00', '2014-07-01 10:00', '2014-07-01 11:00',             
'2014-07-01 12:00', '2014-07-01 13:00'],            
dtype='int64', freq='H')
>>> idx + Hour(2)
PeriodIndex(['2014-07-01 11:00', '2014-07-01 12:00', '2014-07-01 13:00',             
'2014-07-01 14:00', '2014-07-01 15:00'],            
dtype='int64', freq='H')

>>> idx = pd.period_range('2014-07', periods=5, freq='M')
>>> idx
PeriodIndex(['2014-07', '2014-08', '2014-09', '2014-10', '2014-11'], dtype='int64', freq='M')

>>> idx + MonthEnd(3)
PeriodIndex(['2014-10', '2014-11', '2014-12', '2015-01', '2015-02'], dtype='int64', freq='M')


##Date and Time - PeriodIndex String Indexing
prng = pd.period_range('1/1/2011', '1/1/2012', freq='M')
#PeriodIndex can also be used to index pandas objects:
ps = pd.Series(np.random.randn(len(prng)), prng)
>>> ps
2011-01   -1.022670
2011-02    1.371155
2011-03    1.035277
2011-04    1.694400
2011-05   -1.659733
2011-06    0.511432
2011-07    0.433176
2011-08   -0.317955
2011-09   -0.517114
2011-10   -0.310466
2011-11    0.543957
2011-12    0.492003
2012-01    0.193420
Freq: M, dtype: float64
>>> ps['2011-01']
-1.022669594890105

>>> ps[datetime(2011, 12, 25):]
2011-12    0.492003
2012-01    0.193420
Freq: M, dtype: float64

>>> ps['10/31/2011':'12/31/2011']
2011-10   -0.310466
2011-11    0.543957
2011-12    0.492003
Freq: M, dtype: float64


#Passing a string representing a lower frequency than PeriodIndex
#returns partial sliced data.
>>> ps['2011']
2011-01   -1.022670
2011-02    1.371155
2011-03    1.035277
2011-04    1.694400
2011-05   -1.659733
2011-06    0.511432
2011-07    0.433176
2011-08   -0.317955
2011-09   -0.517114
2011-10   -0.310466
2011-11    0.543957
2011-12    0.492003
Freq: M, dtype: float64

#With  DF
>>> dfp = pd.DataFrame(np.random.randn(600,1),                     
    columns=['A'],                     
    index=pd.period_range('2013-01-01 9:00', periods=600, freq='T'))   
>>> dfp
                    A
2013-01-01 09:00  0.197720
2013-01-01 09:01 -0.284769
2013-01-01 09:02  0.061491
2013-01-01 09:03  1.630257
2013-01-01 09:04  2.042442
2013-01-01 09:05 -0.804392
2013-01-01 09:06  0.212760

>>> dfp['2013-01-01 10H']
                    A
2013-01-01 10:00 -0.569936
2013-01-01 10:01 -1.179183
2013-01-01 10:02 -0.838602
2013-01-01 10:03 -1.727539
2013-01-01 10:04  1.334027
2013-01-01 10:05  0.417423
2013-01-01 10:06 -0.221189
...                    ...
2013-01-01 10:53 -0.375925
2013-01-01 10:54  0.212750
2013-01-01 10:55 -0.592417
2013-01-01 10:56 -0.466064
2013-01-01 10:57 -1.715347
2013-01-01 10:58 -0.634913
2013-01-01 10:59 -0.809471
[60 rows x 1 columns]

#As with DatetimeIndex, the endpoints will be included in the result.
>>> dfp['2013-01-01 10H':'2013-01-01 11H']
                    A
2013-01-01 10:00 -0.569936
2013-01-01 10:01 -1.179183
2013-01-01 10:02 -0.838602
2013-01-01 10:03 -1.727539
2013-01-01 10:04  1.334027
2013-01-01 10:05  0.417423
2013-01-01 10:06 -0.221189
...                    ...
2013-01-01 11:53  0.616198
2013-01-01 11:54  2.843156
2013-01-01 11:55  0.572537
2013-01-01 11:56  1.709706
2013-01-01 11:57 -0.205490
2013-01-01 11:58  1.759719
2013-01-01 11:59 -1.181485

[120 rows x 1 columns]



##Date and Time - Frequency Conversion and Resampling with PeriodIndex - use asfreq()
p = pd.Period('2011', freq='A-DEC')
>>> p
Period('2011', 'A-DEC')


#To convert it to a monthly frequency.
#Using the how parameter, we can specify whether to return the starting or ending month:
>>> p.asfreq('M', how='start')  #or how='s'
Period('2011-01', 'M')
>>> p.asfreq('M', how='end')   #or how = 'e'
Period('2011-12', 'M')

#Converting to a 'super-period"
#(e.g., annual frequency is a super-period of quarterly frequency)
#automatically returns the super-period that includes the input period:
>>> p = pd.Period('2011-12', freq='M')

#Note that since we converted to an annual frequency that ends the year in November,
#the monthly period of December 2011 is actually in the 2012 A-NOV period.

>>> p.asfreq('A-NOV')
Period('2012', 'A-NOV')



#Q-DEC define regular calendar quarters:
p = pd.Period('2012Q1', freq='Q-DEC')

>>> p.asfreq('D', 's')
Period('2012-01-01', 'D')

>>> p.asfreq('D', 'e')
Period('2012-03-31', 'D')


#Q-MAR defines fiscal year end in March:
p = pd.Period('2011Q4', freq='Q-MAR')

>>> p.asfreq('D', 's')
Period('2011-01-01', 'D')

>>> p.asfreq('D', 'e')
Period('2011-03-31', 'D')



##Date and Time - Converting between Representations
#Timestamped data can be converted to PeriodIndex-ed data using to_period and 
#vice-versa using to_timestamp:


rng = pd.date_range('1/1/2012', periods=5, freq='M')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
>>> ts
2012-01-31    2.167674
2012-02-29   -1.505130
2012-03-31    1.005802
2012-04-30    0.481525
2012-05-31   -0.352151
Freq: M, dtype: float64

>>> ps = ts.to_period()
>>> ps

2012-01    2.167674
2012-02   -1.505130
2012-03    1.005802
2012-04    0.481525
2012-05   -0.352151
Freq: M, dtype: float64

>>> ps.to_timestamp()
2012-01-01    2.167674
2012-02-01   -1.505130
2012-03-01    1.005802
2012-04-01    0.481525
2012-05-01   -0.352151
Freq: MS, dtype: float64


# 's' and 'e' can be used to return the timestamps 
#at the start or end of the period:
>>> ps.to_timestamp('D', how='s')
2012-01-01    2.167674
2012-02-01   -1.505130
2012-03-01    1.005802
2012-04-01    0.481525
2012-05-01   -0.352151
Freq: MS, dtype: float64


#Example - convert a quarterly frequency with year ending in November to 9 am
#of the end of the month following the quarter end:
prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')
ts = pd.Series(np.random.randn(len(prng)), prng)
ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9

>>> ts.head()
1990-03-01 09:00   -0.608988
1990-06-01 09:00    0.412294
1990-09-01 09:00   -0.715938
1990-12-01 09:00    1.297773
1991-03-01 09:00   -2.260765
Freq: H, dtype: float64



#Representing out-of-bounds spans - use PeriodIndex
>>> span = pd.period_range('1215-01-01', '1381-01-01', freq='D')
>>> span
PeriodIndex(['1215-01-01', '1215-01-02', '1215-01-03', '1215-01-04',             
'1215-01-05', '1215-01-06', '1215-01-07', '1215-01-08',             
'1215-01-09', '1215-01-10',             
...             
'1380-12-23', '1380-12-24', '1380-12-25', '1380-12-26',             
'1380-12-27', '1380-12-28', '1380-12-29', '1380-12-30',             
'1380-12-31', '1381-01-01'],            
dtype='int64', length=60632, freq='D')


#To convert from a int64 based YYYYMMDD representation.
>>> s = pd.Series([20121231, 20141130, 99991231])
>>> s
0    20121231
1    20141130
2    99991231
dtype: int64

def conv(x):    
    return pd.Period(year = x // 10000, month = x//100 % 100, day = x%100, freq='D')

#Note apply arg - Can be ufunc (a NumPy function that applies to the entire Serie s)
#or a Python function that only works on single values

>>> s.apply(conv)
0   2012-12-31
1   2014-11-30
2   9999-12-31
dtype: object

>>> s.apply(conv)[2]
Period('9999-12-31', 'D')


#to a PeriodIndex
span = pd.PeriodIndex(s.apply(conv))
>>> span
PeriodIndex(['2012-12-31', '2014-11-30', '9999-12-31'], dtype='int64', freq='D')



##Date and Time - Time Zone Handling

rng = pd.date_range('3/6/2012 00:00', periods=15, freq='D')
>>> rng.tz is None
True

# pytz
>>> rng_pytz = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='Europe/London')   

>>> rng_pytz.tz
<DstTzInfo 'Europe/London' LMT-1 day, 23:59:00 STD>
# dateutil
>>> rng_dateutil = pd.date_range('3/6/2012 00:00', periods=10, freq='D',                              
tz='dateutil/Europe/London')   
>>> rng_dateutil.tz
tzfile('/usr/share/zoneinfo/Europe/London')

# dateutil - utc special case
>>> rng_utc = pd.date_range('3/6/2012 00:00', periods=10, freq='D',                       
tz=dateutil.tz.tzutc())   

>>> rng_utc.tz
tzutc()



##Date and Time - Time Deltas
#a subclass of datetime.timedelta, and behaves in a similar manner,
#but allows compatibility with np.timedelta64 types

#Timedeltas are differences in times, expressed in difference units,
#e.g. days, hours, minutes, seconds. They can be both positive and negative.

#Parsing
from pandas import * 
# strings
>>> Timedelta('1 days')
Timedelta('1 days 00:00:00')

>>> Timedelta('1 days 00:00:00')
Timedelta('1 days 00:00:00')

>>> Timedelta('1 days 2 hours')
Timedelta('1 days 02:00:00')

>>> Timedelta('-1 days 2 min 3us')
Timedelta('-2 days +23:57:59.999997')

# like datetime.timedelta
# note: these MUST be specified as keyword arguments
>>> Timedelta(days=1, seconds=1)
Timedelta('1 days 00:00:01')

# integers with a unit
>>> Timedelta(1, unit='d')
Timedelta('1 days 00:00:00')

# from a timedelta/np.timedelta64
>>> Timedelta(timedelta(days=1, seconds=1))
Timedelta('1 days 00:00:01')

>>> Timedelta(np.timedelta64(1, 'ms'))
Timedelta('0 days 00:00:00.001000')

# negative Timedeltas have this string repr
# to be more consistent with datetime.timedelta conventions
>>> Timedelta('-1us')
Timedelta('-1 days +23:59:59.999999')

# a NaT
>>> Timedelta('nan')
NaT

>>> Timedelta('nat')
NaT


#using DateOffsets (Day, Hour, Minute, Second, Milli, Micro, Nano)
>>> Timedelta(Second(2))
Timedelta('0 days 00:00:02')


#operations among the scalars yield another scalar Timedelta.
>>> Timedelta(Day(2)) + Timedelta(Second(2)) + Timedelta('00:00:00.000123')
Timedelta('2 days 00:00:02.000123')



##Date and Time -  - using to_timedelta(arg, unit='ns', box=True)
#can convert a scalar, array, list, or Series from a recognized timedelta format / value into a Timedelta typ e.
#It will construct Series if the input is a Series, a scalar if the input is scalar-lik e,
#otherwise will output a TimedeltaIndex.

>>> to_timedelta('1 days 06:05:01.00003')
Timedelta('1 days 06:05:01.000030')

>>> to_timedelta('15.5us')
Timedelta('0 days 00:00:00.000015')


#or a list/array of strings:
>>> to_timedelta(['1 days 06:05:01.00003', '15.5us', 'nan'])
TimedeltaIndex(['1 days 06:05:01.000030', '0 days 00:00:00.000015', NaT], dtype='timedelta64[ns]', freq=None)


#The unit keyword argument specifies the unit of the Timedelta:
>>> to_timedelta(np.arange(5), unit='s')
TimedeltaIndex(['00:00:00', '00:00:01', '00:00:02', '00:00:03', '00:00:04'], dtype='timedelta64[ns]', freq=None)

>>> to_timedelta(np.arange(5), unit='d')
TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'], dtype='timedelta64[ns]', freq=None)



##Date and Time -  Time Deltas - Operations- on Series/DataFrames
#construct timedelta64[ns] Series through subtraction operations on datetime64[ns] Series, or Timestamps.

s = Series(date_range('2012-1-1', periods=3, freq='D'))
td = Series([ Timedelta(days=i) for i in range(3) ])
df = DataFrame(dict(A = s, B = td))

>>> df
        A      B
0 2012-01-01 0 days
1 2012-01-02 1 days
2 2012-01-03 2 days

>>> df['C'] = df['A'] + df['B']
>>> df
        A      B          C
0 2012-01-01 0 days 2012-01-01
1 2012-01-02 1 days 2012-01-03
2 2012-01-03 2 days 2012-01-05

>>> df.dtypes
A     datetime64[ns]
B    timedelta64[ns]
C     datetime64[ns]
dtype: object

>>> s - s.max()
0   -2 days
1   -1 days
2    0 days
dtype: timedelta64[ns]

>>> s - datetime(2011, 1, 1, 3, 5)
0   364 days 20:55:00
1   365 days 20:55:00
2   366 days 20:55:00
dtype: timedelta64[ns]

>>> s + timedelta(minutes=5)
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]

>>> s + Minute(5)
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]

>>> s + Minute(5) + Milli(5)
0   2012-01-01 00:05:00.005
1   2012-01-02 00:05:00.005
2   2012-01-03 00:05:00.005
dtype: datetime64[ns]


#Operations with scalars from a timedelta64[ns] series:
>>> y = s - s[0]
>>> y
0   0 days
1   1 days
2   2 days
dtype: timedelta64[ns]


#Series of timedeltas with NaT values are supported:
>>> y = s - s.shift()
>>> y
0      NaT
1   1 days
2   1 days
dtype: timedelta64[ns]


#Elements can be set to NaT using np.nan analogously to datetimes:
>>> y[1] = np.nan
>>> y
0      NaT
1      NaT
2   1 days
dtype: timedelta64[ns]


#Operands can also appear in a reversed order 
#(a singular object operated with a Series):
>>> s.max() - s
0   2 days
1   1 days
2   0 days
dtype: timedelta64[ns]

>>> datetime(2011, 1, 1, 3, 5) - s
0   -365 days +03:05:00
1   -366 days +03:05:00
2   -367 days +03:05:00
dtype: timedelta64[ns]

>>> timedelta(minutes=5) + s
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]


#min, max and the corresponding idxmin, idxmax operations are supported on frames:
A = s - Timestamp('20120101') - Timedelta('00:05:05')
B = s - Series(date_range('2012-1-2', periods=3, freq='D'))
df = DataFrame(dict(A=A, B=B))

>>> df
                A       B
0 -1 days +23:54:55 -1 days
1   0 days 23:54:55 -1 days
2   1 days 23:54:55 -1 days

>>> df.min()
A   -1 days +23:54:55
B   -1 days +00:00:00
dtype: timedelta64[ns]

>>> df.min(axis=1)
0   -1 days
1   -1 days
2   -1 days
dtype: timedelta64[ns]

>>> df.idxmin()
A    0
B    0
dtype: int64

>>> df.idxmax()
A    2
B    0
dtype: int64


#min, max, idxmin, idxmax operations are supported on Series as well.
#A scalar result will be a Timedelta.
>>> df.min().max()
Timedelta('-1 days +23:54:55')

>>> df.min(axis=1).min()
Timedelta('-1 days +00:00:00')

>>> df.min().idxmax()
'A'

>>> df.min(axis=1).idxmin()
0


##Date and Time -  - fillna on timedeltas.
#Integers will be interpreted as seconds.
#You can pass a timedelta to get a particular value.


>>> y.fillna(0)
0   0 days
1   0 days
2   1 days
dtype: timedelta64[ns]

>>> y.fillna(10)
0   0 days 00:00:10
1   0 days 00:00:10
2   1 days 00:00:00
dtype: timedelta64[ns]

>>> y.fillna(Timedelta('-1 days, 00:00:05'))
0   -1 days +00:00:05
1   -1 days +00:00:05
2     1 days 00:00:00
dtype: timedelta64[ns]


# negate, multiply and use abs on Timedeltas:
td1 = Timedelta('-1 days 2 hours 3 seconds')

>>> td1
Timedelta('-2 days +21:59:57')

>>> -1 * td1
Timedelta('1 days 02:00:03')

>>> - td1
Timedelta('1 days 02:00:03')

>>> abs(td1)
Timedelta('1 days 02:00:03')



#Reductions
#NaT are skipped during evaluation.

>>> y2 = Series(to_timedelta(['-1 days +00:00:05', 'nat', '-1 days +00:00:05', '1 days']))
>>> y2
0   -1 days +00:00:05
1                 NaT
2   -1 days +00:00:05
3     1 days 00:00:00
dtype: timedelta64[ns]

>>> y2.mean()
Timedelta('-1 days +16:00:03.333333')

>>> y2.median()
Timedelta('-1 days +00:00:05')

>>> y2.quantile(.1)
Timedelta('-1 days +00:00:05')

>>> y2.sum()
Timedelta('-1 days +00:00:10')



##Date and Time -  Time Deltas - Frequency Conversion
#Timedelta Series, TimedeltaIndex, and Timedelta scalars can be converted
#to other 'frequencies' by dividing by another timedelta,
#or by astype to a specific timedelta type.

#These operations yield Series and propagate NaT -> na n.
#Note that division by the numpy scalar is true divisio n,
#while astype is equivalent of floor division.


td = Series(date_range('20130101', periods=4)) -  Series(date_range('20121201', periods=4))
td[2] += timedelta(minutes=5, seconds=3)

>>> td[3] = np.nan
>>> td
0   31 days 00:00:00
1   31 days 00:00:00
2   31 days 00:05:03
3                NaT
dtype: timedelta64[ns]

# to days
>>> td / np.timedelta64(1, 'D')
0    31.000000
1    31.000000
2    31.003507
3          NaN
dtype: float64

>>> td.astype('timedelta64[D]')
0    31.0
1    31.0
2    31.0
3     NaN
dtype: float64

# to seconds
>>> td / np.timedelta64(1, 's')
0    2678400.0
1    2678400.0
2    2678703.0
3          NaN
dtype: float64

>>> td.astype('timedelta64[s]')
0    2678400.0
1    2678400.0
2    2678703.0
3          NaN
dtype     : flo     at64

# to months (these are constant months)
>>> td / np.timedelta64(1, 'M')
0    1.018501
1    1.018501
2    1.018617
3         NaN
dtype: float64


#Dividing or multiplying a timedelta64[ns] Series
#by an integer or integer Series yields another timedelta64[ns] dtypes Series.
>>> td * -1
0   -31 days +00:00:00
1   -31 days +00:00:00
2   -32 days +23:54:57
3                  NaT
dtype: timedelta64[ns]

>>> td * Series([1, 2, 3, 4])
0   31 days 00:00:00
1   62 days 00:00:00
2   93 days 00:15:09
3                NaT
dtype: timedelta64[ns]



##Date and Time - Time Deltas - Attributes
#Use attributes days,seconds,microseconds,nanoseconds.

>>> td.dt.days
0    31.0
1    31.0
2    31.0
3     NaN
dtype: float64

>>> td.dt.seconds
0      0.0
1      0.0
2    303.0
3      NaN
dtype: float64


>>> tds = Timedelta('31 days 5 min 3 sec')
>>> tds.days
31

>>> tds.seconds
303

>>> (-tds).seconds
86097


#use the .components property to access a reduced form of the timedelta.

>>> td.dt.components
    days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds
0  31.0    0.0      0.0      0.0           0.0           0.0          0.0
1  31.0    0.0      0.0      0.0           0.0           0.0          0.0
2  31.0    0.0      5.0      3.0           0.0           0.0          0.0
3   NaN    NaN      NaN      NaN           NaN           NaN          NaN

>>> td.dt.components.seconds
0    0.0
1    0.0
2    3.0
3    NaN
Name: seconds, dtype: float64



##Date and Time -  - TimedeltaIndex
#Using TimedeltaIndex you can pass string-like, Timedelta, timedelta, or np.timedelta64 object s.
#Passing np.nan/pd.NaT/nat will represent missing values.


>>> TimedeltaIndex(['1 days', '1 days, 00:00:05',  np.timedelta64(2,'D'), timedelta(days=2,seconds=2)])
TimedeltaIndex(['1 days 00:00:00', '1 days 00:00:05', '2 days 00:00:00',                
'2 days 00:00:02'],               
dtype='timedelta64[ns]', freq=None)


# ranges of a TimedeltaIndex:
>>> timedelta_range(start='1 days', periods=5, freq='D')
TimedeltaIndex(['1 days', '2 days', '3 days', '4 days', '5 days'], dtype='timedelta64[ns]', freq='D')

>>> timedelta_range(start='1 days', end='2 days', freq='30T')
TimedeltaIndex(['1 days 00:00:00', '1 days 00:30:00', '1 days 01:00:00',                
'1 days 01:30:00', '1 days 02:00:00', '1 days 02:30:00',                
'1 days 03:00:00', '1 days 03:30:00', '1 days 04:00:00',                
'1 days 04:30:00', '1 days 05:00:00', '1 days 05:30:00',                
'1 days 06:00:00', '1 days 06:30:00', '1 days 07:00:00',                
'1 days 07:30:00', '1 days 08:00:00', '1 days 08:30:00',                
'1 days 09:00:00', '1 days 09:30:00', '1 days 10:00:00',                
'1 days 10:30:00', '1 days 11:00:00', '1 days 11:30:00',                
'1 days 12:00:00', '1 days 12:30:00', '1 days 13:00:00',                
'1 days 13:30:00', '1 days 14:00:00', '1 days 14:30:00',                
'1 days 15:00:00', '1 days 15:30:00', '1 days 16:00:00',                
'1 days 16:30:00', '1 days 17:00:00', '1 days 17:30:00',                
'1 days 18:00:00', '1 days 18:30:00', '1 days 19:00:00',                
'1 days 19:30:00', '1 days 20:00:00', '1 days 20:30:00',                
'1 days 21:00:00', '1 days 21:30:00', '1 days 22:00:00',                
'1 days 22:30:00', '1 days 23:00:00', '1 days 23:30:00',                
'2 days 00:00:00'],               
dtype='timedelta64[ns]', freq='30T')



#Using the TimedeltaIndex 
s = Series(np.arange(100),               
index=timedelta_range('1 days', periods=100, freq='h'))  


>>> s
1 days 00:00:00     0
1 days 01:00:00     1
1 days 02:00:00     2
1 days 03:00:00     3
1 days 04:00:00     4
1 days 05:00:00     5
1 days 06:00:00     6                   
..
4 days 21:00:00    93
4 days 22:00:00    94
4 days 23:00:00    95
5 days 00:00:00    96
5 days 01:00:00    97
5 days 02:00:00    98
5 days 03:00:00    99
Freq: H, dtype: int64


#selection and slicing
>>> s['1 day':'2 day']
1 days 00:00:00     0
1 days 01:00:00     1
1 days 02:00:00     2
1 days 03:00:00     3
1 days 04:00:00     4
1 days 05:00:00     5
1 days 06:00:00     6                   
..
2 days 17:00:00    41
2 days 18:00:00    42
2 days 19:00:00    43
2 days 20:00:00    44
2 days 21:00:00    45
2 days 22:00:00    46
2 days 23:00:00    47
Freq: H, dtype: int64

>>> s['1 day 01:00:00']
1

>>> s[Timedelta('1 day 1h')]
1


#partial string selection and the range will be inferred:
>>> s['1 day':'1 day 5 hours']
1 days 00:00:00    0
1 days 01:00:00    1
1 days 02:00:00    2
1 days 03:00:00    3
1 days 04:00:00    4
1 days 05:00:00    5
Freq: H, dtype: int64



##Date and Time -  Operations of TimedeltaIndex with DatetimeIndex

tdi = TimedeltaIndex(['1 days', pd.NaT, '2 days'])
>>> tdi.tolist()
[Timedelta('1 days 00:00:00'), NaT, Timedelta('2 days 00:00:00')]

dti = date_range('20130101', periods=3)
>>> dti.tolist()
[Timestamp('2013-01-01 00:00:00', offset='D'), 
Timestamp('2013-01-02 00:00:00', offset='D'), 
Timestamp('2013-01-03 00:00:00', offset='D')]

>>> (dti + tdi).tolist()
[Timestamp('2013-01-02 00:00:00'), NaT, Timestamp('2013-01-05 00:00:00')]

>>> (dti - tdi).tolist()
[Timestamp('2012-12-31 00:00:00'), NaT, Timestamp('2013-01-01 00:00:00')]



#Conversions to yield another Index.
>>> tdi / np.timedelta64(1,'s')
Float64Index([86400.0, nan, 172800.0], dtype='float64')

>>> tdi.astype('timedelta64[s]')
Float64Index([86400.0, nan, 172800.0], dtype='float64')


#Scalars type ops , return a different type of index.
# adding or timedelta and date -> datelike
>>> tdi + Timestamp('20130101')
DatetimeIndex(['2013-01-02', 'NaT', '2013-01-03'], dtype='datetime64[ns]', freq=None)

# subtraction of a date and a timedelta -> datelike
# note that trying to subtract a date from a Timedelta will raise an exception
>>> (Timestamp('20130101') - tdi).tolist()
[Timestamp('2012-12-31 00:00:00'), NaT, Timestamp('2012-12-30 00:00:00')]

# timedelta + timedelta -> timedelta
>>> tdi + Timedelta('10 days')
TimedeltaIndex(['11 days', NaT, '12 days'], dtype='timedelta64[ns]', freq=None)

# division can result in a Timedelta if the divisor is an integer
>>> tdi / 2
TimedeltaIndex(['0 days 12:00:00', NaT, '1 days 00:00:00'], dtype='timedelta64[ns]', freq=None)

# or a Float64Index if the divisor is a Timedelta
>>> tdi / tdi[0]
Float64Index([1.0, nan, 2.0], dtype='float64')



##Date and Time - Time Deltas - Resampling
>>> s.resample('D').mean()
1 days    11.5
2 days    35.5
3 days    59.5
4 days    83.5
5 days    97.5
Freq: D, dtype: float64



















###Pandas - Handling  Time Series

##Time Series - main methods 
DataFrame.rolling(window[, min_periods, ...])       Provides rolling window calculations. 
DataFrame.expanding([min_periods, freq, ...])       Provides expanding transformations. 
DataFrame.ewm([com, span, halflife, alpha, ...])    Provides exponential weighted functions 


DataFrame.rolling(window, min_periods=None, freq=None, 
            center=False, win_type=None, on=None, axis=0, closed=None)
    Provides rolling window calculations.
    Returns Rolling class 
    Parameters:
    window : int, or offset
        Size of the moving window. 
        This is the number of observations used for calculating the statistic. 
        Each window will be a fixed size.
        If its an offset(subclass of DateOffset) then this will be the time period of each window. 
        Each window will be a variable sized based on the observations included 
        in the time-period. 
        This is only valid for datetimelike indexes. 
    min_periods : int, default None
        Minimum number of observations in window required to have a value 
        (otherwise result is NA). 
        For a window that is specified by an offset, this will default to 1.
    freq : string or DateOffset object, optional (default None)
        Deprecated :Frequency to conform the data to before computing the statistic. 
        Specified as a frequency string or DateOffset object.
        The freq keyword is used to conform time series data 
        to a specified frequency by resampling the data. 
        This is done with the default parameters of resample() (i.e. using the mean).
    center : boolean, default False
        Set the labels at the center of the window.
    win_type : string, default None
        Provide a window type.
        The recognized win_types are:
            •boxcar
            •triang
            •blackman
            •hamming
            •bartlett
            •parzen
            •bohman
            •blackmanharris
            •nuttall
            •barthann
            •kaiser (needs beta)
            •gaussian (needs std)
            •general_gaussian (needs power, width)
            •slepian (needs width).
        If win_type=None all points are evenly weighted.
    on : string, optional
        For a DataFrame, column on which to calculate the rolling window, 
        rather than the index
    closed : string, default None
        Make the interval closed on the 'right', 'left', 'both' or 'neither' endpoints. 
        For offset-based windows, it defaults to 'right'. 
        For fixed windows, defaults to 'both'. 
        Remaining cases not implemented for fixed windows.
    axis : int or string, default 0
 
#Examples
>>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})
>>> df
     B
0  0.0
1  1.0
2  2.0
3  NaN
4  4.0

#Rolling sum with a window length of 2, using the 'triang' window type.
>>> df.rolling(2, win_type='triang').sum()
     B
0  NaN
1  1.0
2  2.5
3  NaN
4  NaN

#Rolling sum with a window length of 2, min_periods defaults to the window length.
>>> df.rolling(2).sum()
     B
0  NaN
1  1.0
2  3.0
3  NaN
4  NaN

#Same as above, but explicity set the min_periods
>>> df.rolling(2, min_periods=1).sum()
     B
0  0.0
1  1.0
2  3.0
3  2.0
4  4.0

#A ragged (meaning not-a-regular frequency), time-indexed DataFrame
>>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]},
                    index = [pd.Timestamp('20130101 09:00:00'),
                            pd.Timestamp('20130101 09:00:02'),
                            pd.Timestamp('20130101 09:00:03'),
                            pd.Timestamp('20130101 09:00:05'),
                            pd.Timestamp('20130101 09:00:06')])
>>> df
                       B
2013-01-01 09:00:00  0.0
2013-01-01 09:00:02  1.0
2013-01-01 09:00:03  2.0
2013-01-01 09:00:05  NaN
2013-01-01 09:00:06  4.0

#Contrasting to an integer rolling window, 
#this will roll a variable length window corresponding to the time period. 
#The default for min_periods is 1.
>>> df.rolling('2s').sum()
                       B
2013-01-01 09:00:00  0.0
2013-01-01 09:00:02  1.0
2013-01-01 09:00:03  3.0
2013-01-01 09:00:05  NaN
2013-01-01 09:00:06  4.0



DataFrame.expanding(min_periods=1, freq=None, center=False, axis=0)
    Provides expanding transformations.
    Parameters:
    min_periods : int, default None
        Minimum number of observations in window required to have a value 
        (otherwise result is NA).
    freq : string or DateOffset object, optional (default None)
        Deprecated: Frequency to conform the data to before computing the statistic. 
        Specified as a frequency string or DateOffset object.
    center : boolean, default False
        Set the labels at the center of the window.
    axis : int or string, default 0
 
#Examples
>>> df = DataFrame({'B': [0, 1, 2, np.nan, 4]})
     B
0  0.0
1  1.0
2  2.0
3  NaN
4  4.0
>>> df.expanding(2).sum()
     B
0  NaN
1  1.0
2  3.0
3  3.0
4  7.0


DataFrame.ewm(com=None, span=None, halflife=None, alpha=None, 
            min_periods=0, freq=None, adjust=True, ignore_na=False, axis=0)
    Provides exponential weighted functions
    Returns Exponentially-weighted moving window(EWM) class 
    Parameters:
    com : float, optional
        Specify decay in terms of center of mass,  
        α=1/(1+com), for com≥0
    span : float, optional
        Specify decay in terms of span,  
        α=2/(span+1), for span≥1
    halflife : float, optional
        Specify decay in terms of half-life,  
        α=1−exp(log(0.5)/halflife), for halflife>0
    alpha : float, optional
        Specify smoothing factor 0<α≤1
    min_periods : int, default 0
        Minimum number of observations in window required to have a value 
        (otherwise result is NA).
    freq : None or string alias / date offset object, default=None
        Deprecated: Frequency to conform to before computing statistic
    adjust : boolean, default True
        Divide by decaying adjustment factor in beginning periods to account for imbalance in relative weightings (viewing EWMA as a moving average)
    ignore_na : boolean, default False
        Ignore missing values when calculating weights; specify True to reproduce pre-0.15.0 behavior
 
#Example 
>>> df = DataFrame({'B': [0, 1, 2, np.nan, 4]})
     B
0  0.0
1  1.0
2  2.0
3  NaN
4  4.0
>>> df.ewm(com=0.5).mean()
          B
0  0.000000
1  0.750000
2  1.615385
3  1.615385
4  3.670213



##Time Series - Rolling - methods 
Rolling.count()                 rolling count of number of non-NaN 
Rolling.sum(*args, **kwargs)    rolling sum 
Rolling.mean(*args, **kwargs)   rolling mean 
Rolling.median(**kwargs)        rolling median 
Rolling.var([ddof])             rolling variance 
Rolling.std([ddof])             rolling standard deviation 
Rolling.min(*args, **kwargs)    rolling minimum 
Rolling.max(*args, **kwargs)    rolling maximum 
Rolling.corr([other, pairwise]) rolling sample correlation 
Rolling.cov([other, pairwise, ddof]) rolling sample covariance 
Rolling.skew(**kwargs)              Unbiased rolling skewness 
Rolling.kurt(**kwargs)              Unbiased rolling kurtosis 
Rolling.apply(func[, args, kwargs]) rolling function apply 
Rolling.quantile(quantile, **kwargs) rolling quantile 
Window.mean(*args, **kwargs)        window mean 
Window.sum(*args, **kwargs)         window sum 

##Time Series - Expanding - methods 
Expanding.count(**kwargs)           expanding count of number of non-NaN 
Expanding.sum(*args, **kwargs)      expanding sum 
Expanding.mean(*args, **kwargs)     expanding mean 
Expanding.median(**kwargs)          expanding median 
Expanding.var([ddof])               expanding variance 
Expanding.std([ddof])               expanding standard deviation 
Expanding.min(*args, **kwargs)      expanding minimum 
Expanding.max(*args, **kwargs)      expanding maximum 
Expanding.corr([other, pairwise])   expanding sample correlation 
Expanding.cov([other, pairwise, ddof]) expanding sample covariance 
Expanding.skew(**kwargs)            Unbiased expanding skewness 
Expanding.kurt(**kwargs)            Unbiased expanding kurtosis 
Expanding.apply(func[, args, kwargs]) expanding function apply 
Expanding.quantile(quantile, **kwargs) expanding quantile 

##Time Series - EWM- methods 
EWM.mean(*args, **kwargs) exponential weighted moving average 
EWM.std([bias]) exponential weighted moving stddev 
EWM.var([bias]) exponential weighted moving variance 
EWM.corr([other, pairwise]) exponential weighted sample correlation 
EWM.cov([other, pairwise, bias]) exponential weighted sample covariance 


##Time Series - Time series-related methdos 
Series.asfreq(freq[, method, how, ...])     Convert TimeSeries to specified frequency. 
Series.asof(where[, subset])                The last row without any NaN is taken (or the last row without 
Series.shift([periods, freq, axis])         Shift index by desired number of periods with an optional time freq 
Series.first_valid_index()                  Return index for first non-NA/null value. 
Series.last_valid_index()                   Return index for last non-NA/null value. 
Series.resample(rule[, how, axis, ...])     Convenience method for frequency conversion and resampling of time series. 
Series.tz_convert(tz[, axis, level, copy])  Convert tz-aware axis to target time zone. 
Series.tz_localize(tz[, axis, level, copy, ...]) Localize tz-naive TimeSeries to target time zone. 

DataFrame.asfreq(freq[, method, how, ...])  Convert TimeSeries to specified frequency. 
DataFrame.asof(where[, subset])             The last row without any NaN is taken (or the last row without 
DataFrame.shift([periods, freq, axis])      Shift index by desired number of periods with an optional time freq 
DataFrame.first_valid_index()               Return index for first non-NA/null value. 
DataFrame.last_valid_index()                Return index for last non-NA/null value. 
DataFrame.resample(rule[, how, axis, ...])  Convenience method for frequency conversion and resampling of time series. 
DataFrame.to_period([freq, axis, copy])     Convert DataFrame from DatetimeIndex to PeriodIndex with desired 
DataFrame.to_timestamp([freq, how, axis, copy]) Cast to DatetimeIndex of timestamps, at beginning of period 
DataFrame.tz_convert(tz[, axis, level, copy]) Convert tz-aware axis to target time zone. 




DataFrame.asfreq(freq, method=None, how=None, normalize=False, fill_value=None)
    Convert TimeSeries to specified frequency.
    Parameters:
    freq : DateOffset object, or string
    method : {'backfill'/'bfill', 'pad'/'ffill'}, default None
        Method to use for filling holes in reindexed Series (note this does not fill NaNs that already were present):
        •'pad' / 'ffill': propagate last valid observation forward to next valid
        •'backfill' / 'bfill': use NEXT valid observation to fill
    how : {'start', 'end'}, default end
        For PeriodIndex only, see PeriodIndex.asfreq
    normalize : bool, default False
        Whether to reset output index to midnight
    fill_value: scalar, optional
        Value to use for missing values, applied during upsampling (note this does not fill NaNs that already were present).

#Examples
>>> index = pd.date_range('1/1/2000', periods=4, freq='T')
>>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)
>>> df = pd.DataFrame({'s':series})
>>> df
                       s
2000-01-01 00:00:00    0.0
2000-01-01 00:01:00    NaN
2000-01-01 00:02:00    2.0
2000-01-01 00:03:00    3.0
#Upsample the series into 30 second bins.
>>> df.asfreq(freq='30S')
                       s
2000-01-01 00:00:00    0.0
2000-01-01 00:00:30    NaN
2000-01-01 00:01:00    NaN
2000-01-01 00:01:30    NaN
2000-01-01 00:02:00    2.0
2000-01-01 00:02:30    NaN
2000-01-01 00:03:00    3.0
#Upsample again, providing a fill value.
>>> df.asfreq(freq='30S', fill_value=9.0)
                       s
2000-01-01 00:00:00    0.0
2000-01-01 00:00:30    9.0
2000-01-01 00:01:00    NaN
2000-01-01 00:01:30    9.0
2000-01-01 00:02:00    2.0
2000-01-01 00:02:30    9.0
2000-01-01 00:03:00    3.0
#Upsample again, providing a method.
>>> df.asfreq(freq='30S', method='bfill')
                       s
2000-01-01 00:00:00    0.0
2000-01-01 00:00:30    NaN
2000-01-01 00:01:00    NaN
2000-01-01 00:01:30    2.0
2000-01-01 00:02:00    2.0
2000-01-01 00:02:30    3.0
2000-01-01 00:03:00    3.0
#Another Example 
dr = pd.date_range('1/1/2010', periods=3, freq=3 * datetools.bday)
ts = pd.Series(randn(3), index=dr)

>>> ts
2010-01-01    0.532005
2010-01-06    0.544874
2010-01-11   -1.001788
Freq: 3B, dtype: float64

>>> ts.asfreq(BDay())

2010-01-01    0.532005
2010-01-04         NaN
2010-01-05         NaN
2010-01-06    0.544874
2010-01-07         NaN
2010-01-08         NaN
2010-01-11   -1.001788
Freq: B, dtype: float64

>>> ts.asfreq(BDay(), method='pad')

2010-01-01    0.532005
2010-01-04    0.532005
2010-01-05    0.532005
2010-01-06    0.544874
2010-01-07    0.544874
2010-01-08    0.544874
2010-01-11   -1.001788
Freq: B, dtype: float64

##Difference between asfreq() or resample() - bothe does upsampling or downsampling 
#asfreq() - For a DatetimeIndex, thin wrapper around reindex 
#which generates a date_range and calls reindex

#note reindex(new_index) is used to map old_index with new_index 
#and new values are added as NaN 

#.resample() is a time-based groupby, (returns Resampler class)
#followed by a reduction method on each of its groups

#upsampling - decreasing duration - increasing frequencing - gaps are filled with Nan 
#downsampling - increasing duration - decreasing frequency - time series aggregation 

import pandas as pd 

dr = pd.date_range('1/1/2010', periods=3, freq=3 * pd.tseries.offsets.BDay())
raw = np.random.randn(3)
ts = pd.Series(raw, index=dr)
>>> ts
2010-01-01   -1.948
2010-01-06    0.112
2010-01-11   -0.117
Freq: 3B, dtype: float64

>>> ts.asfreq(pd.tseries.offsets.BDay())
2010-01-01   -1.948
2010-01-04      NaN
2010-01-05      NaN
2010-01-06    0.112
2010-01-07      NaN
2010-01-08      NaN
2010-01-11   -0.117
Freq: B, dtype: float64

>>> ts.resample(pd.tseries.offsets.BDay()).mean() #upsampling 's mean means each point 
2010-01-01   -1.948
2010-01-04      NaN
2010-01-05      NaN
2010-01-06    0.112
2010-01-07      NaN
2010-01-08      NaN
2010-01-11   -0.117
Freq: B, dtype: float64




##Time Series - shift/tshift

#Shifting of values- use shift(periods=1, freq=None, axis=0)
#periods can be + or -ive 
from datetime import datetime
start = datetime(2011, 1, 1)
end = datetime(2012, 1, 1)
rng = pd.date_range(start, end, freq='B')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ts = ts[:5]
>>> ts.head()
2011-01-03    1.267562
2011-01-04    1.172128
2011-01-05   -0.686308
2011-01-06   -1.438843
2011-01-07    0.528596
Freq: B, dtype: float64
>>> ts.shift(1).head()  #shift of values 
2011-01-03         NaN
2011-01-04    1.267562
2011-01-05    1.172128
2011-01-06   -0.686308
2011-01-07   -1.438843
Freq: B, dtype: float64
>>> ts.tshift(1).head() #shifting of index 
2011-01-04    1.267562
2011-01-05    1.172128
2011-01-06   -0.686308
2011-01-07   -1.438843
2011-01-10    0.528596
Freq: B, dtype: float64
>>>

#accepts an freq argument which can accept a DateOffset class
#or other timedelta-like object or a offset alias:
>>> ts.shift(5, freq='D')
2011-01-08   -0.177123
2011-01-09    0.585998
2011-01-10   -0.407661
2011-01-11   -0.328603
2011-01-12    1.682010
Freq: D, dtype: float64

DataFrame.resample(rule, how=None, axis=0, fill_method=None, 
        closed=None, label=None, convention='start', kind=None, loffset=None, 
        limit=None, base=0, on=None, level=None)
    Convenience method for frequency conversion and resampling of time series. 
    Object must have a datetime-like index (DatetimeIndex, PeriodIndex, or TimedeltaIndex), 
    or pass datetime-like values to the on or level keyword.
    Returns Resampler class 
    Parameters:
    rule : string
        the offset string or object representing target conversion
    axis : int, optional, default 0
    closed : {'right', 'left'}
        Which side of bin interval is closed. 
        The default is 'left' for all frequency offsets except 
        for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W' 
        which all have a default of 'right'.
    label : {'right', 'left'}
        Which bin edge label to label bucket with. 
        The default is 'left' for all frequency offsets except 
        for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W' 
        which all have a default of 'right'.
    convention : {'start', 'end', 's', 'e'}
        For PeriodIndex only, controls whether to use the start or end of rule
    loffset : timedelta
        Adjust the resampled time labels
    base : int, default 0
        For frequencies that evenly subdivide 1 day, the 'origin' of the aggregated intervals. For example, for '5min' frequency, base could range from 0 through 4. Defaults to 0
    on : string, optional
        For a DataFrame, column to use instead of index for resampling. Column must be datetime-like.
    level : string or int, optional
        For a MultiIndex, level (name or number) to use for resampling. 
        Level must be datetime-like.

#Examples
>>> index = pd.date_range('1/1/2000', periods=9, freq='T')
>>> series = pd.Series(range(9), index=index)
>>> series
2000-01-01 00:00:00    0
2000-01-01 00:01:00    1
2000-01-01 00:02:00    2
2000-01-01 00:03:00    3
2000-01-01 00:04:00    4
2000-01-01 00:05:00    5
2000-01-01 00:06:00    6
2000-01-01 00:07:00    7
2000-01-01 00:08:00    8
Freq: T, dtype: int64

#Downsample the series into 3 minute bins 
#and sum the values of the timestamps falling into a bin.
>>> series.resample('3T').sum()
2000-01-01 00:00:00     3
2000-01-01 00:03:00    12
2000-01-01 00:06:00    21
Freq: 3T, dtype: int64

#Downsample the series into 3 minute bins as above, 
#but label each bin using the right edge instead of the left. 

#Please note that the value in the bucket used 
#as the label is not included in the bucket, which it labels. 
#For example, in the original series the bucket 2000-01-01 00:03:00 
#contains the value 3, but the summed value in the resampled bucket 
#with the label 2000-01-01 00:03:00 does not include 3 
#(if it did, the summed value would be 6, not 3). 

>>> series.resample('3T', label='right').sum()
2000-01-01 00:03:00     3
2000-01-01 00:06:00    12
2000-01-01 00:09:00    21
Freq: 3T, dtype: int64

#Downsample the series into 3 minute bins as above, 
#but close the right side of the bin interval.
>>> series.resample('3T', label='right', closed='right').sum()
2000-01-01 00:00:00     0
2000-01-01 00:03:00     6
2000-01-01 00:06:00    15
2000-01-01 00:09:00    15
Freq: 3T, dtype: int64

#Upsample the series into 30 second bins.
>>> series.resample('30S').asfreq()[0:5] #select first 5 rows
2000-01-01 00:00:00   0.0
2000-01-01 00:00:30   NaN
2000-01-01 00:01:00   1.0
2000-01-01 00:01:30   NaN
2000-01-01 00:02:00   2.0
Freq: 30S, dtype: float64

#Upsample the series into 30 second bins and fill the NaN values using the pad method.
>>> series.resample('30S').pad()[0:5]
2000-01-01 00:00:00    0
2000-01-01 00:00:30    0
2000-01-01 00:01:00    1
2000-01-01 00:01:30    1
2000-01-01 00:02:00    2
Freq: 30S, dtype: int64

#Upsample the series into 30 second bins and fill the NaN values using the bfill method.
>>> series.resample('30S').bfill()[0:5]
2000-01-01 00:00:00    0
2000-01-01 00:00:30    1
2000-01-01 00:01:00    1
2000-01-01 00:01:30    2
2000-01-01 00:02:00    2
Freq: 30S, dtype: int64

#Pass a custom function via apply
>>> def custom_resampler(array_like):
        return np.sum(array_like)+5
>>> series.resample('3T').apply(custom_resampler)
2000-01-01 00:00:00     8
2000-01-01 00:03:00    17
2000-01-01 00:06:00    26
Freq: 3T, dtype: int64

#For a Series with a PeriodIndex, 
#the keyword convention can be used to control 
#whether to use the start or end of rule.
>>> s = pd.Series([1, 2], index=pd.period_range('2012-01-01',
                                                freq='A',
                                                periods=2))
>>> s
2012    1
2013    2
Freq: A-DEC, dtype: int64

#Resample by month using 'start' convention. 
#Values are assigned to the first month of the period.
>>> s.resample('M', convention='start').asfreq().head()
2012-01    1.0
2012-02    NaN
2012-03    NaN
2012-04    NaN
2012-05    NaN
Freq: M, dtype: float64

#Resample by month using 'end' convention. 
#Values are assigned to the last month of the period.
>>> s.resample('M', convention='end').asfreq()
2012-12    1.0
2013-01    NaN
2013-02    NaN
2013-03    NaN
2013-04    NaN
2013-05    NaN
2013-06    NaN
2013-07    NaN
2013-08    NaN
2013-09    NaN
2013-10    NaN
2013-11    NaN
2013-12    2.0
Freq: M, dtype: float64

#For DataFrame objects, the keyword on can be used to specify the column 
#instead of the index for resampling.
>>> df = pd.DataFrame(data=9*[range(4)], columns=['a', 'b', 'c', 'd'])
>>> df['time'] = pd.date_range('1/1/2000', periods=9, freq='T')
>>> df.resample('3T', on='time').sum()
                     a  b  c  d
time
2000-01-01 00:00:00  0  3  6  9
2000-01-01 00:03:00  0  3  6  9
2000-01-01 00:06:00  0  3  6  9

#For a DataFrame with MultiIndex, 
#the keyword level can be used to specify on level the resampling needs to take place.
>>> time = pd.date_range('1/1/2000', periods=5, freq='T')
>>> df2 = pd.DataFrame(data=10*[range(4)],
                       columns=['a', 'b', 'c', 'd'],
                       index=pd.MultiIndex.from_product([time, [1, 2]])
                       )
>>> df2.resample('3T', level=0).sum()
                     a  b   c   d
2000-01-01 00:00:00  0  6  12  18
2000-01-01 00:03:00  0  4   8  12


##Sparse Resampling 
#when data points are fewer, then don't use up sampling (above)

rng = pd.date_range('2014-1-1', periods=100, freq='D') + pd.Timedelta('1s')
ts = pd.Series(range(100), index=rng)
#WRONG
>>> ts.resample('3T').sum()

2014-01-01 00:00:00     0.0
2014-01-01 00:03:00     NaN
2014-01-01 00:06:00     NaN
2014-01-01 00:09:00     NaN
2014-01-01 00:12:00     NaN
2014-01-01 00:15:00     NaN
2014-01-01 00:18:00     NaN                       
. ..
2014-04-09 23:42:00     NaN
2014-04-09 23:45:00     NaN
2014-04-09 23:48:00     NaN
2014-04-09 23:51:00     NaN
2014-04-09 23:54:00     NaN
2014-04-09 23:57:00     NaN
2014-04-10 00:00:00    99.0
Freq: 3T, dtype: float64


#CORRECT-resample those groups where we have points as follows:
from functools import partial
from pandas.tseries.frequencies import to_offset

def round(t, freq):    
    freq = to_offset(freq)    
    return pd.Timestamp((t.value // freq.delta.value) * freq.delta.value)


>>> ts.groupby(partial(round, freq='3T')).sum()

2014-01-01     0
2014-01-02     1
2014-01-03     2
2014-01-04     3
2014-01-05     4
2014-01-06     5
2014-01-07     6              
..
2014-04-04    93
2014-04-05    94
2014-04-06    95
2014-04-07    96
2014-04-08    97
2014-04-09    98
2014-04-10    99
dtype: int64





## with groupby 
df_re = pd.DataFrame({'A': [1] * 10 + [5] * 10,
                      'B': np.arange(20)})

>>> df_re
    A   B
0   1   0
1   1   1
2   1   2
3   1   3
4   1   4
5   1   5
6   1   6
.. ..  ..
13  5  13
14  5  14
15  5  15
16  5  16
17  5  17
18  5  18
19  5  19
[20 rows x 2 columns]
>>> df_re.groupby('A').rolling(4).B.mean()
A    
1  0      NaN
   1      NaN
   2      NaN
   3      1.5
   4      2.5
   5      3.5
   6      4.5
         ... 
5  13    11.5
   14    12.5
   15    13.5
   16    14.5
   17    15.5
   18    16.5
   19    17.5
Name: B, Length: 20, dtype: float64

#The expanding() method will accumulate a given operation (sum() in the example) 
#for all the members of each particular group.
>>> df_re.groupby('A').expanding().sum()
         A      B
A                
1 0    1.0    0.0
  1    2.0    1.0
  2    3.0    3.0
  3    4.0    6.0
  4    5.0   10.0
  5    6.0   15.0
  6    7.0   21.0
...    ...    ...
5 13  20.0   46.0
  14  25.0   60.0
  15  30.0   75.0
  16  35.0   91.0
  17  40.0  108.0
  18  45.0  126.0
  19  50.0  145.0
[20 rows x 2 columns]

#to use the resample() method to get a daily frequency in each group of dataframe 
#and wish to complete the missing values with the ffill() method.
df_re = pd.DataFrame({'date': pd.date_range(start='2016-01-01',
                              periods=4,
                      freq='W'),
                     'group': [1, 1, 2, 2],
                     'val': [5, 6, 7, 8]}).set_index('date')

>>> df_re
            group  val
date                  
2016-01-03      1    5
2016-01-10      1    6
2016-01-17      2    7
2016-01-24      2    8
>>> df_re.groupby('group').resample('1D').ffill()
                  group  val
group date                  
1     2016-01-03      1    5
      2016-01-04      1    5
      2016-01-05      1    5
      2016-01-06      1    5
      2016-01-07      1    5
      2016-01-08      1    5
      2016-01-09      1    5
...                 ...  ...
2     2016-01-18      2    7
      2016-01-19      2    7
      2016-01-20      2    7
      2016-01-21      2    7
      2016-01-22      2    7
      2016-01-23      2    7
      2016-01-24      2    8
[16 rows x 2 columns]


##Aggregation

#Resampling a DataFrame, 
#the default will be to act on all columns with the same function.

df = pd.DataFrame(np.random.randn(1000, 3),                    
            index=pd.date_range('1/1/2012', freq='S', periods=1000),   #Second                    
            columns=['A', 'B', 'C'])


>>> r = df.resample('3T') #3 min 

>>> r.mean()   #mean for every 3 min 
                        A         B         C
2012-01-01 00:00:00 -0.220339  0.034854 -0.073757
2012-01-01 00:03:00  0.037070  0.040013  0.053754
2012-01-01 00:06:00 -0.041597 -0.144562 -0.007614
2012-01-01 00:09:00  0.043127 -0.076432 -0.032570
2012-01-01 00:12:00 -0.027609  0.054618  0.056878
2012-01-01 00:15:00 -0.014181  0.043958  0.077734


#select a specific column or columns using standard getitem.
>>> r['A'].mean()
2012-01-01 00:00:00   -0.220339
2012-01-01 00:03:00    0.037070
2012-01-01 00:06:00   -0.041597
2012-01-01 00:09:00    0.043127
2012-01-01 00:12:00   -0.027609
2012-01-01 00:15:00   -0.014181
Freq: 3T, Name: A, dtype: float64

>>> r[['A','B']].mean()
                        A         B
2012-01-01 00:00:00 -0.220339  0.034854
2012-01-01 00:03:00  0.037070  0.040013
2012-01-01 00:06:00 -0.041597 -0.144562
2012-01-01 00:09:00  0.043127 -0.076432
2012-01-01 00:12:00 -0.027609  0.054618
2012-01-01 00:15:00 -0.014181  0.043958


#with a list or dict of functions to do aggregation with, outputting a DataFrame:
>>> r['A'].agg([np.sum, np.mean, np.std])  #every 3 min , from resmaple arg 3T 
                        sum      mean       std
2012-01-01 00:00:00 -39.660974 -0.220339  1.033912
2012-01-01 00:03:00   6.672559  0.037070  0.971503
2012-01-01 00:06:00  -7.487453 -0.041597  1.018418
2012-01-01 00:09:00   7.762901  0.043127  1.025842
2012-01-01 00:12:00  -4.969624 -0.027609  0.961649
2012-01-01 00:15:00  -1.418119 -0.014181  0.978847


#With  dict
>>> r['A'].agg({'result1' : np.sum, 'result2' : np.mean})
                      result2    result1
2012-01-01 00:00:00 -0.220339 -39.660974
2012-01-01 00:03:00  0.037070   6.672559
2012-01-01 00:06:00 -0.041597  -7.487453
2012-01-01 00:09:00  0.043127   7.762901
2012-01-01 00:12:00 -0.027609  -4.969624
2012-01-01 00:15:00 -0.014181  -1.418119


>>> r.agg({'A' : np.sum, 'B' : lambda x: np.std(x, ddof=1)}) #x is whole df of every 3Min 
                        A         B
2012-01-01 00:00:00 -39.660974  1.004756
2012-01-01 00:03:00   6.672559  0.963559
2012-01-01 00:06:00  -7.487453  0.950766
2012-01-01 00:09:00   7.762901  0.949182
2012-01-01 00:12:00  -4.969624  1.093736
2012-01-01 00:15:00  -1.418119  1.028869


#The function names can also be strings (implemented on the Resampled object)
>>> r.agg({'A' : 'sum', 'B' : 'std'})
                        A         B
2012-01-01 00:00:00 -39.660974  1.004756
2012-01-01 00:03:00   6.672559  0.963559
2012-01-01 00:06:00  -7.487453  0.950766
2012-01-01 00:09:00   7.762901  0.949182
2012-01-01 00:12:00  -4.969624  1.093736
2012-01-01 00:15:00  -1.418119  1.028869

#with multiple

>>> r.agg({'A' : ['sum','std'], 'B' : ['mean','std'] })
                        A                             B                           
                        sum       std      mean       std
2012-01-01 00:00:00 -39.660974  1.033912  0.034854  1.004756
2012-01-01 00:03:00   6.672559  0.971503  0.040013  0.963559
2012-01-01 00:06:00  -7.487453  1.018418 -0.144562  0.950766
2012-01-01 00:09:00   7.762901  1.025842 -0.076432  0.949182
2012-01-01 00:12:00  -4.969624  0.961649  0.054618  1.093736
2012-01-01 00:15:00  -1.418119  0.978847  0.043958  1.028869


##Time Series - Resampler - Methods 

Resampler.__iter__()                Groupby iterator 
Resampler.groups                    dict {group name -> group labels} 
Resampler.indices                   dict {group name -> group indices} 
Resampler.get_group(name[, obj])    Constructs NDFrame from group with provided name 

Resampler.apply(arg, *args, **kwargs)       Aggregate using callable, string, dict, or list of string/callables 
Resampler.aggregate(arg, *args, **kwargs)   Aggregate using callable, string, dict, or list of string/callables 
Resampler.transform(arg, *args, **kwargs)   Call function producing a like-indexed Series on each group and return 

Resampler.ffill([limit])            Forward fill the values 
Resampler.backfill([limit])         Backward fill the values 
Resampler.bfill([limit])            Backward fill the values 
Resampler.pad([limit])              Forward fill the values 
Resampler.nearest([limit])          Fill values with nearest neighbor starting from center 
Resampler.fillna(method[, limit])   Fill missing values 
Resampler.asfreq([fill_value])      return the values at the new freq, 
Resampler.interpolate([method, axis, limit, ...]) Interpolate values according to different methods. 


Resampler.count([_method])          Compute count of group, excluding missing values 
Resampler.nunique([_method])        Returns number of unique elements in the group 
Resampler.first([_method])          Compute first of group values 
Resampler.last([_method])           Compute last of group values 
Resampler.max([_method])            Compute max of group values 
Resampler.mean([_method])           Compute mean of groups, excluding missing values 
Resampler.median([_method])         Compute median of groups, excluding missing values 
Resampler.min([_method])            Compute min of group values 
Resampler.ohlc([_method])           Compute sum of values, excluding missing values 
Resampler.prod([_method, min_count]) Compute prod of group values 
Resampler.size()                    Compute group sizes 
Resampler.sem([_method])            Compute standard error of the mean of groups, excluding missing values 
Resampler.std([ddof])               Compute standard deviation of groups, excluding missing values 
Resampler.sum([_method, min_count]) Compute sum of group values 
Resampler.var([ddof])               Compute variance of groups, excluding missing values 



























###Statsmodel -  QuickIntro in general steps 
#requires patsy 
•Linear regression models
•Generalized linear models
•Discrete choice models
•Robust linear models
•Many models and functions for time series analysis
•Nonparametric estimators
•Input-output tools for producing tables in a number of formats (Text, LaTex, HTML) 
and for reading Stata files into NumPy and Pandas.
•Plotting functions

##The statistical model is assumed to be
Y = X*beta + mu, where mu ~ N(0,SIGMA)

#Depending on the properties of SIGMA, four classes available:
•GLS : generalized least squares for arbitrary covariance SIGMA
•OLS : ordinary least squares for i.i.d. errors SIGMA = I
•WLS : weighted least squares for heteroskedastic errors in diag(SIGMA)
•GLSAR : feasible generalized least squares with autocorrelated AR(p) errors SIGMA=SIGMA(p)

#All regression models define the same methods and follow the same structure, 
#and can be used in a similar fashion. 
#Some of them contain additional model specific methods and attributes.

#GLS is the superclass of the other regression classes except for RecursiveLS.

##Import 
import statsmodels.api as sm           #General imports 
import statsmodels.formula.api as smf  #for formula support in various regression 

>>> dir(sm)
>>> dir(smf)

#Note 'smf' contains Capital letters class(eg GLS) and small letters method
#Capital letter class is a Model based on data.exog, data.endog 
#small letter is defined as CLASS_NAME.from_formula(..) 
#which is formula based Model 

#eg, in statsmodels.formula.api.py
from statsmodels.regression.linear_model import GLS
gls = GLS.from_formula




##General Steps in using statsmodel 

import statsmodels.api as sm
import statsmodels.formula.api as smf

#load ur data or use exmaples from dataset 
data = sm.datasets.longley.load()  #data.data is full data (np.recarray)
#or load as pandas DF 
data = sm.datasets.longley.load_pandas()  #data.data is pandas DF 
#data.exog is x ie independent variable 
#data.endog is y ie dependent variable 
>>> data.exog_name  #features name 
['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP', 'YEAR']
>>> data.endog_name #dependent/response variable 
'TOTEMP'
>>> data.data  #numpy recarray data 
X = data.exog
y = data.endog

#or directly with pandas 
df = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv', index_col=0)
#or with stats model tools 
#check package names , https://vincentarelbundock.github.io/Rdatasets/datasets.html
df = sm.datasets.get_rdataset("longley", package='datasets', cache=True).data

#findout X and Y 
>>> df.columns
Index(['GNP.deflator', 'GNP', 'Unemployed', 'Armed.Forces', 'Population',
       'Year', 'Employed'],
      dtype='object')
X = df[ ['GNP.deflator', 'GNP', 'Unemployed', 'Armed.Forces', 'Population', 'Year']  ].dropna()
y = df['TOTEMP']

#In statsmodel, many Model  do not include intercept by default. 
#User is expected to manually add one if required by using add_constant
#add_constant: returns original values with a constant (column of ones) as the first column
X = sm.add_constant(X)
#instantiate some model 
model = sm.MODEL_CLASS(y,X)

#formula is based on endog_name ~ combination of exog_name 
model = smf.MODEL_CLASS("formula", data.data)

#or with pandas , Note "formula" automatically includes intercept term 
#formula is based on df.columns 
model = smf.MODEL_CLASS("formula", data = df) #Note data:E.g., a numpy structured or rec array, a dictionary, or a pandas DataFrame.

#fit 
results = model.fit()

#resullts contain many important stats 
dir(results)
results.params  #for params 

#Check regression diagnostics, - http://www.statsmodels.org/dev/diagnostic.html
#Heteroscedasticity, Autocorrelation,Non-Linearity ,Mutlicollinearity
#Normality, Outlier and Influence 

#get summary . Note the P-value<0.05 are significant ie CI does not cross zero
print(results.summary())

#given x , find y 
predicted_endog = model.predict(exog)

#Quick measurement of performance
#Split the data into train and test(use sklearn) and check with Test(true_endog)
#Continuous :statsmodels.tools.eval_measures
mse = statsmodels.tools.eval_measures.mse(predicted_endog, true_endog)

#Discreet: Quick measurement of performance 
#(there is no direct method), note result has .pred_table() but with only train data
np.mean(predicted_endog == true_endog)
#or crosstab 
pd.crosstab(predicted_endog ,true_endog,predicted_endog==true_endog,rownames=['predicted'], colnames=['true'],aggfunc=np.sum,, margins=False, normalize=False)# normalize: boolean, {'all', 'index', 'columns'},  for normalizing output 
#or 
import numpy as np
pred = np.array(mod_fit.predict(test_X) > threshold, dtype=float)
table = np.histogram2d(test_Y, pred, bins=2)[0] 



##RegressionResults generally has below attributes 
aic             Akaike's information criteria, small the better 
bic             Bayes' information criteria, small the better 
fvalue          F-statistic of the fully specified model
f_pvalue        p-value of the F-statistic, <0.05, model is significant 
fittedvalues    The predicted the values for the original 
mse_model       Mean squared error the model
mse_resid       Mean squared error of the residuals
mse_total       Total mean squared error
params          Model Parameters 
pvalues         The two-tailed p values for  params., <0.05, those params are significants 
resid           The residuals of the model.
rsquared        R-squared of a model with an intercept, near 1 is better 
rsquared_adj    Adjusted R-squared.  near 1 is better 


##Models generally have below attributes and methods 
df_model            The model degree of freedom, 
                    defined as the rank of the regressor matrix minus 1 if a constant is included. 
df_resid            The residual degree of freedom, 
                    defined as the number of observations minus the rank of the regressor matrix. 
endog_names         Names of endogenous variables 
exog_names          Names of exogenous variables 

#Important methods 
fit([method, cov_type, cov_kwds, use_t])            
    Full fit of the model. 
    Returns RegressionResults or it's subclss eg OLSResults,QuantRegResults,RecursiveLSResults 
fit_regularized([method, alpha, L1_wt, ...])        
    Return a regularized fit to a linear regression model. 
    Use this for overfitting problem.
    To know overfitting, split data into test/train(use sklearn) and check on test. 
    method : string
        Only the 'elastic_net' approach is currently implemented.
    alpha : scalar or array-like
        The penalty weight. 
        If a scalar, the same penalty weight applies to all variables in the model. 
        If a vector, it must have the same length as params, 
        and contains a penalty weight for each coefficient.
    L1_wt: scalar
        The fraction of the penalty given to the L1 penalty term. 
        Must be between 0 and 1 (inclusive). 
        If 0, the fit is a ridge fit, if 1 it is a lasso fit
from_formula(formula, data[, subset, drop_cols])    
    Create a Model from a formula and dataframe. 
get_distribution(params, scale[, exog, ...])        
    Returns a random number generator for the predictive distribution. 
predict(params[, exog])                             
    Return linear predicted values from a design matrix. 
whiten(X)                                           
    Whiten a series of columns 




###Statsmodel -  QuickIntro - with/without formula OLS  and Regression test
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
# Load data
dat = sm.datasets.get_rdataset("Guerry", "HistData").data
# Fit regression model (using the natural log of one of the regressors)
results = smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=dat).fit()
# Inspect the results
>>> print(results.summary())
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                Lottery   R-squared:                       0.348
Model:                            OLS   Adj. R-squared:                  0.333
Method:                 Least Squares   F-statistic:                     22.20
Date:                Tue, 28 Feb 2017   Prob (F-statistic):           1.90e-08
Time:                        21:38:05   Log-Likelihood:                -379.82
No. Observations:                  86   AIC:                             765.6
Df Residuals:                      83   BIC:                             773.0
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
===================================================================================
                      coef    std err          t      P>|t|      [0.025      0.975]
-----------------------------------------------------------------------------------
Intercept         246.4341     35.233      6.995      0.000     176.358     316.510
Literacy           -0.4889      0.128     -3.832      0.000      -0.743      -0.235
np.log(Pop1831)   -31.3114      5.977     -5.239      0.000     -43.199     -19.424
==============================================================================
Omnibus:                        3.713   Durbin-Watson:                   2.019
Prob(Omnibus):                  0.156   Jarque-Bera (JB):                3.394
Skew:                          -0.487   Prob(JB):                        0.183
Kurtosis:                       3.003   Cond. No.                         702.
==============================================================================
#Understanding 
Df Total            : These are the degrees of freedom associated with the sources of variance
                      ie no of observation 
Df Residuals        :Df total - Df Model 
Df Model            :No of independent variables (excluding intercept), more the better
std err             : also called bse , = standard deviation of parameter estimate
R-squared           :How model fits the data? Towards 1 is better 
                     x= independent variable 
                     y = dependent variable 
                     y^ = predicted y for each y 
                     Ey = mean of y 
                     1.SSR is the "regression sum of squares" 
                         =Sum(y^ - Ey)**2 for i=1..n
                     2.SSE is the "error sum of squares" 
                         =Sum(y - y^)**2 for i=1..n
                     3.SSTO is the "total sum of squares" 
                       . =Sum(y - Ey)^2  for i=1..n
                     #interpretation
                     r2 = SSR/SSTO=1-SSE/SSTO
                     "r2 ×100 percent of the variation in y is reduced by taking into account predictor x
                     or:
                     "r2 ×100 percent of the variation in y is 'explained by' the variation in predictor x."
Adj. R-squared      :adjusts for the number of terms in a model. 
                    If you add more and more useless variables to a model, 
                    adjusted r-squared will decrease. 
                    If you add more useful variables, adjusted r-squared will increase
                    
Prob (F-statistic)  : <0.05, model is significant 
AIC, BIC            : lower the better. Used for comparing two models 
Log-Likelihood      : This is minimized, lower the better 

t                   :These are the t-statistics used in testing 
                     Dividing the coefficient by its standard error calculates a t-value
P>|t|               : < 0.05, coefficient is significant , H0= coefficinet is zero 
[....]              : 95% CI                      

Omnibus test        H0: residuals are from normal distribution 
                    Prob < 0.05, means H0 is rejected 

Jarque-Bera     a goodness-of-fit test with normal distribution (normality test of data)
                HO: sample data have the skewness and kurtosis matching a normal distribution
                Samples from a normal distribution have an expected skewness of 0 and an expected  kurtosis of 3
                Prob < 0.05, reject H0 
                
Durbin-Watson   The Durbin Watson Test is a measure of autocorrelation (also called serial correlation) 
                in residuals from regression analysis. 
                Autocorrelation is the similarity of a time series over successive time intervals. 
                It can lead to underestimates of the standard error 
                and can cause you to think predictors are significant when they are not
                The Durbin Watson test reports a test statistic, with a value from 0 to 4, where:
                    •2 is no autocorrelation.
                    •0 to <2 is positive autocorrelation (common in time series data).
                    •>2 to 4 is negative autocorrelation (less common in time series data).


Condition no    This is used to measure how sensitive a Model is to changes or errors in the input,
                lower the better , very very large is worrysome as system becomes unstable(use regularization, then )
                SO, instead of model.fit(), use model.fit_regularized()

#Understanding 
1.low R-square and low p-value (p-value <= 0.05): This means that the model doesn't explain much of the variation in the response variable, but still this is considered better than having no model to explain the response variable as it is significant as per the p-value.
2.low R-square and high p-value (p-value > 0.05): This means that model doesn't explain much variation in the data and is not significant. We should discard such model as this is the worst scenario.
3.high R-square and low p-value: This means that model explains a lot of variation in the data and is also significant. This scenario is best of the four and the model is considered to be good in this case.
4.high R-square and high p-value: This means that variance in the data is explained by the model but it is not significant. We should not use such model for predictions.
                
                
                
                
##Normality of the residuals
#Jarque-Bera test:
name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']
test = sms.jarque_bera(results.resid)
>>> lzip(name, test)
[('Jarque-Bera', 3.3936080248431706),
 ('Chi^2 two-tail prob.', 0.18326831231663335),
 ('Skew', -0.486580343112234),
 ('Kurtosis', 3.003417757881633)]

#Omni test:
name = ['Chi^2', 'Two-tail probability']
test = sms.omni_normtest(results.resid)
>>> lzip(name, test)
[('Chi^2', 3.7134378115971831), ('Two-tail probability', 0.15618424580304813)]



##Influence tests
#OLSInfluence holds attributes and methods that allow users 
#to assess the influence of each observation. 
#For example, we can compute and extract the first few rows of DFbetas by:
from statsmodels.stats.outliers_influence import OLSInfluence
test_class = OLSInfluence(results)
import math 
>>> 2/math.sqrt(86)
0.21566554640687682
>>> test_class.dfbetas[:5,:]  #> 2/sqrt(N) outliers
array([[-0.00301154,  0.00290872,  0.00118179],
       [-0.06425662,  0.04043093,  0.06281609],
       [ 0.01554894, -0.03556038, -0.00905336],
       [ 0.17899858,  0.04098207, -0.18062352],
       [ 0.29679073,  0.21249207, -0.3213655 ]])

>>> where = test_class.dfbetas > 0.21567
#find index labels-.index[] or index of index row-iloc
test_class.dfbetas.iloc[where]
#or 
test_class.dfbetas.index[where]
#then drop those if required 
new_dat = dat.drop(index=test_class.dfbetas.index[~where], inplace=False)

#Explore other options by typing 
>>> dir(influence_test)

#Useful information on leverage can also be plotted:
#leverage is a measure of how far away the independent variable values 
#of an observation are from those of the other observations.
from statsmodels.graphics.regressionplots import plot_leverage_resid2
fig, ax = plt.subplots(figsize=(8,6))
fig = plot_leverage_resid2(results, ax = ax)


##Multicollinearity- Condition number:
#multicollinearity (also collinearity) : one predictor variable in a multiple regression model 
#can be linearly predicted from the others - which is bad 
>>> np.linalg.cond(results.model.exog)
702.17921454900659

##Autocorrelation Tests
#H0: the errors are uncorrelated between observations
#durbin_watson from summary() 
#or ,Ljung-Box test also the Box-Pierce test results are returned
ljung_test_stats, its_p_value, pierce_test_stats, its_p_value = sms.acorr_ljungbox(results.resid, boxpierce=True)
#or Breusch Godfrey Lagrange Multiplier tests for residual autocorrelation
lagrange_test_stats, its_p_value, f_test_stats, its_p_value = sms.acorr_breusch_godfrey(results)


##Heteroskedasticity tests
#Ho:error term has the same variance  in each observation
#Breush-Pagan test:
name = ['Lagrange multiplier statistic', 'p-value', 
        'f-value', 'f p-value']
test = sms.het_breushpagan(results.resid, results.model.exog)
>>> lzip(name, test)
[('Lagrange multiplier statistic', 4.8932133740940333),
 ('p-value', 0.086586905023518804),
 ('f-value', 2.5037159462564778),
 ('f p-value', 0.087940287826726846)]

#Goldfeld-Quandt test
name = ['F statistic', 'p-value']
test = sms.het_goldfeldquandt(results.resid, results.model.exog)
>>> lzip(name, test)
[('F statistic', 1.1002422436378141), ('p-value', 0.38202950686925286)]

##Linearity
#H0:  the linear specification is correct:
name = ['t value', 'p value']
test = sms.linear_harvey_collier(results)
>>> lzip(name, test)
[('t value', -1.0796490077784473), ('p value', 0.28346392475582971)]







###Statsmodel-QuickIntro - sm.OLS, smf.ols -  ordinary least squares model
#also called lm 
#Simple linear regression model
1. Objective: model the expected value of a continuous variable, Y, as a linear function of the continuous predictor, X, E(Yi) = β0 + β1xi 
2. Model structure: Yi = β0 + β1xi +  ei 
3. Model assumptions: Y is is normally distributed, 
   errors are normally distributed, ei ~ N(0, σ^2), and independent, 
   and X is fixed, and constant variance σ^2. 
4. Model fit: R 2, residual analysis, F-statistic

#important methods 
    •linreg.summary()          # summary of the model
    •linreg.fittedvalues       # fitted value from the model
    •linreg.predict()          # predict
    •linreg.rsquared_adj       # adjusted r-square
#Example 
import numpy as np
import statsmodels.api as sm
Y = [1,3,4,5,2,3,4]
X = range(1,8)
X = sm.add_constant(X)
model = sm.OLS(Y,X)
results = model.fit()
>>> results.params
array([ 2.14285714,  0.25      ])

#H0: coeffcients of intercept and one independent variable are unequal 
>>> print(results.t_test([1, 0]))
<T test: effect=array([ 2.14285714]), sd=array([[ 1.14062282]]), t=array([[ 1.87867287]]), p=array([[ 0.05953974]]), df_denom=5>

#H0: intercept and one independent variable is independent 
>>> print(results.f_test(np.identity(2)))
<F test: F=array([[ 19.46078431]]), p=[[ 0.00437251]], df_denom=5, df_num=2>


#With formula 
df = sm.datasets.get_rdataset("Guerry", "HistData").data
df = df[['Lottery', 'Literacy', 'Wealth', 'Region']].dropna()
df.head()
#C() means Categorical 
res = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region) -1 ', data=df).fit()
print(res.params)
#':' is interaction , a*b = a+b+a:b , -1 means exclude intercept term 
res1 = smf.ols(formula='Lottery ~ Literacy : Wealth - 1', data=df).fit()
res2 = smf.ols(formula='Lottery ~ Literacy * Wealth - 1', data=df).fit()
print(res1.params, res2.params)



##Example - OLS can have non-linear curve but linear in parameters
#because
#y = a*f1(x) + b*f2(x)  +... + e
#OLS is applicable as long as a, b, c ... are linear (ie there is no a*b or a^2 etc)
#f1(x), f2(x) could be non linear
##OLS - Without formula 
%matplotlib inline
from __future__ import print_function
import numpy as np
import statsmodels.api as sm
nsample = 50
sig = 0.25
x1 = np.linspace(0, 20, nsample)
#Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array
X = np.column_stack((x1, np.sin(x1), (x1-5)**2))
X = sm.add_constant(X)

beta = [5., 0.5, 0.5, -0.02] #actual coffiecients 
y_true = np.dot(X, beta)
y = y_true + sig * np.random.normal(size=nsample) #error term 

olsmod = sm.OLS(y, X)
olsres = olsmod.fit()
>>> print(olsres.summary())
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.975
Model:                            OLS   Adj. R-squared:                  0.973
Method:                 Least Squares   F-statistic:                     600.0
Date:                Tue, 28 Feb 2017   Prob (F-statistic):           7.17e-37
Time:                        21:33:42   Log-Likelihood:                -9.5061
No. Observations:                  50   AIC:                             27.01
Df Residuals:                      46   BIC:                             34.66
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.0395      0.104     48.462      0.000       4.830       5.249
x1             0.4963      0.016     30.946      0.000       0.464       0.529
x2             0.5288      0.063      8.388      0.000       0.402       0.656
x3            -0.0199      0.001    -14.100      0.000      -0.023      -0.017
==============================================================================
Omnibus:                        0.820   Durbin-Watson:                   2.161
Prob(Omnibus):                  0.664   Jarque-Bera (JB):                0.796
Skew:                           0.038   Prob(JB):                        0.672
Kurtosis:                       2.387   Cond. No.                         221.
==============================================================================
#In-sample prediction
ypred = olsres.predict(X)
print(ypred)
#predict out of sample
x1n = np.linspace(20.5,25, 10)
#Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array
Xnew = np.column_stack((x1n, np.sin(x1n), (x1n-5)**2)) 
Xnew = sm.add_constant(Xnew)

ynewpred =  olsres.predict(Xnew) # predict out of sample
print(ynewpred)

#Plot comparison
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.plot(x1, y, 'o', label="Data")
ax.plot(x1, y_true, 'b-', label="True")
#hstack - appending a,b 
ax.plot(np.hstack((x1, x1n)), np.hstack((ypred, ynewpred)), 'r', label="OLS prediction")
ax.legend(loc="best");

##Predicting with Formulas
from statsmodels.formula.api import ols
data = {"x1" : x1, "y" : y}
#use the I to indicate use of the Identity transform. 
#Ie., we don't want any expansion magic from using **2
res = ols("y ~ x1 + np.sin(x1) + I((x1-5)**2)", data=data).fit()

>>> res.params
Intercept           5.039514
x1                  0.496312
np.sin(x1)          0.528829
I((x1 - 5) ** 2)   -0.019855
dtype: float64
>>> res.predict(exog=dict(x1=x1n))
0    10.970907
1    10.821642
2    10.554124
3    10.215613
4     9.868320
5     9.574176
6     9.379671
7     9.304465
8     9.336582
9     9.435338
dtype: float64

 



###Statsmodel-QuickIntro - sm.GLM, smf.glm - Generalized Linear Models

1.Linear regression assumes that the response variable is normally distributed. 
  Generalized linear models can have response variables with distributions 
  other than the Normal distribution– they may even be categorical rather than continuous.
2.Relationship between the response and explanatory variables need not be 
  of the simple linear form.

#Imp methods 
    •linreg.summary()          # summary of the model
    •linreg.fittedvalues       # fitted value from the model
    •linreg.predict()          # predict
    •linreg.rsquared_adj       # adjusted r-square
    
#The distribution families (response variable distribution)
Family(link, variance)          The parent class for one-parameter exponential families. 
Binomial([link])                Binomial exponential family distribution. 
Gamma([link])                   Gamma exponential family distribution. 
Gaussian([link])                Gaussian exponential family distribution. 
InverseGaussian([link])         InverseGaussian exponential family. 
NegativeBinomial([link, alpha]) Negative Binomial exponential family. 
Poisson([link])                 Poisson exponential family. 
#Each family has few link functions 
#link function is transformation of response variable 
#such that transformed response variable is linked to independent variables linearly 

#Terminology
The variance function is a smooth function 
which depicts the variance of a random quantity as a function of its mean
A smooth function is a function that has derivatives 
of all orders everywhere in its domain

In parametric modeling, variance functions take on a parametric form 
and explicitly describe the relationship between the variance and the mean of a random quantity. 
In a non-parametric setting, the variance function is assumed 
to be a smooth function.

#Model                  Random              Link                Systematic 
Linear Regression       Normal              Identity            Continuous 
ANOVA                   Normal              Identity            Categorical 
ANCOVA                  Normal              Identity            Mixed 
Logistic Regression     Binomial            Logit               Mixed 
Loglinear               Poisson             Log                 Categorical 
Poisson Regression      Poisson             Log                 Mixed 
Multinomial response    Multinomial         Generalized Logit   Mixed 

#There are three components to any GLM:
1. Random Component – refers to the probability distribution of the response variable (Y); 
   e.g. normal distribution for Y in the linear regression, 
   or binomial distribution for Y in the binary logistic regression.  
   Also called a noise model or error model.  
   How is random error added to the prediction that comes out of the link function?
   Name of the family (such as binomial) specifies the conditional distribution 
   for example, for binomial it is  μ(1−μ)
2. Systematic Component - specifies the explanatory variables (X1, X2, ... Xk) 
   in the model, more specifically their linear combination in creating 
   the so called linear predictor; e.g., β0 + β1x1 + β2x2    
3. Link Function, η or g(μ) - specifies the link between random 
   and systematic components. 
   It says how the expected value of the response relates 
   to the linear predictor of explanatory variables; 
   e.g., η = g(E(Yi)) = E(Yi) for linear regression, 
   or  η = logit(π) for logistic regression.

#Assumptions:
1.The data Y1, Y2, ..., Yn are independently distributed, 
  i.e., cases are independent. 
2.The dependent variable Yi does NOT need to be normally distributed, 
  but it typically assumes a distribution from an exponential family 
  (e.g. binomial, Poisson, multinomial, normal,...) 
3.GLM does NOT assume a linear relationship between the dependent variable 
  and the independent variables, 
  but it does assume linear relationship between the transformed response 
  in terms of the link function and the explanatory variables; 
  e.g., for binary logistic regression logit(π) = β0 + βX. 
4.Independent (explanatory) variables can be even the power terms 
  or some other nonlinear transformations of the original independent variables. 
5.The homogeneity of variance does NOT need to be satisfied. 
  In fact, it is not even possible in many cases given the model structure, 
  and overdispersion (when the observed variance is larger than what the model assumes) 
  maybe present. 
6.Errors need to be independent but NOT normally distributed. 
7.It uses maximum likelihood estimation (MLE) 
  rather than ordinary least squares (OLS) to estimate the parameters, 
  and thus relies on large-sample approximations. 
8.Goodness-of-fit measures rely on sufficiently large samples, 
  where a heuristic rule is that not more than 20% of the expected cells counts 
  are less than 5. 

    
##Example 
import statsmodels.api as sm
data = sm.datasets.scotland.load()
data.exog = sm.add_constant(data.exog)
>>> dir(sm.families.family)
['Binomial', 'FLOAT_EPS', 'Family', 'Gamma', 'Gaussian', 'InverseGaussian', 'L',
 'NegativeBinomial', 'Poisson', 'Tweedie', 'V', '__builtins__', '__cached__', '_
_doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'np',
'special']
>>> sm.families.family.<familyname>.links
# Instantiate a gamma family model with the default link function.
gamma_model = sm.GLM(data.endog, data.exog, family=sm.families.Gamma())
gamma_results = gamma_model.fit()
>>> print(gamma_results.summary())
                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                      y   No. Observations:                   32
Model:                            GLM   Df Residuals:                       24
Model Family:                   Gamma   Df Model:                            7
Link Function:          inverse_power   Scale:                0.00358428317349
Method:                          IRLS   Log-Likelihood:                -83.017
Date:                Tue, 28 Feb 2017   Deviance:                     0.087389
Time:                        21:38:03   Pearson chi2:                   0.0860
No. Iterations:                     4                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0178      0.011     -1.548      0.122      -0.040       0.005
x1          4.962e-05   1.62e-05      3.060      0.002    1.78e-05    8.14e-05
x2             0.0020      0.001      3.824      0.000       0.001       0.003
x3         -7.181e-05   2.71e-05     -2.648      0.008      -0.000   -1.87e-05
x4             0.0001   4.06e-05      2.757      0.006    3.23e-05       0.000
x5         -1.468e-07   1.24e-07     -1.187      0.235   -3.89e-07    9.56e-08
x6            -0.0005      0.000     -2.159      0.031      -0.001   -4.78e-05
x7         -2.427e-06   7.46e-07     -3.253      0.001   -3.89e-06   -9.65e-07
==============================================================================


##GLM -  with formula for logit regression 
from __future__ import print_function
import statsmodels.api as sm
import statsmodels.formula.api as smf
star98 = sm.datasets.star98.load_pandas().data
formula = 'SUCCESS ~ LOWINC + PERASIAN + PERBLACK + PERHISP + PCTCHRT + \
           PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF'
dta = star98[['NABOVE', 'NBELOW', 'LOWINC', 'PERASIAN', 'PERBLACK', 'PERHISP',
              'PCTCHRT', 'PCTYRRND', 'PERMINTE', 'AVYRSEXP', 'AVSALK',
              'PERSPENK', 'PTRATIO', 'PCTAF']].copy()
endog = dta['NABOVE'] / (dta['NABOVE'] + dta.pop('NBELOW'))
del dta['NABOVE']
dta['SUCCESS'] = endog
mod1 = smf.glm(formula=formula, data=dta, family=sm.families.Binomial()).fit()
mod1.summary()
#doubling LOWINC
def double_it(x):
    return 2 * x
formula = 'SUCCESS ~ double_it(LOWINC) + PERASIAN + PERBLACK + PERHISP + PCTCHRT + \
           PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF'
mod2 = smf.glm(formula=formula, data=dta, family=sm.families.Binomial()).fit()
mod2.summary()
>>> print(mod1.params[1])
>>> print(mod2.params[1] * 2)
-0.0203959871548
-0.0203959871548



 



##GLM -  Binomial response data 
#dependent variable is binary/categorical( logistic regression, or logit regression, or logit model)



#Check all variables of the data 
print(sm.datasets.star98.NOTE)


#Load data
data = sm.datasets.star98.load()
data.exog = sm.add_constant(data.exog, prepend=False)  #add a constant term , Note by default, statsmodels does not have intercept term included into model. Add it explicitly

#The dependent variable is N x 2 (Success: NABOVE, Failure: NBELOW):
print(data.endog[:5,:])
[[ 452.  355.] 
[ 144.   40.] 
[ 337.  234.] 
[ 395.  178.] 
[   8.   57.]]

#The independent variables include all the other variables 


#Fit 
glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())
res = glm_binom.fit()
print(res.summary())
                 
Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:           ['y1', 'y2']   No. Observations:                  303
Model:                            GLM   Df Residuals:                      282
Model Family:                Binomial   Df Model:                           20
Link Function:                  logit   Scale:                             1.0
Method:                          IRLS   Log-Likelihood:                -2998.6
Date:                Mon, 17 Nov 2014   Deviance:                       4078.8
Time:                        13:07:02   Pearson chi2:                 4.05e+03
No. Iterations:                     7
==============================================================================                 
                 coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0168      0.000    -38.749      0.000        -0.018    -0.016
x2             0.0099      0.001     16.505      0.000         0.009     0.011
x3            -0.0187      0.001    -25.182      0.000        -0.020    -0.017
x4            -0.0142      0.000    -32.818      0.000        -0.015    -0.013
x5             0.2545      0.030      8.498      0.000         0.196     0.313
x6             0.2407      0.057      4.212      0.000         0.129     0.353
x7             0.0804      0.014      5.775      0.000         0.053     0.108
x8            -1.9522      0.317     -6.162      0.000        -2.573    -1.331
x9            -0.3341      0.061     -5.453      0.000        -0.454    -0.214
x10           -0.1690      0.033     -5.169      0.000        -0.233    -0.105
x11            0.0049      0.001      3.921      0.000         0.002     0.007
x12           -0.0036      0.000    -15.878      0.000        -0.004    -0.003
x13           -0.0141      0.002     -7.391      0.000        -0.018    -0.010
x14           -0.0040      0.000     -8.450      0.000        -0.005    -0.003
x15           -0.0039      0.001     -4.059      0.000        -0.006    -0.002
x16            0.0917      0.015      6.321      0.000         0.063     0.120
x17            0.0490      0.007      6.574      0.000         0.034     0.064
x18            0.0080      0.001      5.362      0.000         0.005     0.011
x19            0.0002   2.99e-05      7.428      0.000         0.000     0.000
x20           -0.0022      0.000     -6.445      0.000        -0.003    -0.002
const          2.9589      1.547      1.913      0.056        -0.073     5.990
==============================================================================



#Quantities of interest
print('Total number of trials:',  data.endog[0].sum())
print('Parameters: ', res.params)
print('T-values: ', res.tvalues)

Total number of trials: 807.0
Parameters:  [-0.01681504  0.00992548 -0.01872421 -0.01423856  0.25448717  0.24069366  
0.08040867 -1.9521605  -0.33408647 -0.16902217  0.0049167  -0.00357996 
-0.01407656 -0.00400499 -0.0039064   0.0917143   0.04898984  0.00804074  
0.00022201 -0.00224925  2.95887793]
T-values:  [-38.74908321  16.50473627 -25.1821894  -32.81791308   8.49827113   
4.21247925   5.7749976   -6.16191078  -5.45321673  -5.16865445   
3.92119964 -15.87825999  -7.39093058  -8.44963886  -4.05916246   
6.3210987    6.57434662   5.36229044   7.42806363  -6.44513698   
1.91301155]



#It uses below methods 
#scipy.stats.scoreatpercentile(a, per )
#Calculate the score at a given percentile of the input sequence, a.
#For example, the score at per=50 is the median

#First differences: 
#We hold all explanatory variables constant at their means 
#and manipulate the percentage of low income households to assess its impact on the response variables:

means = data.exog.mean(axis=0) #DF's mean , axis =0 means each column's means 

means25 = means.copy()
means25[0] = stats.scoreatpercentile(data.exog[:,0], 25)
means75 = means.copy()
means75[0] = lowinc_75per = stats.scoreatpercentile(data.exog[:,0], 75)
resp_25 = res.predict(means25)
resp_75 = res.predict(means75)
diff = resp_75 - resp_25

#The interquartile first difference for the percentage of low income households 
print("%2.4f%%" % (diff*100))
-11.8753%


#Plots
nobs = res.nobs
y = data.endog[:,0]/data.endog.sum(1)  #axis=1
yhat = res.mu


from statsmodels.graphics.api import abline_plot
fig, ax = plt.subplots()
ax.scatter(yhat, y)
line_fit = sm.OLS(y, sm.add_constant(yhat, prepend=True)).fit()
abline_plot(model_results=line_fit, ax=ax)  #Plots a line given an intercept and slope.

ax.set_title('Model Fit Plot')
ax.set_ylabel('Observed values')
ax.set_xlabel('Fitted values');



#Plot yhat vs. Pearson residuals:
fig, ax = plt.subplots()
ax.scatter(yhat, res.resid_pearson)
ax.hlines(0, 0, 1)
ax.set_xlim(0, 1)
ax.set_title('Residual Dependence Plot')
ax.set_ylabel('Pearson Residuals')
ax.set_xlabel('Fitted values')


#Histogram of standardized deviance residuals:
from scipy import stats
fig, ax = plt.subplots()
resid = res.resid_deviance.copy()
resid_std = stats.zscore(resid)
ax.hist(resid_std, bins=25)
ax.set_title('Histogram of standardized deviance residuals');


#QQ Plot of Deviance Residuals:
from statsmodels import graphics
graphics.gofplots.qqplot(resid, line='r')  #Q-Q plot of the quantiles of x versus the quantiles/ppf of a distribution., by default, checks how data matches norm dist 





##GLM - Gamma for proportional count response

#check data
print(sm.datasets.scotland.DESCRLONG)
#Load 
data2 = sm.datasets.scotland.load()
data2.exog = sm.add_constant(data2.exog, prepend=False)
print(data2.exog[:5,:])  #x 
print(data2.endog[:5])   #y 
[[   712.      21.     105.      82.4  13566.      12.3  14952.       1. ] 
[   643.      26.5     97.      80.2  13566.      15.3  17039.5      1. ] 
[   679.      28.3    113.      86.3   9611.      13.9  19215.7      1. ] 
[   801.      27.1    109.      80.4   9483.      13.6  21707.1      1. ] 
[   753.      22.     115.      64.7   9265.      14.6  16566.       1. ]]
[ 60.3  52.3  53.4  57.   68.7]

#fit and summary 
glm_gamma = sm.GLM(data2.endog, data2.exog, family=sm.families.Gamma())
glm_results = glm_gamma.fit()
print(glm_results.summary())                 
Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:                      y   No. Observations:                   32
Model:                            GLM   Df Residuals:                       24
Model Family:                   Gamma   Df Model:                            7
Link Function:          inverse_power   Scale:                0.00358428317349
Method:                          IRLS   Log-Likelihood:                -83.017
Date:                Mon, 17 Nov 2014   Deviance:                     0.087389
Time:                        13:07:07   Pearson chi2:                   0.0860
No. Iterations:                     6
==============================================================================                 
                coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1          4.962e-05   1.62e-05      3.060      0.002      1.78e-05  8.14e-05
x2             0.0020      0.001      3.824      0.000         0.001     0.003
x3         -7.181e-05   2.71e-05     -2.648      0.008        -0.000 -1.87e-05
x4             0.0001   4.06e-05      2.757      0.006      3.23e-05     0.000
x5         -1.468e-07   1.24e-07     -1.187      0.235     -3.89e-07  9.56e-08
x6            -0.0005      0.000     -2.159      0.031        -0.001 -4.78e-05
x7         -2.427e-06   7.46e-07     -3.253      0.001     -3.89e-06 -9.65e-07
const         -0.0178      0.011     -1.548      0.122        -0.040     0.005
==============================================================================






##GLM-  Gaussian distribution with a noncanonical link(link identity for OLS)

>>> sm.families.family.Gaussian.links
[<class 'statsmodels.genmod.families.links.log'>, 
<class 'statsmodels.genmod.families.links.identity'>, 
<class 'statsmodels.genmod.families.links.inverse_power'>]

#generate data 

nobs2 = 100
x = np.arange(nobs2)
np.random.seed(54321)
X = np.column_stack((x,x**2))
X = sm.add_constant(X, prepend=False)

#each response is transformed with exp(), hence use log as link function 
lny = np.exp(-(.03*x + .0001*x**2 - 1.0)) + .001 * np.random.rand(nobs2) 

#fit 
gauss_log = sm.GLM(lny, X, family=sm.families.Gaussian(sm.families.links.log))
gauss_log_results = gauss_log.fit()
print(gauss_log_results.summary())
                 
Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:                      y   No. Observations:                  100
Model:                            GLM   Df Residuals:                       97
Model Family:                Gaussian   Df Model:                            2
Link Function:                    log   Scale:               1.05311425588e-07
Method:                          IRLS   Log-Likelihood:                 662.92
Date:                Mon, 17 Nov 2014   Deviance:                   1.0215e-05
Time:                        13:07:07   Pearson chi2:                 1.02e-05
No. Iterations:                     7
==============================================================================                 
coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0300    5.6e-06  -5361.316      0.000        -0.030    -0.030
x2         -9.939e-05   1.05e-07   -951.091      0.000     -9.96e-05 -9.92e-05
const          1.0003   5.39e-05   1.86e+04      0.000         1.000     1.000
==============================================================================












###Statsmodel-QuickIntro - sm.GLS, smf.gls 
#Generalized least squares model with a general covariance structure.
#GLS can be used to perform linear regression 
#when there is a certain degree of correlation between the residuals in a regression model.


#Example 
import numpy as np
import statsmodels.api as sm
data = sm.datasets.longley.load()
data.exog = sm.add_constant(data.exog)
ols_resid = sm.OLS(data.endog, data.exog).fit().resid
res_fit = sm.OLS(ols_resid[1:], ols_resid[:-1]).fit()
>>> data.data.shape #16 observations 
(16,)
>>> ols_resid.shape #for each observations 
(16,)

rho = res_fit.params
#rho is a consistent estimator of the correlation of the residuals 
#from an OLS fit of the longley data. 
#It is assumed that this is the true rho of the AR process data.
from scipy.linalg import toeplitz
order = toeplitz(np.arange(16))
sigma = rho**order

#sigma is an n x n matrix of the autocorrelation structure of the data.
gls_model = sm.GLS(data.endog, data.exog, sigma=sigma)
gls_results = gls_model.fit()
print(gls_results.summary())




###Statsmodel-QuickIntro - sm.GLSAR, smf.glsar
#A regression model with an autoregressive , AR(p) covariance structure
#The autoregressive model specifies that the output variable depends linearly on its own previous values 
#when there is a AR(p) correlation between the residuals in a regression model.


#Example 
import statsmodels.api as sm
X = range(1,8)
X = sm.add_constant(X)  #X= intercepts, one_var 
Y = [1,3,4,5,8,10,9]
model = sm.GLSAR(Y, X, rho=2) #rho : Order(p) of the autoregressive covariance, AR coefficients
for i in range(6):
    results = model.fit()
    print("AR coefficients: {0}".format(model.rho))
    rho, sigma = sm.regression.yule_walker(results.resid,order=model.order) #Estimate AR(p) parameters from a sequence X using Yule-Walker equation.
    model = sm.GLSAR(Y, X, rho)
#Output 
AR coefficients: [ 0.  0.]
AR coefficients: [-0.52571491 -0.84496178]
AR coefficients: [-0.6104153  -0.86656458]
AR coefficients: [-0.60439494 -0.857867  ]
AR coefficients: [-0.6048218  -0.85846157]
AR coefficients: [-0.60479146 -0.85841922]
>>> results.params
array([-0.66661205,  1.60850853])
>>> results.tvalues
array([ -2.10304127,  21.8047269 ])

#H0:coeffcients of intercept and one independent variable are unequal 
>>> print(results.t_test([1, 0]))
<T test: effect=array([-0.66661205]), sd=array([[ 0.31697526]]), t=array([[-2.10304127]]), p=array([[ 0.06309969]]), df_denom=3>

#H0: intercept and one independent variable is independent 
>>> print(results.f_test(np.identity(2)))
<F test: F=array([[ 1815.23061844]]), p=[[ 0.00002372]], df_denom=3, df_num=2>

#Or, equivalently
model2 = sm.GLSAR(Y, X, rho=2)
res = model2.iterative_fit(maxiter=6)
>>> model2.rho
array([-0.60479146, -0.85841922])


   
###Statsmodel-QuickIntro - sm.WLS, smf.wls
#Weighted Least square 
#A regression model with diagonal but non-identity covariance structure
#A special case of generalized least squares 
#when all the off-diagonal entries of correlation matrix of the residuals) are null; 
#the variances of the observations (along the covariance matrix diagonal) may still be unequal (heteroscedasticity).


#Example 
%matplotlib inline
from __future__ import print_function
import numpy as np
from scipy import stats
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.sandbox.regression.predstd import wls_prediction_std
from statsmodels.iolib.table import (SimpleTable, default_txt_fmt)
np.random.seed(1024)

#Artificial data: Heteroscedasticity 2 groups
#Model assumptions:
#•Misspecification: true model is quadratic, estimate only linear
#•Independent noise/error term
#•Two groups for error variance, low and high variance groups

nsample = 50
x = np.linspace(0, 20, nsample)
X = np.column_stack((x, (x - 5)**2)) #convert 1D arrays to 2D array via columnwise appending 
X = sm.add_constant(X)
beta = [5., 0.5, -0.01] #actual intercept, x1, x2
sig = 0.5

w = np.ones(nsample)
w[nsample * 6//10:] = 3   #first few elements  1, next elements 3 
y_true = np.dot(X, beta)
e = np.random.normal(size=nsample)
y = y_true + sig * w * e  #Heteroscedasticity 2 groups
X = X[:,[0,1]]

#WLS knowing the true variance ratio of heteroscedasticity
mod_wls = sm.WLS(y, X, weights=1./w)
res_wls = mod_wls.fit()
>>> print(res_wls.summary())
                            WLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.910
Model:                            WLS   Adj. R-squared:                  0.909
Method:                 Least Squares   F-statistic:                     487.9
Date:                Tue, 28 Feb 2017   Prob (F-statistic):           8.52e-27
Time:                        21:35:25   Log-Likelihood:                -57.048
No. Observations:                  50   AIC:                             118.1
Df Residuals:                      48   BIC:                             121.9
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.2726      0.185     28.488      0.000       4.900       5.645
x1             0.4379      0.020     22.088      0.000       0.398       0.478
==============================================================================
Omnibus:                        5.040   Durbin-Watson:                   2.242
Prob(Omnibus):                  0.080   Jarque-Bera (JB):                6.431
Skew:                           0.024   Prob(JB):                       0.0401
Kurtosis:                       4.756   Cond. No.                         17.0
==============================================================================

#Estimate an OLS model for comparison:
In [4]:
res_ols = sm.OLS(y, X).fit()
print(res_ols.params)
print(res_wls.params)
[ 5.24256099  0.43486879]
[ 5.27260714  0.43794441]

#Compare the WLS standard errors to heteroscedasticity corrected OLS standard errors:
#vertically stack 
#bse : standard errors of the parameter estimates.
#HCn_se : heteroskedasticity robust standard errors
se = np.vstack([[res_wls.bse], [res_ols.bse], [res_ols.HC0_se], 
                [res_ols.HC1_se], [res_ols.HC2_se], [res_ols.HC3_se]])
se = np.round(se,4)
colnames = ['x1', 'const']
rownames = ['WLS', 'OLS', 'OLS_HC0', 'OLS_HC1', 'OLS_HC3', 'OLS_HC3']
tabl = SimpleTable(se, colnames, rownames, txt_fmt=default_txt_fmt)
>>> print(tabl)
=====================
          x1   const 
---------------------
WLS     0.1851 0.0198
OLS     0.2707 0.0233
OLS_HC0 0.194  0.0281
OLS_HC1 0.198  0.0287
OLS_HC3 0.2003 0.029 
OLS_HC3 0.207   0.03 
---------------------

#Calculate OLS prediction interval:

#Returns the variance/covariance matrix of  the estimates of params 
covb = res_ols.cov_params()
#mse_resid:Mean squared error of the residuals
prediction_var = res_ols.mse_resid + (X * np.dot(covb,X.T).T).sum(1) #axis=1 , rowwise 
prediction_std = np.sqrt(prediction_var)
tppf = stats.t.ppf(0.975, res_ols.df_resid)  #Percent point function (inverse of cdf) , given q, gives x 

'''
def wls_prediction_std(res, exog=None, weights=None, alpha=0.05):
    calculate standard deviation and confidence interval for prediction

    applies to WLS and OLS, not to general GLS,
    that is independently but not identically distributed observations

    Parameters
    ----------
    res : regression result instance
        results of WLS or OLS regression required attributes see notes
    exog : array_like (optional)
        exogenous variables for points to predict
    weights : scalar or array_like (optional)
        weights as defined for WLS (inverse of variance of observation)
    alpha : float (default: alpha = 0.05)
        confidence level for two-sided hypothesis

    Returns
    -------
    predstd : array_like, 1d
        standard error of prediction
        same length as rows of exog
    interval_l, interval_u : array_like
        lower und upper confidence bounds
'''
prstd_ols, iv_l_ols, iv_u_ols = wls_prediction_std(res_ols)

#Draw a plot to compare predicted values in WLS and OLS:
prstd, iv_l, iv_u = wls_prediction_std(res_wls)

fig, ax = plt.subplots(figsize=(8,6))
ax.plot(x, y, 'o', label="Data")
ax.plot(x, y_true, 'b-', label="True")
# OLS
ax.plot(x, res_ols.fittedvalues, 'r--')
ax.plot(x, iv_u_ols, 'r--', label="OLS")
ax.plot(x, iv_l_ols, 'r--')
# WLS
ax.plot(x, res_wls.fittedvalues, 'g--.')
ax.plot(x, iv_u, 'g--', label="WLS")
ax.plot(x, iv_l, 'g--')
ax.legend(loc="best");
 
##Feasible Weighted Least Squares (2-stage FWLS)
resid1 = res_ols.resid[w==1.]
var1 = resid1.var(ddof=int(res_ols.df_model)+1)
resid2 = res_ols.resid[w!=1.]
var2 = resid2.var(ddof=int(res_ols.df_model)+1)
w_est = w.copy()
w_est[w!=1.] = np.sqrt(var2) / np.sqrt(var1)

res_fwls = sm.WLS(y, X, 1./w_est).fit()
>>> print(res_fwls.summary())
                            WLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.914
Model:                            WLS   Adj. R-squared:                  0.912
Method:                 Least Squares   F-statistic:                     507.1
Date:                Tue, 28 Feb 2017   Prob (F-statistic):           3.65e-27
Time:                        21:35:26   Log-Likelihood:                -55.777
No. Observations:                  50   AIC:                             115.6
Df Residuals:                      48   BIC:                             119.4
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.2710      0.177     29.828      0.000       4.916       5.626
x1             0.4390      0.019     22.520      0.000       0.400       0.478
==============================================================================
Omnibus:                        4.076   Durbin-Watson:                   2.251
Prob(Omnibus):                  0.130   Jarque-Bera (JB):                4.336
Skew:                           0.003   Prob(JB):                        0.114
Kurtosis:                       4.443   Cond. No.                         16.5
==============================================================================
Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.




###Statsmodel-QuickIntro - sm.RecursiveLS
#Recursive least squares is an expanding window version of ordinary least squares. 
#regression coefficients are computed recursively, 

#the RLS exhibits extremely fast convergence. 
#However, this benefit comes at the cost of high computational complexity

#This model applies the Kalman filter to compute recursive estimates of the coefficients and recursive residuals

#Example 
%matplotlib inline
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from pandas_datareader.data import DataReader
np.set_printoptions(suppress=True)

#Example 1: Copper
#We first consider parameter stability in the copper dataset (description below).

print(sm.datasets.copper.DESCRLONG)
dta = sm.datasets.copper.load_pandas().data
dta.index = pd.date_range('1951-01-01', '1975-01-01', freq='AS')
endog = dta['WORLDCONSUMPTION']
# To the regressors in the dataset, we add a column of ones for an intercept
exog = sm.add_constant(dta[['COPPERPRICE', 'INCOMEINDEX', 'ALUMPRICE', 'INVENTORYINDEX']])


mod = sm.RecursiveLS(endog, exog)
res = mod.fit()
>>> print(res.summary())
                           Statespace Model Results                           
==============================================================================
Dep. Variable:       WORLDCONSUMPTION   No. Observations:                   25
Model:                    RecursiveLS   Log Likelihood                -153.737
Date:                Tue, 28 Feb 2017   AIC                            317.474
Time:                        21:33:53   BIC                            323.568
Sample:                    01-01-1951   HQIC                           319.164
                         - 01-01-1975                                         
Covariance Type:            nonrobust                                         
==================================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
----------------------------------------------------------------------------------
const          -6513.9917   2367.677     -2.751      0.006   -1.12e+04   -1873.430
COPPERPRICE      -13.6553     15.035     -0.908      0.364     -43.123      15.813
INCOMEINDEX     1.209e+04    762.598     15.853      0.000    1.06e+04    1.36e+04
ALUMPRICE         70.1441     32.668      2.147      0.032       6.117     134.171
INVENTORYINDEX   275.2802   2120.295      0.130      0.897   -3880.423    4430.983
===================================================================================
Ljung-Box (Q):                       14.53   Jarque-Bera (JB):                 1.91
Prob(Q):                              0.75   Prob(JB):                         0.39
Heteroskedasticity (H):               3.48   Skew:                            -0.74
Prob(H) (two-sided):                  0.12   Kurtosis:                         2.65
===================================================================================
Warnings:
[1] Parameters and covariance matrix estimates are RLS estimates conditional on the entire sample.


#recursive_coefficients:Estimates of regression coefficients, recursively estimated 
#.filtered[n] - gives Estimates of regression coefficients at iteration=n
print(res.recursive_coefficients.filtered[0])
res.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10,6));
#Output 
[    2.88890056     4.94425548  1505.26965677  1856.55054283  1598.00138036
  2171.98758114  -889.37441819   122.17728201 -4184.25981611 -6242.72030126
 -7111.44631736 -6400.3801834  -6090.45249953 -7154.96507467 -6290.92418756
 -5805.25729045 -6219.31739926 -6684.49572441 -6430.13765533 -5957.57703202
 -6407.05926045 -5983.49245301 -5224.7166129  -5286.62118319 -6513.99172374]
 
#CUSUM : Cumulative sum of standardized recursive residuals statistics
#but usually it is more convenient to visually check 
#for parameter stability using the plot_cusum method. 

#In the plot below, the CUSUM statistic does not move outside of the 5% significance bands, 
#so we fail to reject the null hypothesis of stable parameters at the 5% level.

print(res.cusum)
fig = res.plot_cusum();
#output 
[ 0.27819009  0.51898841  1.05399638  1.94931307  2.44814772  3.37264805
  3.04780591  2.47334163  2.96189656  2.58229863  1.49435136 -0.50007183
 -2.01111298 -1.75501921 -0.90602604 -1.41034654 -0.8038184   0.71255314
  1.19153109 -0.93368668]
 
#cusum_squares: Cumulative sum of squares of standardized recursive residuals statistics
#but it is similarly more convenient to check it visually, 
#using the plot_cusum_squares method. 

#In the plot below, the CUSUM of squares statistic does not move outside of the 5% significance bands, 
#so we fail to reject the null hypothesis of stable parameters at the 5% level.
res.plot_cusum_squares();




###Statsmodel-QuickIntro - Discrete Choice Models
sm.Logit, smf.logit                       Binary choice logit model
                                          Uses the cumulative distribution function of the logistic distribution
sm.MNLogit, smf.mnlogit                   Multinomial logit model
                                          Generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes
sm.NegativeBinomial,smt.negativebinomial  Discreet model for count data
                                          Assumes that the mean and variance are not the same 
                                          Note it is a generalized linear model with a NegativeBinomial
sm.Poisson,smf.poisson                    Discreet model for count data 
                                          Assumes that the mean and variance are the same
                                          Note it is a generalized linear model with a Poisson distribution and a log link
sm.Probit, smf.probit                     Binary choice probit model
                                          Uses the  cumulative distribution function of the standard normal distribution

#important methods 
•logitreg.summary()          # summary of the model
•logitreg.fittedvalues       # fitted value from the model
•logitreg.predict()          # predict
•logfitreg.pred_table()      # confusion matrix


##Example with non-formula 
from __future__ import print_function
import numpy as np
import statsmodels.api as sm
#Load data from Spector and Mazzeo (1980). 
#Examples follow Greene's Econometric Analysis Ch. 21 (5th Edition).
spector_data = sm.datasets.spector.load()
spector_data.exog = sm.add_constant(spector_data.exog, prepend=False)
print(spector_data.exog[:5,:])
print(spector_data.endog[:5])
#Linear Probability Model (OLS)
lpm_mod = sm.OLS(spector_data.endog, spector_data.exog)
lpm_res = lpm_mod.fit()
>>> print('Parameters: ', lpm_res.params[:-1])
Parameters:  [ 0.46385168  0.01049512  0.37855479]

#Logit Model
logit_mod = sm.Logit(spector_data.endog, spector_data.exog)
logit_res = logit_mod.fit(disp=0)
>>> print('Parameters: ', logit_res.params)
Parameters:  [  2.82611259   0.09515766   2.37868766 -13.02134686]
>>> print(logit_res.summary())
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                      y   No. Observations:                   32
Model:                          Logit   Df Residuals:                       28
Method:                           MLE   Df Model:                            3
Date:                Tue, 28 Feb 2017   Pseudo R-squ.:                  0.3740
Time:                        21:32:53   Log-Likelihood:                -12.890
converged:                       True   LL-Null:                       -20.592
                                        LLR p-value:                  0.001502
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1             2.8261      1.263      2.238      0.025       0.351       5.301
x2             0.0952      0.142      0.672      0.501      -0.182       0.373
x3             2.3787      1.065      2.234      0.025       0.292       4.465
const        -13.0213      4.931     -2.641      0.008     -22.687      -3.356
==============================================================================

#Marginal Effects
#measure of the instantaneous effect that a change in a particular independent variable 
#has on the predicted probability of , when the other covariates are kept fixed
#H0: no effect.
margeff = logit_res.get_margeff()
>>> print(margeff.summary())
        Logit Marginal Effects       
=====================================
Dep. Variable:                      y
Method:                          dydx
At:                           overall
==============================================================================
                dy/dx    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1             0.3626      0.109      3.313      0.001       0.148       0.577
x2             0.0122      0.018      0.686      0.493      -0.023       0.047
x3             0.3052      0.092      3.304      0.001       0.124       0.486
==============================================================================



##l1 regularized logit
#The regularization parameter alpha should be a scalar 
#or have the same shape as results.params


alpha = 0.1 * len(spector_data.endog) * np.ones(spector_data.exog.shape[1])
#Choose not to regularize the constant
alpha[-1] = 0

logit_l1_res = logit_mod.fit_regularized(method='l1', alpha=alpha)
Optimization terminated successfully.    (Exit mode 0)
            Current function value: 0.610664637583
            Iterations: 13
            Function evaluations: 15
            Gradient evaluations: 13

print(logit_l1_res.summary())
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                      y   No. Observations:                   32
Model:                          Logit   Df Residuals:                       30
Method:                           MLE   Df Model:                            1
Date:                Wed, 14 Aug 2013   Pseudo R-squ.:                 0.07470
Time:                        17:18:27   Log-Likelihood:                -19.054
converged:                       True   LL-Null:                       -20.592
                                        LLR p-value:                   0.07944
==============================================================================
                 coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1                  0        nan        nan        nan           nan       nan
x2             0.1524      0.112      1.361      0.174        -0.067     0.372
x3                  0        nan        nan        nan           nan       nan
const         -4.0457      2.566     -1.577      0.115        -9.075     0.984
==============================================================================


#Probit Model
probit_mod = sm.Probit(spector_data.endog, spector_data.exog)
probit_res = probit_mod.fit()
probit_margeff = probit_res.get_margeff()
print('Parameters: ', probit_res.params)
print('Marginal effects: ')
print(probit_margeff.summary())
#Output 
#the partial effects of each explanatory variable 
#on the probability of the observed dependent variable

Optimization terminated successfully.
         Current function value: 0.400588
         Iterations 6
Parameters:  [ 1.62581004  0.05172895  1.42633234 -7.45231965]
Marginal effects: 
       Probit Marginal Effects       
=====================================
Dep. Variable:                      y
Method:                          dydx
At:                           overall
==============================================================================
                dy/dx    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1             0.3608      0.113      3.182      0.001       0.139       0.583
x2             0.0115      0.018      0.624      0.533      -0.025       0.048
x3             0.3165      0.090      3.508      0.000       0.140       0.493
==============================================================================

#Multinomial Logit
#Load data from the American National Election Studies:
anes_data = sm.datasets.anes96.load()
anes_exog = anes_data.exog
anes_exog = sm.add_constant(anes_exog, prepend=False)

mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)
mlogit_res = mlogit_mod.fit()
>>> print(mlogit_res.params)
#Output 
Optimization terminated successfully.
         Current function value: 1.548647
         Iterations 7
[[ -1.15359746e-02  -8.87506530e-02  -1.05966699e-01  -9.15567017e-02
   -9.32846040e-02  -1.40880692e-01]
 [  2.97714352e-01   3.91668642e-01   5.73450508e-01   1.27877179e+00
    1.34696165e+00   2.07008014e+00]
 [ -2.49449954e-02  -2.28978371e-02  -1.48512069e-02  -8.68134503e-03
   -1.79040689e-02  -9.43264870e-03]
 [  8.24914421e-02   1.81042758e-01  -7.15241904e-03   1.99827955e-01
    2.16938850e-01   3.21925702e-01]
 [  5.19655317e-03   4.78739761e-02   5.75751595e-02   8.44983753e-02
    8.09584122e-02   1.08894083e-01]
 [ -3.73401677e-01  -2.25091318e+00  -3.66558353e+00  -7.61384309e+00
   -7.06047825e+00  -1.21057509e+01]]
   


#Marginal Effects #p-value >0.05, that variable not significant 
mlogit_margeff = mlogit_res.get_margeff()
#the partial effects of each explanatory variable 
#on the probability of the observed dependent variable
print(mlogit_margeff.summary())
       MNLogit Marginal Effects      
=====================================
Dep. Variable:                      y
Method:                          dydx
At:                           overall
==============================================================================
       y=0      dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1             0.0087      0.004      2.250      0.024         0.001     0.016
x2            -0.0978      0.008    -12.153      0.000        -0.114    -0.082
x3             0.0027      0.001      3.856      0.000         0.001     0.004
x4            -0.0199      0.008     -2.420      0.016        -0.036    -0.004
x5            -0.0060      0.002     -2.977      0.003        -0.010    -0.002
------------------------------------------------------------------------------
       y=1      dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1             0.0070      0.004      1.791      0.073        -0.001     0.015
x2            -0.0502      0.007     -6.824      0.000        -0.065    -0.036
x3            -0.0021      0.001     -2.789      0.005        -0.004    -0.001
x4            -0.0054      0.008     -0.636      0.525*        -0.022     0.011
x5            -0.0055      0.002     -2.707      0.007        -0.010    -0.002
------------------------------------------------------------------------------
       y=2      dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0039      0.003     -1.246      0.213*        -0.010     0.002
x2            -0.0282      0.006     -4.972      0.000        -0.039    -0.017
x3            -0.0010      0.001     -1.523      0.128*        -0.002     0.000
x4             0.0066      0.007      0.964      0.335*        -0.007     0.020
x5             0.0010      0.002      0.530      0.596*        -0.003     0.005
------------------------------------------------------------------------------
       y=3      dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0018      0.002     -0.940      0.347*       -0.006     0.002
x2            -0.0057      0.003     -1.798      0.072*        -0.012     0.001
x3         -4.249e-05      0.000     -0.110      0.912*        -0.001     0.001
x4            -0.0055      0.004     -1.253      0.210*        -0.014     0.003
x5             0.0005      0.001      0.469      0.639*        -0.002     0.003
------------------------------------------------------------------------------
       y=4      dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0010      0.003     -0.330      0.741        -0.007     0.005
x2             0.0199      0.005      3.672      0.000         0.009     0.030
x3             0.0005      0.001      0.815      0.415        -0.001     0.002
x4             0.0017      0.006      0.268      0.789        -0.011     0.014
x5             0.0021      0.002      1.119      0.263        -0.002     0.006
------------------------------------------------------------------------------
       y=5      dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0015      0.004     -0.429      0.668        -0.009     0.005
x2             0.0376      0.007      5.404      0.000         0.024     0.051
x3            -0.0007      0.001     -0.949      0.343        -0.002     0.001
x4             0.0047      0.008      0.604      0.546        -0.011     0.020
x5             0.0025      0.002      1.140      0.254        -0.002     0.007
------------------------------------------------------------------------------
       y=6      dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0074      0.003     -2.205      0.027        -0.014    -0.001
x2             0.1246      0.008     14.875      0.000         0.108     0.141
x3             0.0006      0.001      0.942      0.346        -0.001     0.002
x4             0.0177      0.007      2.403      0.016         0.003     0.032
x5             0.0054      0.002      2.490      0.013         0.001     0.010
==============================================================================

##l1 regularized Multinomial Logit
#The regularization parameter alpha should be a scalar or have the same shape as as results.params

alpha = 10 * np.ones((mlogit_mod.K, mlogit_mod.J - 1))

#Choose not to regularize the constant
alpha[-1, :] = 0
mlogit_mod2 = sm.MNLogit(anes_data.endog, anes_exog)
mlogit_l1_res = mlogit_mod2.fit_regularized(method='l1', alpha=alpha)
Optimization terminated successfully.    (Exit mode 0)
            Current function value: 1.61249508025
            Iterations: 122
            Function evaluations: 144
            Gradient evaluations: 122

print(mlogit_l1_res.summary())  #>0.05, that value not significant 
                          MNLogit Regression Results                          
==============================================================================
Dep. Variable:                      y   No. Observations:                  944
Model:                        MNLogit   Df Residuals:                      912
Method:                           MLE   Df Model:                           26
Date:                Wed, 14 Aug 2013   Pseudo R-squ.:                  0.1558
Time:                        17:18:35   Log-Likelihood:                -1477.7
converged:                       True   LL-Null:                       -1750.3
                                        LLR p-value:                 1.464e-98
==============================================================================
       y=1       coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1             0.0010      0.033      0.030      0.976        -0.065     0.067
x2                  0        nan        nan        nan           nan       nan
x3            -0.0218      0.006     -3.494      0.000        -0.034    -0.010
x4                  0        nan        nan        nan           nan       nan
x5                  0        nan        nan        nan           nan       nan
const          0.9100      0.328      2.774      0.006         0.267     1.553
------------------------------------------------------------------------------
       y=2       coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0586      0.038     -1.530      0.126        -0.134     0.017
x2             0.0319      0.090      0.353      0.724        -0.145     0.208
x3            -0.0199      0.008     -2.577      0.010        -0.035    -0.005
x4             0.0331      0.075      0.440      0.660        -0.114     0.180
x5             0.0445      0.020      2.197      0.028         0.005     0.084
const         -0.5008      0.686     -0.730      0.466        -1.846     0.845
------------------------------------------------------------------------------
       y=3       coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0615      0.057     -1.086      0.277        -0.172     0.049
x2             0.1205      0.139      0.868      0.385        -0.151     0.392
x3            -0.0081      0.011     -0.741      0.459        -0.029     0.013
x4                  0        nan        nan        nan           nan       nan
x5             0.0325      0.029      1.108      0.268        -0.025     0.090
const         -2.0828      0.929     -2.242      0.025        -3.904    -0.262
------------------------------------------------------------------------------
       y=4       coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0477      0.042     -1.144      0.252        -0.129     0.034
x2             0.8321      0.105      7.918      0.000         0.626     1.038
x3            -0.0049      0.008     -0.613      0.540        -0.020     0.011
x4             0.0236      0.082      0.288      0.774        -0.137     0.185
x5             0.0766      0.024      3.233      0.001         0.030     0.123
const         -5.2613      0.842     -6.246      0.000        -6.912    -3.610
------------------------------------------------------------------------------
       y=5       coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0522      0.037     -1.413      0.158        -0.125     0.020
x2             0.9233      0.092     10.012      0.000         0.743     1.104
x3            -0.0140      0.007     -1.972      0.049        -0.028 -8.34e-05
x4             0.0549      0.071      0.769      0.442        -0.085     0.195
x5             0.0727      0.020      3.618      0.000         0.033     0.112
const         -4.8678      0.727     -6.693      0.000        -6.293    -3.442
------------------------------------------------------------------------------
       y=6       coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.0952      0.039     -2.442      0.015        -0.172    -0.019
x2             1.5681      0.116     13.472      0.000         1.340     1.796
x3            -0.0056      0.007     -0.752      0.452        -0.020     0.009
x4             0.1466      0.076      1.925      0.054        -0.003     0.296
x5             0.0968      0.022      4.376      0.000         0.053     0.140
const         -9.3154      0.913    -10.207      0.000       -11.104    -7.527
==============================================================================

  
   

##Poisson
#Load the Rand data
rand_data = sm.datasets.randhie.load()
rand_exog = rand_data.exog.view(float).reshape(len(rand_data.exog), -1)
rand_exog = sm.add_constant(rand_exog, prepend=False)
poisson_mod = sm.Poisson(rand_data.endog, rand_exog)
poisson_res = poisson_mod.fit(method="newton")
>>> print(poisson_res.summary())
Optimization terminated successfully.
         Current function value: 3.091609
         Iterations 12
                          Poisson Regression Results                          
==============================================================================
Dep. Variable:                      y   No. Observations:                20190
Model:                        Poisson   Df Residuals:                    20180
Method:                           MLE   Df Model:                            9
Date:                Tue, 28 Feb 2017   Pseudo R-squ.:                 0.06343
Time:                        21:32:54   Log-Likelihood:                -62420.
converged:                       True   LL-Null:                       -66647.
                                        LLR p-value:                     0.000
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1            -0.0525      0.003    -18.216      0.000      -0.058      -0.047
x2            -0.2471      0.011    -23.272      0.000      -0.268      -0.226
x3             0.0353      0.002     19.302      0.000       0.032       0.039
x4            -0.0346      0.002    -21.439      0.000      -0.038      -0.031
x5             0.2717      0.012     22.200      0.000       0.248       0.296
x6             0.0339      0.001     60.098      0.000       0.033       0.035
x7            -0.0126      0.009     -1.366      0.172      -0.031       0.005
x8             0.0541      0.015      3.531      0.000       0.024       0.084
x9             0.2061      0.026      7.843      0.000       0.155       0.258
const          0.7004      0.011     62.741      0.000       0.678       0.722
==============================================================================

#Marginal Effects
poisson_margeff = poisson_res.get_margeff()
#the partial effects of each explanatory variable 
#on the probability of the observed dependent variable
print poisson_margeff.summary()
       Poisson Marginal Effects      
=====================================
Dep. Variable:                      y
Method:                          dydx
At:                           overall
==============================================================================
                dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
x1            -0.1503      0.008    -18.164      0.000        -0.166    -0.134
x2            -0.7068      0.031    -23.164      0.000        -0.767    -0.647
x3             0.1009      0.005     19.240      0.000         0.091     0.111
x4            -0.0989      0.005    -21.354      0.000        -0.108    -0.090
x5             0.7772      0.035     22.106      0.000         0.708     0.846
x6             0.0971      0.002     58.303      0.000         0.094     0.100
x7            -0.0361      0.026     -1.366      0.172        -0.088     0.016
x8             0.1546      0.044      3.530      0.000         0.069     0.240
x9             0.5896      0.075      7.839      0.000         0.442     0.737
==============================================================================


#l1 regularized Poisson model

poisson_mod2 = sm.Poisson(rand_data.endog, rand_exog)
alpha = 0.1 * len(rand_data.endog) * np.ones(rand_exog.shape[1])
alpha[-1] = 0

poisson_l1_res = poisson_mod2.fit_regularized(method='l1', alpha=alpha)
Optimization terminated successfully.    (Exit mode 0)
            Current function value: 3.13647450062
            Iterations: 19
            Function evaluations: 35
            Gradient evaluations: 19









#Negative Binomial
mod_nbin = sm.NegativeBinomial(rand_data.endog, rand_exog)
res_nbin = mod_nbin.fit(disp=False)
>>> print(res_nbin.summary())
/private/tmp/statsmodels/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
  "Check mle_retvals", ConvergenceWarning)
                     NegativeBinomial Regression Results                      
==============================================================================
Dep. Variable:                      y   No. Observations:                20190
Model:               NegativeBinomial   Df Residuals:                    20180
Method:                           MLE   Df Model:                            9
Date:                Tue, 28 Feb 2017   Pseudo R-squ.:                 0.01845
Time:                        21:32:56   Log-Likelihood:                -43384.
converged:                      False   LL-Null:                       -44199.
                                        LLR p-value:                     0.000
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1            -0.0580      0.006     -9.517      0.000      -0.070      -0.046
x2            -0.2678      0.023    -11.802      0.000      -0.312      -0.223
x3             0.0412      0.004      9.937      0.000       0.033       0.049
x4            -0.0381      0.003    -11.219      0.000      -0.045      -0.031
x5             0.2690      0.030      8.981      0.000       0.210       0.328
x6             0.0382      0.001     26.081      0.000       0.035       0.041
x7            -0.0441      0.020     -2.200      0.028      -0.083      -0.005
x8             0.0172      0.036      0.477      0.633      -0.054       0.088
x9             0.1780      0.074      2.397      0.017       0.032       0.324
const          0.6636      0.025     26.787      0.000       0.615       0.712
alpha          1.2930      0.019     69.477      0.000       1.256       1.329
==============================================================================

##Alternative solvers
#The default method for fitting discrete data MLE models is Newton-Raphson. 
#You can use other solvers by using the method argument:
mlogit_res = mlogit_mod.fit(method='bfgs', maxiter=100)
>>> print(mlogit_res.summary())
Warning: Maximum number of iterations has been exceeded.
         Current function value: 1.548650
         Iterations: 100
         Function evaluations: 106
         Gradient evaluations: 106
                          MNLogit Regression Results                          
==============================================================================
Dep. Variable:                      y   No. Observations:                  944
Model:                        MNLogit   Df Residuals:                      908
Method:                           MLE   Df Model:                           30
Date:                Tue, 28 Feb 2017   Pseudo R-squ.:                  0.1648
Time:                        21:32:56   Log-Likelihood:                -1461.9
converged:                      False   LL-Null:                       -1750.3
                                        LLR p-value:                1.827e-102
==============================================================================
       y=1       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1            -0.0116      0.034     -0.338      0.735      -0.079       0.056
x2             0.2973      0.094      3.175      0.001       0.114       0.481
x3            -0.0250      0.007     -3.825      0.000      -0.038      -0.012
x4             0.0821      0.074      1.116      0.264      -0.062       0.226
x5             0.0052      0.018      0.294      0.769      -0.029       0.040
const         -0.3689      0.630     -0.586      0.558      -1.603       0.866
------------------------------------------------------------------------------
       y=2       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1            -0.0888      0.039     -2.269      0.023      -0.166      -0.012
x2             0.3913      0.108      3.615      0.000       0.179       0.603
x3            -0.0229      0.008     -2.897      0.004      -0.038      -0.007
x4             0.1808      0.085      2.120      0.034       0.014       0.348
x5             0.0478      0.022      2.145      0.032       0.004       0.091
const         -2.2451      0.763     -2.942      0.003      -3.741      -0.749
------------------------------------------------------------------------------
       y=3       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1            -0.1062      0.057     -1.861      0.063      -0.218       0.006
x2             0.5730      0.159      3.614      0.000       0.262       0.884
x3            -0.0149      0.011     -1.313      0.189      -0.037       0.007
x4            -0.0075      0.126     -0.060      0.952      -0.255       0.240
x5             0.0575      0.034      1.711      0.087      -0.008       0.123
const         -3.6592      1.156     -3.164      0.002      -5.926      -1.393
------------------------------------------------------------------------------
       y=4       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1            -0.0914      0.044     -2.085      0.037      -0.177      -0.005
x2             1.2826      0.129      9.937      0.000       1.030       1.536
x3            -0.0085      0.008     -1.008      0.314      -0.025       0.008
x4             0.2012      0.094      2.136      0.033       0.017       0.386
x5             0.0850      0.026      3.240      0.001       0.034       0.136
const         -7.6589      0.960     -7.982      0.000      -9.540      -5.778
------------------------------------------------------------------------------
       y=5       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1            -0.0934      0.039     -2.374      0.018      -0.170      -0.016
x2             1.3451      0.117     11.485      0.000       1.116       1.575
x3            -0.0180      0.008     -2.362      0.018      -0.033      -0.003
x4             0.2161      0.085      2.542      0.011       0.049       0.383
x5             0.0808      0.023      3.517      0.000       0.036       0.126
const         -7.0401      0.844     -8.344      0.000      -8.694      -5.387
------------------------------------------------------------------------------
       y=6       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
x1            -0.1409      0.042     -3.345      0.001      -0.224      -0.058
x2             2.0686      0.143     14.433      0.000       1.788       2.349
x3            -0.0095      0.008     -1.164      0.244      -0.025       0.006
x4             0.3216      0.091      3.532      0.000       0.143       0.500
x5             0.1087      0.025      4.299      0.000       0.059       0.158
const        -12.0913      1.059    -11.415      0.000     -14.167     -10.015
==============================================================================
/private/tmp/statsmodels/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
  "Check mle_retvals", ConvergenceWarning)

##Formula based Discrete choice model 
#Fair's Affair data
#A survey of women only was conducted in 1974 by Redbook 
#asking about extramarital affairs.

%matplotlib inline
from __future__ import print_function
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.formula.api import logit, probit, poisson, ols

>>> print(sm.datasets.fair.SOURCE)
Fair, Ray. 1978. "A Theory of Extramarital Affairs," `Journal of Political
Economy`, February, 45-61.
The data is available at http://fairmodel.econ.yale.edu/rayfair/pdf/2011b.htm
In [3]:
print( sm.datasets.fair.NOTE)
::
    Number of observations: 6366
    Number of variables: 9
    Variable name definitions:
        rate_marriage   : How rate marriage, 1 = very poor, 2 = poor, 3 = fair,
                        4 = good, 5 = very good
        age             : Age
        yrs_married     : No. years married. Interval approximations. See
                        original paper for detailed explanation.
        children        : No. children
        religious       : How relgious, 1 = not, 2 = mildly, 3 = fairly,
                        4 = strongly
        educ            : Level of education, 9 = grade school, 12 = high
                        school, 14 = some college, 16 = college graduate,
                        17 = some graduate school, 20 = advanced degree
        occupation      : 1 = student, 2 = farming, agriculture; semi-skilled,
                        or unskilled worker; 3 = white-colloar; 4 = teacher
                        counselor social worker, nurse; artist, writers;
                        technician, skilled worker, 5 = managerial,
                        administrative, business, 6 = professional with
                        advanced degree
        occupation_husb : Husband's occupation. Same as occupation.
        affairs         : measure of time spent in extramarital affairs
    See the original paper for more details.


dta = sm.datasets.fair.load_pandas().data
dta['affair'] = (dta['affairs'] > 0).astype(float)

>>> print(dta.head(10))
   rate_marriage   age  yrs_married  children  religious  educ  occupation  \
0            3.0  32.0          9.0       3.0        3.0  17.0         2.0   
1            3.0  27.0         13.0       3.0        1.0  14.0         3.0   
2            4.0  22.0          2.5       0.0        1.0  16.0         3.0   
3            4.0  37.0         16.5       4.0        3.0  16.0         5.0   
4            5.0  27.0          9.0       1.0        1.0  14.0         3.0   
5            4.0  27.0          9.0       0.0        2.0  14.0         3.0   
6            5.0  37.0         23.0       5.5        2.0  12.0         5.0   
7            5.0  37.0         23.0       5.5        2.0  12.0         2.0   
8            3.0  22.0          2.5       0.0        2.0  12.0         3.0   
9            3.0  27.0          6.0       0.0        1.0  16.0         3.0   
   occupation_husb   affairs  affair  
0              5.0  0.111111     1.0  
1              4.0  3.230769     1.0  
2              5.0  1.400000     1.0  
3              5.0  0.727273     1.0  
4              4.0  4.666666     1.0  
5              4.0  4.666666     1.0  
6              4.0  0.852174     1.0  
7              3.0  1.826086     1.0  
8              3.0  4.799999     1.0  
9              5.0  1.333333     1.0  

>>> print(dta.describe())
       rate_marriage          age  yrs_married     children    religious  \
count    6366.000000  6366.000000  6366.000000  6366.000000  6366.000000   
mean        4.109645    29.082862     9.009425     1.396874     2.426170   
std         0.961430     6.847882     7.280120     1.433471     0.878369   
min         1.000000    17.500000     0.500000     0.000000     1.000000   
25%         4.000000    22.000000     2.500000     0.000000     2.000000   
50%         4.000000    27.000000     6.000000     1.000000     2.000000   
75%         5.000000    32.000000    16.500000     2.000000     3.000000   
max         5.000000    42.000000    23.000000     5.500000     4.000000   
              educ   occupation  occupation_husb      affairs       affair  
count  6366.000000  6366.000000      6366.000000  6366.000000  6366.000000  
mean     14.209865     3.424128         3.850141     0.705374     0.322495  
std       2.178003     0.942399         1.346435     2.203374     0.467468  
min       9.000000     1.000000         1.000000     0.000000     0.000000  
25%      12.000000     3.000000         3.000000     0.000000     0.000000  
50%      14.000000     3.000000         4.000000     0.000000     0.000000  
75%      16.000000     4.000000         5.000000     0.484848     1.000000  
max      20.000000     6.000000         6.000000    57.599991     1.000000  

affair_mod = logit("affair ~ occupation + educ + occupation_husb" 
                   "+ rate_marriage + age + yrs_married + children"
                   " + religious", dta).fit()

>>> print(affair_mod.summary())
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                 affair   No. Observations:                 6366
Model:                          Logit   Df Residuals:                     6357
Method:                           MLE   Df Model:                            8
Date:                Tue, 28 Feb 2017   Pseudo R-squ.:                  0.1327
Time:                        21:32:44   Log-Likelihood:                -3471.5
converged:                       True   LL-Null:                       -4002.5
                                        LLR p-value:                5.807e-224
===================================================================================
                      coef    std err          z      P>|z|      [0.025      0.975]
-----------------------------------------------------------------------------------
Intercept           3.7257      0.299     12.470      0.000       3.140       4.311
occupation          0.1602      0.034      4.717      0.000       0.094       0.227
educ               -0.0392      0.015     -2.533      0.011      -0.070      -0.009
occupation_husb     0.0124      0.023      0.541      0.589      -0.033       0.057
rate_marriage      -0.7161      0.031    -22.784      0.000      -0.778      -0.655
age                -0.0605      0.010     -5.885      0.000      -0.081      -0.040
yrs_married         0.1100      0.011     10.054      0.000       0.089       0.131
children           -0.0042      0.032     -0.134      0.893      -0.066       0.058
religious          -0.3752      0.035    -10.792      0.000      -0.443      -0.307
===================================================================================

#How well are we predicting?
#pred_table[i,j] refers to the number of times 'i' was observed and the model predicted 'j'. 
#Correct predictions are along the diagonal
>>> affair_mod.pred_table() 
array([[ 3882.,   431.],
       [ 1326.,   727.]])
       
#The coefficients of the discrete choice model do not tell us much. 
#But  marginal effects.
#Ho: no effect 
mfx = affair_mod.get_margeff()
>>> print(mfx.summary())
        Logit Marginal Effects       
=====================================
Dep. Variable:                 affair
Method:                          dydx
At:                           overall
===================================================================================
                     dy/dx    std err          z      P>|z|      [0.025      0.975]
-----------------------------------------------------------------------------------
occupation          0.0293      0.006      4.744      0.000       0.017       0.041
educ               -0.0072      0.003     -2.538      0.011      -0.013      -0.002
occupation_husb     0.0023      0.004      0.541      0.589      -0.006       0.010
rate_marriage      -0.1308      0.005    -26.891      0.000      -0.140      -0.121
age                -0.0110      0.002     -5.937      0.000      -0.015      -0.007
yrs_married         0.0201      0.002     10.327      0.000       0.016       0.024
children           -0.0008      0.006     -0.134      0.893      -0.012       0.011
religious          -0.0685      0.006    -11.119      0.000      -0.081      -0.056
===================================================================================

respondent1000 = dta.ix[1000]
>>> print(respondent1000)
rate_marriage       4.000000
age                37.000000
yrs_married        23.000000
children            3.000000
religious           3.000000
educ               12.000000
occupation          3.000000
occupation_husb     4.000000
affairs             0.521739
affair              1.000000
Name: 1000, dtype: float64


resp = dict(zip(range(1,9), respondent1000[["occupation", "educ", 
                                            "occupation_husb", "rate_marriage", 
                                            "age", "yrs_married", "children", 
                                            "religious"]].tolist()))
resp.update({0 : 1})
>>> print(resp)
{1: 3.0, 2: 12.0, 3: 4.0, 4: 4.0, 5: 37.0, 6: 23.0, 7: 3.0, 8: 3.0, 0: 1}
In [13]:
mfx = affair_mod.get_margeff(atexog=resp)
>>> print(mfx.summary())
        Logit Marginal Effects       
=====================================
Dep. Variable:                 affair
Method:                          dydx
At:                           overall
===================================================================================
                     dy/dx    std err          z      P>|z|      [0.025      0.975]
-----------------------------------------------------------------------------------
occupation          0.0400      0.008      4.711      0.000       0.023       0.057
educ               -0.0098      0.004     -2.537      0.011      -0.017      -0.002
occupation_husb     0.0031      0.006      0.541      0.589      -0.008       0.014
rate_marriage      -0.1788      0.008    -22.743      0.000      -0.194      -0.163
age                -0.0151      0.003     -5.928      0.000      -0.020      -0.010
yrs_married         0.0275      0.003     10.256      0.000       0.022       0.033
children           -0.0011      0.008     -0.134      0.893      -0.017       0.014
religious          -0.0937      0.009    -10.722      0.000      -0.111      -0.077
===================================================================================

>>> affair_mod.predict(respondent1000)
#ERROR 
>>> affair_mod.fittedvalues[1000]
0.075161592850562342

>>> affair_mod.model.cdf(affair_mod.fittedvalues[1000])
0.51878155721214692

##probit 
affair_mod_prob = probit("affair ~ occupation + educ + occupation_husb" 
                   "+ rate_marriage + age + yrs_married + children"
                   " + religious", dta).fit()

>>> print(affair_mod_prob.summary())

#The coefficients of the discrete choice model do not tell us much. 
#But  marginal effects.
#Ho: no effect 
mfx = affair_mod_prob.get_margeff()
>>> print(mfx.summary())

##Logit vs Probit
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111)
support = np.linspace(-6, 6, 1000)
ax.plot(support, stats.logistic.cdf(support), 'r-', label='Logistic')
ax.plot(support, stats.norm.cdf(support), label='Probit')
ax.legend();
 

fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111)
support = np.linspace(-6, 6, 1000)
ax.plot(support, stats.logistic.pdf(support), 'r-', label='Logistic')
ax.plot(support, stats.norm.pdf(support), label='Probit')
ax.legend();
 
##Genarlized Linear Model Example

>>> print(sm.datasets.star98.SOURCE)
Jeff Gill's `Generalized Linear Models: A Unified Approach`
http://jgill.wustl.edu/research/books.html

>>> print(sm.datasets.star98.DESCRLONG)
This data is on the California education policy and outcomes (STAR program
results for 1998.  The data measured standardized testing by the California
Department of Education that required evaluation of 2nd - 11th grade students
by the the Stanford 9 test on a variety of subjects.  This dataset is at
the level of the unified school district and consists of 303 cases.  The
binary response variable represents the number of 9th graders scoring
over the national median value on the mathematics exam.
The data used in this example is only a subset of the original source.

>>> print(sm.datasets.star98.NOTE)
::
    Number of Observations - 303 (counties in California).
    Number of Variables - 13 and 8 interaction terms.
    Definition of variables names::
        NABOVE   - Total number of students above the national median for the
                   math section.
        NBELOW   - Total number of students below the national median for the
                   math section.
        LOWINC   - Percentage of low income students
        PERASIAN - Percentage of Asian student
        PERBLACK - Percentage of black students
        PERHISP  - Percentage of Hispanic students
        PERMINTE - Percentage of minority teachers
        AVYRSEXP - Sum of teachers' years in educational service divided by the
                number of teachers.
        AVSALK   - Total salary budget including benefits divided by the number
                   of full-time teachers (in thousands)
        PERSPENK - Per-pupil spending (in thousands)
        PTRATIO  - Pupil-teacher ratio.
        PCTAF    - Percentage of students taking UC/CSU prep courses
        PCTCHRT  - Percentage of charter schools
        PCTYRRND - Percentage of year-round schools
        The below variables are interaction terms of the variables defined
        above.
        PERMINTE_AVYRSEXP
        PEMINTE_AVSAL
        AVYRSEXP_AVSAL
        PERSPEN_PTRATIO
        PERSPEN_PCTAF
        PTRATIO_PCTAF
        PERMINTE_AVTRSEXP_AVSAL
        PERSPEN_PTRATIO_PCTAF

dta = sm.datasets.star98.load_pandas().data
>>> print(dta.columns)
Index(['NABOVE', 'NBELOW', 'LOWINC', 'PERASIAN', 'PERBLACK', 'PERHISP',
       'PERMINTE', 'AVYRSEXP', 'AVSALK', 'PERSPENK', 'PTRATIO', 'PCTAF',
       'PCTCHRT', 'PCTYRRND', 'PERMINTE_AVYRSEXP', 'PERMINTE_AVSAL',
       'AVYRSEXP_AVSAL', 'PERSPEN_PTRATIO', 'PERSPEN_PCTAF', 'PTRATIO_PCTAF',
       'PERMINTE_AVYRSEXP_AVSAL', 'PERSPEN_PTRATIO_PCTAF'],
      dtype='object')

>>> print(dta[['NABOVE', 'NBELOW', 'LOWINC', 'PERASIAN', 'PERBLACK', 'PERHISP', 'PERMINTE']].head(10))
   NABOVE  NBELOW    LOWINC   PERASIAN   PERBLACK    PERHISP   PERMINTE
0   452.0   355.0  34.39730  23.299300  14.235280  11.411120  15.918370
1   144.0    40.0  17.36507  29.328380   8.234897   9.314884  13.636360
2   337.0   234.0  32.64324   9.226386  42.406310  13.543720  28.834360
3   395.0   178.0  11.90953  13.883090   3.796973  11.443110  11.111110
4     8.0    57.0  36.88889  12.187500  76.875000   7.604167  43.589740
5  1348.0   899.0  20.93149  28.023510   4.643221  13.808160  15.378490
6   477.0   887.0  53.26898   8.447858  19.374830  37.905330  25.525530
7   565.0   347.0  15.19009   3.665781   2.649680  13.092070   6.203008
8   205.0   320.0  28.21582  10.430420   6.786374  32.334300  13.461540
9   469.0   598.0  32.77897  17.178310  12.484930  28.323290  27.259890


formula = 'NABOVE + NBELOW ~ LOWINC + PERASIAN + PERBLACK + PERHISP + PCTCHRT '
formula += '+ PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF'

##Using GLM and Binomial distribution
#Toss a six-sided die 5 times, what's the probability of exactly 2 fours?
>>> stats.binom(5, 1./6).pmf(2)  #pdf=pmf in discrete model 
0.16075102880658435

from scipy.misc import comb
>>> comb(5,2) * (1/6.)**2 * (5/6.)**3 #comb(n,k): The number of combinations of N things taken k at a time
0.1607510288065844

from statsmodels.formula.api import glm
glm_mod = glm(formula, dta, family=sm.families.Binomial()).fit()

>>> print(glm_mod.summary())
                  Generalized Linear Model Regression Results                   
================================================================================
Dep. Variable:     ['NABOVE', 'NBELOW']   No. Observations:                  303
Model:                              GLM   Df Residuals:                      282
Model Family:                  Binomial   Df Model:                           20
Link Function:                    logit   Scale:                             1.0
Method:                            IRLS   Log-Likelihood:                -2998.6
Date:                  Tue, 28 Feb 2017   Deviance:                       4078.8
Time:                          21:32:46   Pearson chi2:                     9.60
No. Iterations:                       5                                         
============================================================================================
                               coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------------
Intercept                    2.9589      1.547      1.913      0.056      -0.073       5.990
LOWINC                      -0.0168      0.000    -38.749      0.000      -0.018      -0.016
PERASIAN                     0.0099      0.001     16.505      0.000       0.009       0.011
PERBLACK                    -0.0187      0.001    -25.182      0.000      -0.020      -0.017
PERHISP                     -0.0142      0.000    -32.818      0.000      -0.015      -0.013
PCTCHRT                      0.0049      0.001      3.921      0.000       0.002       0.007
PCTYRRND                    -0.0036      0.000    -15.878      0.000      -0.004      -0.003
PERMINTE                     0.2545      0.030      8.498      0.000       0.196       0.313
AVYRSEXP                     0.2407      0.057      4.212      0.000       0.129       0.353
PERMINTE:AVYRSEXP           -0.0141      0.002     -7.391      0.000      -0.018      -0.010
AVSALK                       0.0804      0.014      5.775      0.000       0.053       0.108
PERMINTE:AVSALK             -0.0040      0.000     -8.450      0.000      -0.005      -0.003
AVYRSEXP:AVSALK             -0.0039      0.001     -4.059      0.000      -0.006      -0.002
PERMINTE:AVYRSEXP:AVSALK     0.0002   2.99e-05      7.428      0.000       0.000       0.000
PERSPENK                    -1.9522      0.317     -6.162      0.000      -2.573      -1.331
PTRATIO                     -0.3341      0.061     -5.453      0.000      -0.454      -0.214
PERSPENK:PTRATIO             0.0917      0.015      6.321      0.000       0.063       0.120
PCTAF                       -0.1690      0.033     -5.169      0.000      -0.233      -0.105
PERSPENK:PCTAF               0.0490      0.007      6.574      0.000       0.034       0.064
PTRATIO:PCTAF                0.0080      0.001      5.362      0.000       0.005       0.011
PERSPENK:PTRATIO:PCTAF      -0.0022      0.000     -6.445      0.000      -0.003      -0.002
============================================================================================

#The number of trials
>>> glm_mod.model.data.orig_endog.sum(1) #axis =1, rowwise 
0      807.0
1      184.0
2      571.0
3      573.0
4       65.0
       ...  
298    342.0
299    154.0
300    595.0
301    709.0
302    156.0
dtype: float64

>>> glm_mod.fittedvalues * glm_mod.model.data.orig_endog.sum(1)
0      470.732584
1      138.266178
2      285.832629
3      392.702917
4       20.963146
          ...    
298    111.464708
299     61.037884
300    235.517446
301    290.952508
302     53.312851

##First differences: 
#We hold all explanatory variables constant at their means 
#and manipulate the percentage of low income households 
#to assess its impact on the response variables:
>>> exog = glm_mod.model.data.orig_exog # get the dataframe
means25 = exog.mean()
>>> print(means25)
Intercept                    1.000000
LOWINC                      41.409877
PERASIAN                     5.896335
PERBLACK                     5.636808
PERHISP                     34.398080
                             ...     
PERSPENK:PTRATIO            96.295756
PCTAF                       33.630593
PERSPENK:PCTAF             147.235740
PTRATIO:PCTAF              747.445536
PERSPENK:PTRATIO:PCTAF    3243.607568
dtype: float64

means25['LOWINC'] = exog['LOWINC'].quantile(.25)
>>> print(means25)
Intercept                    1.000000
LOWINC                      26.683040
PERASIAN                     5.896335
PERBLACK                     5.636808
PERHISP                     34.398080
                             ...     
PERSPENK:PTRATIO            96.295756
PCTAF                       33.630593
PERSPENK:PCTAF             147.235740
PTRATIO:PCTAF              747.445536
PERSPENK:PTRATIO:PCTAF    3243.607568
dtype: float64

means75 = exog.mean()
means75['LOWINC'] = exog['LOWINC'].quantile(.75)
>>> print(means75)
Intercept                    1.000000
LOWINC                      55.460075
PERASIAN                     5.896335
PERBLACK                     5.636808
PERHISP                     34.398080
                             ...     
PERSPENK:PTRATIO            96.295756
PCTAF                       33.630593
PERSPENK:PCTAF             147.235740
PTRATIO:PCTAF              747.445536
PERSPENK:PTRATIO:PCTAF    3243.607568
dtype: float64

resp25 = glm_mod.predict(means25)
resp75 = glm_mod.predict(means75)
diff = resp75 - resp25

#The interquartile first difference for the percentage of low income households 
#in a school district is:
>>> print("%2.4f%%" % (diff[0]*100))


nobs = glm_mod.nobs
y = glm_mod.model.endog
yhat = glm_mod.mu

from statsmodels.graphics.api import abline_plot
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111, ylabel='Observed Values', xlabel='Fitted Values')
ax.scatter(yhat, y)
y_vs_yhat = sm.OLS(y, sm.add_constant(yhat, prepend=True)).fit()
fig = abline_plot(model_results=y_vs_yhat, ax=ax)
 
##Plot fitted values vs Pearson residuals
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111, title='Residual Dependence Plot', xlabel='Fitted Values',
                          ylabel='Pearson Residuals')
ax.scatter(yhat, stats.zscore(glm_mod.resid_pearson))
ax.axis('tight')
ax.plot([0.0, 1.0],[0.0, 0.0], 'k-');
 
#Histogram of standardized deviance residuals with Kernel Density Estimate overlayed

resid = glm_mod.resid_deviance
resid_std = stats.zscore(resid) 
kde_resid = sm.nonparametric.KDEUnivariate(resid_std)
kde_resid.fit()

fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111, title="Standardized Deviance Residuals")
ax.hist(resid_std, bins=25, normed=True);
ax.plot(kde_resid.support, kde_resid.density, 'r');
 
##QQ-plot of deviance residuals
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111)
fig = sm.graphics.qqplot(resid, line='r', ax=ax)





###Statsmodel-QuickIntro - sm.MixedLM, smf.mixedlm
#Linear mixed-effects models are extensions of linear regression models for data 
#that are collected and summarized in groups. 
#These models describe the relationship between a response variable 
#and independent variables, with coefficients that can vary 
#with respect to one or more grouping variables

#Example 
import statsmodels.api as sm
import statsmodels.formula.api as smf
data = sm.datasets.get_rdataset("dietox", "geepack").data
md = smf.mixedlm("Weight ~ Time", data, groups=data["Pig"])
mdf = md.fit()
>>> print(mdf.summary())
         Mixed Linear Model Regression Results
========================================================
Model:            MixedLM Dependent Variable: Weight    
No. Observations: 861     Method:             REML      
No. Groups:       72      Scale:              11.3669   
Min. group size:  11      Likelihood:         -2404.7753
Max. group size:  12      Converged:          Yes       
Mean group size:  12.0                                  
--------------------------------------------------------
             Coef.  Std.Err.    z    P>|z| [0.025 0.975]
--------------------------------------------------------
Intercept    15.724    0.788  19.952 0.000 14.179 17.268
Time          6.943    0.033 207.939 0.000  6.877  7.008
groups RE    40.394    2.149                            
========================================================






###Statsmodel-QuickIntro - sm.GEE,smf.gee, sm.cov_struct - Generalized Estimating Equations

#Note GLM assumes that data are independent given the covariates( independent variable)
#Instead, there will almost certainly be autocorrelations, 
#for example, two observations of the same individual closer in time 
#will be more similar than two observations further apart in time

#Second, GLM uses an enormous number of degrees of freedom(ie no of observations)
#estimating a parameter for each covariate

#GEE  is specified with a correlational structure (such as AR(1)), 
#GEE estimates the population mean association, , 
#hence does not uses huge degree of freedom for each covariate

#Assumptions 
1.The responses Y1,Y2,…,Yn  are correlated or clustered
2.There is a linear relationship between the covariates 
   and a transformation of the response, described by the link function g 
3.Within-subject covariance has some structure ('working covariance'):
  Note coefficients would be estimated correctly 
  even if the working covariance structure is wrong
  But the standard errors computed from this will be wrong
  To fix this , use GEE with the Huber-White 'sandwich estimator' for robustness. 
  Few structures are 
    •independence (observations over time are independent)
    •exchangeable (all observations over time have the same correlation)
    •AR(1) (correlation decreases as a power of how many timepoints apart two observations are)
    •unstructured (correlation between all timepoints may be different)


#The dependence structures currently implemented are
#sm.cov_struct
#check detailed help via help(sm.cov_struct.CLASS_NAME)
CovStruct([cov_nearest_method])         A base class for correlation and covariance structures of grouped data. 
Autoregressive([dist_func])             A first-order autoregressive working dependence structure. 
Exchangeable()                          An exchangeable working dependence structure. 
GlobalOddsRatio(endog_type)             Estimate the global odds ratio for a GEE with ordinal or nominal data. 
Independence([cov_nearest_method])      An independence working dependence structure. 
Nested([cov_nearest_method])            A nested working dependence structure. 
Equivalence                             A covariance structure defined in terms of equivalence classes
NominalIndependence                     An independence covariance structure for ordinal models.
OrdinalIndependence                     An independence covariance structure for ordinal models.
Stationary                              A stationary covariance structure.




#Example 
import statsmodels.api as sm
import statsmodels.formula.api as smf
data = sm.datasets.get_rdataset('epil', package='MASS').data
>>> dir(sm.families.family)
['Binomial', 'FLOAT_EPS', 'Family', 'Gamma', 'Gaussian', 'InverseGaussian', 'L',
 'NegativeBinomial', 'Poisson', 'Tweedie', 'V', '__builtins__', '__cached__', '_
_doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'np',
'special']
>>> sm.families.family.<familyname>.links
fam = sm.families.Poisson()
ind = sm.cov_struct.Exchangeable()
mod = smf.gee("y ~ age + trt + base", "subject", data,cov_struct=ind, family=fam)
res = mod.fit()
>>> print(res.summary())
                               GEE Regression Results                              
===================================================================================
Dep. Variable:                           y   No. Observations:                  236
Model:                                 GEE   No. clusters:                       59
Method:                        Generalized   Min. cluster size:                   4
                      Estimating Equations   Max. cluster size:                   4
Family:                            Poisson   Mean cluster size:                 4.0
Dependence structure:         Exchangeable   Num. iterations:                    51
Date:                     Tue, 28 Feb 2017   Scale:                           1.000
Covariance type:                    robust   Time:                         21:36:10
====================================================================================
                       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------------
Intercept            0.5730      0.361      1.589      0.112      -0.134       1.280
trt[T.progabide]    -0.1519      0.171     -0.888      0.375      -0.487       0.183
age                  0.0223      0.011      1.960      0.050    2.11e-06       0.045
base                 0.0226      0.001     18.451      0.000       0.020       0.025
==============================================================================
Skew:                          3.7823   Kurtosis:                      28.6672
Centered skew:                 2.7597   Centered kurtosis:             21.9865
==============================================================================

##Marginal effect 
marg = result.get_margeff()
#or 
from statsmodels.genmod.generalized_estimating_equations import GEEMargins
geem = GEEMargins(results)
print(geem.summary())



###Statsmodel-QuickIntro - sm.NominalGEE, smf.nominal_gee
#GEE for nominal response variable(ie categorical - binomial or multimonial)
#nominal scales are kind of like 'names' or labels or class


#Example 
#np.r_ : appending all elements 
endog = np.r_[0, 0, 0, 0, 1, 1, 1, 1]
#8 observations of two features , 8x2
exog = np.ones((8, 2))
#all rows, second column 
exog[:, 1] = np.r_[1, 2, 1, 1, 2, 1, 2, 2]

groups = np.arange(8)
model = sm.NominalGEE(endog, exog, groups)
result = model.fit(cov_type='naive', start_params=[3.295837, -2.197225])
marg = result.get_margeff()
fig = result.plot_distribution()





###Statsmodel-QuickIntro - sm.OrdinalGEE, smf.ordinal_gee
#GEE for ordinal response variable 
#ordinal : order of the values is what's important and significant, 
#          but the differences between each one is not really known
#Ordinal scales are typically measures of non-numeric concepts like satisfaction, happiness, discomfort, etc.
#http://www.mymarketresearchmethods.com/types-of-data-nominal-ordinal-interval-ratio/

from statsmodels.compat import lrange
from statsmodels.compat.testing import skipif

import numpy as np
import os

from numpy.testing import (assert_almost_equal, assert_equal, assert_allclose,
                           assert_array_less, assert_raises, assert_, dec)
from statsmodels.genmod.generalized_estimating_equations import (
    GEE, OrdinalGEE, NominalGEE, NominalGEEResults, OrdinalGEEResults,
    NominalGEEResultsWrapper, OrdinalGEEResultsWrapper)
from statsmodels.genmod.families import Gaussian, Binomial, Poisson
from statsmodels.genmod.cov_struct import (Exchangeable, Independence,
                                           GlobalOddsRatio, Autoregressive,
                                           Nested, Stationary)
import pandas as pd
import statsmodels.formula.api as smf
import statsmodels.api as sm
from scipy.stats.distributions import norm
import warnings
def load_data(fname, icept=True):
    """
    Load a data set from the results directory.  The data set should
    be a CSV file with the following format:

    Column 0: Group indicator
    Column 1: endog variable
    Columns 2-end: exog variables

    If `icept` is True, an intercept is prepended to the exog
    variables.
    """
    cur_dir = os.path.dirname(os.path.abspath(__file__))
    Z = np.genfromtxt(os.path.join(cur_dir, 'results', fname), delimiter=",")
    group = Z[:, 0]
    endog = Z[:, 1]
    exog = Z[:, 2:]
    if icept:
        exog = np.concatenate((np.ones((exog.shape[0], 1)), exog),axis=1)
    return endog, exog, group

#Example 
family = Binomial()
endog, exog, groups = load_data("gee_ordinal_1.csv",icept=False)

va = GlobalOddsRatio("ordinal")
mod = OrdinalGEE(endog, exog, groups, None, family, va)
rslt = mod.fit()
>>> rslt.params
[1.09250002, 0.0217443, -0.39851092, -0.01812116, 0.03023969, 1.18258516, 0.01803453, -1.10203381]
fig = rslt.plot_distribution()
marg = rslt.get_margeff()
        
#Another example 
np.random.seed(434)
n = 40
y = np.random.randint(0, 3, n)
groups = np.arange(n)
x1 = np.random.normal(size=n)
x2 = np.random.normal(size=n)

df = pd.DataFrame({"y": y, "groups": groups, "x1": x1, "x2": x2})

# smoke test
model = OrdinalGEE.from_formula("y ~ 0 + x1 + x2", groups, data=df)
model.fit()
fig = rslt.plot_distribution()


        
        
        
        
        
        
        
###Statsmodel-QuickIntro - sm.SurvfuncRight
#Estimation and inference for a survival function for right censored data 


#A typical example is a medical study in which the origin is the time 
#at which a subject is diagnosed with some condition, 
#and the event of interest is death (or disease progression, recovery, etc.).
#Right censored means some subjects  do not achieve event of interest during duration of obs

#http://blog.minitab.com/blog/michelle-paret/the-difference-between-right-left-and-interval-censored-data


#Example 
import statsmodels.api as sm
data = sm.datasets.get_rdataset("flchain", "survival").data
#all columns for rows where sex is 'F' 
df = data.loc[data.sex == "F", :]
sf = sm.SurvfuncRight(df["futime"], df["death"])

#Summary 
sf.summary().head()

#point estimates and confidence intervals for quantiles of the survival distribution. 
#Since only around 30% of the subjects died during this study, 
#we can only estimate quantiles below the 0.3 probability point:
sf.quantile(0.25)
sf.quantile_ci(0.25)

#To plot a single survival function
sf.plot()

#Since this is a large dataset with a lot of censoring, 
#we may wish to not plot the censoring symbols:
fig = sf.plot()
ax = fig.get_axes()[0]
pt = ax.get_lines()[1]
pt.set_visible(False)

#We can also add a 95% simultaneous confidence band to the plot.
#Typically these bands only plotted for central part of the distribution.
fig = sf.plot()
lcb, ucb = sf.simultaneous_cb()
ax = fig.get_axes()[0]
ax.fill_between(sf.surv_times, lcb, ucb, color='lightgrey')
ax.set_xlim(365, 365*10)
ax.set_ylim(0.7, 1)
ax.set_ylabel("Proportion alive")
ax.set_xlabel("Days since enrollment")

#Here we plot survival functions for two groups (females and males) 
#on the same axes:
gb = data.groupby("sex")
ax = plt.axes()
sexes = []
for g in gb:
    sexes.append(g[0])
    sf = sm.SurvfuncRight(g[1]["futime"], g[1]["death"])
    sf.plot(ax)
li = ax.get_lines()
li[1].set_visible(False)
li[3].set_visible(False)
plt.figlegend((li[0], li[2]), sexes, "center right")
plt.ylim(0.6, 1)
ax.set_ylabel("Proportion alive")
ax.set_xlabel("Days since enrollment")


#compare two survival distributions with survdiff, 
#The default procedure is the logrank test:
stat, pv = sm.duration.survdiff(data.futime, data.death, data.sex)

# Fleming-Harrington with p=1, i.e. weight by pooled survival time
stat, pv = sm.duration.survdiff(data.futime, data.death, data.sex, weight_type='fh', fh_p=1)

# Gehan-Breslow, weight by number at risk
stat, pv = sm.duration.survdiff(data.futime, data.death, data.sex, weight_type='gb')
# Tarone-Ware, weight by the square root of the number at risk
stat, pv = sm.duration.survdiff(data.futime, data.death, data.sex, weight_type='tw')




###Statsmodel-QuickIntro - sm.PHReg, smf.phreg
#Fit the Cox proportional hazards regression model for right censored data.
#Right censoring means during study time, failures don't happen 


#Survival models relate the time that passes before some event occurs to one 
#or more covariates that may be associated with that quantity of time. 

#In a proportional hazards model, the effect of  unit increase in a covariate 
#is multiplicative with respect to the hazard rate. 

#For example, taking a drug may halve one's hazard rate for a stroke occurring, 
#or, changing the material from which a manufactured component is constructed 
#may double its hazard rate for failure. 

#Other types of survival models such as accelerated failure time models 
#do not exhibit proportional hazards. 

#The accelerated failure time model describes a situation 
#where the biological or mechanical life history of an event is accelerated.

#It has similar methods fo any regression class 
statsmodels.duration.hazard_regression.PHReg(endog, exog[, status, entry, strata, ...])
    Fit the Cox proportional hazards regression model for right censored data. 
    ties : string,The method used to handle tied times, must be either 'breslow' or 'efron'.
    status : array-like,The censoring status values; status=1 indicates that an event occured (e.g. failure or death), status=0 indicates that the observation was right censored.


#The result class is- It has similar methods of any result class 
statsmodels.duration.hazard_regression.PHRegResults(model, params, cov_params[, ...]) 
#methods of result 
bse()                                               Returns the standard errors of the parameter estimates. 
conf_int([alpha, cols, method])                     Returns the confidence interval of the fitted parameters. 
cov_params([r_matrix, column, scale, cov_p, ...])   Returns the variance/covariance matrix. 
normalized_cov_params() 
weighted_covariate_averages()                       The average covariate values within the at-risk set at each event time point, weighted by hazard. 
f_test(r_matrix[, cov_p, scale, invcov])            Compute the F-test for a joint linear hypothesis. 
t_test(r_matrix[, cov_p, scale, use_t])             Compute a t-test for a each linear hypothesis of the form Rb = q 
wald_test(r_matrix[, cov_p, scale, invcov, ...])    Compute a Wald-test for a joint linear hypothesis. 
martingale_residuals()                              The martingale residuals. 
schoenfeld_residuals()                              A matrix containing the Schoenfeld residuals. 
score_residuals()                                   A matrix containing the score residuals. 
standard_errors()                                   Returns the standard errors of the parameter estimates.
predict([endog, exog, strata, offset, pred_type])   Returns predicted values from the fitted proportional hazards regression model. 
pvalues()  
tvalues()                                           Return the t-statistic for a given parameter estimate. 
summary([yname, xname, title, alpha])               Summarize the proportional hazards regression results. 


##Example 
import statsmodels.api as sm
import statsmodels.formula.api as smf
data = sm.datasets.get_rdataset("flchain", "survival").data
del data["chapter"]
data = data.dropna()
data["lam"] = data["lambda"]
data["female"] = (data["sex"] == "F").astype(int)
data["year"] = data["sample.yr"] - min(data["sample.yr"])
status = data["death"].values
mod = smf.phreg("futime ~ 0 + age + female + creatinine + "
                "np.sqrt(kappa) + np.sqrt(lam) + year + mgus",
                data, status=status, ties="efron")
rslt = mod.fit()
print(rslt.summary())







##Another Example  -with dummy data 
#check other data sets - https://vincentarelbundock.github.io/Rdatasets/datasets.html




import numpy as np
import pandas as pd
from statsmodels.duration.hazard_regression import PHReg




#simulated data set with 1000 observations and no censoring. 
#The event times for the first 500 observations are distributed as standard exponential values, 
#which implies that the hazard function has a constant value of 1, 
#and the cumulative hazard function has the graph y = x. 

#The event times in the second set of 500 values are exponentially distributed 
#with mean 2, which implies that the hazard function has a constant value of 1/2, 
#and the cumulative hazard function follows the graph y = x/2.


exog = np.zeros(1000)
exog[500:] = 1
endog = -np.exp(np.log(2)*exog) * np.log(np.random.uniform(size=1000))

mod = PHReg(endog, exog)
rslt = mod.fit()
>>> print(rslt.summary())
                     Results: PHReg
========================================================
Model:                   PH Reg     Sample size:    1000
Dependent variable:      y          Num. events:    1000
Ties:                    Breslow                        
--------------------------------------------------------
    log HR log HR SE   HR      t    P>|t|  [0.025 0.975]
--------------------------------------------------------
x1 -0.6573    0.0664 0.5183 -9.8917 0.0000 0.4550 0.5904
========================================================
Confidence intervals are for the hazard ratios


#The baseline hazard function corresponds to a subject with exog == 0, 
#and having a cumulative hazard function equal to y = x.
bch = rslt.baseline_cumulative_hazard
bch = bch[0] # Only one stratum here
time, cumhaz, surv = tuple(bch)
plt.clf()
plt.plot(time, cumhaz, '-o', alpha=0.6)
plt.grid(True)
plt.xlabel("Time")
plt.ylabel("Cumulative hazard")

# we can use the predict function to recover the cumulative hazard functions 
#for the two subgroups of the data corresponding to exog == 0 and exog == 1.
chaz = rslt.predict(pred_type='cumhaz')
chaz = chaz.predicted_values
plt.clf()
plt.plot(endog[0:500], chaz[0:500], 'o', label='x=0')
plt.plot(endog[500:], chaz[500:], 'o', label='x=1')
leg = plt.legend(numpoints=1, handletextpad=0.0001, loc='lower right')
leg.draw_frame(False)
plt.grid(True)
plt.xlabel("Time")
plt.ylabel("Cumulative hazard")


#Here are plots of the estimated survival functions for these two groups.
chaz = rslt.predict(pred_type='surv')
chaz = chaz.predicted_values
plt.clf()
plt.plot(endog[0:500], chaz[0:500], 'o', label='x=0')
plt.plot(endog[500:], chaz[500:], 'o', label='x=1')
leg = plt.legend(numpoints=1, handletextpad=0.0001, loc='upper right')
leg.draw_frame(False)
plt.grid(True)
plt.xlabel("Time")
plt.ylabel("Survival probability")










###Statsmodel-QuickIntro - sm.QuantReg, smf.quantreg
#Estimate a quantile regression model using iterative reweighted least squares

#Whereas the method of least squares results in 
#estimates of the conditional mean of the response variable 
#given certain values of the predictor variables, 
#quantile regression aims at estimating either the conditional median 
#or other quantiles of the response variable. 

#quantile regression estimates are more robust against outliers in the response measurements

#Example 
%matplotlib inline
from __future__ import print_function
import patsy
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
from statsmodels.regression.quantile_regression import QuantReg
data = sm.datasets.engel.load_pandas().data
data.head()
mod = smf.quantreg('foodexp ~ income', data)
res = mod.fit(q=.5)
print(res.summary())

#Prepare data for plotting

quantiles = np.arange(.05, .96, .1)
def fit_model(q):
    res = mod.fit(q=q)
    return [q, res.params['Intercept'], res.params['income']] + \
            res.conf_int().ix['income'].tolist()
    
models = [fit_model(x) for x in quantiles]
models = pd.DataFrame(models, columns=['q', 'a', 'b','lb','ub'])
ols = smf.ols('foodexp ~ income', data).fit()
ols_ci = ols.conf_int().ix['income'].tolist()
ols = dict(a = ols.params['Intercept'],
           b = ols.params['income'],
           lb = ols_ci[0],
           ub = ols_ci[1])
print(models)
print(ols)

##First plot
#This plot compares best fit lines for 10 quantile regression models 
#to the least squares fit. 
#result 
1.Food expenditure increases with income
2.The dispersion of food expenditure increases with income
3.The least squares estimates fit low income observations quite poorly (i.e. the OLS line passes over most low income households)

x = np.arange(data.income.min(), data.income.max(), 50)
get_y = lambda a, b: a + b * x
fig, ax = plt.subplots(figsize=(8, 6))
for i in range(models.shape[0]):
    y = get_y(models.a[i], models.b[i])
    ax.plot(x, y, linestyle='dotted', color='grey')
    
y = get_y(ols['a'], ols['b'])
ax.plot(x, y, color='red', label='OLS')
ax.scatter(data.income, data.foodexp, alpha=.2)
ax.set_xlim((240, 3000))
ax.set_ylim((240, 2000))
legend = ax.legend()
ax.set_xlabel('Income', fontsize=16)
ax.set_ylabel('Food expenditure', fontsize=16);

##Second plot
#The dotted black lines form 95% point-wise confidence band around 10 quantile regression estimates (solid black line). 
#The red lines represent OLS regression results along with their 95% confindence interval.
#In most cases, the quantile regression point estimates lie outside the OLS confidence interval, 
#which suggests that the effect of income on food expenditure may not be constant across the distribution.

n = models.shape[0]
p1 = plt.plot(models.q, models.b, color='black', label='Quantile Reg.')
p2 = plt.plot(models.q, models.ub, linestyle='dotted', color='black')
p3 = plt.plot(models.q, models.lb, linestyle='dotted', color='black')
p4 = plt.plot(models.q, [ols['b']] * n, color='red', label='OLS')
p5 = plt.plot(models.q, [ols['lb']] * n, linestyle='dotted', color='red')
p6 = plt.plot(models.q, [ols['ub']] * n, linestyle='dotted', color='red')
plt.ylabel(r'$\beta_{income}$')
plt.xlabel('Quantiles of the conditional food expenditure distribution')
plt.legend()
plt.show()




###Statsmodel-QuickIntro - sm.RLM, smf.rlm
#Robust linear models with support for the M-estimators listed under Norms
#robust against outliers
#eg Heteroscedasticity allows the variance of error term to be dependent on x


#It requires Norm and SCale 
#M-estimator  minimizes a function of influence observations based on residuals with a scale
#robust estimates are computed by the iterative WLS of M-Estimators
#These weights of WLS are called Norms , each norm has default scale function
norms = sm.robust.norms


#Norms - use with prefix sm.robust.norms.
AndrewWave([a])         Andrew's wave for M estimation. 
Hampel([a, b, c])       Hampel function for M-estimation. 
HuberT([t])             Huber's T for M estimation. 
LeastSquares            Least squares rho for M-estimation and its derived functions. 
RamsayE([a])            Ramsay's Ea for M estimation. 
RobustNorm              The parent class for the norms used for robust regression. 
TrimmedMean([c])        Trimmed mean function for M-estimation. 
TukeyBiweight([c])      Tukey's biweight function for M-estimation. 
estimate_location(a, scale[, norm, axis, ...]) M-estimator of location using self.norm and a current estimator of scale. 


#Scale - use with sm.robust.scale 
Huber([c, tol, maxiter, norm])      Huber's proposal 2 for estimating location and scale jointly. 
HuberScale([d, tol, maxiter])       Huber's scaling for fitting robust linear models. 
mad(a[, c, axis, center])           The Median Absolute Deviation along given axis of an array 
huber                               Huber's proposal 2 for estimating location and scale jointly. 
hubers_scale                        Huber's scaling for fitting robust linear models. 
stand_mad(a[, c, axis]) 




##Example 
from __future__ import print_function
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.sandbox.regression.predstd import wls_prediction_std


data = sm.datasets.stackloss.load()
data.exog = sm.add_constant(data.exog)

#Huber's T norm with the (default) median absolute deviation scaling
huber_t = sm.RLM(data.endog, data.exog, M=sm.robust.norms.HuberT())
hub_results = huber_t.fit()
print(hub_results.params)
print(hub_results.bse)
print(hub_results.summary(yname='y',
            xname=['var_%d' % i for i in range(len(hub_results.params))]))

#uber's T norm with 'H2' covariance matrix
hub_results2 = huber_t.fit(cov="H2")
print(hub_results2.params)
print(hub_results2.bse)
#output 
[-41.02649835   0.82938433   0.92606597  -0.12784672]
[ 9.08950419  0.11945975  0.32235497  0.11796313]

#Andrew's Wave norm with Huber's Proposal 2 scaling and 'H3' covariance matrix
andrew_mod = sm.RLM(data.endog, data.exog, M=sm.robust.norms.AndrewWave())
andrew_results = andrew_mod.fit(scale_est=sm.robust.scale.HuberScale(), cov="H3")
print('Parameters: ', andrew_results.params)
Parameters:  [-40.8817957    0.79276138   1.04857556  -0.13360865]

>>> help(sm.RLM.fit) for more options 
>>> dir(sm.robust.scale) # for scale options


##Comparing OLS and RLM

nsample = 50
x1 = np.linspace(0, 20, nsample)
X = np.column_stack((x1, (x1-5)**2))
X = sm.add_constant(X)
sig = 0.3   # smaller error variance makes OLS<->RLM contrast bigger
beta = [5, 0.5, -0.0]
y_true2 = np.dot(X, beta)
y2 = y_true2 + sig*1. * np.random.normal(size=nsample)
y2[[39,41,43,45,48]] -= 5   # add some outliers (10% of nsample)

#Example 1: quadratic function with linear truth
#Note that the quadratic term in OLS regression will capture outlier effects.
res = sm.OLS(y2, X).fit()
print(res.params)
print(res.bse)
print(res.predict())

#Estimate RLM:
resrlm = sm.RLM(y2, X).fit()
print(resrlm.params)
print(resrlm.bse)  #SE terms ie std deviation of each parameter estimation(ie mean)
[  4.98830133e+00   5.21124476e-01  -3.95637732e-03]
[ 0.1177946   0.0181859   0.00160917]

#Draw a plot to compare OLS estimates to the robust estimates:
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111)
ax.plot(x1, y2, 'o',label="data")
ax.plot(x1, y_true2, 'b-', label="True")
prstd, iv_l, iv_u = wls_prediction_std(res)
ax.plot(x1, res.fittedvalues, 'r-', label="OLS")
ax.plot(x1, iv_u, 'r--')
ax.plot(x1, iv_l, 'r--')
ax.plot(x1, resrlm.fittedvalues, 'g.-', label="RLM")
ax.legend(loc="best")


#Example 2: linear function with linear truth
#Fit a new OLS model using only the linear term and the constant:
X2 = X[:,[0,1]] 
res2 = sm.OLS(y2, X2).fit()
print(res2.params)
print(res2.bse)
#output 
[ 5.59260775  0.39611497]
[ 0.39150044  0.03373326]
#Estimate RLM:
resrlm2 = sm.RLM(y2, X2).fit()
print(resrlm2.params)
print(resrlm2.bse)
#output 
[ 5.11833224  0.48612219]
[ 0.09516649  0.00819993]
#Draw a plot to compare OLS estimates to the robust estimates:
prstd, iv_l, iv_u = wls_prediction_std(res2)
fig, ax = plt.subplots(figsize=(8,6))
ax.plot(x1, y2, 'o', label="data")
ax.plot(x1, y_true2, 'b-', label="True")
ax.plot(x1, res2.fittedvalues, 'r-', label="OLS")
ax.plot(x1, iv_u, 'r--')
ax.plot(x1, iv_l, 'r--')
ax.plot(x1, resrlm2.fittedvalues, 'g.-', label="RLM")
legend = ax.legend(loc="best")
    
    
    
    
 

###Statsmodel-QuickIntro - sm.MICE, sm.MICEData
#Multiple Imputation with Chained Equations.
#(to impute - to assign )


#Note (imputed data set - missing value is assigned via some method(called MICE) )
* If the goal is only to produce imputed data sets, the MICEData class
can be used to wrap a data frame, providing facilities for doing the
imputation.  Summary plots are available for assessing the performance
of the imputation.

* If the imputed data sets are to be used to fit an additional
'analysis model', a MICE instance can be used.  After specifying the
MICE instance and running it, the results are combined using the
`combine` method.  Results and various summary plots are then
available.

#Note below data is used 
def gendat():
    """
    Create a data set with missing values.
    """
    np.random.seed(34243)
    n = 200
    p = 5
    exog = np.random.normal(size=(n, p))
    exog[:, 0] = exog[:, 1] - exog[:, 2] + 2*exog[:, 4]
    exog[:, 0] += np.random.normal(size=n)
    exog[:, 2] = 1*(exog[:, 2] > 0)
    endog = exog.sum(1) + np.random.normal(size=n)

    df = pd.DataFrame(exog)
    df.columns = ["x%d" % k for k in range(1, p+1)]
    df["y"] = endog
    df.x1[0:60] = np.nan
    df.x2[0:40] = np.nan
    df.x3[10:30:2] = np.nan
    df.x4[20:50:3] = np.nan
    df.x5[40:45] = np.nan
    df.y[30:100:2] = np.nan

    return df
    
    
data = gendat()


#sm.MICE can be used to fit most Statsmodels models to data sets 
#with missing values using the 'multiple imputation with chained equations' (MICE) approach..

#Run all MICE steps and obtain results
#Fit fml to the model OLS, but with imputed data, imp
>>> imp = sm.MICEData(data)
>>> fml = 'y ~ x1 + x2 + x3 + x4'
>>> mice = sm.MICE(fml, sm.OLS, imp)
#Or another example 
mice = sm.MICE("x3 ~ x1 + x2", sm.GLM, imp,init_kwds={"family": sm.families.Binomial()})

#n_burnin : int,The number of burn-in cycles to skip.
#n_imputations : int,The number of data sets to impute
>>> results = mice.fit(10, 10)  #MICEResults, subclass of LikelihoodModelResults
>>> print(results.summary())



                              Results: MICE
    =================================================================
    Method:                    MICE       Sample size:           1000
    Model:                     OLS        Scale                  1.00
    Dependent variable:        y          Num. imputations       10
    -----------------------------------------------------------------
               Coef.  Std.Err.    t     P>|t|   [0.025  0.975]  FMI
    -----------------------------------------------------------------
    Intercept -0.0234   0.0318  -0.7345 0.4626 -0.0858  0.0390 0.0128
    x1         1.0305   0.0578  17.8342 0.0000  0.9172  1.1437 0.0309
    x2        -0.0134   0.0162  -0.8282 0.4076 -0.0451  0.0183 0.0236
    x3        -1.0260   0.0328 -31.2706 0.0000 -1.0903 -0.9617 0.0169
    x4        -0.0253   0.0336  -0.7520 0.4521 -0.0911  0.0406 0.0269
    =================================================================


#Example 
>>> imp = mice.MICEData(data)
>>> fml = 'y ~ x1 + x2 + x3 + x4'
>>> mice = mice.MICE(fml, sm.OLS, imp)
>>> results = []
>>> for k in range(10):
        #A single MICE iteration updates all missing values using their
        #respective imputation models, then fits the analysis model to the imputed data.
        #Use `fit` to run all MICE steps together and obtain summary results
        x = mice.next_sample()
        results.append(x)

#Draw 20 imputations from a data set called `data` and save them in
#separate files with filename pattern `dataXX.csv`.  The variables
#other than x1,x2,x3 are imputed using linear models fit with OLS, with
#mean structures containing main effects of all other variables in
#`data`.  The variable named `x1`,x2,x3 are imputed with given formula 
>>> imp = sm.MICEData(data)
imp.set_imputer('x1', 'x3 + x4 + x3*x4')
imp.set_imputer('x2', 'x4 + I(x5**2)')
imp.set_imputer('x3', model_class=sm.GLM, init_kwds={"family": sm.families.Binomial()})
>>> for j in range(20):
        imp.update_all()
        imp.data.to_csv('data%02d.csv' % j)

#Impute using default models, using the MICEData object as an iterator.
>>> imp = sm.MICEData(data)
>>> j = 0
>>> for data in imp:
        imp.data.to_csv('data%02d.csv' % j)
        j += 1
        
##Methods of sm.MICEData
next_sample()        
    Returns the next imputed dataset in the imputation process.
set_imputer(endog_name, formula=None, init_kwds={})
    Specify the imputation process for a single variable.
    endog_name : string
        Name of the variable to be imputed.
    formula : string
        Conditional formula for imputation. Defaults to a formula
        with main effects for all other variables in dataset.  The
        formula should only include an expression for the mean
        structure, e.g. use 'x1 + x2' not 'x4 ~ x1 + x2'     
update_all(n_iter=1):
    Perform a specified number of MICE iterations  
    The imputed values are stored in the class attribute self.data
plot_missing_pattern(ax=None)
    A figure containing a plot of the missing data pattern.
plot_bivariate(self, col1_name, col2_name,ax=None):
    Plot observed and imputed values for two variables.
    Displays a scatterplot of one variable against another.  The
    points are colored according to whether the values are
    observed or imputed.       
plot_fit_obs(self, col_name, ax=None):
    Plot fitted versus imputed or observed values as a scatterplot.        
def plot_imputed_hist(self, col_name, ax=None)
    Display imputed values for one variable as a histogram.   

    
###Statsmodel -QuickIntro - cross validation 
statsmodels.sandbox.tools.cross_val
    class LeaveOneOut(n):
        Leave-One-Out cross validation iterator:
        Provides train/test indexes to split data in train test sets
            Parameters
            ----------
            n: int
                Total number of elements

            Examples
            --------
            >>> from scikits.learn import cross_val
            >>> X = [[1, 2], [3, 4]]
            >>> y = [1, 2]
            >>> loo = cross_val.LeaveOneOut(2)
            >>> for train_index, test_index in loo:
            ...    print "TRAIN:", train_index, "TEST:", test_index
            ...    X_train, X_test, y_train, y_test = cross_val.split(train_index, test_index, X, y)
            ...    print X_train, X_test, y_train, y_test
            TRAIN: [False  True] TEST: [ True False]
            [[3 4]] [[1 2]] [2] [1]
            TRAIN: [ True False] TEST: [False  True]
            [[1 2]] [[3 4]] [1] [2]
            
    class LeavePOut(n, p):
        Leave-P-Out cross validation iterator:
        Provides train/test indexes to split data in train test sets
            Parameters
            ----------
            n: int
                Total number of elements
            p: int
                Size test sets

            Examples
            --------
            >>> from scikits.learn import cross_val
            >>> X = [[1, 2], [3, 4], [5, 6], [7, 8]]
            >>> y = [1, 2, 3, 4]
            >>> lpo = cross_val.LeavePOut(4, 2)
            >>> for train_index, test_index in lpo:
            ...    print "TRAIN:", train_index, "TEST:", test_index
            ...    X_train, X_test, y_train, y_test = cross_val.split(train_index, test_index, X, y)
            TRAIN: [False False  True  True] TEST: [ True  True False False]
            TRAIN: [False  True False  True] TEST: [ True False  True False]
            TRAIN: [False  True  True False] TEST: [ True False False  True]
            TRAIN: [ True False False  True] TEST: [False  True  True False]
            TRAIN: [ True False  True False] TEST: [False  True False  True]
            TRAIN: [ True  True False False] TEST: [False False  True  True]
            
    class KFold(n, k):
        K-Folds cross validation iterator:
        Provides train/test indexes to split data in train test sets
            Parameters
            ----------
            n: int
                Total number of elements
            k: int
                number of folds

            Examples
            --------
            >>> from scikits.learn import cross_val
            >>> X = [[1, 2], [3, 4], [1, 2], [3, 4]]
            >>> y = [1, 2, 3, 4]
            >>> kf = cross_val.KFold(4, k=2)
            >>> for train_index, test_index in kf:
            ...    print "TRAIN:", train_index, "TEST:", test_index
            ...    X_train, X_test, y_train, y_test = cross_val.split(train_index, test_index, X, y)
            TRAIN: [False False  True  True] TEST: [ True  True False False]
            TRAIN: [ True  True False False] TEST: [False False  True  True]

            
    class LeaveOneLabelOut(labels):
        Leave-One-Label_Out cross-validation iterator:
        Provides train/test indexes to split data in train test sets
            Parameters
            ----------
            labels : list
                    List of labels

            Examples
            --------
            >>> from scikits.learn import cross_val
            >>> X = [[1, 2], [3, 4], [5, 6], [7, 8]]
            >>> y = [1, 2, 1, 2]
            >>> labels = [1, 1, 2, 2]
            >>> lol = cross_val.LeaveOneLabelOut(labels)
            >>> for train_index, test_index in lol:
            ...    print "TRAIN:", train_index, "TEST:", test_index
            ...    X_train, X_test, y_train, y_test = cross_val.split(train_index, \
                test_index, X, y)
            ...    print X_train, X_test, y_train, y_test
            TRAIN: [False False  True  True] TEST: [ True  True False False]
            [[5 6]
            [7 8]] [[1 2]
            [3 4]] [1 2] [1 2]
            TRAIN: [ True  True False False] TEST: [False False  True  True]
            [[1 2]
            [3 4]] [[5 6]
            [7 8]] [1 2] [1 2]

            
    class KStepAhead( n, k=1, start=None, kall=True, return_slice=True):
        KStepAhead cross validation iterator:
        Provides fit/test indexes to split data in sequential sets
            Parameters
            ----------
            n: int
                Total number of elements
            k : int
                number of steps ahead
            start : int
                initial size of data for fitting
            kall : boolean
                if true. all values for up to k-step ahead are included in the test index.
                If false, then only the k-th step ahead value is returnd

            Examples
            --------
            >>> from scikits.learn import cross_val
            >>> X = [[1, 2], [3, 4]]
            >>> y = [1, 2]
            >>> loo = cross_val.LeaveOneOut(2)
            >>> for train_index, test_index in loo:
            ...    print "TRAIN:", train_index, "TEST:", test_index
            ...    X_train, X_test, y_train, y_test = cross_val.split(train_index, test_index, X, y)
            ...    print X_train, X_test, y_train, y_test
            TRAIN: [False  True] TEST: [ True False]
            [[3 4]] [[1 2]] [2] [1]
            TRAIN: [ True False] TEST: [False  True]
            [[1 2]] [[3 4]] [1] [2]

        
        

###Statsmodel-QuickIntro - sm.PCA
#Principal Component Analysis

PCA(data, ncomp=None, standardize=True, demean=True, normalize=True, 
            gls=False, weights=None, method='svd', missing=None, tol=5e-08, max_iter=1000, tol_em=5e-08, max_em_iter=100)[source]
    Parameters:
        data : array-like
            Variables in columns, observations in rows
        ncomp : int, optional
            Number of components to return. 
            If None, returns the as many as the smaller of the number of rows or columns in data
        standardize: bool, optional
            Flag indicating to use standardized data with mean 0 and unit variance. 
            standardized being True implies demean. 
            Using standardized data is equivalent to computing principal components 
            from the correlation matrix of data
        demean : bool, optional
            Flag indicating whether to demean data before computing principal components.
            demean is ignored if standardize is True. 
            Demeaning data but not standardizing is equivalent to computing 
            principal components from the covariance matrix of data
        normalize : bool , optional
            Indicates whether th normalize the factors to have unit inner product. 
            If False, the loadings will have unit inner product.
#Attributes 
factors (array or DataFrame) nobs by ncomp array of of principal components (scores) 
scores ( array or DataFrame) nobs by ncomp array of of principal components - identical to factors 
cols (array)                Array of indices indicating columns used in the PCA 
rows (array)                Array of indices indicating rows used in the PCA 
rsquare (array or Series)   ncomp array where the element in the ith position is the R-square of including the first i principal components. Note: values are calculated on the transformed data, not the original data 
ic (array or DataFrame)     ncomp by 3 array containing the Bai and Ng (2003) Information criteria. Each column is a different criteria, and each row represents the number of included factors. 


plot_scree([ncomp, log_scale, cumulative, ax])  Plot of the ordered eigenvalues 
plot_rsquare([ncomp, ax])                       Box plots of the individual series R-square against the number of PCs 
project([ncomp, transform, unweight])           Project series onto a specific number of factors 
                                                ie returns orginal data if used with infinite ncomp
                                                Returns:nobs by nvar array of the projection onto ncomp factors
 



#Example 
import statsmodels.api as sm

data = sm.datasets.fertility.load_pandas().data

columns = list(map(str, range(1960, 2012)))  #'1960'...'2012'
data.set_index('Country Name', inplace=True)

dta = data[columns]
dta = dta.dropna()
pca_model = sm.PCA(dta.T, standardize=False, demean=True)
pca_model.plot_scree()
>>> pca_model.factors.shape
>>> pca_model.project() # wth pcomp=None , get back original data 

#with 5 most important comp 
>>> pca_model = sm.PCA(dta.T, ncomp = 5, standardize=False, demean=True)
>>> pca_model.factors.shape
(52, 5)
>>> dta.head()
                       1960   1961   1962   1963   1964   1965   1966   1967  \
Country Name
Aruba                 4.820  4.655  4.471  4.271  4.059  3.842  3.625  3.417
Afghanistan           7.671  7.671  7.671  7.671  7.671  7.671  7.671  7.671
Angola                7.316  7.354  7.385  7.410  7.425  7.430  7.422  7.403
Albania               6.186  6.076  5.956  5.833  5.711  5.594  5.483  5.376
United Arab Emirates  6.928  6.910  6.893  6.877  6.861  6.841  6.816  6.783

>>> pca_model.project().T.head()
                          1960      1961      1962      1963      1964  \
Country Name
Aruba                 4.680162  4.585257  4.469843  4.318748  4.137293
Afghanistan           7.673737  7.664769  7.666142  7.653747  7.656124
Angola                7.394137  7.395607  7.394740  7.389772  7.383758
Albania               5.999994  5.979182  5.942319  5.884406  5.807943
United Arab Emirates  6.862526  6.891905  6.903761  6.909959  6.898611

>>> pca_model.rsquare
ncomp
0    0.000000
1    0.889399
2    0.969752
3    0.986427
4    0.992945
5    0.995896
Name: rsquare, dtype: float64
>>> pca_model.ic
          IC_p1     IC_p2     IC_p3
ncomp
0      9.366278  9.366278  9.366278
1      7.255162  7.261019  7.240440
2      6.049370  6.061085  6.019926
3      5.338730  5.356302  5.294564
4      4.775056  4.798486  4.716168
5      4.323965  4.353252  4.250355



#Principal Component Regression
statsmodels.sandbox.tools.pca(data, keepdim=0, normalize=0, demean=True)
    principal components with eigenvector decomposition
    similar to princomp in matlab
statsmodels.sandbox.tools.pcasvd(data, keepdim=0, demean=True)
    principal components with svd
    
    Parameters
    ----------
    data : ndarray, 2d
        data with observations by rows and variables in columns
    keepdim : integer
        number of eigenvectors to keep
        if keepdim is zero, then all eigenvectors are included
    normalize : boolean
        if true, then eigenvectors are normalized by sqrt of eigenvalues
    demean : boolean
        if true, then the column mean is subtracted from the data
    Returns
    -------
    xreduced : ndarray, 2d, (nobs, nvars)
        projection of the data x on the kept eigenvectors
    factors : ndarray, 2d, (nobs, nfactors) ** this is reduced factors 
        factor matrix, given by np.dot(x, evecs)
    evals : ndarray, 2d, (nobs, nfactors)
        eigenvalues
    evecs : ndarray, 2d, (nobs, nfactors)
        eigenvectors, normalized if normalize is true


#Example 
* simulate model with 2 factors and 4 explanatory variables
* use pca to extract factors from data,
* run OLS on factors,
* use information criteria to choose "best" model

#Warning: pca sorts factors by explaining variance in explanatory variables,
#which are not necessarily the most important factors for explaining the
#endogenous variable.

# try out partial correlation for dropping (or adding) factors
# get algorithm for partial least squares as an alternative to PCR

import numpy as np
from numpy.testing import assert_array_almost_equal
import statsmodels.api as sm
from statsmodels.sandbox.tools import pca
from statsmodels.sandbox.tools.cross_val import LeaveOneOut

nobs = 1000
#np.c_=columnwise append 
f0 = np.c_[np.random.normal(size=(nobs,2)), np.ones((nobs,1))]
#np.repeat(array,repeats,axis)
f2xcoef = np.c_[np.repeat(np.eye(2),2,0),np.arange(4)[::-1]].T 
f2xcoef = np.array([[ 1.,  1.,  0.,  0.],
                    [ 0.,  0.,  1.,  1.],
                    [ 3.,  2.,  1.,  0.]])
f2xcoef = np.array([[ 0.1,  3.,  1.,    0.],
                    [ 0.,  0.,  1.5,   0.1],
                    [ 3.,  2.,  1.,    0.]])
x0 = np.dot(f0, f2xcoef)
x0 += 0.1*np.random.normal(size=x0.shape)
ytrue = np.dot(f0,[1., 1., 1.])  #actual coeff 
y0 = ytrue + 0.1*np.random.normal(size=ytrue.shape)
xred, fact, eva, eve  = pca(x0, keepdim=0)
print(eve)
print(fact[:5])
print(f0[:5])
import statsmodels.api as sm
res = sm.OLS(y0, sm.add_constant(x0, prepend=False)).fit()
print('OLS on original data')
print(res.params)
print(res.aic)
print(res.rsquared)
#print 'OLS on Factors'
#for k in range(x0.shape[1]):
#    xred, fact, eva, eve  = pca(x0, keepdim=k, normalize=1)
#    fact_wconst = sm.add_constant(fact)
#    res = sm.OLS(y0, fact_wconst).fit()
#    print 'k =', k
#    print res.params
#    print 'aic:  ', res.aic
#    print 'bic:  ', res.bic
#    print 'llf:  ', res.llf
#    print 'R2    ', res.rsquared
#    print 'R2 adj', res.rsquared_adj
print('OLS on Factors')
results = []
xred, fact, eva, eve  = pca(x0, keepdim=0, normalize=1)
for k in range(0, x0.shape[1]+1):
    #xred, fact, eva, eve  = pca(x0, keepdim=k, normalize=1)
    # this is faster and same result
    fact_wconst = sm.add_constant(fact[:,:k], prepend=False)
    res = sm.OLS(y0, fact_wconst).fit()
##    print 'k =', k
##    print res.params
##    print 'aic:  ', res.aic
##    print 'bic:  ', res.bic
##    print 'llf:  ', res.llf
##    print 'R2    ', res.rsquared
##    print 'R2 adj', res.rsquared_adj
    prederr2 = 0.
    for inidx, outidx in LeaveOneOut(len(y0)):
        resl1o = sm.OLS(y0[inidx], fact_wconst[inidx,:]).fit()
        #print data.endog[outidx], res.model.predict(data.exog[outidx,:]),
        prederr2 += (y0[outidx] - resl1o.predict(fact_wconst[outidx,:]))**2.
    results.append([k, res.aic, res.bic, res.rsquared_adj, prederr2])
results = np.array(results)
print(results)
print('best result for k, by AIC, BIC, R2_adj, L1O')
print(np.r_[(np.argmin(results[:,1:3],0), np.argmax(results[:,3],0),
             np.argmin(results[:,-1],0))])
from statsmodels.iolib.table import (SimpleTable, default_txt_fmt,
                        default_latex_fmt, default_html_fmt)
headers = 'k, AIC, BIC, R2_adj, L1O'.split(', ')
numformat = ['%6d'] + ['%10.3f']*4 #'%10.4f'
txt_fmt1 = dict(data_fmts = numformat)
tabl = SimpleTable(results, headers, None, txt_fmt=txt_fmt1)
print("PCA regression on simulated data,")
print("DGP: 2 factors and 4 explanatory variables")
print(tabl)
print("Notes: k is number of components of PCA,")
print("       constant is added additionally")
print("       k=0 means regression on constant only")
print("       L1O: sum of squared prediction errors for leave-one-out")








###Statsmodel-QuickIntro - sm.add_constant

statsmodels.tools.tools.add_constant(data, prepend=True, has_constant='skip')
    Adds a column of ones to an array
    Required because linear model does not add intercept term. 
    Manually needs to be added by this class 
    When the input is recarray or a pandas Series or DataFrame,
    the added column's name is 'const'.
    data : array-like
    data is the column-ordered design matrix
    prepend : bool
        If true, the constant is in the first column. 
        Else the constant is appended (last column).
    has_constant : str {'raise', 'add', 'skip'}
        Behavior if data already has a constant. 
        The default will return data without adding another constant. 
        If 'raise', will raise an error if a constant is present. 
        Using 'add' will duplicate the constant, if one is present.
 

 
 
###Statsmodel-QuickIntro - sm.categorical
statsmodels.tools.tools.categorical(data, col=None, dictnames=False, drop=False)
    Returns a dummy matrix given an array of categorical variables.
    Parameters:
    data : array
        A structured array, recarray, or array. 
        This can be either a 1d vector of the categorical variable 
        or a 2d array with the column specifying the categorical variable 
        specified by the col argument.
    col : 'string', int, or None
        If data is a structured array or a recarray, 
        col can be a string that is the name of the column 
        that contains the variable. 
        For all arrays col can be an int that is the (zero-based) column index number. col can only be None for a 1d array. The default is None.
    dictnames : bool, optional
        If True, a dictionary mapping the column number to the categorical name is returned. 
        Used to have information about plain arrays.
    drop : bool
        Whether or not keep the categorical variable in the returned matrix.
    Returns:
        dummy_matrix, [dictnames, optional]
            A matrix of dummy (indicator/binary) float variables 
            for the categorical data. 
            If dictnames is True, then the dictionary is returned as well.
            This returns a dummy variable for EVERY distinct variable. 
            So if the a variable 'vote' had answers as 'yes' or 'no' 
            then the returned array would have to new variables– 'vote_yes' and 'vote_no'. 

#Examples
>>> import numpy as np
>>> import statsmodels.api as sm

#Univariate examples
>>> import string
>>> string_var = [string.ascii_lowercase[0:5],string.ascii_lowercase[5:10],                       string.ascii_lowercase[10:15],                       string.ascii_lowercase[15:20],                         string.ascii_lowercase[20:25]]
>>> string_var *= 5
>>> string_var = np.asarray(sorted(string_var))
>>> design = sm.tools.categorical(string_var, drop=True)

#Or for a numerical categorical variable
>>> instr = np.floor(np.arange(10,60, step=2)/10)
>>> design = sm.tools.categorical(instr, drop=True)

#With a structured array
>>> num = np.random.randn(25,2)
>>> struct_ar = np.zeros((25,1), dtype=[('var1', 'f4'),('var2', 'f4'),                      ('instrument','f4'),('str_instr','a5')])
>>> struct_ar['var1'] = num[:,0][:,None]
>>> struct_ar['var2'] = num[:,1][:,None]
>>> struct_ar['instrument'] = instr[:,None]
>>> struct_ar['str_instr'] = string_var[:,None]
>>> design = sm.tools.categorical(struct_ar, col='instrument', drop=True)
#Or
>>> design2 = sm.tools.categorical(struct_ar, col='str_instr', drop=True)




###Statsmodel-QuickIntro - sm.datasets
#provides data sets (i.e. data and meta-data) 

#from STATA
webuse(data[, baseurl, as_df])      Download and return an example dataset from Stata. 


#Using Datasets from R
import statsmodels.api as sm
duncan_prestige = sm.datasets.get_rdataset("Duncan", "car")
print(duncan_prestige.__doc__)


#Available Datasets in stats model 
    •American National Election Survey 1996
    •Breast Cancer Data
    •Bill Greene's credit scoring data.
    •Smoking and lung cancer in eight cities in China.
    •Mauna Loa Weekly Atmospheric CO2 Data
    •First 100 days of the US House of Representatives 1995
    •World Copper Market 1951-1975 Dataset
    •US Capital Punishment dataset.
    •El Nino - Sea Surface Temperatures
    •Engel (1857) food expenditure data
    •Affairs dataset
    •World Bank Fertility Data
    •Grunfeld (1950) Investment Data
    •Transplant Survival Data
    •Longley dataset
    •United States Macroeconomic data
    •Travel Mode Choice
    •Nile River flows at Ashwan 1871-1970
    •RAND Health Insurance Experiment Data
    •Taxation Powers Vote for the Scottish Parliamant 1997
    •Spector and Mazzeo (1980) - Program Effectiveness Data
    •Stack loss data
    •Star98 Educational Dataset
    •Statewide Crime Data 2009
    •U.S. Strike Duration Data
    •Yearly sunspots data 1700-2008

import statsmodels.api as sm
data = sm.datasets.longley.load()
#View Code
#The full dataset is available in the data attribute.
data.data

#attributes endog and exog for y and x
>>> data.endog[:5]
array([ 60323.,  61122.,  60171.,  61187.,  63221.])
data.exog[:5,:]

#Univariate datasets, however, do not have an exog attribute.
>>> data.endog_name
'TOTEMP'
>>> data.exog_name
['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP', 'YEAR']
>>> type(data.data)
numpy.recarray
>>> type(data.raw_data)
numpy.recarray
>>> data.names
['TOTEMP', 'GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP', 'YEAR']

#Loading data as pandas objects
data = sm.datasets.longley.load_pandas()
data.exog
data.endog
data.data  #DF


#Extra Information
>>> dir(sm.datasets.longley)[:6]
['COPYRIGHT', 'DESCRLONG', 'DESCRSHORT', 'NOTE', 'SOURCE', 'TITLE']






###Statsmodel-QuickIntro - sm.distributions
#various additional functions and methods for statistical distributions.

#Empirical Distributions
ECDF(x[, side])                             Return the Empirical CDF of an array as a step function. 
StepFunction(x, y[, ival, sorted, side])    A basic step function. 
monotone_fn_inverter(fn, x[, vectorized])   Given a monotone function fn (no checking is done to verify monotonicity) and a set of x values, return an linearly interpolated approximation to its inverse from its values on x. 
#Example 
import numpy as np
from statsmodels.distributions.empirical_distribution import ECDF

ecdf = ECDF([3, 3, 1, 4])
>>> ecdf([3, 55, 0.5, 1.5])
array([ 0.75,  1.  ,  0.  ,  0.25])


#Skew Distributions
SkewNorm_gen()                              univariate Skew-Normal distribution of Azzalini 
SkewNorm2_gen([momtype, a, b, xtol, ...])   univariate Skew-Normal distribution of Azzalini 
ACSkewT_gen()                               univariate Skew-T distribution of Azzalini 
skewnorm2                                   univariate Skew-Normal distribution of Azzalini 

#Distributions based on Gram-Charlier expansion
pdf_moments_st(cnt)             Return the Gaussian expanded pdf function given the list of central moments (first one is mean). 
pdf_mvsk(mvsk)                  Return the Gaussian expanded pdf function given the list of 1st, 2nd moment and skew and Fisher (excess) kurtosis. 
pdf_moments(cnt)                Return the Gaussian expanded pdf function given the list of central moments (first one is mean). 
NormExpan_gen(args, **kwds)     Gram-Charlier Expansion of Normal distribution 

#cdf of multivariate normal wrapper for scipy.stats
mvstdnormcdf(lower, upper, corrcoef, **kwds)    standardized multivariate normal cumulative distribution function 
mvnormcdf(upper, mu, cov[, lower])              multivariate normal cumulative distribution function 

#Univariate Distributions by non-linear Transformations
#Univariate distributions can be generated 
#from a non-linear transformation of an existing univariate distribution. 

#Transf_gen is a class that can generate a new distribution 
#from a monotonic transformation, 

#TransfTwo_gen can use hump-shaped or u-shaped transformation, 
#such as abs or square. The remaining objects are special cases.
 
TransfTwo_gen(kls, func, funcinvplus, ...)      Distribution based on a non-monotonic (u- or hump-shaped transformation) 
Transf_gen(kls, func, funcinv, *args, **kwargs) a class for non-linear monotonic transformation of a continuous random variable 
ExpTransf_gen(kls, *args, **kwargs)             Distribution based on log/exp transformation 
LogTransf_gen(kls, *args, **kwargs)             Distribution based on log/exp transformation 

SquareFunc          class to hold quadratic function with inverse function and derivative 
absnormalg          Distribution based on a non-monotonic (u- or hump-shaped transformation) 
invdnormalg         a class for non-linear monotonic transformation of a continuous random variable 
loggammaexpg        univariate distribution of a non-linear monotonic transformation of a 
lognormalg a        class for non-linear monotonic transformation of a continuous random variable 
negsquarenormalg    Distribution based on a non-monotonic (u- or hump-shaped transformation) 
squarenormalg       Distribution based on a non-monotonic (u- or hump-shaped transformation) 
squaretg            Distribution based on a non-monotonic (u- or hump-shaped transformation) 




###Statsmodel-QuickIntro - sm.duration
#Survival function and duration analysis 

SurvfuncRight(time, status[, entry, title, ...]) 
    Estimation and inference for a survival function. 
survdiff(time, status, group, weight_type=None, strata=None,
             entry=None, **kwargs):
    Test for the equality of two survival distributions.
    
PHReg(endog, exog[, status, entry, strata, ...]) 
    Fit the Cox proportional hazards regression model for right censored data. 
PHRegResults(model, params, cov_params[, ...]) 
    The proportional hazards regression result


    
    
###Statsmodel-QuickIntro - sm.emplike
#Empirical likelihood is a method of nonparametric inference 
#and estimation that lifts the obligation of having to specify 
#a family of underlying distributions

#Module Reference
descriptive.DescStat(endog)     Returns an instance to conduct inference on descriptive statistics via empirical likelihood. 
descriptive.DescStatUV(endog)   A class to compute confidence intervals and hypothesis tests involving mean, variance, kurtosis and skewness of a univariate random variable. 
descriptive.DescStatMV(endog)   A class for conducting inference on multivariate means and correlation. 


import numpy as np
import statsmodels.api as sm
# Generate Data
x = np.random.standard_normal(50)
# initiate EL
el = sm.emplike.DescStat(x)
# confidence interval for the mean
>>> el.ci_mean()
(-0.284592989912553, 0.32792432989204601)
# test variance is 1
>>> el.test_var(1)
(1.5489915746765002, 0.21328437351474738)

#SV and MV 
from statsmodels.datasets import star98
data = star98.load()
desc_stat_data = data.exog[:50, 5]   #UV 
mv_desc_stat_data = data.exog[:50, 5:7]  # mv = multivariate
res1 = DescStat(desc_stat_data)
mvres1 = DescStat(mv_desc_stat_data)
#Check methods 
>>> dir(mvres1)
res1.test_mean(14)
res1.ci_mean()
res1.test_var(3)
res1.ci_var()
res1.test_skew(0)
res1.ci_skew()
res1.test_kurt(0)
res1.ci_kurt(upper_bound=.5, lower_bound=-1.5)
res1.test_joint_skew_kurt(0, 0)

mvres1.mv_test_mean(np.array([14, 56]))
mvres1.test_corr(.5)
mvres1.ci_corr()

#Other functionalities in empLike 
ANOVA(endog)
    A class for ANOVA and comparing means.
    endog : list of arrays
        endog should be a list containing 1 dimensional arrays.  Each array
        is the data collected from a certain group.
    Methods 
    ----------
    compute_ANOVA(self, mu=None, mu_start=0, return_weights=0):
        returns The log-likelihood, p-value and estimate for the common mean.
        mu : float
            If a mu is specified, ANOVA is conducted with mu as the
            common mean.  Otherwise, the common mean is the maximum
            empirical likelihood estimate of the common mean.
            Default is None.

        mu_start : float
            Starting value for commean mean if specific mu is not specified.
            Default = 0

        return_weights : bool
            if TRUE, returns the weights on observations that maximize the
            likelihood.  Default is FALSE

#Example 
from statsmodels.datasets import star98
from statsmodels.emplike.elanova import ANOVA

self.data = star98.load().exog[:30, 1:3]
self.res1 = ANOVA([self.data[:, 0], self.data[:, 1]])
self.res1.compute_ANOVA()
        
        
ELOriginRegress(endog, exog)
    This module implements empirical likelihood regression that is forced through the origin.
    This class inherits from RegressionResults but inference is
    conducted via empirical likelihood.  Therefore, any methods that
    require an estimate of the covariance matrix will not function.  Instead
    use el_test and conf_int_el to conduct inference.

#Examples
import statsmodels.api as sm
data = sm.datasets.bc.load()
model = sm.emplike.ELOriginRegress(data.endog, data.exog)
fitted = model.fit()
>>> fitted.params #  0 is the intercept term.
array([ 0.        ,  0.00351813])
#b0_vals : 1darray,The hypothesized value of the parameter to be tested
#param_nums : 1darray,The parameter number to be tested
#returns -2 times the log-likelihood ratio for the hypothesized values and p-value 
>>> fitted.el_test(np.array([.0034]), np.array([1])) #0.0034 for x1
(3.6696503297979302, 0.055411808127497755)
#param_num : float,The parameter for which the confidence interval is desired
>>> fitted.conf_int_el(1) #for x1
(0.0033971871114706867, 0.0036373150174892847)

# No covariance matrix so normal inference is not valid
>>> fitted.conf_int()
TypeError: unsupported operand type(s) for *: 'instancemethod' and 'float'
    
    
    
emplikeAFT(endog, exog, censors) 
    Accelerated Failure Time (AFT) Model with empirical likelihood inference.
    AFT regression analysis is applicable when the researcher has access
    to a randomly right censored dependent variable, a matrix of exogenous
    variables and an indicatior variable (delta) that takes a value of 0 if the
    observation is censored and 1 otherwise.
        endog: nx1 array
            Response variables that are subject to random censoring

        exog: nxk array
            Matrix of covariates

        censors: nx1 array
            array with entries 0 or 1.  0 indicates a response was
            censored.

    Methods 
    ----------        
    fit(): 
        Fits an AFT model and returns AFTResults instance
    predict(params, endog=None)
    
    #AFTResults Methods 
        test_beta(b0_vals, param_nums, ftol=10 ** - 5, maxiter=30,print_weights=1):
            Tests if beta = b0 for any vector b0.
            Returns the log likelihood and p_value for regression parameters 'param_num' at 'b0_vals.'
        ci_beta(param_num, beta_high, beta_low, sig=.05):
            Returns the confidence interval for a regression parameter in the AFT model.
        
#Example 
import statsmodels.api as sm
import numpy as np

# Test parameter is .05 in one regressor no intercept model
data=sm.datasets.heart.load()
y = np.log10(data.endog)
x = data.exog
cens = data.censors
model = sm.emplike.emplikeAFT(y, x, cens)
res1 = model.fit()
res1.params()
res1.test_beta([-.04], [1])  #-0.04 for x1
res1.test_beta([3.5, -.035], [0, 1]) #3.5 for intercept and -0.35 for x1
ci = self.res1.ci_beta(1, -.06, 0) 
ll = ci[0]
ul = ci[1]
ll_pval = res1.test_beta([ll], [1])[1]
ul_pval = res1.test_beta([ul], [1])[1]





###Statsmodel-QuickIntro - sm.families
#contains families for GLM 
>>> dir(sm.families)
['Binomial', 'Family', 'Gamma', 'Gaussian', 'InverseGaussian', 'NegativeBinomial
', 'Poisson', 'Tweedie', '__builtins__', '__cached__', '__doc__', '__file__', '_



###Statsmodel-QuickIntro - sm.formula
#Contains implementation of formula based regression analysis 
>>> dir(sm.formula)
[ 'gee', 'glm', 'gls', 'glsar', 'logit', 'm
ixedlm', 'mnlogit', 'negativebinomial', 'nominal_gee', 'ols', 'ordinal_gee', 'ph
reg', 'poisson', 'probit', 'quantreg', 'rlm', 'wls']



###Statsmodel-QuickIntro - sm.genmod
#contains cov_struct, GEE and GLM 







###Statsmodel-QuickIntro - sm.ProbPlot
#Class for convenient construction of Q-Q, P-P, and probability plots.

#Example 
import statsmodels.api as sm
from matplotlib import pyplot as plt
# example 1
data = sm.datasets.longley.load()
data.exog = sm.add_constant(data.exog)
model = sm.OLS(data.endog, data.exog)
mod_fit = model.fit()
res = mod_fit.resid # residuals
probplot = sm.ProbPlot(res)
probplot.qqplot()
plt.show()


#qqplot of the residuals against quantiles of t-distribution 
#with 4 degrees of freedom:
# example 2
import scipy.stats as stats
probplot = sm.ProbPlot(res, stats.t, distargs=(4,))
fig = probplot.qqplot()
plt.show()

#qqplot against same as above, but with mean 3 and std 10:
# example 3
probplot = sm.ProbPlot(res, stats.t, distargs=(4,), loc=3, scale=10)
fig = probplot.qqplot()
plt.show()

#Automatically determine parameters for t distribution including the loc and scale:
# example 4
probplot = sm.ProbPlot(res, stats.t, fit=True)
fig = probplot.qqplot(line='45')
plt.show()

#A second ProbPlot object can be used to compare two seperate sample sets 
#by using the other kwarg in the qqplot and ppplot methods.
# example 5
import numpy as np
x = np.random.normal(loc=8.25, scale=2.75, size=37)
y = np.random.normal(loc=8.75, scale=3.25, size=37)
pp_x = sm.ProbPlot(x, fit=True)
pp_y = sm.ProbPlot(y, fit=True)
fig = pp_x.qqplot(line='45', other=pp_y)
plt.show()


###Statsmodel-QuickIntro - sm.qqline,sm.qqplot,sm.qqplot_2samples
#Goodness of Fit Plots
gofplots.qqplot(data[, dist, distargs, a, ...]) Q-Q plot of the quantiles of x versus the quantiles/ppf of a distribution. 
gofplots.qqline(ax, line[, x, y, dist, fmt])    Plot a reference line for a qqplot. 
gofplots.qqplot_2samples(data1, data2[, ...])   Q-Q Plot of two samples' quantiles. 

#Example 
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
test = np.random.normal(0, 1, 1000)
pp = sm.ProbPlot(test, fit=True)
qq = pp.qqplot(marker='.', markerfacecolor='k', markeredgecolor='k', alpha=0.3)
sm.qqline(qq.axes[0], line='45', fmt='k--')
plt.show()

#Example of qqplot_2samples
x = np.random.normal(loc=8.5, scale=2.5, size=37)
y = np.random.normal(loc=8.0, scale=3.0, size=37)
pp_x = sm.ProbPlot(x)
pp_y = sm.ProbPlot(y)
qqplot_2samples(pp_x, pp_y)


###Statsmodel-QuickIntro - sm.graphics
#Goodness of Fit Plots
gofplots.qqplot(data[, dist, distargs, a, ...])     Q-Q plot of the quantiles of x versus the quantiles/ppf of a distribution. 
gofplots.qqline(ax, line[, x, y, dist, fmt])        Plot a reference line for a qqplot. 
gofplots.qqplot_2samples(data1, data2[, ...])       Q-Q Plot of two samples' quantiles. 
gofplots.ProbPlot(data[, dist, fit, ...])           Class for convenient construction of Q-Q, P-P, and probability plots. 

#Boxplots
boxplots.violinplot(data[, ax, labels, ...])    Make a violin plot of each dataset in the data sequence. 
boxplots.beanplot(data[, ax, labels, ...])      Make a bean plot of each dataset in the data sequence. 

#Correlation Plots
correlation.plot_corr(dcorr[, xnames, ...])         Plot correlation of many variables in a tight color grid. 
correlation.plot_corr_grid(dcorrs[, titles, ...])   Create a grid of correlation plots. 
plot_grids.scatter_ellipse(data[, level, ...])  Create a grid of scatter plots with confidence ellipses. 

#Functional Plots
functional.fboxplot(data[, xdata, labels, ...])     Plot functional boxplot. 
functional.rainbowplot(data[, xdata, depth, ...])   Create a rainbow plot for a set of curves. 
functional.banddepth(data[, method])                Calculate the band depth for a set of functional curves. 


#Time Series Plots
tsaplots.plot_acf(x[, ax, lags, alpha, ...])    Plot the autocorrelation function 
tsaplots.plot_pacf(x[, ax, lags, alpha, ...])   Plot the partial autocorrelation function 
tsaplots.month_plot(x[, dates, ylabel, ax])     Seasonal plot of monthly data 
tsaplots.quarter_plot(x[, dates, ylabel, ax])   Seasonal plot of quarterly data 

#Other Plots
factorplots.interaction_plot(x, trace, response) Interaction plot for factor level statistics. 
mosaicplot.mosaic(data[, index, ax, ...])       Create a mosaic plot from a contingency table. 



#Example - qqplot
import statsmodels.api as sm
from matplotlib import pyplot as plt
data = sm.datasets.longley.load()
data.exog = sm.add_constant(data.exog)
mod_fit = sm.OLS(data.endog, data.exog).fit()
res = mod_fit.resid # residuals
fig = sm.qqplot(res)
plt.show()

#qqplot of the residuals against quantiles of t-distribution with 4 degrees of freedom:
import scipy.stats as stats
fig = sm.qqplot(res, stats.t, distargs=(4,))
plt.show()

#qqplot against same as above, but with mean 3 and std 10:
fig = sm.qqplot(res, stats.t, distargs=(4,), loc=3, scale=10)
plt.show()

#Automatically determine parameters for t distribution including the loc and scale:
>>> fig = sm.qqplot(res, stats.t, fit=True, line='45')
>>> plt.show()

#Example - beanplot 
#We use the American National Election Survey 1996 dataset, 
#which has Party Identification of respondents as independent variable 
#and (among other data) age as dependent variable.
>>> data = sm.datasets.anes96.load_pandas()
>>> party_ID = np.arange(7)
>>> labels = ["Strong Democrat", "Weak Democrat", "Independent-Democrat",
           "Independent-Indpendent", "Independent-Republican",
           "Weak Republican", "Strong Republican"]
#Group age by party ID, and create a violin plot with it:
plt.rcParams['figure.subplot.bottom'] = 0.23  # keep labels visible
age = [data.exog['age'][data.endog == id] for id in party_ID]
fig = plt.figure()
ax = fig.add_subplot(111)
sm.graphics.beanplot(age, ax=ax, labels=labels,
                     plot_opts={'cutoff_val':5, 'cutoff_type':'abs',
                                'label_fontsize':'small',
                                'label_rotation':30})
ax.set_xlabel("Party identification of respondent.")
ax.set_ylabel("Age")
plt.show()

#Exmaple - plot_corr
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.graphics.api as smg
hie_data = sm.datasets.randhie.load_pandas()
corr_matrix = np.corrcoef(hie_data.data.T)
smg.plot_corr(corr_matrix, xnames=hie_data.names)
plt.show()

#Examples of rainbowplot
#Load the El Nino dataset. 
#Consists of 60 years worth of Pacific Ocean sea surface temperature data.
import matplotlib.pyplot as plt
import statsmodels.api as sm
data = sm.datasets.elnino.load()

fig = plt.figure()
ax = fig.add_subplot(111)
res = sm.graphics.rainbowplot(data.raw_data[:, 1:], ax=ax)
ax.set_xlabel("Month of the year")
ax.set_ylabel("Sea surface temperature (C)")
ax.set_xticks(np.arange(13, step=3) - 1)
ax.set_xticklabels(["", "Mar", "Jun", "Sep", "Dec"])
ax.set_xlim([-0.2, 11.2])
plt.show()

#Examples of month_plot
import statsmodels.api as sm
import pandas as pd
dta = sm.datasets.elnino.load_pandas().data
dta['YEAR'] = dta.YEAR.astype(int).astype(str)
dta = dta.set_index('YEAR').T.unstack()
dates = pd.to_datetime(list(map(lambda x : '-'.join(x) + '-1',
                                       dta.index.values)))
dta.index = pd.DatetimeIndex(dates, freq='MS')
fig = sm.graphics.tsa.month_plot(dta)

#Examples of mosiac
#The most simple use case is to take a dictionary and plot the result
data = {'a': 10, 'b': 15, 'c': 16}
mosaic(data, title='basic dictionary')
pylab.show()

#A more useful example is given by a dictionary with multiple indices. 
#In this case we use a wider gap to a better visual separation of the resulting plot
data = {('a', 'b'): 1, ('a', 'c'): 2, ('d', 'b'): 3, ('d', 'c'): 4}
mosaic(data, gap=0.05, title='complete dictionary')
pylab.show()

#The same data can be given as a simple or hierarchical indexed Series
rand = np.random.random
from itertools import product

tuples = list(product(['bar', 'baz', 'foo', 'qux'], ['one', 'two']))
index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])
data = pd.Series(rand(8), index=index)
mosaic(data, title='hierarchical index series')
pylab.show()

#The third accepted data structureis the np array, 
#for which a very simple index will be created.
rand = np.random.random
data = 1+rand((2,2))
mosaic(data, title='random non-labeled array')
pylab.show()

#If you need to modify the labeling and the coloring 
#you can give a function tocreate the labels 
#and one with the graphical properties starting from the key tuple
data = {'a': 10, 'b': 15, 'c': 16}
props = lambda key: {'color': 'r' if 'a' in key else 'gray'}
labelizer = lambda k: {('a',): 'first', ('b',): 'second',                                ('c',): 'third'}[k]
mosaic(data, title='colored dictionary',                 properties=props, labelizer=labelizer)
pylab.show()


#Using a DataFrame as source, specifying the name of the columns of interest 
gender = ['male', 'male', 'male', 'female', 'female', 'female'] 
pet = ['cat', 'dog', 'dog', 'cat', 'dog', 'cat'] 
data = pandas.DataFrame({'gender': gender, 'pet': pet}) 
mosaic(data, ['pet', 'gender']) 
pylab.show()

#Exmaple of interaction_plot
import numpy as np
np.random.seed(12345)
weight = np.random.randint(1,4,size=60)
duration = np.random.randint(1,3,size=60)
days = np.log(np.random.randint(1,30, size=60))
fig = interaction_plot(weight, duration, days,
            colors=['red','blue'], markers=['D','^'], ms=10)
import matplotlib.pyplot as plt
plt.show()


###Statsmodel -  QuickIntro - sm.graphics - Regression Plots
#Regression Plots
regressionplots.plot_fit(results, exog_idx)     Plot fit against one regressor. 
regressionplots.plot_regress_exog(results, ...) Plot regression results against one regressor. 
regressionplots.plot_partregress(endog, ...)    Plot partial regression for a single regressor. 
regressionplots.plot_ccpr(results, exog_idx)    Plot CCPR against one regressor. 
regressionplots.abline_plot([intercept, ...])   Plots a line given an intercept and slope. 
regressionplots.influence_plot(results[, ...])  Plot of influence in regression. 
regressionplots.plot_leverage_resid2(results)   Plots leverage statistics vs. 
#Example 
%matplotlib inline
from __future__ import print_function
from statsmodels.compat import lzip
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.formula.api import ols

#Duncan's Prestige Dataset
prestige = sm.datasets.get_rdataset("Duncan", "car", cache=True).data
prestige.head()
prestige_model = ols("prestige ~ income + education", data=prestige).fit()
>>> print(prestige_model.summary())
                            OLS Regression Results                            
==============================================================================
Dep. Variable:               prestige   R-squared:                       0.828
Model:                            OLS   Adj. R-squared:                  0.820
Method:                 Least Squares   F-statistic:                     101.2
Date:                Tue, 28 Feb 2017   Prob (F-statistic):           8.65e-17
Time:                        21:34:02   Log-Likelihood:                -178.98
No. Observations:                  45   AIC:                             364.0
Df Residuals:                      42   BIC:                             369.4
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -6.0647      4.272     -1.420      0.163     -14.686       2.556
income         0.5987      0.120      5.003      0.000       0.357       0.840
education      0.5458      0.098      5.555      0.000       0.348       0.744
==============================================================================
Omnibus:                        1.279   Durbin-Watson:                   1.458
Prob(Omnibus):                  0.528   Jarque-Bera (JB):                0.520
Skew:                           0.155   Prob(JB):                        0.771
Kurtosis:                       3.426   Cond. No.                         163.
==============================================================================
Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

#Influence plots
#Influence plots show the (externally) studentized residuals 
#vs. the leverage of each observation as measured by the hat matrix.
#The influence of each point can be visualized by the criterion keyword argument. Options are Cook's distance and DFFITS, two measures of influence.
fig, ax = plt.subplots(figsize=(12,8))
fig = sm.graphics.influence_plot(prestige_model, ax=ax, criterion="cooks")
 
#As you can see there are a few worrisome observations. 
#Both contractor and reporter have low leverage but a large residual. 
#RR.engineer has small residual and large leverage. 
#Conductor and minister have both high leverage and large residuals, and, therefore, large influence.


#Partial Regression Plots
#Since we are doing multivariate regressions, 
#we cannot just look at individual bivariate plots to discern relationships. 
#Instead, we want to look at the relationship of the dependent variable 
#and independent variables conditional on the other independent variables. 
#We can do this through using partial regression plots, 

fig, ax = plt.subplots(figsize=(12,8))
fig = sm.graphics.plot_partregress("prestige", "income", ["income", "education"], data=prestige, ax=ax)
 

fix, ax = plt.subplots(figsize=(12,14))
fig = sm.graphics.plot_partregress("prestige", "income", ["education"], data=prestige, ax=ax)
 
#As you can see the partial regression plot confirms 
#the influence of conductor, minister, and RR.engineer 
#on the partial relationship between income and prestige. 
#The cases greatly decrease the effect of income on prestige. 
#Dropping these cases confirms this.

subset = ~prestige.index.isin(["conductor", "RR.engineer", "minister"])
prestige_model2 = ols("prestige ~ income + education", data=prestige, subset=subset).fit()
>>> print(prestige_model2.summary())
                            OLS Regression Results                            
==============================================================================
Dep. Variable:               prestige   R-squared:                       0.876
Model:                            OLS   Adj. R-squared:                  0.870
Method:                 Least Squares   F-statistic:                     138.1
Date:                Tue, 28 Feb 2017   Prob (F-statistic):           2.02e-18
Time:                        21:34:04   Log-Likelihood:                -160.59
No. Observations:                  42   AIC:                             327.2
Df Residuals:                      39   BIC:                             332.4
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -6.3174      3.680     -1.717      0.094     -13.760       1.125
income         0.9307      0.154      6.053      0.000       0.620       1.242
education      0.2846      0.121      2.345      0.024       0.039       0.530
==============================================================================
Omnibus:                        3.811   Durbin-Watson:                   1.468
Prob(Omnibus):                  0.149   Jarque-Bera (JB):                2.802
Skew:                          -0.614   Prob(JB):                        0.246
Kurtosis:                       3.303   Cond. No.                         158.
==============================================================================
Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

#For a quick check of all the regressors, you can use plot_partregress_grid. 
#These plots will not label the  points, but you can use them to identify problems 
#and then use plot_partregress to get more information.
fig = plt.figure(figsize=(12,8))
fig = sm.graphics.plot_partregress_grid(prestige_model, fig=fig)
 
#Component-Component plus Residual (CCPR) Plots
#The CCPR plot provides a way to judge the effect of one regressor on the 
#response variable by taking into account the effects of the other 
#independent variables. The partial residuals plot is defined as 

fig, ax = plt.subplots(figsize=(12, 8))
fig = sm.graphics.plot_ccpr(prestige_model, "education", ax=ax)
 
#As you can see the relationship between the variation in prestige 
#explained by education conditional on income seems to be linear, 
#though you can see there are some observations 
#that are exerting considerable influence on the relationship. 

#We can quickly look at more than one variable by using plot_ccpr_grid.
fig = plt.figure(figsize=(12, 8))
fig = sm.graphics.plot_ccpr_grid(prestige_model, fig=fig)
 
#Regression Plots
#The plot_regress_exog function is a convenience function 
#that gives a 2x2 plot containing the dependent variable 
#and fitted values with confidence intervals vs. the independent variable chosen, 
#the residuals of the model vs. the chosen independent variable, 
#a partial regression plot, and a CCPR plot. 

#This function can be used for quickly checking modeling assumptions 
#with respect to a single regressor.

fig = plt.figure(figsize=(12,8))
fig = sm.graphics.plot_regress_exog(prestige_model, "education", fig=fig)
 
#Fit Plot
#The plot_fit function plots the fitted values versus a chosen independent variable. 
#It includes prediction confidence intervals and optionally plots the true dependent variable.

fig, ax = plt.subplots(figsize=(12, 8))
fig = sm.graphics.plot_fit(prestige_model, "education", ax=ax)
 
#Regresssion plot example 
#Statewide Crime 2009 Dataset

dta = sm.datasets.statecrime.load_pandas().data
crime_model = ols("murder ~ urban + poverty + hs_grad + single", data=dta).fit()
>>> print(crime_model.summary())
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 murder   R-squared:                       0.813
Model:                            OLS   Adj. R-squared:                  0.797
Method:                 Least Squares   F-statistic:                     50.08
Date:                Tue, 28 Feb 2017   Prob (F-statistic):           3.42e-16
Time:                        21:34:09   Log-Likelihood:                -95.050
No. Observations:                  51   AIC:                             200.1
Df Residuals:                      46   BIC:                             209.8
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -44.1024     12.086     -3.649      0.001     -68.430     -19.774
urban          0.0109      0.015      0.707      0.483      -0.020       0.042
poverty        0.4121      0.140      2.939      0.005       0.130       0.694
hs_grad        0.3059      0.117      2.611      0.012       0.070       0.542
single         0.6374      0.070      9.065      0.000       0.496       0.779
==============================================================================
Omnibus:                        1.618   Durbin-Watson:                   2.507
Prob(Omnibus):                  0.445   Jarque-Bera (JB):                0.831
Skew:                          -0.220   Prob(JB):                        0.660
Kurtosis:                       3.445   Cond. No.                     5.80e+03
==============================================================================
Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 5.8e+03. This might indicate that there are
strong multicollinearity or other numerical problems.

#Partial Regression Plots
fig = plt.figure(figsize=(12,8))
fig = sm.graphics.plot_partregress_grid(crime_model, fig=fig)
 
fig, ax = plt.subplots(figsize=(12,8))
fig = sm.graphics.plot_partregress("murder", "hs_grad", ["urban", "poverty", "single"],  ax=ax, data=dta)
 
#Leverage-Resid2 Plot
#Closely related to the influence_plot is the leverage-resid2 plot.

fig, ax = plt.subplots(figsize=(8,6))
fig = sm.graphics.plot_leverage_resid2(crime_model, ax=ax)
 
#Influence Plot
fig, ax = plt.subplots(figsize=(8,6))
fig = sm.graphics.influence_plot(crime_model, ax=ax)
 
#Using robust regression to correct for outliers.

from statsmodels.formula.api import rlm

rob_crime_model = rlm("murder ~ urban + poverty + hs_grad + single", data=dta, 
                      M=sm.robust.norms.TukeyBiweight(3)).fit(conv="weights")
>>> print(rob_crime_model.summary())
                    Robust linear Model Regression Results                    
==============================================================================
Dep. Variable:                 murder   No. Observations:                   51
Model:                            RLM   Df Residuals:                       46
Method:                          IRLS   Df Model:                            4
Norm:                   TukeyBiweight                                         
Scale Est.:                       mad                                         
Cov Type:                          H1                                         
Date:                Tue, 28 Feb 2017                                         
Time:                        21:34:12                                         
No. Iterations:                    50                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -4.2986      9.494     -0.453      0.651     -22.907      14.310
urban          0.0029      0.012      0.241      0.809      -0.021       0.027
poverty        0.2753      0.110      2.499      0.012       0.059       0.491
hs_grad       -0.0302      0.092     -0.328      0.743      -0.211       0.150
single         0.2902      0.055      5.253      0.000       0.182       0.398
==============================================================================

#There isn't yet an influence diagnostics method as part of RLM, but we can recreate them. 
#(This depends on the status of issue #888)
weights = rob_crime_model.weights
idx = weights > 0
X = rob_crime_model.model.exog[idx.values]
ww = weights[idx] / weights[idx].mean()
hat_matrix_diag = ww*(X*np.linalg.pinv(X).T).sum(1)
resid = rob_crime_model.resid
resid2 = resid**2
resid2 /= resid2.sum()
nobs = int(idx.sum())
hm = hat_matrix_diag.mean()
rm = resid2.mean()

from statsmodels.graphics import utils
fig, ax = plt.subplots(figsize=(12,8))
ax.plot(resid2[idx], hat_matrix_diag, 'o')
ax = utils.annotate_axes(range(nobs), labels=rob_crime_model.model.data.row_labels[idx], 
                    points=lzip(resid2[idx], hat_matrix_diag), offset_points=[(-5,5)]*nobs,
                    size="large", ax=ax)
ax.set_xlabel("resid2")
ax.set_ylabel("leverage")
ylim = ax.get_ylim()
ax.vlines(rm, *ylim)
xlim = ax.get_xlim()
ax.hlines(hm, *xlim)
ax.margins(0,0)





###Statsmodel-QuickIntro - sm.iolib
#statsmodels offers some functions for input and output

foreign.StataReader(fname[, missing_values, ...]) Stata .dta file reader. 
foreign.StataWriter(fname, data[, ...])         A class for writing Stata binary dta files from array-like objects 
foreign.genfromdta(fname[, missing_flt, ...])   Returns an ndarray or DataFrame from a Stata .dta file. 
foreign.savetxt(fname, X[, names, fmt, ...])    Save an array to a text file. 
table.SimpleTable(data[, headers, stubs, ...])  Produce a simple ASCII, CSV, HTML, or LaTeX table from a rectangular (2d!) array of data, not necessarily numerical. 
table.csv2st(csvfile[, headers, stubs, title])  Return SimpleTable instance, created from the data in csvfile, which is in comma separated values format. 
smpickle.save_pickle(obj, fname)                Save the object to file via pickling. 
smpickle.load_pickle(fname)                     Load a previously saved object from file 
summary.Summary()                               class to hold tables for result summary presentation 

#Example  
mydata = [[11,12],[21,22]]  # data MUST be 2-dimensional
myheaders = [ "Column 1", "Column 2" ]
mystubs = [ "Row 1", "Row 2" ]
tbl = text.SimpleTable(mydata, myheaders, mystubs, title="Title")
print( tbl )
print( tbl.as_html() )
# set column specific data formatting
tbl = text.SimpleTable(mydata, myheaders, mystubs,
    data_fmts=["%3.2f","%d"])
print( tbl.as_csv() )
with open('c:/temp/temp.tex','w') as fh:
    fh.write( tbl.as_latex_tabular() )
 
 
 
 
class statsmodels.iolib.summary2.Summary
    Any result.summary() returns instance of Summary
    add_array(array[, align, float_format])         Add the contents of a Numpy array to summary table 
    add_base(results[, alpha, float_format, ...])   Try to construct a basic summary instance. 
    add_df(df[, index, header, float_format, align]) Add the contents of a DataFrame to summary table 
    add_dict(d[, ncols, align, float_format])       Add the contents of a Dict to summary table 
    add_text(string)                                Append a note to the bottom of the summary table. 
    add_title([title, results])                     Insert a title on top of the summary table. 
    as_html()   Generate HTML Summary Table 
    as_latex()  Generate LaTeX Summary Table 
    as_text()   Generate ASCII Summary Table 

 
 
 
###Statsmodel-QuickIntro - sm.load
#equivalent to smpickle.load_pickle(fname)

#Example 
import statsmodels.api as sm
data = sm.datasets.longley.load_pandas()
data.exog['constant'] = 1
results = sm.OLS(data.endog, data.exog).fit()
results.save("longley_results.pickle")

from statsmodels.regression.linear_model import OLSResults
new_results = OLSResults.load("longley_results.pickle")
# or
from statsmodels.iolib.smpickle import load_pickle
new_results = load_pickle("longley_results.pickle")
#or 
new_results = sm.load('longley_results.pickle')



###Statsmodel-QuickIntro - sm.nonparametric
#This includes kernel density estimation for univariate and multivariate data, 
#kernel regression and locally weighted scatterplot smoothing (lowess).

smoothers_lowess.lowess(endog, exog[, frac, ...]) LOWESS (Locally Weighted Scatterplot Smoothing) 
kde.KDEUnivariate(endog)                          Univariate Kernel Density Estimator. 
kernel_density.KDEMultivariate(data, var_type)    Multivariate kernel density estimator. 
kernel_density.KDEMultivariateConditional(...)    Conditional multivariate kernel density estimator. 
kernel_density.EstimatorSettings([...])           Object to specify settings for density estimation or regression. 
kernel_regression.KernelReg(endog, exog, ...)     Nonparametric kernel regression class. 
kernel_regression.KernelCensoredReg(endog, ...)   Nonparametric censored regression. 

#helper functions for kernel bandwidths
bandwidths.bw_scott(x[, kernel])            Scott's Rule of Thumb 
bandwidths.bw_silverman(x[, kernel])        Silverman's Rule of Thumb 
bandwidths.select_bandwidth(x, bw, kernel)  Selects bandwidth for a selection rule bw 


#LOWESS (Locally Weighted Scatterplot Smoothing)
#A lowess function that outs smoothed estimates of endog 
#at the given exog values from points (exog, endog)

#The below allows a comparison between how different the fits 
#from lowess for different values of frac can be.
import numpy as np
import statsmodels.api as sm
lowess = sm.nonparametric.lowess
x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)
y = np.sin(x) + np.random.normal(size=len(x))
z = lowess(y, x)
w = lowess(y, x, frac=1./3)

#This gives a similar comparison for when it is 0 vs not.
import numpy as np
import scipy.stats as stats
import statsmodels.api as sm
lowess = sm.nonparametric.lowess
x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)
y = np.sin(x) + stats.cauchy.rvs(size=len(x))
z = lowess(y, x, frac= 1./3, it=0)
w = lowess(y, x, frac=1./3)


class statsmodels.nonparametric.kde.KDEUnivariate(endog)
    Univariate Kernel Density Estimator.
    Methods
    cdf()           Returns the cumulative distribution function evaluated at the support. 
    cumhazard()     Returns the hazard function evaluated at the support. 
    entropy()       Returns the differential entropy evaluated at the support 
    evaluate(point) Evaluate density at a single point. 
    fit([kernel, bw, fft, weights, gridsize, ...])  Attach the density estimate to the KDEUnivariate class. 
    icdf()      Inverse Cumulative Distribution (Quantile) Function 
    sf()        Returns the survival function evaluated at the support.

#Exmaple 
import statsmodels.api as sm
import matplotlib.pyplot as plt
nobs = 300
np.random.seed(1234)  # Seed random generator
dens = sm.nonparametric.KDEUnivariate(np.random.normal(size=nobs))
dens.fit()
plt.plot(dens.cdf)
plt.show()
 
class statsmodels.nonparametric.kernel_density.KDEMultivariate(data, var_type, bw=None, defaults=<statsmodels.nonparametric._kernel_base.EstimatorSettings object>)
    Multivariate kernel density estimator.
    Methods
    cdf([data_predict])     Evaluate the cumulative distribution function. 
    imse(bw)                Returns the Integrated Mean Square Error for the unconditional KDE. 
    loo_likelihood(bw[, func]) Returns the leave-one-out likelihood function. 
    pdf([data_predict])     Evaluate the probability density function.

#Example 
import statsmodels.api as sm
nobs = 300
np.random.seed(1234)  # Seed random generator
c1 = np.random.normal(size=(nobs,1))
c2 = np.random.normal(2, 1, size=(nobs,1))
#Estimate a bivariate distribution and display the bandwidth found:
dens_u = sm.nonparametric.KDEMultivariate(data=[c1,c2],
    var_type='cc', bw='normal_reference')
>>> dens_u.bw
array([ 0.39967419,  0.38423292])



 

###Statsmodel-QuickIntro - sm.regression
#implementation of OLS,GLS etc 


###Statsmodel-QuickIntro - sm.robust
#Implementation of RLM 


###Statsmodel-QuickIntro - sm.show_versions
>>> sm.show_versions()
INSTALLED VERSIONS
------------------
Python: 3.5.2.final.0
Statsmodels
===========
Installed: 0.8.0 (c:\python35\lib\site-packages\statsmodels)

###Statsmodel-QuickIntro - sm.version
>>> help(sm.version)
Help on module statsmodels.version in statsmodels:
NAME
    statsmodels.version - # THIS FILE IS GENERATED FROM SETUP.PY
DATA
    full_version = '0.8.0'
    git_revision = 'Unknown'
    release = True
    short_version = '0.8.0'
    version = '0.8.0'

###Statsmodel-QuickIntro - sm.test
#Running the Test Suite
#You can run all the tests by:
import statsmodels.api as sm
sm.test()
#You can test submodules by:
sm.discrete.test()



###Statsmodel-QuickIntro - sm.tools

#Basic tools tools
tools.add_constant(data[, prepend, has_constant])   Adds a column of ones to an array 
tools.categorical(data[, col, dictnames, drop])     Returns a dummy matrix given an array of categorical variables. 
tools.clean0(matrix)    Erase columns of zeros: can save some time in pseudoinverse. 
tools.fullrank(X[, r])  Return a matrix whose column span is the same as X. 
tools.isestimable(C, D) True if (Q, P) contrast C is estimable for (N, P) design D 
tools.rank(X[, cond])   Return the rank of a matrix X based on its generalized inverse, not the SVD. 
tools.recipr(X)         Return the reciprocal of an array, setting all entries less than or equal to 0 to 0. 
tools.recipr0(X)        Return the reciprocal of an array, setting all entries equal to 0 as 0. 
tools.unsqueeze(data, axis, oldshape)   Unsqueeze a collapsed array 

#Numerical Differentiation
numdiff.approx_fprime(x, f[, epsilon, args, ...])   Gradient of function, or Jacobian if function f returns 1d array 
numdiff.approx_fprime_cs(x, f[, epsilon, ...])      Calculate gradient or Jacobian with complex step derivative approximation 
numdiff.approx_hess1(x, f[, epsilon, args, ...])    Calculate Hessian with finite difference derivative approximation 
numdiff.approx_hess2(x, f[, epsilon, args, ...])    Calculate Hessian with finite difference derivative approximation 
numdiff.approx_hess3(x, f[, epsilon, args, ...])    Calculate Hessian with finite difference derivative approximation 
numdiff.approx_hess_cs(x, f[, epsilon, ...])        Calculate Hessian with complex-step derivative approximation 

#Measure for fit performance eval_measures
#The function with _sigma suffix take the error sum of squares as argument, 
#those without, take the value of the log-likelihood, llf, as argument.
eval_measures.aic(llf, nobs, df_modelwc)            Akaike information criterion 
eval_measures.aic_sigma(sigma2, nobs, df_modelwc)   Akaike information criterion 
eval_measures.aicc(llf, nobs, df_modelwc)           Akaike information criterion (AIC) with small sample correction 
eval_measures.aicc_sigma(sigma2, nobs, ...)         Akaike information criterion (AIC) with small sample correction 
eval_measures.bic(llf, nobs, df_modelwc)            Bayesian information criterion (BIC) or Schwarz criterion 
eval_measures.bic_sigma(sigma2, nobs, df_modelwc)   Bayesian information criterion (BIC) or Schwarz criterion 
eval_measures.hqic(llf, nobs, df_modelwc)           Hannan-Quinn information criterion (HQC) 
eval_measures.hqic_sigma(sigma2, nobs, ...)         Hannan-Quinn information criterion (HQC) 

eval_measures.bias(x1, x2[, axis])          bias, mean error 
eval_measures.iqr(x1, x2[, axis])           interquartile range of error 
eval_measures.maxabs(x1, x2[, axis])        maximum absolute error 
eval_measures.meanabs(x1, x2[, axis])       mean absolute error 
eval_measures.medianabs(x1, x2[, axis])     median absolute error 
eval_measures.medianbias(x1, x2[, axis])    median bias, median error 
eval_measures.mse(x1, x2[, axis])           mean squared error 
eval_measures.rmse(x1, x2[, axis])          root mean squared error 
eval_measures.stde(x1, x2[, ddof, axis])    standard deviation of error 
eval_measures.vare(x1, x2[, ddof, axis])    variance of error 


#Example 
from statsmodels.compat.python import zip
import numpy as np
from numpy.testing import assert_equal, assert_almost_equal, assert_
from statsmodels.tools.eval_measures import (
    maxabs, meanabs, medianabs, medianbias, mse, rmse, stde, vare,
    aic, aic_sigma, aicc, aicc_sigma, bias, bic, bic_sigma,
    hqic, hqic_sigma, iqr)
x = np.arange(20).reshape(4,5)
y = np.ones((4,5))
assert_equal(iqr(x, y), 5*np.ones(5))
assert_equal(iqr(x, y, axis=1), 2*np.ones(4))
assert_equal(iqr(x, y, axis=None), 9)
assert_equal(mse(x, y),
             np.array([  73.5,   87.5,  103.5,  121.5,  141.5]))
assert_equal(mse(x, y, axis=1),
             np.array([   3.,   38.,  123.,  258.]))
assert_almost_equal(rmse(x, y),
                    np.array([  8.5732141 ,   9.35414347,  10.17349497,
                               11.02270384,  11.89537725]))
assert_almost_equal(rmse(x, y, axis=1),
                    np.array([  1.73205081,   6.164414,
                               11.09053651,  16.0623784 ]))
assert_equal(maxabs(x, y),
             np.array([ 14.,  15.,  16.,  17.,  18.]))
assert_equal(maxabs(x, y, axis=1),
             np.array([  3.,   8.,  13.,  18.]))
assert_equal(meanabs(x, y),
             np.array([  7. ,   7.5,   8.5,   9.5,  10.5]))
assert_equal(meanabs(x, y, axis=1),
             np.array([  1.4,   6. ,  11. ,  16. ]))
assert_equal(meanabs(x, y, axis=0),
             np.array([  7. ,   7.5,   8.5,   9.5,  10.5]))
assert_equal(medianabs(x, y),
             np.array([  6.5,   7.5,   8.5,   9.5,  10.5]))
assert_equal(medianabs(x, y, axis=1),
             np.array([  1.,   6.,  11.,  16.]))
assert_equal(bias(x, y),
             np.array([  6.5,   7.5,   8.5,   9.5,  10.5]))
assert_equal(bias(x, y, axis=1),
             np.array([  1.,   6.,  11.,  16.]))
assert_equal(medianbias(x, y),
             np.array([  6.5,   7.5,   8.5,   9.5,  10.5]))
assert_equal(medianbias(x, y, axis=1),
             np.array([  1.,   6.,  11.,  16.]))
assert_equal(vare(x, y),
             np.array([ 31.25,  31.25,  31.25,  31.25,  31.25]))
assert_equal(vare(x, y, axis=1),
             np.array([ 2.,  2.,  2.,  2.]))
                 


##Statsmodel-QuickIntro - sm.webdoc
webdoc(arg=None, stable=None)
    Opens a browser and displays online documentation
    Parameters
    ----------
    arg, optional : string or statsmodels function
        Either a string to search the documentation or a function
        
#Example 
>>> import statsmodels.api as sm
>>> sm.webdoc()  # Documention site
>>> sm.webdoc('glm')  # Search for glm in docs
>>> sm.webdoc(sm.OLS, stable=False)  # Go to generated help for OLS, devel
             
                 

###Statsmodel- Statistics- sm.stats

#Residual Diagnostics and Specification Tests
#Examples in 'QuickIntro - with/without formula OLS  and Regression test'
durbin_watson(resids[, axis])       Calculates the Durbin-Watson statistic 
jarque_bera(resids[, axis])         Calculates the Jarque-Bera test for normality 
omni_normtest(resids[, axis])       Omnibus test for normality 

medcouple(y[, axis])                Calculates the medcouple robust measure of skew. 
robust_skewness(y[, axis])          Calculates the four skewness measures in Kim & White 
robust_kurtosis(y[, axis, ab, dg, excess])  Calculates the four kurtosis measures in Kim & White 
expected_robust_kurtosis([ab, dg])  Calculates the expected value of the robust kurtosis measures in Kim and White assuming the data are normally distributed. 
 
acorr_ljungbox(x[, lags, boxpierce])            Ljung-Box test for no autocorrelation 
acorr_breusch_godfrey(results[, nlags, store])  Breusch Godfrey Lagrange Multiplier tests for residual autocorrelation 
HetGoldfeldQuandt                               test whether variance is the same in 2 subsamples 
het_goldfeldquandt                              see class docstring 
het_breuschpagan(resid, exog_het)               Breusch-Pagan Lagrange Multiplier test for heteroscedasticity 
het_white(resid, exog[, retres])                White's Lagrange Multiplier Test for Heteroscedasticity 
het_arch(resid[, maxlag, autolag, store, ...])  Engle's Test for Autoregressive Conditional Heteroscedasticity (ARCH) 
linear_harvey_collier(res)                      Harvey Collier test for linearity 
linear_rainbow(res[, frac])                     Rainbow test for linearity 
linear_lm(resid, exog[, func])                  Lagrange multiplier test for linearity against functional alternative 
breaks_cusumolsresid(olsresidual[, ddof])       cusum test for parameter stability based on ols residuals 
breaks_hansen(olsresults)                       test for model stability, breaks in parameters for ols, Hansen 1992 
recursive_olsresiduals(olsresults[, skip, ...]) calculate recursive ols with residuals and cusum test statistic 

CompareCox Cox          Test for non-nested models 
compare_cox Cox         Test for non-nested models 
CompareJ                J-Test for comparing non-nested models 
compare_j               J-Test for comparing non-nested models 
unitroot_adf(x[, maxlag, trendorder, ...])  
normal_ad(x[, axis])            Anderson-Darling test for normal distribution unknown mean and variance 
kstest_normal(x[, pvalmethod])  lilliefors test for normality, 
lilliefors(x[, pvalmethod])     lilliefors test for normality, 

#Outliers and influence measures
#Examples in 'QuickIntro - with/without formula OLS  and Regression test'
OLSInfluence(results)                       class to calculate outlier and influence measures for OLS result 
variance_inflation_factor(exog, exog_idx)   variance inflation factor, VIF, for one exogenous variable 



#Goodness of Fit Tests and Measures
#some tests for goodness of fit for univariate distributions
powerdiscrepancy(observed, expected[, ...])     Calculates power discrepancy, a class of goodness-of-fit tests as a measure of discrepancy between observed and expected data. 
gof_chisquare_discrete(distfn, arg, rvs, ...)   perform chisquare test for random sample of a discrete distribution 
gof_binning_discrete(rvs, distfn, arg[, nsupp]) get bins for chisquare type gof tests for a discrete distribution 
chisquare_effectsize(probs0, probs1[, ...])     effect size for a chisquare goodness-of-fit test 
normal_ad(x[, axis])                            Anderson-Darling test for normal distribution unknown mean and variance 
kstest_normal(x[, pvalmethod])                  lilliefors test for normality, 
lilliefors(x[, pvalmethod])                     lilliefors test for normality, 


#Interrater Reliability and Agreement
cohens_kappa(table[, weights, ...])     Compute Cohen's kappa with variance and equal-zero test 
fleiss_kappa(table)                     Fleiss' kappa multi-rater agreement measure 
to_table(data[, bins])                  convert raw data with shape (subject, rater) to (rater1, rater2) 
aggregate_raters(data[, n_cat])         convert raw data with shape (subject, rater) to (subject, cat_counts) 

#Sandwich Robust Covariances
#The following functions calculate covariance matrices 
#and standard errors for the parameter estimates 
#that are robust to heteroscedasticity and autocorrelation in the errors. 

#these methods are designed for use with OLSResults
sandwich_covariance.cov_hac(results[, ...])         heteroscedasticity and autocorrelation robust covariance matrix (Newey-West) 
sandwich_covariance.cov_nw_panel(results, ...)      Panel HAC robust covariance matrix 
sandwich_covariance.cov_nw_groupsum(results, ...)   Driscoll and Kraay Panel robust covariance matrix 
sandwich_covariance.cov_cluster(results, group)     cluster robust covariance matrix 
sandwich_covariance.cov_cluster_2groups(...)        cluster robust covariance matrix for two groups/clusters 
sandwich_covariance.cov_white_simple(results)       heteroscedasticity robust covariance matrix (White) 

sandwich_covariance.cov_hc0(results) 
sandwich_covariance.cov_hc1(results) 
sandwich_covariance.cov_hc2(results) 
sandwich_covariance.cov_hc3(results) 
sandwich_covariance.se_cov(cov)             get standard deviation from covariance matrix 




#Moment Helpers
#The following three functions can be used to find a correlation or covariance 
#matrix that is positive definite and close to the original matrix.
corr_clipped(corr[, threshold])             Find a near correlation matrix that is positive semi-definite 
corr_nearest(corr[, threshold, n_fact])     Find the nearest correlation matrix that is positive semi-definite. 
corr_nearest_factor(corr, rank[, ctol, ...]) Find the nearest correlation matrix with factor structure to a given square matrix. 
corr_thresholded(data[, minabs, max_elt])   Construct a sparse matrix containing the thresholded row-wise correlation matrix from a data array. 
cov_nearest(cov[, method, threshold, ...])  Find the nearest covariance matrix that is postive (semi-) definite 
cov_nearest_factor_homog(cov, rank)         Approximate an arbitrary square matrix with a factor-structured matrix of the form k*I + XX'. 
FactoredPSDMatrix(diag, root)               Representation of a positive semidefinite matrix in factored form. 
#These are utility functions to convert between central 
#and non-central moments, skew, kurtosis and cummulants.
cum2mc(kappa)       convert non-central moments to cumulants 
mc2mnc(mc)          convert central to non-central moments, uses recursive formula 
mc2mvsk(args)       convert central moments to mean, variance, skew, kurtosis 
mnc2cum(mnc)        convert non-central moments to cumulants 
mnc2mc(mnc[, wmean]) convert non-central to central moments, uses recursive formula 
mnc2mvsk(args)      convert central moments to mean, variance, skew, kurtosis 
mvsk2mc(args)       convert mean, variance, skew, kurtosis to central moments 
mvsk2mnc(args)      convert mean, variance, skew, kurtosis to non-central moments 

cov2corr(cov[, return_std]) convert covariance matrix to correlation matrix 
corr2cov(corr, std) convert correlation matrix to covariance matrix given standard deviation 
se_cov(cov)         get standard deviation from covariance matrix 






###statsmodel - sm.stats - Mediation Analysis
#Rather than a direct causal relationship between the independent variable 
#and the dependent variable, 
#a mediation model proposes that the independent variable influences 
#the (non-observable) mediator variable, 
#which in turn influences the dependent variable.


#Mediation analysis focuses on the relationships among three key variables: 
#an 'outcome', a 'treatment', and a 'mediator'. 


class statsmodels.stats.mediation.Mediation(outcome_model, mediator_model, exposure, mediator=None, moderators=None, outcome_fit_kwargs=None, mediator_fit_kwargs=None)[source]
    Conduct a mediation analysis.
    Parameters:
    outcome_model : statsmodels model
        Regression model for the outcome. 
        Predictor variables include the treatment/exposure, the mediator, and any other variables of interest.
    mediator_model : statsmodels model
        Regression model for the mediator variable. 
        Predictor variables include the treatment/exposure and any other variables of interest.
    exposure : string or (int, int) tuple
        The name or column position of the treatment/exposure variable. 
        If positions are given, the first integer is the column position of the exposure variable in the outcome model 
        and the second integer is the position of the exposure variable in the mediator model. If a string is given, it must be the name of the exposure variable in both regression models.
    mediator : string or int
        The name or column position of the mediator variable in the outcome regression model. 
        If None, infer the name from the mediator model formula (if present).
    moderators : dict
        Map from variable names or index positions to values of moderator variables 
        that are held fixed when calculating mediation effects   
    Methods
    fit([method, n_rep]) 
        Fit a regression model to assess mediation. 
        Returns      MediationResults
        
MediationResults(indirect_effects, ...)         
    A class for holding the results of a mediation analysis. 
        summary(alpha=0.05)
            Provide a summary of a mediation analysis.


#Example 
#A basic mediation analysis using formulas:

import statsmodels.api as sm
import statsmodels.genmod.families.links as links
import pandas as pd 

data = pd.read_csv("framing.csv")

probit = links.probit
outcome_model = sm.GLM.from_formula("cong_mesg ~ emo + treat + age + educ + gender + income",
                                    data, family=sm.families.Binomial(link=probit))
mediator_model = sm.OLS.from_formula("emo ~ treat + age + educ + gender + income", data)
med = Mediation(outcome_model, mediator_model, "treat", "emo").fit()
med.summary()


#A moderated mediation analysis.  The mediation effect is computed
#for people of age 20.

fml = "cong_mesg ~ emo + treat*age + emo*age + educ + gender + income",
outcome_model = sm.GLM.from_formula(fml, data,
                                     family=sm.families.Binomial())
mediator_model = sm.OLS.from_formula("emo ~ treat*age + educ + gender + income", data)
moderators = {"age" : 20}
med = Mediation(outcome_model, mediator_model, "treat", "emo",
                moderators=moderators).fit()
med.summary()


###statsmodel - sm.stats - Non-Parametric Tests
mcnemar(x[, y, exact, correction])          McNemar test 
symmetry_bowker(table)                      Test for symmetry of a (k, k) square contingency table 
median_test_ksample(x, groups)              chisquare test for equality of median/location 
runstest_1samp(x[, cutoff, correction])     use runs test on binary discretized data above/below cutoff 
runstest_2samp(x[, y, groups, correction])  Wald-Wolfowitz runstest for two samples 
cochrans_q(x)                               Cochran's Q test for identical effect of k treatments 
Runs(x)                                     class for runs in a binary sequence 
sign_test(samp[, mu0])                      Signs test. 



##Non parametric 1 sample/2 sample run Test for randomness-  use with statsmodels.sandbox.stats.runs.
#Non-Parametric Tests - tests not assuming uniform distribution

runstest_1samp(x[, cutoff, correction])     use runs test on binary discretized data above/below cutoff
runstest_2samp(x[, y, groups, correction])  Wald-Wolfowitz runstest for two samples                                            


statsmodels.sandbox.stats.runs.runstest_1samp(x, cutoff='mean', correction=True)
    is x (binary discretized data above/below cutoff) random?
    H0:  sequence is random, p-value < 0.05, reject H0 
    cutoff : {'mean', 'median'} or number        
        This specifies the cutoff to split the data into two values 
    Returns (statistic, p-value). Note p-value is always last like numpy/scipy

#Example
import statsmodels.api as sm
x = np.array([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1])
>>> sms.runstest_1samp(x, cutoff=x2.mean(), correction=False)
(1.386750490563073, 0.16551785869746993) #>0.05, don't reject  H0

#add some noise and then check
x2 = x - 0.5 + np.random.uniform(-0.1, 0.1, size=len(x)) 

>>> sm.stats.runstest_1samp(x2, cutoff=0, correction=False)
(1.386750490563073, 0.16551785869746993)   #>0.05, don't reject  H0
>>> sms.runstest_1samp(x2, cutoff='mean', correction=False)
(1.386750490563073, 0.16551785869746993)   #>0.05, don't reject  H0
>>> sms.runstest_1samp(x2, cutoff=x2.mean(), correction=False)
(1.386750490563073, 0.16551785869746993) #>0.05, don't reject  H0


#2 sample- Wald-Wolfowitz runstest for two samples
statsmodels.sandbox.stats.runs.runstest_2samp(x, y=None, groups=None, correction=True)
    H0: two samples come from the same distribution(ie with same mean)
    groups 
        1-dimensional array of [0s,1s], with 0 means data from x, 
        1 means data from y 
    Returns (statistic, p-value). Note p-value is always last like numpy/scipy

#Example 
x = [31.8, 32.8, 39.2, 36, 30, 34.5, 37.4]
y = [35.5, 27.6, 21.3, 24.8, 36.7, 30]
y[-1] += 1e-6  #avoid tie that creates warning
#append zeros, ones 
groups = np.concatenate((np.zeros(len(x)), np.ones(len(y))))
>>> runs.runstest_2samp(x, y)   #H0: x,y came from same distribution
(0.022428065200812752, 0.98210649318649212)   #>0.05, don't reject  H0
>>> xy = np.concatenate((x, y))       #append x,y
>>> runs.runstest_1samp(xy)
(0.022428065200812752, 0.98210649318649212)
>>> runs.runstest_1samp(xy, xy.mean())
(0.022428065200812752, 0.98210649318649212)
>>> runs.runstest_2samp(xy, groups=groups)
(0.022428065200812752, 0.98210649318649212)



###statsmodel - sm.stats - Multiple Tests and Multiple Comparison Procedures
#multipletests is a function for p-value correction, 
#which also includes p-value correction based on fdr in fdrcorrection. 
#tukeyhsd performs simulatenous testing for the comparison of (independent) means. 

multipletests(pvals[, alpha, method, ...])          test results and p-value correction for multiple tests 
fdrcorrection0(pvals[, alpha, method, is_sorted])   pvalue correction for false discovery rate 
GroupsStats(x[, useranks, uni, intlab])             statistics by groups (another version) 
MultiComparison(data, groups[, group_order])        Tests for multiple comparisons 
TukeyHSDResults(mc_object, results_table, q_crit)   Results from Tukey HSD test, with additional plot methods 
 
pairwise_tukeyhsd(endog, groups[, alpha])           calculate all pairwise comparisons with TukeyHSD confidence intervals 
 
local_fdr(zscores[, null_proportion, ...])          Calculate local FDR values for a list of Z-scores. 
fdrcorrection_twostage(pvals[, alpha, ...])         (iterated) two stage linear step-up procedure with estimation of number of true 
NullDistribution(zscores[, null_lb, ...])           Estimate a Gaussian distribution for the null Z-scores. 


##Nonparametric comparing pairwise means   - use with statsmodels.stats.multicomp.
#TukeyHSD -  Tukey's studentized range test (HSD)
#tukeyhsd performs simulatenous testing for the comparison of (independent) means(ie from different distribution)
#Tukey's studentized range test - checks range, hence non parametric
#H0: all  pairs having same mean (ie from same distribution)

MultiComparison(data, groups[, group_order])        Tests for multiple comparisons
TukeyHSDResults(mc_object, results_table, q_crit)   Results from Tukey HSD test, with additional plot methods
pairwise_tukeyhsd(endog, groups[, alpha])           shortcut method of return MultiComparison(endog, groups).tukeyhsd(alpha=alpha)
                                                    calculate all pairwise comparisons with TukeyHSD confidence intervals
                                                    Returns TukeyHSDResults instance


#Methods of MultiComparison
allpairtest(testfunc[, alpha, method, pvalidx])     run a pairwise test on all pairs with multiple test correction 
                                                    returns tupe(SimpleTable_instance, pure_result),
                                                    SimpleTable_instance has as_csv(),as_html(), as_text() methods 
getranks()                                          convert data to rankdata and attach 
kruskal([pairs, multimethod])                       *Unfinished* pairwise comparison for kruskal-wallis test 
tukeyhsd([alpha])                                   Tukey's range test to compare means of all pairs of groups 
                                                    returns TukeyHSDResults instance

#Methods of TukeyHSDResults
plot_simultaneous([comparison_name, ax, ...])   Plot a universal confidence interval of each group mean 
summary()                                       Summary table that can be printed 


#Example - we have three different treatments,
#Test whether the differences in means for the three pairs of treatments are different.
#(means - do three threatments have different effectiveness?)
#H0: Each pairs have same mean 

#code
import numpy as np
from scipy import stats
from statsmodels.stats.multicomp import pairwise_tukeyhsd, MultiComparison

#Create a record array
dta2 = np.rec.array([
(  1,   'mental',  2 ),
(  2,   'mental',  2 ),
(  3,   'mental',  3 ),
(  4,   'mental',  4 ),
(  5,   'mental',  4 ),
(  6,   'mental',  5 ),
(  7,   'mental',  3 ),
(  8,   'mental',  4 ),
(  9,   'mental',  4 ),
( 10,   'mental',  4 ),
( 11, 'physical',  4 ),
( 12, 'physical',  4 ),
( 13, 'physical',  3 ),
( 14, 'physical',  5 ),
( 15, 'physical',  4 ),
( 16, 'physical',  1 ),
( 17, 'physical',  1 ),
( 18, 'physical',  2 ),
( 19, 'physical',  3 ),
( 20, 'physical',  3 ),
( 21,  'medical',  1 ),
( 22,  'medical',  2 ),
( 23,  'medical',  2 ),
( 24,  'medical',  2 ),
( 25,  'medical',  3 ),
( 26,  'medical',  2 ),
( 27,  'medical',  3 ),
( 28,  'medical',  1 ),
( 29,  'medical',  3 ),
( 30,  'medical',  1 ) ],
dtype=[('idx', '<i4'),('Treatment', '|S8'),('StressReduction', '<i4')])


#tukeyHSD

res2 = pairwise_tukeyhsd(dta2['StressReduction'], dta2['Treatment'])
>>> print(res2) ##H0: both are same
Multiple Comparison of Means - Tukey HSD,FWER=0.05
=============================================== 
group1  group2  meandiff  lower  upper  reject
-----------------------------------------------
medical  mental    1.5     0.3217 2.6783  True   #reject H0, soe medical and mental effectivness are different 
medical physical   1.0    -0.1783 2.1783 False   #don't reject H0 
mental physical   -0.5   -1.6783 0.6783 False    #don't reject H0
-----------------------------------------------
#plot
import matplotlib.pyplot as plt
res2.plot_simultaneous()
plt.plot()
#shows medical and mental , no overlapping, hence their means are different



#OR using MultiComparison
mod = MultiComparison(dta2['StressReduction'], dta2['Treatment'])
>>> print(mod.tukeyhsd())
Multiple Comparison of Means - Tukey HSD,FWER=0.05
=============================================== 
group1  group2  meandiff  lower  upper  reject
-----------------------------------------------
medical  mental    1.5     0.3217 2.6783  True
medical physical   1.0    -0.1783 2.1783 False 
mental physical   -0.5   -1.6783 0.6783 False
-----------------------------------------------

>>> mod.groupsunique
rec.array(['medical', 'mental', 'physical'],      
dtype='|S8')




##Parameteric comparing pairwise means   - Pairwise T-tests 
#(parameteric-hence normal dist) - alternate to TukeyHSD -  Tukey's studentized range test (HSD)
#part of MultiComparison class 
#run t-tests on all pairs, calculate the p-values
#Both statsmodels has  several options to adjust the p-values.
#Use  Holm and Bonferroni adjustments.
#H0: all pairs same

from scipy import stats
mod = MultiComparison(dta2['StressReduction'], dta2['Treatment'])
rtp = mod.allpairtest(stats.ttest_rel, method='Holm') #from stats.ttest_rel, paired t-test
>>> print(rtp[0])
Test Multiple Comparison ttest_rel
FWER=0.05 method=Holm
alphacSidak=0.02, alphacBonf=0.017
================================================ 
group1  group2    stat   pval  pval_corr reject
------------------------------------------------
medical  mental  -4.0249 0.003    0.009    True    #medical and mental 's mean are significantly different 
medical physical -1.9365 0.0848   0.1696  False 
mental physical  0.8321 0.4269   0.4269  False
------------------------------------------------

#Using Independent two Samples test (T-test)

>>> print(mod.allpairtest(stats.ttest_ind, method='b')[0])
Test Multiple Comparison ttest_ind
FWER=0.05 method=b
alphacSidak=0.02, alphacBonf=0.017
================================================ 
group1  group2    stat   pval  pval_corr reject
------------------------------------------------
medical  mental   -3.737 0.0015   0.0045   True
medical physical -2.0226 0.0582   0.1747  False 
mental physical  0.9583 0.3506    1.0    False
------------------------------------------------



###statsmodel - sm.stats -  Comparing means(parametric) - use with statsmodels.stats.weightstats.
ttest_ind(x1, x2[, alternative, usevar, ...])       ttest independent sample 
ttost_ind(x1, x2, low, upp[, usevar, ...])          test of (non-)equivalence for two independent samples 
ttost_paired(x1, x2, low, upp[, transform, ...])    test of (non-)equivalence for two dependent, paired sample 
ztest(x1[, x2, value, alternative, usevar, ddof])   test for mean based on normal distribution, one or two samples 
ztost(x1, low, upp[, x2, usevar, ddof])             Equivalence test based on normal distribution 
zconfint(x1[, x2, value, alpha, ...])               confidence interval based on normal distribution z-test 


##T-Test vs Z-Test vs F-test(ANOVA) - all parameteric ie assumes normal distribution, hence mean and sd are valid
Test      sample size       sigma
Z-Test      >30             known
T-test      <30             unknown
F-Test      Comparing three or more means under normal distribution
            for 1 way, use scipy.stats.f_one_away or statsmodels.stats.anova.anova_lm
            for 2 way, use statsmodel. stats.anova.anova_lm
            
##One sample vs 2 sample
One sample -  one sample comparing with given mean , H0:mean=given mu
two sample -  One sample comparing with another sample to check thier mean difference , H0:mean1=mean2
paired two sample - Like two sample, but each observation point of both is measured on same subject ,H0:mean1=mean2

       
        
##TOST: two one-sided t tests
    H0: the difference between the two samples is larger than the thresholds 
    given by low and upp.
    H0: m1 - m2 < low or m1 - m2 > upp 
    alternative hypothesis: low < m1 - m2 < upp
    where m1, m2 are the means, expected values of the two samples.

statsmodels.stats.weightstats.ttost_ind(x1, x2, low, upp, usevar='pooled', weights=(None, None), transform=None)[source]
    for two independent samples
statsmodels.stats.weightstats.ttost_paired(x1, x2, low, upp, transform=None, weights=None)[source]
    for two dependent, paired sample
    Parameters:
    x1, x2 : array_like, 1-D or 2-D
        two independent samples, 
        If d1 and d2 have the same number of columns, 
        then each column of the data in d1 is compared with the corresponding column in d2.
    low, upp : float
        equivalence interval low < m1 - m2 < upp
    usevar : string, 'pooled' or 'unequal'
        If pooled, then the standard deviation of the samples is assumed to be the same. 
        If unequal, then Welsh ttest with Satterthwait degrees of freedom is used
    weights : tuple of None or ndarrays
        Case weights for the two samples. 
    transform : None or function
        If None (default), then the data is not transformed. 
        Given a function, sample data and thresholds are transformed. 
        If transform is log, then the equivalence interval is in ratio: low < m1 / m2 < upp
    Returns:
    pvalue : float
        pvalue of the non-equivalence test
    t1, pv1, df1(only for ttost_paired) : tuple
        test statistic, pvalue and degrees of freedom for lower threshold test
    t2, pv2, df2(only for ttost_paired) : tuple
        test statistic, pvalue and degrees of freedom for upper threshold test
 
statsmodels.stats.weightstats.ztost(x1, low, upp, x2=None, usevar='pooled', ddof=1.0)[source]
    Equivalence test based on normal distribution
    1 sample case or 2 sample case
    Parameters:
    x1 : array_like
        one sample or first sample for 2 independent samples
    low, upp : float
        equivalence interval low < m1 - m2 < upp
    x1 : array_like or None
        second sample for 2 independent samples test. 
        If None, then a one-sample test is performed.
        
##ztest
Use statsmodels.stats.weightstats.ztest to compare two means,
    assuming they are independent and have the same standard deviation.
use CompareMeans.ztest_ind to compare means from distributions
    with different standard deviation

    
statsmodels.stats.weightstats.ztest(x1, x2=None, value=0, alternative='two-sided', usevar='pooled', ddof=1.0)
    test for mean based on normal distribution, one or two samples
    Parameters:
    x1, x2 : array_like, 1-D or 2-D
        two independent samples, 
        If d1 and d2 have the same number of columns, 
        then each column of the data in d1 is compared with the corresponding column in d2.
    value : float
        In the one sample case, value is the mean of x1 under the Null hypothesis. 
        In the two sample case, value is the difference between mean of x1 and mean of x2 under the Null hypothesis. 
        The test statistic is x1_mean - x2_mean - value.
    usevar : usevar not implemented, is always pooled in two sample case use CompareMeans instead
        string, 'pooled' or 'unequal'
        If pooled, then the standard deviation of the samples is assumed to be the same. 
        If unequal, then Welsh ttest with Satterthwait degrees of freedom is used
    Returns 
    zstat : float
        test statisic
    pvalue : float
        pvalue 
 

statsmodels.stats.weightstats.zconfint(x1, x2=None, value=0, alpha=0.05, alternative='two-sided', usevar='pooled', ddof=1.0)[source]
    confidence interval of mean  based on normal distribution z-test
    Parameters:
    x1, x2 : array_like, 1-D or 2-D
        two independent samples
    value : float
        In the one sample case, value is the mean of x1 under the Null hypothesis. 
        In the two sample case, value is the difference between mean of x1 and mean of x2 under the Null hypothesis. 
        The test statistic is x1_mean - x2_mean - value.
 
##T-Test
One sample  - use scipy.stats.ttest_1samp
2 sample    - use statsmodels.stats.weightstats.ttest_ind or scipy.stats.ttest_ind
paired      - use scipy.stats.ttest_rel

statsmodels.stats.weightstats.ttest_ind(x1, x2, alternative='two-sided ',   
            usevar='pooled', weights=(None, None), value=0)
    H0 mean1 = mean2 or mean1-mean2=0 or value (true mean) #p <0.05, rejectH0
    alternative : string
        'two-sided': H1: difference in means not equal to value (default) H0:mean1=mean2
        'larger' : H1: difference in means larger than value    , H0: mean1<mean2
        'smaller' : H1: difference in means smaller than value   , H0: mean1>mean2
    Returns:
    tstat : float
        test statisic
    pvalue : float
        pvalue of the t-test
    df : int or float
        degrees of freedom used in the t-test
 
#Example
import pandas
data = pandas.read_csv('data/brain_size.csv', sep=';', na_values=".")

>>> print data.head()    
Unnamed: 0  Gender  FSIQ  VIQ  PIQ  Weight  Height  MRI_Count
0            1  Female   133  132  124     118    64.5     816932
1            2    Male   140  150  124     NaN    72.5    1001121
2            3    Male   139  123  150     143    73.3    1038437
3            4    Male   133  129  128     172    68.8     965353
4            5  Female   137  132  134     147    65.0     951545

gender_data = data.groupby('Gender')
>>> print gender_data.mean()        
Unnamed: 0   FSIQ     VIQ     PIQ      Weight     Height  MRI_Count
Gender
Female       19.65  111.9  109.45  110.45  137.200000  65.765000   862654.6
Male         21.35  115.0  115.25  111.60  166.444444  71.431579   954855.4


from pandas.tools import plotting
import matplotlib.pyplot as plt
plotting.scatter_matrix(data[ ['Weight', 'Height', 'MRI_Count']    ])
plt.show()

#We have seen above that the mean VIQ in the male and female populations were different.
#To test if this is significant
female_viq = data[data['Gender'] == 'Female']['VIQ']
male_viq = data[data['Gender'] == 'Male']['VIQ']
import  statsmodels.stats.weightstats
>>> statsmodels.stats.weightstats.ttest_ind(female_viq, male_viq)
(-0.77261617232750124, 0.44452876778583217, 38.0)  #middle value is p-value, >0.05, don't reject H0, H0: same mean

#Example of Paired tests
#Paired T-Test
from scipy import stats 
>>> stats.ttest_rel(data['FSIQ'], data['PIQ'])
Ttest_relResult(statistic=1.7842019405859857, pvalue=0.082172638183642358) #>0.05, don't reject H0, H0: same mean

#T-tests assume Gaussian errors. Use a Wilcoxon signed-rank test, that relaxes this assumption:
from scipy import stats
>>> stats.wilcoxon(data['FSIQ'], data['PIQ'])
WilcoxonResult(statistic=274.5, pvalue=0.10659492713506856)  #>0.05, don't reject H0, H0: same mean

#Example - 1-sample t-test H0: observations are drawn from a Gaussian distributions of given population mean. It returns the T statistic, and the p-value (see the function's help):
>> stats.ttest_1samp(data['VIQ'], 0)
Ttest_1sampResult(statistic=30.088099970849328, pvalue=1.3289196468728067e-28) #<0.05, reject H0, H0:Gaussian dist








###statsmodel - sm.stats - Basic Statistics and compare Means(parametric) -class based  - use with statsmodels.stats.weightstats.
DescrStatsW(data[, weights, ddof])                  descriptive statistics and tests with weights for case weights 
CompareMeans(d1, d2)                                class for two sample comparison 


class statsmodels.stats.weightstats.DescrStatsW(data, weights=None, ddof=0)
    Assumes that the data is 1d or 2d with (nobs, nvars) 
    observations in rows, variables in columns, 
    and that the same weight applies to each column.
    If degrees of freedom correction is used, 
    then weights should add up to the number of observations. 
    ttest also assumes that the sum of weights corresponds to the sample size.
    This is essentially the same as replicating each observations by its weight, 
    if the weights are integers, often called case or frequency weights.
    Parameters:
    data : array_like, 1-D or 2-D
        dataset
    weights : None or 1-D ndarray
        weights for each observation, with same length as zero axis of data
    ddof : int
        default ddof=0, degrees of freedom correction used for second moments, var, std, cov, corrcoef. 
        However, statistical tests are independent of ddof, based on the standard formulas.


#DescrStatsW Methods
asrepeats()         get array that has repeats given by floor(weights)
corrcoef()          weighted correlation with default ddof
cov()               weighted covariance of data if data is 2 dimensional
demeaned()          data with weighted mean subtracted
mean()              weighted mean of data
nobs()              alias for number of observations/cases, equal to sum of weights
std()               standard deviation with default degrees of freedom correction
std_ddof([ddof])    standard deviation of data with given ddof
std_mean()          standard deviation of weighted mean
sum()               weighted sum of data
sum_weights  ()
sumsquares()                        weighted sum of squares of demeaned data
tconfint_mean([alpha, alternative]) two-sided confidence interval for weighted mean of data
ttest_mean([value, alternative])    ttest of Null hypothesis that mean is equal to value.
ttost_mean(low, upp)                test of (non-)equivalence of one sample
var()                               variance with default degrees of freedom correction
var_ddof([ddof])                    variance of data given ddof
zconfint_mean([alpha, alternative]) two-sided confidence interval for weighted mean of data
ztest_mean([value, alternative])    z-test of Null hypothesis that mean is equal to value.
ztost_mean(low, upp)
get_compare(other[, weights])       return an instance of CompareMeans with self and other , instance DescrStatsW

#CompareMeans
statsmodels.stats.weightstats.CompareMeans(d1, d2) 
    class for two sample comparison
    d1, d2 : instances of DescrStatsW

#statsmodels.stats.weightstats.CompareMeans - Methods
dof_satt()                                      degrees of freedom of Satterthwaite for unequal variance
std_meandiff_pooledvar()                        variance assuming equal variance in both data sets
std_meandiff_separatevar  ()
tconfint_diff([alpha, alternative, usevar])     confidence interval for the difference in means
ttest_ind([alternative, usevar, value])         ttest for the null hypothesis of identical means
ttost_ind(low, upp[, usevar])                   test of equivalence for two independent samples, base on t-test
zconfint_diff([alpha, alternative, usevar])     confidence interval for the difference in means
ztest_ind([alternative, usevar, value])         z-test for the null hypothesis of identical means
ztost_ind(low, upp[, usevar])                   test of equivalence for two independent samples, based on z-test
summary([use_t, alpha, usevar, value])          summarize the results of the hypothesis test 
from_data(data1, data2[, weights1, ...])        construct a CompareMeans object from data 

#Example of descriptive statistics - statsmodels.stats.weightstats.DescrStatsW

x1_2d = 1.0 + np.random.randn(20, 3)  #20x3 std normal random number , mean=0, sd=1
w1 = np.random.randint(1,4, 20)       #20 uniform random number between 1, 4
d1 = statsmodels.stats.weightstats.DescrStatsW(x1_2d, weights=w1)
>>> d1.mean
array([ 1.42739844,  1.23174284,  1.083753  ])
>>> d1.var
array([ 0.94855633,  0.52074626,  1.12309325])
>>> d1.std_mean
array([ 0.14682676,  0.10878944,  0.15976497])

>>> tstat, pval, df = d1.ttest_mean(0) #mean =0
>>> tstat; pval; df
array([  9.72165021,  11.32226471,   6.78342055])
array([  1.58414212e-12,   1.26536887e-14,   2.37623126e-08]) #<0.05, reject H0: H0:mean=0
44.0
>>> d1.ttest_mean(1)   #>0.5, don't reject H0, H0:mean = 1
(array([-1.31663973,  0.23744113,  2.36860934]), array([ 0.19761089,  0.81387685,  0.02426928]), 31.
0)


>>> tstat, pval, df = d1.ttest_mean([0, 1, 1])
>>> tstat; pval; df
array([ 9.72165021,  2.13019609,  0.52422632])
array([  1.58414212e-12,   3.87842808e-02,   6.02752170e-01])
44.0









###statsmodel - sm.stats - Power and Sample Size Calculations - statsmodels.stats.power.
#Implements power and sample size calculations for the t-tests,
#normal based test, F-tests and Chisquare goodness of fit test.

#power of test calculation - More the better 
The power or sensitivity of hypothesis test is the probability
that the test correctly rejects the null hypothesis (H0) (ie  (H1) is true.)

A type I error, also known as an error of the first kind, occurs
when the null hypothesis (H0) is true, but is rejected(alpha)
The probability of type I error is called the significance level of the hypothesis testing

A type II error, also known as an error of the second kind, occurs
when the null hypothesis is false, but  fails to be rejected(beta)
(the power =  1-beta)

#Methods for Statistical Power calculations - statsmodels.stats.power.
TTestIndPower(**kwds)       for t-test for two independent sample
                            currently only uses pooled variance
TTestPower(**kwds)          for one sample or paired sample t-test
GofChisquarePower(**kwds)   for one sample chisquare test
NormalIndPower([ddof])      for z-test for two independent samples.
FTestAnovaPower(**kwds)     F-test for one factor balanced ANOVA
FTestPower(**kwds)          for generic F-test

#All above have below methods
plot_power([dep_var, nobs, effect_size, ...])    
    plot power with number of observations or effect size on x-axis
power(effect_size, nobs1, alpha, ratio=1)        
    Calculate the power of a t-test for two independent sample
    ratio = nobs2/nobs1, =1 means equal sample size     
solve_power(effect_size=None, nobs=None,alpha=None, power=None, alternative='two-sided') 
    solve for any one parameter of the power
    Exactly one needs to be None, all others need numeric values.
    
#Quick methods 
statsmodels.stats.power.tt_solve_power = <bound method TTestPower.solve_power of <statsmodels.stats.power.TTestPower object>>
    solve for any one parameter of the power of a one sample t-test
    This test can also be used for a paired t-test, 
    where effect size is defined in terms of the mean difference, 
    and nobs is the number of pairs
statsmodels.stats.power.tt_ind_solve_power = <bound method TTestIndPower.solve_power of <statsmodels.stats.power.TTestIndPower object>>
    solve for any one parameter of the power of a two sample t-test
statsmodels.stats.power.zt_ind_solve_power = <bound method NormalIndPower.solve_power of <statsmodels.stats.power.NormalIndPower object>>
    solve for any one parameter of the power of a two sample z-test
    
    
#The one-sided test can be either 'larger', 'smaller'.
#larger = upper-tailed test = H1: actual value > given value (eg mu0)
#smaller = lower-tailed test = H1: actual value < given value (eg mu0)
#two-sided = H1: actual value != given value(mu0)
#a one-tailed test with alpha set at .05 has approximately the same power as a two-tailed test with alpha set at .10


# Power Analysis - Confidence of correct result

#The following four quantities are related
#Given any three, we can determine the fourth.
1. sample size, nobs 
2. effect size, Effect size emphasises the size of the difference between two groups
3. significance level = P(Type I error) = alpha , low the better 
4. power = 1 - P(Type II error) = 1 - beta = confidence of correct result, high the better

#Effect size for T-tests and Z-tests(d)
Cohen suggests that d values of 0.2, 0.5, and 0.8 represent small, medium, and large effect sizes respectively.
 
#Effect size for one way Anova and F-Test(f)
Cohen suggests that f values of 0.1, 0.25, and 0.4 represent small, medium, and large effect sizes respectively.

#Effect size for Chi-square Tests(w)
Cohen suggests that w values of 0.1, 0.3, and 0.5 represent small, medium, and large effect sizes respectively.

#Example
import statsmodels.stats.power as smp
>>> smp.TTestPower().power(effect_size=0.2, nobs=60, alpha=0.1, alternative='larger')
0.601340316701

>>> smp.TTestPower().power(effect_size=0.2, nobs=160, alpha=0.1, alternative='larger')
0.89282992525257199

#Calculate Power
>>> smp.TTestPower().solve_power(effect_size =0.55, nobs=40, alpha=0.05, alternative='two-sided')
0.93152481603307669
>>> smp.TTestPower().power(effect_size =0.55, nobs=40, alpha=0.05, alternative='two-sided')
0.93152481603307669


#Calculating the sample size to achieve a given power - 2 sample T-test
>>> smp.TTestIndPower().solve_power(effect_size=0.3, power=0.75, ratio=1, alpha=0.05, alternative='larger')
array([ 120.22320279])

#For unequal sample size in 2 sample T-Test  - use ratio
#for  example with twice as many observations in group 2 as in group 1
>>> smp.TTestIndPower().solve_power(0.3, power=0.75, ratio=2, alpha=0.05, alternative='larger')
array([ 90.11015096])
# 90 observations in sample 1, and about 180 observations in sample 2


# Proportion for one sample (Normal Approximation)
from statsmodels.stats._proportion import proportion_effectsize
#proportion_effectsize(prop1, prop2[, method]) 
#effect size for a test comparing two proportions 
>>> proportion_effectsize(0.4, 0.6)
0.20135792079033088
>>> smp.NormalIndPower().solve_power(0.2013579207903309, nobs1=60, alpha=0.05, ratio=0)
0.34470140912721525

# F-test for ANOVA
>>> smp.FTestAnovaPower().power(0.28, nobs=80, alpha=0.05, k_groups=4) #in each group 20 samples
0.51498349798649223

#Solving for the sample size (for each group in pwr, for total in statsmodels
>>> smp.FTestAnovaPower().solve_power(0.28, alpha=0.05, power=0.8, k_groups=4)
array([ 143.03088865])
>>> _ / 4
array([ 35.75772216])

#F-Test for linear Model
>>> smp.FTestPower().solve_power(effect_size=np.sqrt(0.1/(1-0.1)), df_num=89, df_denom=5, alpha=0.05)
0.67358904832076627

#Chi-square goodness-of-fit

>>> p0 = np.ones(4.) / 4
>>> p1 = np.concatenate(([0.375], np.ones(3.) * (1- 0.375) / 3))
>>> import statsmodels.stats.api as sms
>>> sms.chisquare_effectsize(p0, p1)  #get effectsize as first 
>>> es = sms.chisquare_effectsize(p0, p1)
>>> es
0.28867513459481292
>>> smp.GofChisquarePower().solve_power(es, nobs=100, n_bins=4, alpha=0.05)
0.67398350421626085

#If there are no parameters in the distribution that are estimated,
#then df = nobs -  1.
#If there are parameters that are (appropriately) estimated, then df = nobs - 1 - ddof,
#where ddof is the number of estimated parameters.


#Chisquare test for contingency tables
>>> smp.GofChisquarePower().solve_power(0.346, nobs=140, n_bins=(2-1)*(3-1) + 1, alpha=0.01)
0.88540533954389766


#Finding the sample size to obtain a given power

>>> smp.GofChisquarePower().solve_power(0.1, n_bins=(5-1)*(6-1) + 1, alpha=0.05, power=0.8)
array([ 2096.07846526])

#Correlation
#with Fisher z-transform approximation
>>> smp.NormalIndPower().power(np.arctanh(0.3), nobs1=50-3, alpha=0.05, ratio=0, alternative='two-sided')
0.56436763896354003
>>> smp.NormalIndPower().power(np.arctanh(0.3), nobs1=50-3, alpha=0.05, ratio=0, alternative='larger')
0.68335663306958949

#sample size calculations:
>>> [smp.NormalIndPower().solve_power(np.arctanh(r), alpha=0.05, ratio=0,            
            power= 0.8, alternative='two-sided') + 3 for r in [0.3, 0.5, 0.1]]
[array([ 84.92761044]), array([ 29.01223669]), array([ 782.64821794])]










###statsmodel - sm.stats - Proportion test - for discrete probs -  use statsmodels.stats.proportion.
import statsmodels.stats.proportion as smprop

proportion_confint(count, nobs[, alpha, method])    confidence interval for a binomial proportion
proportion_effectsize(prop1, prop2[, method])       effect size for a test comparing two proportions

binom_test(count, nobs[, prop, alternative])        Perform a test that the probability of success is  p.
binom_test_reject_interval(value, nobs[, ...])      rejection region for binomial test for one sample proportion
binom_tost(count, nobs, low, upp)                   exact TOST test for one proportion using binomial distribution
binom_tost_reject_interval(low, upp, nobs[, ...])   rejection region for binomial TOST

proportions_ztest(count, nobs[, value, ...])        test for proportions based on normal (z) test
proportions_ztost(count, nobs, low, upp[, ...])     Equivalence test based on normal distribution
proportions_chisquare(count, nobs[, value])         test for proportions based on chisquare test
proportions_chisquare_allpairs(count, nobs)         chisquare test of proportions for all pairs of k samples
proportions_chisquare_pairscontrol(count, nobs)     chisquare test of proportions for pairs of k samples compared to control
proportion_effectsize(prop1, prop2[, method])       effect size for a test comparing two proportions

power_binom_tost(low, upp, nobs[, p_alt, alpha  ])
power_ztost_prop(low, upp, nobs, p_alt[, ...])      Power of proportions equivalence test based on normal distribution
samplesize_confint_proportion(proportion, ...)      find sample size to get desired confidence interval length


#method to use for confidence interval, currently available methods :
•normal : asymptotic normal approximation
•agresti_coull : Agresti-Coull interval
•beta : Clopper-Pearson interval based on Beta distribution
•wilson : Wilson Score interval
•jeffrey : Jeffrey's Bayesian Interval
•binom_test : experimental, inversion of binom_test 


## Compare two for same sucess rate 
statsmodels.stats.proportion.proportions_ztest(count, nobs, value=None, 
            alternative='two-sided', prop_var=False)
    count : integer or array_like
        the number of successes in nobs trials. 
        If this is array_like, then the assumption is that 
        this represents the number of successes for each independent sample
    nobs : integer or array-like
        the number of trials or observations, with the same length as count.
    value : float, array_like or None, optional
        This is the value of the null hypothesis equal to the proportion 
        in the case of a one sample test. 
        In the case of a two-sample test, the null hypothesis is that 
        prop[0] - prop[1] = value, 
        where prop is the proportion in the two samples. 
        If not provided value = 0 and the null is prop[0] = prop[1]
statsmodels.stats.proportion.proportions_chisquare(count, nobs, value=None)
    test for proportions based on chisquare test
    If value is given, then all proportions are jointly tested against this value. 
    If value is not given and count and nobs are not scalar, 
    then the null hypothesis is that all samples have the same proportion.
    Parameters:
    count : integer or array_like
        the number of successes in nobs trials. 
        If this is array_like, then the assumption is that this represents the number of successes for each independent sample
    nobs : integer
        the number of trials or observations, with the same length as count.
    value : None or float or array_like     
    Returns:
    chi2stat : float
        test statistic for the chisquare test
    p-value : float
        p-value for the chisquare test
    (table, expected)
    table is a (k, 2) contingency table, 
    expected is the corresponding table of counts that are expected 
    under independence with given margins        
        
        
        
#Example - 5 is success out of 83, 12 is success out of 99 - are they same ?
import statsmodels.stats.proportion as smprop
count = np.array([5, 12])
nobs = np.array([83, 99  ])
>>> smprop.proportions_ztest(count, nobs, value=0)
(-1.4078304151258787, 0.15918129181156992)  #2nd value = pvalue, don't reject H0 , H0:same        
>>> smprop.proportions_chisquare(count, nobs)
(1.9819864777535043, 0.15918129181156587, (array([[ 5, 78],
       [12, 87]]), array([[ 7.75274725, 75.24725275],
       [ 9.24725275, 89.75274725]])))
       
       
##test of proportions for all pairs of k samples
statsmodels.stats.proportion.proportions_chisquare_allpairs(count, nobs, multitest_method='hs')
    chisquare test of proportions for all pairs of k samples
    Performs a chisquare test for proportions for all pairwise comparisons. 
    H0: pairs have same proportion 
    The alternative is two-sided
    Parameters:
    count : integer or array_like
        the number of successes in nobs trials.
    nobs : integer
        the number of trials or observations.
    Returns:
    result : AllPairsResults instance
        Methods 
            pval_corrected([method]) p-values corrected for multiple testing problem 
            pval_table() create a (n_levels, n_levels) array with corrected p_values 
            summary() returns text summarizing the results 
 
 
#Example 
n_success = np.array([ 73,  90, 114,  75])  #index 0,1,2,3
nobs = np.array([ 86,  93, 136,  82])
ppt = smprop.proportions_chisquare_allpairs(n_success, nobs)
>>> print(ppt.summary())  #H0: pair has same proportion 
Corrected p-values using Holm-Sidak p-value correction

Pairs  p-values
(0, 1)  0.02641  #reject H0
(0, 2)  0.8328   #accept H0
(0, 3)  0.3658   #accept H0
(1, 2)  0.0121   #reject H0
(1, 3)  0.3658   #accept H0
(2, 3)  0.3658   #accept H0

## To find sample size to get desired confidence interval length
statsmodels.stats.proportion.proportion_confint(count, nobs, alpha=0.05, method='normal')[source]
    confidence interval for a binomial proportion
    Parameters:
    count : int or array
        number of successes
    nobs : int
        total number of trials 
        
statsmodels.stats.proportion.samplesize_confint_proportion(proportion, half_length, alpha=0.05, method='normal')[source]
    find sample size to get desired confidence interval length
    Parameters:
    proportion : float in (0, 1)
        proportion or quantile
    half_length : float in (0, 1)
        desired half length of the confidence interval
        
#method to use for confidence interval, currently available methods :
•normal : asymptotic normal approximation
•agresti_coull : Agresti-Coull interval
•beta : Clopper-Pearson interval based on Beta distribution
•wilson : Wilson Score interval
•jeffrey : Jeffrey's Bayesian Interval
•binom_test : experimental, inversion of binom_test 

#Example - What is the CI  of success rate 12 for 20 observations (ie p =12/20=0.6)
nobs = 20
ci = smprop.proportion_confint(12, nobs, alpha=0.05, method='normal')
samplesize = smprop.samplesize_confint_proportion(12./nobs, (ci[1] - ci[0]) / 2)
>>> ci
(0.38529670275394107, 0.81470329724605883)
>>> samplesize #== nobs 
20.0  


statsmodels.stats.proportion.multinomial_proportions_confint(counts, alpha=0.05, method='goodman')
    Confidence intervals for multinomial proportions.
    A categorical response variable can take on k different values. 
    If you have a random sample from a multinomial response, 
    the sample proportions estimate the proportion of each category in the population
Parameters:
counts : array_like of int, 1-D
    Number of observations in each category.
alpha : float in (0, 1), optional
    Significance level, defaults to 0.05.
method : {'goodman', 'sison-glaz'}, optional
    Method to use to compute the confidence intervals; available methods are:
        •goodman: based on a chi-squared approximation, valid if all values in counts are greater or equal to 5 [R53]
        •sison-glaz: less conservative than goodman, but only valid if counts has 7 or more categories (len(counts) >= 7) [R54]
Returns:
confint : ndarray, 2-D
    Array of [lower, upper] confidence levels for each category, 
    such that overall coverage is (approximately) 1-alpha.
 
#Example
data Psych is k sample where each category 
Neurotic              91
Depressed             49
Schizophrenic         37
Personality disorder  43
Other                 10

import statsmodels.stats.proportion as smprop  
proportions = [91,49,37,43,10]    
smprop.multinomial_proportions_confint(proportions, 0.05)
>>> smprop.multinomial_proportions_confint(proportions, 0.05)
array([[0.31664566, 0.48051021],
       [0.15205626, 0.29012238],
       [0.10812595, 0.23263062],
       [0.12986268, 0.26160492],
       [0.01981054, 0.09274638]])
       
       
##Perform a test that the probability of success is   p.
binom_test(count, nobs, prop=0.5, alternative='two-sided')
    exact TOST test for one proportion using binomial distribution
    Parameters:
    count : integer or array_like
        the number of successes in nobs trials.
    nobs : integer
        the number of trials or observations.
    low, upp : floats
        lower and upper limit of equivalence region
    Returns:
    pvalue : float
        p-value of equivalence test
    pval_low, pval_upp : floats
        p-values of lower and upper one-sided tests
#Example 
smprop.binom_test(51, 235, prop=1. / 6)
0.043747970182413345  #<0.05 reject H0, H0: rate = 1/6
# R binom_test returns Copper-Pearson confint
smprop.proportion_confint(51, 235, alpha=0.05, method='beta')
(0.16606332980830396, 0.27526836402892607) #barely includes 1/6=.16666


##compare two prop for effectsize and use it to get power of test 
statsmodels.stats.proportion.proportion_effectsize(prop1, prop2, method='normal')[source]
    effect size for a test comparing two proportions
    To use in power calculation
    
import statsmodels.api as sm
>>> sm.stats.proportion_effectsize(0.5, 0.4)
0.20135792079033088
>>> sm.stats.proportion_effectsize([0.3, 0.4, 0.5], 0.4)
array([-0.21015893,  0.        ,  0.20135792])
import statsmodels.stats.power as smp
#NormalIndPower([ddof])      for z-test for two independent samples.
#ratio=nobs for 1st samples/nobs for 2nd samples 
>>> smp.NormalIndPower().solve_power(0.2013579207903309, nobs1=60, alpha=0.05, ratio=1)








###statsmodel -  Formula syntax  - uses patsy 
#http://statsmodels.sourceforge.net/devel/example_formulas.html

#Formula example (from patsy)
y ~ a + a:b + np.log(x)  #  plus an invisible intercept term
   
#Syntax    
    All operations are left-associative    
    (so a - b - c means the same as (a - b) - c, not a - (b - c)).

~   Separates the left-hand side and right-hand side of a formula. Optional.    
    If not present, then the formula is considered to contain a right-hand side only.    

+   Takes the set of terms given on the left of operator and the set of terms given on the right,    
    and returns a set of terms that combines both (i.e., it computes a set union ).    
    Note that this means that a + a is just a.    

-   Takes the set of terms given on the left and removes any terms    
    which are given on the right (i.e., it computes a set difference).

*   a * b is short-hand for a + b + a: b,    
    Standard ANOVA models are of the form a * b * c * ...    

/   a / b is shorthand for a + a:b    
    a / (b + c) is equivalent to a + a:b + a:c    
    (a + b)/c is equivalent to a + b + a:b:c (not same as a/c + b/c : / is not leftward distributive over +)

:   This takes two sets of terms,    
    and computes the interaction between each term on the left 
    and each term on the right    
    (a + b):(c + d) is the same as a:c + a:d + b:c + b:d    
    : takes the union of factors within two terms,    
    while + takes the union of two sets of terms.    
    a:a is just a, and (a:b):(a:c) is the same as a:b:c.     

**  This takes a set of terms on the left, and an integer n on the right,    
    and computes the * of that set of terms with itself n times.    
    (a + b + c + d) ** 3    
    is expanded to:    
    (a + b + c + d) * (a + b + c + d) * (a + b + c + d)    
    Note that an equivalent way to write this particular expression would be    
    a*b*c*d - a:b:c:d    

#Special case
C(x)            x is Categorical 

I(x)            The identity function. Simply returns its input unchanged.                
                I(x1 + x2) inside a formula to represent the sum of x1 and x2  

Q(name)         A way to 'quote' variable names, especially ones that do not otherwise meet Python's variable name rules
                y ~ weight.in.kg is y ~ Q("weight.in.kg")  

np.method(x)    Any np.method(x) can be used, Infact any vectorised method can be used

Intercept       y ~ x  is processed like y ~ 1 + x (with intercept term)                
                y ~ x - 1 does not have intercept term                
                0 and -1  represent the 'no-intercept"                
                -1 (which means the same as 0) and -0 (which means the same as 1)                
                Below all have no intercept term                
                y ~ x - 1                
                y ~ x + -1                
                y ~ -1 + x                
                y ~ 0 + x                
                y ~ x - (-0)       

##Check the design matrix  for a equation 
from patsy import  *
data = demo_data("a", "b", "y")
mat2 = dmatrices("y ~ 1 + a + b + a:b", data)[1] #dmatrices(formula, data) 
>>> mat2
DesignMatrix with shape (8, 4)
Intercept  a[T.a2]  b[T.b2]  a[T.a2]:b[T.b2]        
1            0        0                0        
1            0        1                0        
1            1        0                0        
1            1        1                1        
1            0        0                0        
1            0        1                0        
1            1        0                0        
1            1        1                1
Terms:
'Intercept' (column 0), 'a' (column 1), 'b' (column 2), 'a:b' (column 3)  


##Utility - patsy.balanced(factor_name=num_levels[, factor_name=num_levels, ..., repeat=1])
#Create simple balanced factorial designs for testing.
#Given some factor names and the number of desired levels for each, 
#generates a balanced factorial design in the form of a data dictionary

>>> balanced(a=2, b=3)
{'a': ['a1', 'a1', 'a1', 'a2', 'a2', 'a2'],
 'b': ['b1', 'b2', 'b3', 'b1', 'b2', 'b3']}



##Categorical values - use C(data, contrast=None, levels=None)
#Marks some data as being categorical, and specifies how to interpret it.

#•To explicitly mark some data as categorical
dmatrix("a", {"a": [1, 2, 3]})  #dmatrix(formula, data)
#use below 
dmatrix("C(a)", {"a": [1, 2, 3]})

#Types of coding(contrast) for Categorical values - patsy.builtins.
Treatment               Treatment coding (also known as dummy coding)default 
Sum(omit=None)          Deviation coding (also known as sum-to-zero coding).
Poly(scores=None)       Orthogonal polynomial contrast coding.
Helmert                 Helmert contrasts.
Diff                    Backward difference coding.  




#•To explicitly set the levels or override the default level ordering for categorical data, e.g.:
dmatrix("C(a, levels=["a2", "a1"])", balanced(a=2))

#Example of various coding 

# reduced rank
>>> dmatrix("C(a, Treatment)", balanced(a=3))
DesignMatrix with shape (3, 3)
  Intercept  C(a, Treatment)[T.a2]  C(a, Treatment)[T.a3]
          1                      0                      0
          1                      1                      0
          1                      0                      1
  Terms:
    'Intercept' (column0)
    'C(a, Treatment)' (columns 1:3)

# full rank
>>> dmatrix("0 + C(a, Treatment)", balanced(a=3))
DesignMatrix with shape (3, 3)
  C(a, Treatment)[a1]  C(a, Treatment)[a2]  C(a, Treatment)[a3]
                    1                    0                    0
                    0                    1                    0
                    0                    0                    1
  Terms:
    'C(a, Treatment)' (columns 0:3)

# Reduced rank with Sum -each column sum is zero 
>>> dmatrix("C(a, Sum)", balanced(a=4))
DesignMatrix with shape (4, 4)
  Intercept  C(a, Sum)[S.a1]  C(a, Sum)[S.a2]  C(a, Sum)[S.a3]
          1                1                0                0
          1                0                1                0
          1                0                0                1
          1               -1               -1               -1
  Terms:
    'Intercept' (column 0)
    'C(a, Sum)' (columns 1:4)

# Full rank
>>> dmatrix("0 + C(a, Sum)", balanced(a=4))
DesignMatrix with shape (4, 4)
  C(a, Sum)[mean]  C(a, Sum)[S.a1]  C(a, Sum)[S.a2]  C(a, Sum)[S.a3]
                1                1                0                0
                1                0                1                0
                1                0                0                1
                1               -1               -1               -1
  Terms:
    'C(a, Sum)' (columns 0:4)

# Omit a different level
>>> dmatrix("C(a, Sum(1))", balanced(a=3))
DesignMatrix with shape (3, 3)
  Intercept  C(a, Sum(1))[S.a1]  C(a, Sum(1))[S.a3]
          1                   1                   0
          1                  -1                  -1
          1                   0                   1
  Terms:
    'Intercept' (column 0)
    'C(a, Sum(1))' (columns 1:3)

>>> dmatrix("C(a, Sum('a1'))", balanced(a=3))
DesignMatrix with shape (3, 3)
  Intercept  C(a, Sum('a1'))[S.a2]  C(a, Sum('a1'))[S.a3]
          1                     -1                     -1
          1                      1                      0
          1                      0                      1
  Terms:
    'Intercept' (column 0)
    "C(a, Sum('a1'))" (columns 1:3)







##Namespaces for executing formula
#The default is to use the caller's namespace.
#The namespace used can be controlled via the eval_env keyword.
#For example, you may want to give a custom namespace using the patsy.EvalEnvironment
#or you may want to use a 'clean' namespace, which we provide by passing eval_func=- 1. 


# Using formulas with models that do not  support them
#Use dmatrix from patsy
# Those matrices can then be fed to the fitting function as endog and exog arguments.

import patsy
f = 'Lottery ~ Literacy * Wealth'
y,X = patsy.dmatrices(f, df, return_type='dataframe')
print y[:5]
print X[:5]
print smf.OLS(y, X).fit().summary()


##Example 
import statsmodels.formula.api as smf
import statsmodels as  sm
import numpy as np
import pandas

#print a list of available models
dir(smf)

#call signature of statsmodels formula API :
small_letter_model(formula, data, subset=None, *args, **kwargs)


#OLS regression using formulas
df = sm.datasets.get_rdataset("Guerry", "HistData").data
df = df[['Lottery', 'Literacy', 'Wealth', 'Region']].dropna()
>>> df.head()   
Lottery  Literacy  Wealth Region
0       41        37      73      E
1       38        51      22      N
2       66        13      61      C
3       80        46      76      E
4       79        69      83      E



mod = smf.ols(formula='Lottery ~ Literacy + Wealth + Region', data=df)
res = mod.fit()  # check result class attributes from http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.html#statsmodels.regression.linear_model.OLSResults
>>> print res.summary()

OLS Regression Results
==============================================================================
Dep. Variable:                Lottery   R-squared:                       0.338
Model:                            OLS   Adj. R-squared:                  0.287
Method:                 Least Squares   F-statistic:                     6.636
Date:                Wed, 10 Aug 2016   Prob (F-statistic):           1.07e-05
Time:                        19:13:49   Log-Likelihood:                -375.30
No. Observations:                  85   AIC:                             764.6
Df Residuals:                      78   BIC:                             781.7
Df Model:                           6
Covariance Type:            nonrobust
===============================================================================                  
               coef      std err          t      P>|t|      [95.0% Conf. Int.]
-------------------------------------------------------------------------------
Intercept      38.6517      9.456      4.087      0.000        19.826    57.478
Region[T.E]   -15.4278      9.727     -1.586      0.117       -34.793     3.938
Region[T.N]   -10.0170      9.260     -1.082      0.283       -28.453     8.419
Region[T.S]    -4.5483      7.279     -0.625      0.534       -19.039     9.943
Region[T.W]   -10.0913      7.196     -1.402      0.165       -24.418     4.235
Literacy       -0.1858      0.210     -0.886      0.378        -0.603     0.232
Wealth          0.4515      0.103      4.390      0.000         0.247     0.656
==============================================================================
Omnibus:                        3.049   Durbin-Watson:                   1.785
Prob(Omnibus):                  0.218   Jarque-Bera (JB):                2.694
Skew:                          -0.340   Prob(JB):                        0.260
Kurtosis:                       2.454   Cond. No.                         371.
==============================================================================



##Using  Categorical variables
#patsy determined that elements of Region were text strings,
#so it treated Region as a categorical variable by default

#Use C() to treat explicitly as categorical
#for example, if Region has interger category values
res = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region)', data=df).fit()
print res.params
Intercept         38.651655
C(Region)[T.E]   -15.427785   #Region == E
C(Region)[T.N]   -10.016961   #Region == N
C(Region)[T.S]    -4.548257   #Region == S
C(Region)[T.W]   -10.091276   #Region == W
Literacy          -0.185819
Wealth             0.451475


##Removing variables - for example removing intercept
res = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region) -1 ', data=df).fit()
print res.params



##Multiplicative interactions - by using :
#':' adds a new column to the design matrix with the product of the other two columns.
#'*' will also include the individual columns that were multiplied together

#lower aic/bic is bettern compare by res1.aic(), res2.bic()
res1 = smf.ols(formula='Lottery ~ Literacy : Wealth - 1', data=df).fit()
res2 = smf.ols(formula='Lottery ~ Literacy * Wealth - 1', data=df).fit()
print res1.params, '\n'
print res2.params

#Adding vectorized Functions to models
res = smf.ols(formula='Lottery ~ np.log(Literacy)', data=df).fit()
print res.params

# Adding a custom function:
def log_plus_1(x):    
    return np.log(x) + 1.    

res = smf.ols(formula='Lottery ~ log_plus_1(Literacy)', data=df).fit()
print res.params

##Another example of C ()

url = 'https://github.com/rpruim/OpenIntro/blob/master/data/hsb2.csv'
hsb2 = pd.read_table(url, delimiter=",")
hsb2 = pd.read_csv("data/hsb2.csv", delimiter=",")
>> hsb2.head()
   id  gender   race     ses  schtyp        prog  read  write  math  science  \

   70    male  white     low  public     general    57     52    41       47

  121  female  white  middle  public  vocational    68     59    53       63

   86    male  white    high  public     general    44     33    54       58

  141    male  white    high  public  vocational    63     44    47       53

  172    male  white  middle  public    academic    47     52    57       53

>>> hsb2.race.value_counts()
white               145
hispanic             24
african american     20
asian                11
Name: race, dtype: int64

from statsmodels.formula.api import ols
mod = ols("write ~ C(race, Treatment)", data=hsb2)
res = mod.fit()  # check result class attributes from http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.html#statsmodels.regression.linear_model.OLSResults
print(res.summary())
#relevant section
#P>|t| if <0.05, accept that term, rest reject
#equivalent to if [95.0% Conf. Int.] contains 0 .0
>>> print(res.summary())
                            OLS Regression Results
==============================================================================
Dep. Variable:                  write   R-squared:                       0.107
Model:                            OLS   Adj. R-squared:                  0.093
Method:                 Least Squares   F-statistic:                     7.833
Date:                Mon, 29 Jan 2018   Prob (F-statistic):           5.78e-05
Time:                        10:33:30   Log-Likelihood:                -721.77
No. Observations:                 200   AIC:                             1452.
Df Residuals:                     196   BIC:                             1465.
Df Model:                           3
Covariance Type:            nonrobust
================================================================================
==================
                                     coef    std err          t      P>|t|
[0.025      0.975]
--------------------------------------------------------------------------------
------------------
Intercept                         48.2000      2.018     23.884      0.000
44.220      52.180
C(race, Treatment)[T.asian]        9.8000      3.388      2.893      0.004
 3.119      16.481
C(race, Treatment)[T.hispanic]    -1.7417      2.732     -0.637      0.525
-7.131       3.647
C(race, Treatment)[T.white]        5.8552      2.153      2.720      0.007
 1.610      10.101
==============================================================================
Omnibus:                       10.487   Durbin-Watson:                   1.779
Prob(Omnibus):                  0.005   Jarque-Bera (JB):               11.031
Skew:                          -0.551   Prob(JB):                      0.00402
Kurtosis:                       2.670   Cond. No.                         8.82
==============================================================================
#Means, race is Categorical
C(race, Treatment)[T.asian]   means when race= asian ,=A(say) , contrast=[1,0,0]
C(race, Treatment)[T.hispanic]   means when race= hispanic = B   contrast=[0,1,0]
C(race, Treatment)[T.white]   means when race= white = C  contrast=[0,0,1]

#the fitments equation would be (ignoring non significant coeff)
write = 48.2000 + 9.8000*A + 5.8552 *C

#To get full rank , remove intercept 
mod = ols("write ~ 0 + C(race, Treatment)", data=hsb2)
res = mod.fit()  # check result class attributes from http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.html#statsmodels.regression.linear_model.OLSResults
print(res.summary())











###statsmodel - Linear Model - Result Class 

#Model Classes
OLS(endog[, exog, missing, hasconst])           A simple ordinary least squares model. 
GLS(endog, exog[, sigma, missing, hasconst])    Generalized least squares model with a general covariance structure. 
WLS(endog, exog[, weights, missing, hasconst])  A regression model with diagonal but non-identity covariance structure. 
GLSAR(endog[, exog, rho, missing])              A regression model with an AR(p) covariance structure. 
yule_walker(X[, order, method, df, inv, demean]) Estimate AR(p) parameters from a sequence X using Yule-Walker equation. 
QuantReg(endog, exog, **kwargs)                 Quantile Regression 
RecursiveLS(endog, exog, **kwargs)              Recursive least squares 

#Results Classes
RegressionResults(model, params[, ...])             This class summarizes the fit of a linear regression model. 
OLSResults(model, params[, ...])                    Results class for for an OLS model. 
QuantRegResults(model, params[, ...])               Results instance for the QuantReg model 
RecursiveLSResults(model, params, filter_results)   Class to hold results from fitting a recursive least squares model. 


##Result Methods and Attributes 
#Note base class is RegressionResults and other classes are derived from it
>>> sm.regression.linear_model.OLSResults.__mro__
(<class 'statsmodels.regression.linear_model.OLSResults'>, 
<class 'statsmodels.regression.linear_model.RegressionResults'>, 
<class 'statsmodels.base.model.LikelihoodModelResults'>, 
<class 'statsmodels.base.model.Results'>, <class 'object'>)
 
##Result Methods and Attributes 
ssr() :                     Sum of squared (whitened) residuals.
                            
mse_total() :               Total mean squared error. High is bad   
                            
resid() :                   The residuals of each observations
                            it's pandas.core.series.Series    
                            
fittedvalues() :            The predicted the values for the original (unwhitened) design.
                            it's pandas.core.series.Series   
                            
params() :                  The linear coefficients that minimize the least squares criterion. This is usually called Beta for the classical linear model.
                            It's pandas.core.series.Series , use accessing methods from Series 
                            
pvalues() :                 The two-tailed p values for the t-stats of the params.
                            it's pandas.core.series.Series 
                            
bse                         The standard errors of the parameter estimates
                            it's pandas.core.series.Series
                            
HC0_se() :                  White's (1980) heteroskedasticity robust standard errors, High is bad
HC1_se() :                  MacKinnon and White's (1985) alternative heteroskedasticity robust standard errors
HC2_se() :                  MacKinnon and White's (1985) alternative heteroskedasticity robust standard errors
HC3_se() :                  MacKinnon and White's (1985) alternative heteroskedasticity robust standard errors
                            
conf_int([alpha, cols])     Returns the confidence interval of the fitted parameters
                            it's pandas.core.frame.DataFrame
condition_number()          Return condition number of exogenous matrix. 
save(fname[, remove_data])  save a pickle of this instance 
load(fname)                 load a pickle, (class method) 
llf()                       Log likelihood
cov_params([r_matrix, column, scale, cov_p, ...])   
                            Returns covariance matrix of the parameter estimates 
                            or of linear combination of parameter estimates 
                            
aic                         Akaike's information criteria
bic                         Bayes' information criteria
cov_HC0                     Heteroscedasticity robust covariance matrix
cov_HC1                     Heteroscedasticity robust covariance matrix. 
cov_HC2                     Heteroscedasticity robust covariance matrix.
cov_HC3                     Heteroscedasticity robust covariance matrix.
cov_type                    Parameter covariance estimator used for standard errors and t-stats
df_model                    Model degress of freedom. The number of regressors p. Does not include the constant if one is present
df_resid                    Residual degrees of freedom. n - p - 1, if a constant is present. n - p if a constant is not included.
ess                         Explained sum of squares. 
fvalue                      F-statistic of the fully specified model. 
f_pvalue                    p-value of the F-statistic
mse_model                   Mean squared error the model. 
mse_resid                   Mean squared error of the residuals. 
mse_total                   Total mean squared error. 
params
resid                       The residuals of the model.
resid_pearson               wresid normalized to have unit variance.
rsquared                    R-squared of a model with an intercept. 
rsquared_adj                Adjusted R-squared.
ssr                         Sum of squared (whitened) residuals.


RegressionResults.summary(yname=None, xname=None, title=None, alpha=0.05)[source]
    Summarize the Regression Results
    Parameters:
    yname : string, optional
        Default is y
    xname : list of strings, optional
        Default is var_## for ## in p the number of regressors
    title : string, optional
        Title for the top table. If not None, then this replaces the default title
    alpha : float
        significance level for the confidence intervals
    Returns:
    smry : Summary instance
        this holds the summary tables and text, 
        which can be printed or converted to various output formats.
 
RegressionResults.predict(exog=None, transform=True, *args, **kwargs)
    Call self.model.predict with self.params as the first argument.
    Parameters:
    exog : array-like, optional
        The values for which you want to predict.
    transform : bool, optional
        If the model was fit via a formula, 
        do you want to pass exog through the formula. 
        Default is True. 
        E.g., if you fit a model y ~ log(x1) + log(x2), and transform is True, 
        then you can pass a data structure that contains x1 and x2 
        in their original form. Otherwise, you'd need to log the data first.
    Returns:
        prediction : ndarray, pandas.Series or pandas.DataFrame
 


RegressionResults.wald_test(r_matrix, cov_p=None, scale=1.0, 
                invcov=None, use_f=None)
    Compute a Wald-test for a joint linear hypothesis.
OLSResults.t_test(r_matrix, cov_p=None, scale=None, use_t=None)
    Compute a t-test for a each linear hypothesis of the form Rb = q
OLSResults.f_test(r_matrix, cov_p=None, scale=1.0, invcov=None)
    Compute the F-test for a joint linear hypothesis.
    This is a special case of wald_test that always uses the F distribution.
    Parameters:
    r_matrix : array-like, str, or tuple
        •array : An r x k array where r is the number of restrictions to test 
                and k is the number of regressors. 
                It is assumed that the linear combination is equal to zero.
        •str : The full hypotheses to test can be given as a string. 
        •tuple : A tuple of arrays in the form (R, q), 
                q can be either a scalar or a length k row vector.
    cov_p : array-like, optional
        An alternative estimate for the parameter covariance matrix. 
        If None is given, self.normalized_cov_params is used.
    scale : float, optional
        Default is 1.0 for no scaling.
    invcov : array-like, optional
        A q x q array to specify an inverse covariance matrix 
        based on a restrictions matrix.
    Returns:
        res : ContrastResults instance
            conf_int([alpha])               Returns the confidence interval of the value, effect of the constraint. 
            summary([xname, alpha, title])  Summarize the Results of the hypothesis test 
            summary_frame([xname, alpha])   Return the parameter table as a pandas DataFrame 
#Examples
import numpy as np
import statsmodels.api as sm
data = sm.datasets.longley.load()
data.exog = sm.add_constant(data.exog)
results = sm.OLS(data.endog, data.exog).fit()
A = np.identity(len(results.params))
A = A[1:,:]  #remove first row 

#H1: each coefficient is jointly statistically significantly different from zero.
>>> print(results.f_test(A))
<F test: F=array([[ 330.28533923]]), p=4.984030528700946e-10, df_denom=9, df_num=6>
#Compare this to
>>> results.fvalue
330.2853392346658
>>> results.f_pvalue
4.98403096572e-10

#H0: the coefficient on the 2nd and 3rd regressors are equal 
#and jointly that the coefficient on the 5th and 6th regressors are equal.
#coeff index      :0 1 2  3 4 5 6   0 1 2 3 4 5  6
B = np.array(([0,0,1,-1,0,0,0],[0,0,0,0,0,1,-1]))
>>> print(results.f_test(B))
<F test: F=array([[ 9.74046187]]), p=0.005605288531708235, df_denom=9, df_num=2>

##specify the hypothesis tests using a string
from statsmodels.datasets import longley
from statsmodels.formula.api import ols
dta = longley.load_pandas().data
formula = 'TOTEMP ~ GNPDEFL + GNP + UNEMP + ARMED + POP + YEAR'
results = ols(formula, dta).fit()
hypotheses = '(GNPDEFL = GNP), (UNEMP = 2), (YEAR/1829 = 1)' #H0
f_test = results.f_test(hypotheses)
>>> print(f_test)  #reject H0
<F test: F=array([[ 144.17976065]]), p=6.322026217355609e-08, df_denom=9, df_num=3>



#Examples of OLSResults.t_test
import numpy as np
import statsmodels.api as sm
data = sm.datasets.longley.load()
data.exog = sm.add_constant(data.exog)
results = sm.OLS(data.endog, data.exog).fit()
r = np.zeros_like(results.params)
r[5:] = [1,-1]
>>> print(r)
[ 0.  0.  0.  0.  0.  1. -1.]
#H0: coefficients on the 5th and 6th regressors are the same.
>>> T_test = results.t_test(r)
>>> print(T_test)
                             Test for Constraints
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
c0         -1829.2026    455.391     -4.017      0.003   -2859.368    -799.037
==============================================================================
>>> T_test.effect
-1829.2025687192481
>>> T_test.sd
455.39079425193762
>>> T_test.tvalue
-4.0167754636411717
>>> T_test.pvalue
0.0015163772380899498

#specify the hypothesis tests using a string
>>> from statsmodels.formula.api import ols
>>> dta = sm.datasets.longley.load_pandas().data
>>> formula = 'TOTEMP ~ GNPDEFL + GNP + UNEMP + ARMED + POP + YEAR'
>>> results = ols(formula, dta).fit()
>>> hypotheses = 'GNPDEFL = GNP, UNEMP = 2, YEAR/1829 = 1'
>>> t_test = results.t_test(hypotheses)
>>> print(t_test)
                             Test for Constraints
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
c0            15.0977     84.937      0.178      0.863    -177.042     207.238
c1            -2.0202      0.488     -8.231      0.000      -3.125      -0.915
c2             1.0001      0.249      0.000      1.000       0.437       1.563
==============================================================================


##comparison with another model which is nested inside 'this' model , 
#called restricted model
compare_f_test(restricted)      use F test to test whether restricted model is correct
compare_lm_test(restricted)     Use Lagrange Multiplier test to test whether restricted model is correct
compare_lr_test(restricted)     Likelihood ratio test to test whether restricted model is correct

OLSResults.compare_lr_test(restricted, large_sample=False)
    Likelihood ratio test to test whether restricted model is better 
    H0: both are same 
    Parameters:
    restricted : Result instance
        The restricted model is assumed to be nested in the current model. 
    large_sample : bool
        Flag indicating whether to use a heteroskedasticity robust version of the LR test, which is a modified LM test.
    Returns:
    lr_stat : float
        likelihood ratio, chisquare distributed with df_diff degrees of freedom
    p_value : float
        p-value of the test statistic
    df_diff : int
        degrees of freedom of the restriction, i.e. difference in df between models
 
 
 
OLSResults.el_test(b0_vals, param_nums, return_weights=0, ret_params=0, method='nm', stochastic_exog=1, return_params=0)
    Tests single or joint hypotheses of the regression parameters using Empirical Likelihood.
    Parameters:
    b0_vals : 1darray
        The hypothesized value of the parameter to be tested
    param_nums : 1darray
        The parameter number to be tested
    print_weights : bool
        If true, returns the weights that optimize the likelihood ratio at b0_vals. Default is False
    ret_params : bool
        If true, returns the parameter vector that maximizes the likelihood ratio at b0_vals. Also returns the weights. Default is False
    method : string
        Can either be 'nm' for Nelder-Mead or 'powell' for Powell. The optimization method that optimizes over nuisance parameters. Default is 'nm'
    stochastic_exog : bool
        When TRUE, the exogenous variables are assumed to be stochastic. When the regressors are nonstochastic, moment conditions are placed on the exogenous variables. Confidence intervals for stochastic regressors are at least as large as non-stochastic regressors. Default = TRUE
    Returns:
    res : tuple
        The p-value and -2 times the log-likelihood ratio for the hypothesized values.
     
#Examples
import statsmodels.api as sm
data = sm.datasets.stackloss.load()
endog = data.endog
exog = sm.add_constant(data.exog)
model = sm.OLS(endog, exog)
fitted = model.fit()
>>> fitted.params
>>> array([-39.91967442,   0.7156402 ,   1.29528612,  -0.15212252])
>>> fitted.rsquared
>>> 0.91357690446068196
>>> # H0: the slope on the first variable is 0
>>> fitted.el_test([0], [1])
>>> (27.248146353888796, 1.7894660442330235e-07)


OLSResults.outlier_test(method='bonf', alpha=0.05)
    Test observations for outliers according to method
    Parameters:
        method : str
            •bonferroni : one-step correction
            •sidak : one-step correction
            •holm-sidak :
            •holm :
            •simes-hochberg :
            •hommel :
            •fdr_bh : Benjamini/Hochberg
            •fdr_by : Benjamini/Yekutieli
    See statsmodels.stats.multitest.multipletests for details.
    alpha : float
        familywise error rate
    Returns:
    table : ndarray or DataFrame
        Returns either an ndarray or a DataFrame if labels is not None. 
        Will attempt to get labels from model_results if available. 
        The columns are the Studentized residuals, the unadjusted p-value, 
        and the corrected p-value according to method.
 

#Example 
import numpy as np
import statsmodels.api as sm
spector_data = sm.datasets.spector.load()
spector_data.exog = sm.add_constant(spector_data.exog, prepend=False)

# Fit and summarize OLS model
mod = sm.OLS(spector_data.endog, spector_data.exog)
res = mod.fit()
>>> print(res.outlier_test()) #H0: no outlier 
[[ 0.14357861  0.88689947  1.        ]
 [-0.19227041  0.84896922  1.        ]
 [-0.72912515  0.47220308  1.        ]
 [ 0.05198279  0.9589251   1.        ]
 [ 1.23157405  0.22872763  1.        ]
 [-0.01871656  0.98520485  1.        ]
 [ 0.10521307  0.91698449  1.        ]
 [-0.14030488  0.88946058  1.        ]
 [-0.45228749  0.65467489  1.        ]
 [ 1.09463596  0.2833455   1.        ]
 [ 0.18083173  0.85785021  1.        ]
 [-0.7488271   0.46043569  1.        ]
 [-1.08086619  0.28931688  1.        ]
 [ 2.05350432  0.04982625  1.        ]
 [-1.12947014  0.26863464  1.        ]
 [ 0.07296402  0.9423726   1.        ]
 [-0.1081957   0.91464051  1.        ]
 [-0.0370469   0.97072017  1.        ]
 [-1.56148898  0.13005455  1.        ]
 [ 1.05772986  0.2995507   1.        ]
 [-0.19804147  0.84449616  1.        ]
 [ 0.40242494  0.6905397   1.        ]
 [-1.08222827  0.28872225  1.        ]
 [-2.28529074  0.0303718   0.97189753]
 [ 0.60419872  0.55075518  1.        ]
 [ 1.50769308  0.14324589  1.        ]
 [ 1.05011971  0.30297199  1.        ]
 [-1.02423915  0.31481197  1.        ]
 [ 0.56348861  0.57775259  1.        ]
 [ 0.06388793  0.94953012  1.        ]
 [-1.47457371  0.15189302  1.        ]
 [ 2.47780029  0.01976799  0.63257573]]








###statsmodel -  Regression Diagnostics and Specification Tests


#Multicollinearity - highly correlated  variables(columns of x)
# affects the stability of  coefficient estimates

#when the independent variables are so highly correlated 
#that they contain redundant information, which confuses the regression proces


#longley - highly correlated data
#This data set contains a variety of measurements about the population of the US in an attempt to predict employment
from statsmodels.datasets.longley import load_pandas
y = load_pandas().endog
X = load_pandas().exog
X = sm.add_constant(X)  #add a constant term , Note by default, statsmodels does not have intercept term included into model. Add it explicitly

ols_model = sm.OLS(y, X)
ols_results = ols_model.fit()
print(ols_results.summary())
==============================================================================                 
                coef    std err          t      P>|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
const      -3.482e+06    8.9e+05     -3.911      0.004      -5.5e+06 -1.47e+06   
GNPDEFL       15.0619     84.915      0.177      0.863      -177.029   207.153
GNP           -0.0358      0.033     -1.070      0.313        -0.112     0.040
UNEMP         -2.0202      0.488     -4.136      0.003        -3.125    -0.915
ARMED         -1.0332      0.214     -4.822      0.001        -1.518    -0.549
POP           -0.0511      0.226     -0.226      0.826        -0.563     0.460
YEAR        1829.1515    455.478      4.016      0.003       798.788  2859.515
Warnings:
[2] The condition number is large, 4.86e+0 9.
This might indicate that there are strong multicollinearity or other numerical problems.



##Condition number- assess multicollinearity is to compute the condition number.
#Values over 20 are worrisome

##multicollinearity effect: 
#dropping a single observation can have a dramatic effect on the coefficient estimates:

ols_results2 = sm.OLS(y.ix[:14], X.ix[:14]).fit()  #only 14 rows 
>>> print("Percentage change %4.2f%%\n"*7 % tuple([i for i in (ols_results2.params - ols_results.params)/ols_results.params*100]))
Percentage change -13.35%
Percentage change -236.18%
Percentage change -23.69%
Percentage change -3.36%
Percentage change -7.26%
Percentage change -200.46%
Percentage change -13.34%

#Do pairwise scatter matrix to check corelation of independent variables

from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
scatter_matrix(X.iloc[:,1:], alpha=0.2, figsize=(6, 6), color='black')
plt.show()

#check plot to find corelated Columns (eg linear relation)
#find it out by VIF 
statsmodels.stats.outliers_influence.variance_inflation_factor(exog, exog_idx)
    variance inflation factor, VIF, for one exogenous variable
    The variance inflation factor is a measure for the increase of the variance 
    of the parameter estimates if an additional variable, 
    given by exog_idx is added to the linear regression. 
    It is a measure for multicollinearity of the design matrix, exog.
    One recommendation is that if VIF is greater than 5, 
    then the explanatory variable given by exog_idx is highly collinear 
    with the other explanatory variables, 
    and the parameter estimates will have large standard errors because of this.
    Parameters:
    exog : ndarray, (nobs, k_vars)
        design matrix with all explanatory variables, 
    exog_idx : int
        index of the exogenous variable in the columns of exog
    Returns:
    vif : float
        variance inflation factor
    #Example 
    from patsy import dmatrices
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    import statsmodels.api as sm
    # Break into left and right hand side; y and X
    y, X = dmatrices(formula="medv ~ crim + zn + nox + ptratio + black + rm ", data=boston, return_type="dataframe")
    # For each Xi, calculate VIF
    vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    # Fit X to y
    result = sm.OLS(y, X).fit()

        
        
#longley    data 
X_vif = X.iloc[:,1:].as_matrix()
for i in range(X_vif.shape[1]):
    print(X.columns[i+1],statsmodels.stats.outliers_influence.variance_inflation_factor(X_vif,i))
#output 
GNPDEFL 12425.514335354637
GNP 10290.435436791722
UNEMP 136.2243535629104
ARMED 39.98338558291202
POP 101193.16199321792
YEAR 84709.9504430369
#Removing two highest 
X_m = X.iloc[:,[0,2,3,4,6]]
ols_model2 = sm.OLS(y, X_m)
ols_results2 = ols_model2.fit()
print(ols_results2.summary())


##One of formal statistics for Outliers is  DFBETAS
#a standardized measure of how much each coefficient changes 
#when that observation is removed

#Calculate influence table and find out influential observations
infl = ols_results.get_influence()  #type of statsmodels.stats.outliers_influence.OLSInfluence

# DBETAS in absolute value greater than 2/sqrt{N} to be influential observations
2./len(X)**.5  #0.5

>>> print(infl.summary_frame().filter(regex="dfb")) 
#check abs(values) > 0.5 to know influencial observation    
    dfb_const  dfb_GNPDEFL   dfb_GNP  dfb_UNEMP  dfb_ARMED   dfb_POP  dfb_YEAR
0   -0.016406    -0.234566 -0.045095  -0.121513  -0.149026  0.211057  0.013388
1   -0.020608    -0.289091  0.124453   0.156964   0.287700 -0.161890  0.025958
2   -0.008382     0.007161 -0.016799   0.009575   0.002227  0.014871  0.008103
3    0.018093     0.907968 -0.500022  -0.495996   0.089996  0.711142 -0.040056
4    1.871260    -0.219351  1.611418   1.561520   1.169337 -1.081513 -1.864186
5   -0.321373    -0.077045 -0.198129  -0.192961  -0.430626  0.079916  0.323275
6    0.315945    -0.241983  0.438146   0.471797  -0.019546 -0.448515 -0.307517
7    0.015816    -0.002742  0.018591   0.005064  -0.031320 -0.015823 -0.015583
8   -0.004019    -0.045687  0.023708   0.018125   0.013683 -0.034770  0.005116
9   -1.018242    -0.282131 -0.412621  -0.663904  -0.715020 -0.229501  1.035723
10   0.030947    -0.024781  0.029480   0.035361   0.034508 -0.014194 -0.030805
11   0.005987    -0.079727  0.030276  -0.008883  -0.006854 -0.010693 -0.005323
12  -0.135883     0.092325 -0.253027  -0.211465   0.094720  0.331351  0.129120
13   0.032736    -0.024249  0.017510   0.033242   0.090655  0.007634 -0.033114
14   0.305868     0.148070  0.001428   0.169314   0.253431  0.342982 -0.318031
15  -0.538323     0.432004 -0.261262  -0.143444  -0.360890 -0.467296  0.552421

[16 rows x 7 columns]
>>> print(np.abs(infl.summary_frame().filter(regex="dfb")) > 0.5)
    dfb_const  dfb_GNPDEFL  dfb_GNP  dfb_UNEMP  dfb_ARMED  dfb_POP  dfb_YEAR
0       False        False    False      False      False    False     False
...

>>> indexes = (np.abs(infl.summary_frame().filter(regex="dfb")) > 0.5).astype(np.float).sum(1)
0     0.0
1     0.0
2     0.0
3     3.0
4     6.0
5     0.0
6     0.0
7     0.0
8     0.0
9     4.0
10    0.0
11    0.0
12    0.0
13    0.0
14    0.0
15    2.0
dtype: float64

# the discovery of an influential observation in a regression provides 
#no justification for removing that point from the model; 
#it simply helps to identify observations which may deserve further study; 
#in fact, the influential points are often perfectly valid observations 
#that simply do a good job of representing the relationships 
#that occur in the data. 
#So when an influential point is identified, it simply means that it should be examined more closely to see whether or not it is a potential problem. 

#Drop them and then try fitting 
X1 = X.drop(indexes[ indexes > 0 ].index.tolist(), axis=0).reset_index(drop=True)
y1 = y.drop(indexes[ indexes > 0 ].index.tolist(), axis=0).reset_index(drop=True)
ols_model1 = sm.OLS(y1, X1)
ols_results1 = ols_model1.fit()
print(ols_results1.summary())
infl1 = ols_results1.get_influence()
print(infl1.summary_frame().filter(regex="dfb")) 
indexes1 = (np.abs(infl1.summary_frame().filter(regex="dfb")) > 0.5).astype(np.float).sum(1)
#Not improved!!!
>>> indexes1
0     0.0
1     0.0
2     0.0
3     5.0
4     3.0
5     0.0
6     1.0
7     7.0
8     0.0
9     0.0
10    1.0
11    6.0
dtype: float64
#Use Other eg RLM 
rlm_model = sm.RLM(y, X, M=sm.robust.norms.HuberT())
rlm_results = rlm_model.fit()
print(rlm_results.summary())  #not imporved 

#Other methods of statsmodels.stats.outliers_influence.OLSInfluence
#when comparing take absolute values, 
#N = number of observations, p= number of parameters

cooks_distance()                Cooks distance , > 4/(N-p-1) outliers

cov_ratio()                     covariance ratio between LOOO and original                                
                                Uses leave-one-observation-out (LOOO) auxiliary regression                                
                                COVRATIO statistic measures the change in the determinant of the covariance matr ix                                
                                of the estimates by deleting the ith observation                                
                                Use if dataset is not large , >(1+3P/N) requires attiontion
                                
det_cov_params_not_obsi()       determinant of cov_params of all LOOO regressions
params_not_obsi()               parameter estimates for all LOOO regressions
sigma2_not_obsi()               error variance for all LOOO regressions

dfbetas()                       dfbetas , > 2/sqrt(N) outtliers
dffits()                        dffits measure for influence of an observation                                
                                > 2/sqrt(p/N) requires attention
                                
dffits_internal()               dffits measure for influence of an observation

resid_press()                   PRESS residuals
ess_press()                     predicted residual error sum of squares (PRESS) statistic                                
                                each observation in turn is removed and the model is refitted using the remaining observation s.                                
                                The out-of-sample predicted value is calculated for the omitted observation in each cas e,                                
                                and the PRESS statistic is calculated as the sum of the squares of all the resulting prediction errors                                
                                Lower PRESS statistic is better (can be used to compare two models)                                

get_resid_studentized_external([sigma])     calculate studentized residuals                                            
                                             >2 is outliers
                                             
resid_studentized_external()                studentized residuals using LOOO varian ce
resid_studentized_internal()                studentized residuals using variance from O LS

hat_diag_factor()               factor of diagonal of hat_matrix used in influence
hat_matrix_diag()               diagonal of the hat_matrix for OLS                                
                                Maps the vector of dependent variable values to the vector of  predicted value s.                                
                                It describes the influence each response value has on each fitted value.                                
                                The diagonal elements of the hat matrix are the leverages,                                
                                which describe the influence each response value has on the fitted val ue                                
                                for that same observation                                
                                ith diagonal > 1/N means ith observation has more levergae/influence on model                                

influence()                     influence measure

resid_std()                     estimate of standard deviation of the residua ls
resid_var()                     estimate of variance of the residuals                                
                                the variance of residuals must be constant, 
                                and they must have a mean of zero                                
                                Can be seen from plt.scatter(np.arange(i.resid_var.size),i.resid_var)                                
                                OR from scipy.stats.describe(i.resid_var)                                

summary_frame()                 Creates a DataFrame with all available influence results.
summary_table([float_fmt])      create a summary table with all influence and outlier measur es

##Also, Can use below to get outlier

statsmodels.stats.outliers_influence.outlier_test(model_results, method='bonf', alpha=.05, labels=None,  order=False ):

method : str    
- `bonferroni` : one-step correction    
- `sidak` : one-step correction    
- `holm-sidak` :    
- `holm` :    
- `simes-hochberg` :    
- `hommel` :    
- `fdr_bh` : Benjamini/Hochberg    
- `fdr_by` : Benjamini/Yekutieli    

#returns ,  each observation is tested with H0: no outlier , <0.05, reject H0
#abs(student_resid) > 2 requires attention     
     student_resid   unadj_p  bonf(p)
0        -0.227953  0.819922      1.0
1         0.548815  0.583760      1.0
2        -2.368408  0.018843      1.0  #Attention!!!




##Solution of removing outliers - use Robust Regression, RLM,

import statsmodels.api as sm

#Example for using Huber's T norm with the default
# median absolute deviation scaling

data = sm.datasets.stackloss.load()
data.exog = sm.add_constant(data.exog)   #add a constant term , Note by default, statsmodels does not have intercept term included into model. Add it explicitly
huber_t = sm.RLM(data.endog, data.exog, M=sm.robust.norms.HuberT())
hub_results = huber_t.fit()
print(hub_results.weights) #weights used in WLS 

##More advanced methods are Generalized Additive Models (gam)
#Instead of fitting a single linear parameter to try to explain the relationship 
#between independent variables and dependent variables, 
#GAM models perform spline smooths on selected variables, 
#and use these smoothed versions of the independent variables 
#to try to explain the values of the dependent variables
 
#python does not have good GAM 

#below Work in progress, hence dont use 
from statsmodels.sandbox.gam import AdditiveModel
gam = AdditiveModel(X.values) #takes ndarray 
gam_r = gam.fit(y.values,maxiter=100)
print(gam_r.summary())

#use below , requires scikit-parse for large models  , but not available in prebuilt
$ pip install pygam
$ conda install scikit-sparse nose  #check 
#https://github.com/dswah/pyGAM
#https://codeburst.io/pygam-getting-started-with-generalized-additive-models-in-python-457df5b4705f
#Models
    GAM (base class for constructing custom models)
    LinearGAM
    LogisticGAM
    GammaGAM
    PoissonGAM
    InvGaussGAM
#You can mix and match distributions with link functions to create custom models
gam = GAM(distribution='gamma', link='inverse')
#Distributions
    Normal
    Binomial
    Gamma
    Poisson
    Inverse Gaussian
#Link Functions
#Link functions take the distribution mean to the linear prediction. 
#These are the canonical link functions for the above distributions:
    Identity
    Logit
    Inverse
    Log
    Inverse-squared
#Callbacks
#Callbacks are performed during each optimization iteration. 
    deviance - model deviance
    diffs - differences of coefficient norm
    accuracy - model accuracy for LogisticGAM
    coef - coefficient logging
#You can check a callback by inspecting:
plt.plot(gam.logs_['deviance'])
 


from pygam import LinearGAM
from pygam.utils import generate_X_grid

from statsmodels.datasets.longley import load_pandas
y = load_pandas().endog
X = load_pandas().exog

#can call fit(X,y) but it searches grid for optimization with default grid 
#gam = LinearGAM(n_splines=10).fit(X, y) 
gam = LinearGAM(n_splines=10).gridsearch(X, y)  
>>> print(gam.summary())
Model Statistics
-------------------------
edof               15.923
AIC               188.521
AICc              -126.88
GCV                 77.37
loglikelihood     -77.337
deviance            0.077
scale            2501.005

Pseudo-R^2
--------------------------
explained_deviance     1.0
McFadden               1.0
McFadden_adj           1.0
None

#Because of additive nature, 
#we can explore and interpret individual features by holding others at their mean. 
#Find out relation between y vs each feature - linear , non linear or what 

XX = generate_X_grid(gam)

plt.rcParams['figure.figsize'] = (28, 8)
fig, axs = plt.subplots(1, len(X.columns[0:6]))
titles = X.columns
for i, ax in enumerate(axs):
    pdep, confi = gam.partial_dependence(XX, feature=i+1, width=.95)
    ax.plot(XX[:, i], pdep)
    ax.plot(XX[:, i], confi[0][:, 0], c='grey', ls='--')
    ax.plot(XX[:, i], confi[0][:, 1], c='grey', ls='--')
    ax.set_title(titles[i])

plt.show()
    
#Prediction 
index = [1]
predictions = gam.predict(X.iloc[index])
print(predictions, y[ index])

##get test, train data 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
gam2 = LinearGAM(n_splines=10).gridsearch(X_train, y_train)  
>>> print(gam2.summary())
predictions = gam2.predict(X_test)
print(predictions,  y_test)
plt.scatter(y_test, predictions) #ideal is linear 
plt.xlabel('True Values')
plt.ylabel('Predictions')

#Performance measure for Continuous data 
statsmodels.tools.eval_measures.bias(x1, x2[, axis]) bias, mean error 
statsmodels.tools.eval_measures.iqr(x1, x2[, axis]) interquartile range of error 
statsmodels.tools.eval_measures.maxabs(x1, x2[, axis]) maximum absolute error 
statsmodels.tools.eval_measures.meanabs(x1, x2[, axis]) mean absolute error 
statsmodels.tools.eval_measures.medianabs(x1, x2[, axis]) median absolute error 
statsmodels.tools.eval_measures.medianbias(x1, x2[, axis]) median bias, median error 
statsmodels.tools.eval_measures.mse(x1, x2[, axis]) mean squared error 
statsmodels.tools.eval_measures.rmse(x1, x2[, axis]) root mean squared error 
statsmodels.tools.eval_measures.stde(x1, x2[, ddof, axis]) standard deviation of error 
statsmodels.tools.eval_measures.vare(x1, x2[, ddof, axis]) variance of error 
#example , good !!
m, iqr = statsmodels.tools.eval_measures.meanabs(predictions,  y_test), statsmodels.tools.eval_measures.iqr(predictions,  y_test)



  


###statsmodel - One-way/2-way  ANOVA 
statsmodels.stats.anova.anova_lm(*args, **kwargs)
    ANOVA table for one or more fitted linear models.
    Parameters:
    args : fitted linear model results instance
        One or more fitted linear models
    scale : float
        Estimate of variance, If None, will be estimated from the largest model. Default is None.
    test : str {'F', 'Chisq', 'Cp'} or None
        Test statistics to provide. Default is 'F'.
    typ : str or int {'I','II','III'} or {1,2,3}
        The type of ANOVA test to perform. See notes.
    robust : {None, 'hc0', 'hc1', 'hc2', 'hc3'}
        Use heteroscedasticity-corrected coefficient covariance matrix. 
        If robust covariance is desired, it is recommended to use hc3.
     

#Example - 2way - 2 categorical 

import statsmodels.api as sm
from statsmodels.formula.api import ols
moore = sm.datasets.get_rdataset("Moore", "car", cache=True) # load
data = moore.data
data = data.rename(columns={"partner.status" :
                            "partner_status"}) # make name pythonic
moore_lm = ols('conformity ~ C(fcategory, Sum)*C(partner_status, Sum)', data=data).fit()
table = sm.stats.anova_lm(moore_lm, typ=2) # Type 2 ANOVA DataFrame
>>> print(table)  #H0=no impact 
                                              sum_sq    df          F  \
C(fcategory, Sum)                          11.614700   2.0   0.276958
C(partner_status, Sum)                    212.213778   1.0  10.120692
C(fcategory, Sum):C(partner_status, Sum)  175.488928   2.0   4.184623
Residual                                  817.763961  39.0        NaN

                                            PR(>F)
C(fcategory, Sum)                         0.759564
C(partner_status, Sum)                    0.002874  *significant impact 
C(fcategory, Sum):C(partner_status, Sum)  0.022572  *significant impact
Residual                                       NaN


#Example - one way 
from __future__ import print_function
from statsmodels.compat import urlopen
import numpy as np
np.set_printoptions(precision=4, suppress=True)
import statsmodels.api as sm
import pandas as pd
pd.set_option("display.width", 100)
import matplotlib.pyplot as plt
from statsmodels.formula.api import ols
from statsmodels.graphics.api import interaction_plot, abline_plot
from statsmodels.stats.anova import anova_lm

try:    
    rehab_table = pd.read_csv('data/rehab.table')
except:    
    url = 'http://stats191.stanford.edu/data/rehab.csv'    
    rehab_table = pd.read_table(url, delimiter=",")    
    rehab_table.to_csv('data/rehab.table')

fig, ax = plt.subplots(figsize=(8,6))
fig = rehab_table.boxplot('Time', 'Fitness', ax=ax, grid=False)

#Fit
rehab_lm = ols('Time ~ C(Fitness)', data=rehab_table).fit()
table9 = anova_lm(rehab_lm)
>>> print(table9)            
            df  sum_sq     mean_sq          F    PR(>F)
C(Fitness)   2     672  336.000000  16.961538  0.000041  #<0.05 , significant or influential 
Residual    21     416   19.809524        NaN       NaN

>>> print(rehab_lm.model.data.orig_exog) #Design matrix 
    Intercept  C(Fitness)[T.2]  C(Fitness)[T.3] 
0           1                0                0
1           1                0                0
2           1                0                0
3           1                0                0
4           1                0                0
5           1                0                0
6           1                0                0
7           1                0                0
8           1                1                0
9           1                1                0
10          1                1                0
11          1                1                0
12          1                1                0
13          1                1                0
14          1                1                0
15          1                1                0
16          1                1                0
17          1                1                0
18          1                0                1
19          1                0                1
20          1                0                1
21          1                0                1
22          1                0                1
23          1                0                1

[24 rows x 3 columns]

#C(Fitness)[T.2]  C(Fitness)[T.3] 
# Fitness -> 2  , -> 3
>>> print(rehab_lm.summary())
                           
OLS Regression Results
==============================================================================
Dep. Variable:                   Time   R-squared:                       0.618
Model:                            OLS   Adj. R-squared:                  0.581
Method:                 Least Squares   F-statistic:                     16.96
Date:                Sun, 01 Feb 2015   Prob (F-statistic):           4.13e-05
Time:                        09:30:58   Log-Likelihood:                -68.286
No. Observations:                  24   AIC:                             142.6
Df Residuals:                      21   BIC:                             146.1
Df Model:                           2
Covariance Type:            nonrobust
===================================================================================                      
coef    std err          t      P>|t|      [95.0% Conf. Int.]
-----------------------------------------------------------------------------------
Intercept          38.0000      1.574     24.149      0.000        34.728    41.272
C(Fitness)[T.2]    -6.0000      2.111     -2.842      0.010       -10.390    -1.610
C(Fitness)[T.3]   -14.0000      2.404     -5.824      0.000       -18.999    -9.001
==============================================================================
Omnibus:                        0.163   Durbin-Watson:                   2.209
Prob(Omnibus):                  0.922   Jarque-Bera (JB):                0.211
Skew:                          -0.163   Prob(JB):                        0.900
Kurtosis:                       2.675   Cond. No.                         3.80
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.



##Example - Two-way ANOVA
try:    
    rehab_table = pd.read_csv('data/kidney.table')
except:    
    url = 'http://stats191.stanford.edu/data/kidney.table'    
    rehab_table = pd.read_table(url, delimiter=",")    
    rehab_table.to_csv('data/kidney.table')
    


>>> kidney_table.groupby(['Weight', 'Duration']).size()
Weight  Duration
1       1           10        
2           10
2       1           10        
2           10
3       1           10        
2           10
dtype: int64


#Balanced panel
kt = kidney_table
plt.figure(figsize=(8,6))
fig = interaction_plot(kt['Weight'], kt['Duration'], np.log(kt['Days']+1), colors=['red', 'blue'], markers=['D','^'], ms=10, ax=plt.gca())


#fit
kidney_lm = ols('np.log(Days+1) ~ C(Duration) * C(Weight)', data=kt).fit()
table10 = anova_lm(kidney_lm)

#two way anova - H0: all models are same 
>>> print(anova_lm(ols('np.log(Days+1) ~ C(Duration) + C(Weight)',  data=kt).fit(), kidney_lm))
     df_resid    ssr      df_diff   ss_diff        F    Pr(>F)
0        56  29.624856        0       NaN      NaN       NaN
1        54  28.989198        2  0.635658  0.59204  0.556748  #accept H0

[2 rows x 6 columns]   

>>> print(anova_lm(ols('np.log(Days+1) ~ C(Duration)', data=kt).fit(), 
               ols('np.log(Days+1) ~ C(Duration) + C(Weight, Sum)',data=kt).fit()))
df_resid        ssr  df_diff    ss_diff          F    Pr(>F)
0        58  46.596147        0        NaN        NaN       NaN
1        56  29.624856        2  16.971291  16.040454  0.000003 #reject H0

>>> print(anova_lm(ols('np.log(Days+1) ~ C(Weight)', data=kt).fit(),               
                ols('np.log(Days+1) ~ C(Duration) + C(Weight, Sum)',                   
                data=kt).fit()))
df_resid        ssr  df_diff   ss_diff         F   Pr(>F)
0        57  31.964549        0       NaN       NaN      NaN
1        56  29.624856        1  2.339693  4.422732  0.03997 #reject H0

[2 rows x 6 columns]


#Sum of squares- use of different types of sums of squares (I,II,II) 
#and the Sum contrast can be used to produce the same output between the 3.

#When data is unbalanced(ie nobs for factor A and B are different), 
#there are different ways to calculate 
#the sums of squares for ANOVA
#Type I for two factors A,B = consider A first, then B and then AB 
#type II = consider A first and then B , no interaction 
#Type II = consider AB, then B and then A  and consider AB , then A and then A 

#Types I II and III SS are equivalent under a balanced design.
#Don't use Type III with non-orthogonal contrast - ie., Treatment, use with Sum 

#contrast is a linear combination of variables (parameters or statistics) 
#whose coefficients add up to zero, allowing comparison of different treatments


#Using  Sum contrast can be used to produce the same output between the 3.

sum_lm = ols('np.log(Days+1) ~ C(Duration, Sum) * C(Weight, Sum)',data=kt).fit()

>>> print(anova_lm(sum_lm))
                                  df     sum_sq   mean_sq          F    PR(>F)
C(Duration, Sum)                  1   2.339693  2.339693   4.358293  0.041562
C(Weight, Sum)                    2  16.971291  8.485645  15.806745  0.000004
C(Duration, Sum):C(Weight, Sum)   2   0.635658  0.317829   0.592040  0.556748
Residual                         54  28.989198  0.536837        NaN       NaN

[4 rows x 5 columns] 

>>> print(anova_lm(sum_lm, typ=2))                                   
                                sum_sq  df          F    PR(>F)
C(Duration, Sum)                  2.339693   1   4.358293  0.041562
C(Weight, Sum)                   16.971291   2  15.806745  0.000004
C(Duration, Sum):C(Weight, Sum)   0.635658   2   0.592040  0.556748
Residual                         28.989198  54        NaN       NaN

[4 rows x 4 columns] 

>>> print(anova_lm(sum_lm, typ=3))
                                    sum_sq  df           F        PR(>F)
Intercept                        156.301830   1  291.153237  2.077589e-23
C(Duration, Sum)                   2.339693   1    4.358293  4.156170e-02
C(Weight, Sum)                    16.971291   2   15.806745  3.944502e-06
C(Duration, Sum):C(Weight, Sum)    0.635658   2    0.592040  5.567479e-01
Residual                          28.989198  54         NaN           NaN

[5 rows x 4 columns]



#and how the Treatment contrast can be used to produce 
#the same output between the 3.
nosum_lm = ols('np.log(Days+1) ~ C(Duration, Treatment) * C(Weight, Treatment)', data=kt).fit()
>>> print(anova_lm(nosum_lm))
                                            df     sum_sq   mean_sq          F    PR(>F)
C(Duration, Treatment)                        1   2.339693  2.339693   4.358293  0.041562
C(Weight, Treatment)                          2  16.971291  8.485645  15.806745  0.000004
C(Duration, Treatment):C(Weight, Treatment)   2   0.635658  0.317829   0.592040  0.556748
Residual                                     54  28.989198  0.536837        NaN       NaN

[4 rows x 5 columns]                                                

>>> print(anova_lm(nosum_lm, typ=2))
                                                sum_sq  df          F    PR(>F)
C(Duration, Treatment)                        2.339693   1   4.358293  0.041562
C(Weight, Treatment)                         16.971291   2  15.806745  0.000004
C(Duration, Treatment):C(Weight, Treatment)   0.635658   2   0.592040  0.556748
Residual                                     28.989198  54        NaN       NaN

[4 rows x 4 columns]  

>>> print(anova_lm(nosum_lm, typ=3))                                             
                                                sum_sq  df          F    PR(>F)
Intercept                                    10.427596   1  19.424139  0.000050
C(Duration, Treatment)                        0.054293   1   0.101134  0.751699
C(Weight, Treatment)                         11.703387   2  10.900317  0.000106
C(Duration, Treatment):C(Weight, Treatment)   0.635658   2   0.592040  0.556748
Residual                                     28.989198  54        NaN       NaN

[5 rows x 4 columns]




###statsmodel - Selecting best models using ANOVA

#Download and format data:
#Do an ANOVA check, to compare models- H0: all models are same 

from __future__ import print_function
from statsmodels.compat import urlopen
import numpy as np
np.set_printoptions(precision=4, suppress=True)
import statsmodels.api as sm
import pandas as pd
pd.set_option("display.width", 100)
import matplotlib.pyplot as plt
from statsmodels.formula.api import ols
from statsmodels.graphics.api import interaction_plot, abline_plot
from statsmodels.stats.anova import anova_lm

try:    
    salary_table = pd.read_csv('data/salary.table')
except:  # recent pandas can read URL without urlopen    
    url = 'http://stats191.stanford.edu/data/salary.table'    
    fh = urlopen(url)    
    salary_table = pd.read_table(fh)    
    salary_table.to_csv('salary.table')

E = salary_table.E
M = salary_table.M
X = salary_table.X
S = salary_table.S

#draw few plots on raw data 
plt.figure(figsize=(6,6))
symbols = ['D', '^']
colors = ['r', 'g', 'blue']
factor_groups = salary_table.groupby(['E','M'])
for values, group in factor_groups:    
    i,j = values    
    plt.scatter(group['X'], group['S'], marker=symbols[j], color=colors[i-1], s=144)
plt.xlabel('Experience');
plt.ylabel('Salary');


#Fit a linear model:
formula = 'S ~ C(E) + C(M) + X'    #E, M are categorical 
lm = ols(formula, salary_table).fit()  #OLS for (y,x) and ols for formula 
>>> print(lm.summary())                            
OLS Regression Results
==============================================================================
Dep. Variable:                      S   R-squared:                       0.957
Model:                            OLS   Adj. R-squared:                  0.953
Method:                 Least Squares   F-statistic:                     226.8
Date:                Sun, 01 Feb 2015   Prob (F-statistic):           2.23e-27
Time:                        09:30:45   Log-Likelihood:                -381.63
No. Observations:                  46   AIC:                             773.3
Df Residuals:                      41   BIC:                             782.4
Df Model:                           4
Covariance Type:            nonrobust
==============================================================================                 
coef    std err          t      P>|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept   8035.5976    386.689     20.781      0.000      7254.663  8816.532
C(E)[T.2]   3144.0352    361.968      8.686      0.000      2413.025  3875.045
C(E)[T.3]   2996.2103    411.753      7.277      0.000      2164.659  3827.762
C(M)[T.1]   6883.5310    313.919     21.928      0.000      6249.559  7517.503
X            546.1840     30.519     17.896      0.000       484.549   607.819
==============================================================================
Omnibus:                        2.293   Durbin-Watson:                   2.237
Prob(Omnibus):                  0.318   Jarque-Bera (JB):                1.362
Skew:                          -0.077   Prob(JB):                        0.506
Kurtosis:                       2.171   Cond. No.                         33.5
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

#note 
C(E)[T.2]  => E== 2
C(E)[T.3]  => E== 3
C(M)[T.1]  => M== 1

#look at the created design matrix:
>>> lm.model.exog[:5]
array([[ 1.,  0.,  0.,  1.,  1.],       
[ 1.,  0.,  1.,  0.,  1.],       
[ 1.,  0.,  1.,  1.,  1.],       
[ 1.,  1.,  0.,  0.,  1.],       
[ 1.,  0.,  1.,  0.,  1.]])


#Or since we initially passed in a DataFrame, original design matrix 
lm.model.data.orig_exog[:5]

#reference to the original untouched data in
lm.model.data.frame[:5]

#Influence statistics
infl = lm.get_influence()
>>> print(infl.summary_table())
==================================================================================================       
obs      endog     fitted     Cook's   student.   hat diag    dffits   ext.stud.     dffits                           
value          d   residual              internal   residual
--------------------------------------------------------------------------------------------------         
0  13876.000  15465.313      0.104     -1.683      0.155     -0.722     -1.723     -0.739         
1  11608.000  11577.992      0.000      0.031      0.130      0.012      0.031      0.012         
2  18701.000  18461.523      0.001      0.247      0.109      0.086      0.244      0.085         
3  11283.000  11725.817      0.005     -0.458      0.113     -0.163     -0.453     -0.162         
4  11767.000  11577.992      0.001      0.197      0.130      0.076      0.195      0.075         
5  20872.000  19155.532      0.092      1.787      0.126      0.678      1.838      0.698         
6  11772.000  12272.001      0.006     -0.513      0.101     -0.172     -0.509     -0.170         
7  10535.000   9127.966      0.056      1.457      0.116      0.529      1.478      0.537         
8  12195.000  12124.176      0.000      0.074      0.123      0.028      0.073      0.027         
9  12313.000  12818.185      0.005     -0.516      0.091     -0.163     -0.511     -0.161        
10  14975.000  16557.681      0.084     -1.655      0.134     -0.650     -1.692     -0.664        
11  21371.000  19701.716      0.078      1.728      0.116      0.624      1.772      0.640        
12  19800.000  19553.891      0.001      0.252      0.096      0.082      0.249      0.081        
13  11417.000  10220.334      0.033      1.227      0.098      0.405      1.234      0.408        
14  20263.000  20100.075      0.001      0.166      0.093      0.053      0.165      0.053        
15  13231.000  13216.544      0.000      0.015      0.114      0.005      0.015      0.005        
16  12884.000  13364.369      0.004     -0.488      0.082     -0.146     -0.483     -0.145        
17  13245.000  13910.553      0.007     -0.674      0.075     -0.192     -0.669     -0.191        
18  13677.000  13762.728      0.000     -0.089      0.113     -0.032     -0.087     -0.031        
19  15965.000  17650.049      0.082     -1.747      0.119     -0.642     -1.794     -0.659        
20  12336.000  11312.702      0.021      1.043      0.087      0.323      1.044      0.323        
21  21352.000  21192.443      0.001      0.163      0.091      0.052      0.161      0.051        
22  13839.000  14456.737      0.006     -0.624      0.070     -0.171     -0.619     -0.170        
23  22884.000  21340.268      0.052      1.579      0.095      0.511      1.610      0.521        
24  16978.000  18742.417      0.083     -1.822      0.111     -0.644     -1.877     -0.664        
25  14803.000  15549.105      0.008     -0.751      0.065     -0.199     -0.747     -0.198        
26  17404.000  19288.601      0.093     -1.944      0.110     -0.684     -2.016     -0.709        
27  22184.000  22284.811      0.000     -0.103      0.096     -0.034     -0.102     -0.033        
28  13548.000  12405.070      0.025      1.162      0.083      0.350      1.167      0.352        
29  14467.000  13497.438      0.018      0.987      0.086      0.304      0.987      0.304        
30  15942.000  16641.473      0.007     -0.705      0.068     -0.190     -0.701     -0.189        
31  23174.000  23377.179      0.001     -0.209      0.108     -0.073     -0.207     -0.072        
32  23780.000  23525.004      0.001      0.260      0.092      0.083      0.257      0.082        
33  25410.000  24071.188      0.040      1.370      0.096      0.446      1.386      0.451        
34  14861.000  14043.622      0.014      0.834      0.091      0.263      0.831      0.262        
35  16882.000  17733.841      0.012     -0.863      0.077     -0.249     -0.860     -0.249        
36  24170.000  24469.547      0.003     -0.312      0.127     -0.119     -0.309     -0.118        
37  15990.000  15135.990      0.018      0.878      0.104      0.300      0.876      0.299        
38  26330.000  25163.556      0.035      1.202      0.109      0.420      1.209      0.422        
39  17949.000  18826.209      0.017     -0.897      0.093     -0.288     -0.895     -0.287        
40  25685.000  26108.099      0.008     -0.452      0.169     -0.204     -0.447     -0.202        
41  27837.000  26802.108      0.039      1.087      0.141      0.440      1.089      0.441        
42  18838.000  19918.577      0.033     -1.119      0.117     -0.407     -1.123     -0.408        
43  17483.000  16774.542      0.018      0.743      0.138      0.297      0.739      0.295        
44  19207.000  20464.761      0.052     -1.313      0.131     -0.511     -1.325     -0.515        
45  19346.000  18959.278      0.009      0.423      0.208      0.216      0.419      0.214
==================================================================================================


#or get a dataframe
df_infl = infl.summary_frame()
df_infl[:5]



#plot the reiduals within the groups separately:
resid = lm.resid
plt.figure(figsize=(6,6));
for values, group in factor_groups:    
    i,j = values    
    group_num = i*2 + j - 1  # for plotting purposes    
    x = [group_num] * len(group)    
    plt.scatter(x, resid[group.index], marker=symbols[j], color=colors[i-1],s=144, edgecolors='black')

plt.xlabel('Group');
plt.ylabel('Residuals');


#test some interactions using anova or f_test - a*b = a + b + a:b
interX_lm = ols("S ~ C(E) * X + C(M)", salary_table).fit() #OLS for (y,x) and ols for formula 
>>> print(interX_lm.summary())                            
OLS Regression Results
==============================================================================
Dep. Variable:                      S   R-squared:                       0.961
Model:                            OLS   Adj. R-squared:                  0.955
Method:                 Least Squares   F-statistic:                     158.6
Date:                Sun, 01 Feb 2015   Prob (F-statistic):           8.23e-26
Time:                        09:30:48   Log-Likelihood:                -379.47
No. Observations:                  46   AIC:                             772.9
Df Residuals:                      39   BIC:                             785.7
Df Model:                           6
Covariance Type:            nonrobust
===============================================================================                  
coef    std err          t      P>|t|      [95.0% Conf. Int.]
-------------------------------------------------------------------------------
Intercept    7256.2800    549.494     13.205      0.000      6144.824  8367.736
C(E)[T.2]    4172.5045    674.966      6.182      0.000      2807.256  5537.753
C(E)[T.3]    3946.3649    686.693      5.747      0.000      2557.396  5335.333
C(M)[T.1]    7102.4539    333.442     21.300      0.000      6428.005  7776.903
X             632.2878     53.185     11.888      0.000       524.710   739.865
C(E)[T.2]:X  -125.5147     69.863     -1.797      0.080      -266.826    15.796
C(E)[T.3]:X  -141.2741     89.281     -1.582      0.122      -321.861    39.313
==============================================================================
Omnibus:                        0.432   Durbin-Watson:                   2.179
Prob(Omnibus):                  0.806   Jarque-Bera (JB):                0.590
Skew:                           0.144   Prob(JB):                        0.744
Kurtosis:                       2.526   Cond. No.                         69.7
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


#Do an ANOVA check, to compare models- H0: all models are same 

from statsmodels.stats.api import anova_lm

table1 = anova_lm(lm, interX_lm)
>>> print(table1)
    df_resid    ssr             df_diff         ss_diff      F    Pr(>F)
0        41  43280719.492876        0             NaN       NaN       NaN
1        39  39410679.807560        2  3870039.685316  1.914856    0.160964 #accept H0 

[2 rows x 6 columns]      

#another model 
interM_lm = ols("S ~ X + C(E)*C(M)", data=salary_table).fit()
>>> print(interM_lm.summary())

#Prob < 0.5, so model is significant 
#all coeff are significant (P < 0.05 or there is no zero in conf interval)                    
OLS Regression Results
==============================================================================
Dep. Variable:                      S   R-squared:                       0.999
Model:                            OLS   Adj. R-squared:                  0.999
Method:                 Least Squares   F-statistic:                     5517.
Date:                Sun, 01 Feb 2015   Prob (F-statistic):           1.67e-55
Time:                        09:30:48   Log-Likelihood:                -298.74
No. Observations:                  46   AIC:                             611.5
Df Residuals:                      39   BIC:                             624.3
Df Model:                           6
Covariance Type:            nonrobust
=======================================================================================                          
                       coef        std err      t        P>|t|      [95.0% Conf. Int.]
---------------------------------------------------------------------------------------
Intercept            9472.6854     80.344    117.902      0.000      9310.175  9635.196
C(E)[T.2]            1381.6706     77.319     17.870      0.000      1225.279  1538.063
C(E)[T.3]            1730.7483    105.334     16.431      0.000      1517.690  1943.806
C(M)[T.1]            3981.3769    101.175     39.351      0.000      3776.732  4186.022
C(E)[T.2]:C(M)[T.1]  4902.5231    131.359     37.322      0.000      4636.825  5168.222
C(E)[T.3]:C(M)[T.1]  3066.0351    149.330     20.532      0.000      2763.986  3368.084
X                     496.9870      5.566     89.283      0.000       485.728   508.246
==============================================================================
Omnibus:                       74.761   Durbin-Watson:                   2.244
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1037.873
Skew:                          -4.103   Prob(JB):                    4.25e-226
Kurtosis:                      24.776   Cond. No.                         79.0
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.   

#Do an ANOVA check, to compare models- H0: all models are same 
table2 = anova_lm(lm, interM_lm)
>>> print(table2) #reject H0, interM_lm is better 

    df_resid              ssr  df_diff          ss_diff           F        Pr(>F)
0        41  43280719.492876        0              NaN         NaN           NaN
1        39   1178167.864864        2  42102551.628012  696.844466  3.025504e-31  #reject H0, this model is better

[2 rows x 6 columns]


#The design matrix as a DataFrame
interM_lm.model.data.orig_exog[:5]

#The design matrix as an ndarray
interM_lm.model.exog
interM_lm.model.exog_names

#Get influence of model 
infl = interM_lm.get_influence()
resid = infl.resid_studentized_internal
plt.figure(figsize=(6,6))
for values, group in factor_groups:    
    i,j = values    
    idx = group.index    
    plt.scatter(X[idx], resid[idx], marker=symbols[j], color=colors[i-1], s=144, edgecolors='black')

plt.xlabel('X');
plt.ylabel('standardized resids');


#Looks like one observation is an outlier.
#Drop that observation

drop_idx = abs(resid).argmax()
print(drop_idx)              # zero-based index
idx = salary_table.index.drop(drop_idx)
lm32 = ols('S ~ C(E) + X + C(M)', data=salary_table, subset=idx).fit()
print(lm32.summary())
print('\n')

interX_lm32 = ols('S ~ C(E) * X + C(M)', data=salary_table, subset=idx).fit()
print(interX_lm32.summary())
print('\n')


table3 = anova_lm(lm32, interX_lm32)
print(table3)
print('\n')


interM_lm32 = ols('S ~ X + C(E) * C(M)', data=salary_table, subset=idx).fit()

table4 = anova_lm(lm32, interM_lm32)
print(table4)
print('\n')




#Replot the residuals

try:    
    resid = interM_lm32.get_influence().summary_frame()['standard_resid']
except:    
    resid = interM_lm32.get_influence().summary_frame()['standard_resid']

plt.figure(figsize=(6,6))
for values, group in factor_groups:    
    i,j = values    
    idx = group.index    
    plt.scatter(X[idx], resid[idx], marker=symbols[j], color=colors[i-1], s=144, edgecolors='black')
plt.xlabel('X[~[32]]');
plt.ylabel('standardized resids');



#Plot the fitted values

lm_final = ols('S ~ X + C(E)*C(M)', data = salary_table.drop([drop_idx])).fit()
mf = lm_final.model.data.orig_exog
lstyle = ['-','--']

plt.figure(figsize=(6,6))
for values, group in factor_groups:    
    i,j = values    
    idx = group.index    
    plt.scatter(X[idx], S[idx], marker=symbols[j], color=colors[i-1],s=144, edgecolors='black')    
    # drop NA because there is no idx 32 in the final model    
    plt.plot(mf.X[idx].dropna(), lm_final.fittedvalues[idx].dropna(),ls=lstyle[j], color=colors[i-1])

plt.xlabel('Experience');
plt.ylabel('Salary');




           
###Statsmodel-QuickIntro - sm.tsa
#Time Series analysis tsa
#The module structure is within statsmodels.tsa is
•stattools :    empirical properties and tests, acf, pacf, granger-causality, adf unit root test, kpss test, bds test, ljung-box test and others.
•ar_model :     univariate autoregressive process, estimation with conditional and exact maximum likelihood and conditional least-squares
•arima_model :  univariate ARMA process, estimation with conditional and exact maximum likelihood and conditional least-squares
•vector_ar, var : vector autoregressive process (VAR) estimation models, impulse response analysis, forecast error variance decompositions, and data visualization tools
•kalmanf :      estimation classes for ARMA and other models with exact MLE using Kalman Filter
•arma_process : properties of arma processes with given parameters, this includes tools to convert between ARMA, MA and AR representation as well as acf, pacf, spectral density, impulse response function and similar
•sandbox.tsa.fftarma : similar to arma_process but working in frequency domain
•tsatools :     additional helper functions, to create arrays of lagged variables, construct regressors for trend, detrend and similar.
•filters :      helper function for filtering time series
•regime_switching : Markov switching dynamic regression and autoregression models


#Descriptive Statistics and Tests
stattools.acovf(x[, unbiased, demean, fft, ...])    Autocovariance for 1D 
stattools.acf(x[, unbiased, nlags, qstat, ...])     Autocorrelation function for 1d arrays. 
stattools.pacf(x[, nlags, method, alpha])           Partial autocorrelation estimated 
stattools.pacf_yw(x[, nlags, method])               Partial autocorrelation estimated with non-recursive yule_walker 
stattools.pacf_ols(x[, nlags])                      Calculate partial autocorrelations 
stattools.ccovf(x, y[, unbiased, demean])           crosscovariance for 1D 
stattools.ccf(x, y[, unbiased])                     cross-correlation function for 1d 

stattools.periodogram(X)                            Returns the periodogram for the natural frequency of X 
stattools.adfuller(x[, maxlag, regression, ...])    Augmented Dickey-Fuller unit root test 
stattools.kpss(x[, regression, lags, store])        Kwiatkowski-Phillips-Schmidt-Shin test for stationarity. 
stattools.coint(y0, y1[, trend, method, ...])       Test for no-cointegration of a univariate equation 
stattools.bds(x[, max_dim, epsilon, distance])      Calculate the BDS test statistic for independence of a time series 
stattools.q_stat(x, nobs[, type])                   Return's Ljung-Box Q Statistic 
stattools.grangercausalitytests(x, maxlag[, ...])   four tests for granger non causality of 2 timeseries 
stattools.levinson_durbin(s[, nlags, isacov])       Levinson-Durbin recursion for autoregressive processes 

stattools.arma_order_select_ic(y[, max_ar, ...])    Returns information criteria for many ARMA models 
x13.x13_arima_select_order(endog[, ...])            Perform automatic seaonal ARIMA order identification using x12/x13 ARIMA. 
                                                    X-12-ARIMA was the U.S. Census Bureau's software package for seasonal adjustment
x13.x13_arima_analysis(endog[, maxorder, ...])      Perform x13-arima analysis for monthly or quarterly data. 

#Estimation
#Univariate Autogressive Processes (AR)
ar_model.AR(endog[, dates, freq, missing])  Autoregressive AR(p) model 
ar_model.ARResults(model, params[, ...])    Class to hold results from fitting an AR model. 

#Autogressive Moving-Average Processes (ARMA) and Kalman Filter
arima_model.ARMA(endog, order[, exog, ...])     Autoregressive Moving Average ARMA(p,q) Model 
arima_model.ARMAResults(model, params[, ...])   Class to hold results from fitting an ARMA model. 
arima_model.ARIMA(endog, order[, exog, ...])    Autoregressive Integrated Moving Average ARIMA(p,d,q) Model 
arima_model.ARIMAResults(model, params[, ...]) 

#Vector Autogressive Processes (VAR)
vector_ar.var_model.VAR(endog[, dates, ...])        Fit VAR(p) process and do lag order selection 
vector_ar.var_model.VARResults(endog, ...[, ...])   Estimate VAR(p) process with fixed number of lags 
vector_ar.dynamic.DynamicVAR(data[, ...])           Estimates time-varying vector autoregression (VAR(p)) using 

 
#Vector Autogressive Processes (VAR)
vector_ar.var_model.VAR(endog[, dates, ...])        Fit VAR(p) process and do lag order selection 
vector_ar.var_model.VARProcess(coefs, ...[, ...])   Class represents a known VAR(p) process 
vector_ar.var_model.VARResults(endog, ...[, ...])   Estimate VAR(p) process with fixed number of lags 
vector_ar.irf.IRAnalysis(model[, P, ...])           Impulse response analysis class. 
vector_ar.var_model.FEVD(model[, P, periods])       Compute and plot Forecast error variance decomposition and asymptotic 
vector_ar.dynamic.DynamicVAR(data[, ...])           Estimates time-varying vector autoregression (VAR(p)) using 

 
#Regime switching models
regime_switching.markov_regression.MarkovRegression(...)        First-order k-regime Markov switching regression model 
regime_switching.markov_autoregression.MarkovAutoregression(...) Markov switching regression model 

#ARMA Process
arima_process.ArmaProcess(ar, ma[, nobs])       Represent an ARMA process for given lag-polynomials 
arima_process.ar2arma(ar_des, p, q[, n, ...])   find arma approximation to ar process 
arima_process.arma2ar(ar, ma[, nobs])           get the AR representation of an ARMA process 
arima_process.arma2ma(ar, ma[, nobs])           get the impulse response function (MA representation) for ARMA process 
arima_process.arma_acf(ar, ma[, nobs])          theoretical autocorrelation function of an ARMA process 
arima_process.arma_acovf(ar, ma[, nobs])        theoretical autocovariance function of ARMA process 
arima_process.arma_generate_sample(ar, ma, ...) Generate a random sample of an ARMA process 
arima_process.arma_impulse_response(ar, ma)     get the impulse response function (MA representation) for ARMA process 
arima_process.arma_pacf(ar, ma[, nobs])         partial autocorrelation function of an ARMA process 
arima_process.arma_periodogram(ar, ma[, ...])   periodogram for ARMA process given by lag-polynomials ar and ma 
arima_process.deconvolve(num, den[, n])         Deconvolves divisor out of signal, division of polynomials for n terms 
arima_process.index2lpol(coeffs, index)         expand coefficients to lag poly 
arima_process.lpol2index(ar)                    remove zeros from lagpolynomial, squeezed representation with index 
arima_process.lpol_fiar(d[, n])                 AR representation of fractional integration 
arima_process.lpol_fima(d[, n])                 MA representation of fractional integration 
arima_process.lpol_sdiff(s)                     return coefficients for seasonal difference (1-L^s) 
sandbox.tsa.fftarma.ArmaFft(ar, ma, n)          fft tools for arma processes 

#Time Series Filters
filters.bk_filter.bkfilter(X[, low, high, K])       Baxter-King bandpass filter 
filters.hp_filter.hpfilter(X[, lamb])               Hodrick-Prescott filter 
filters.cf_filter.cffilter(X[, low, high, drift])   Christiano Fitzgerald asymmetric, random walk filter 
filters.filtertools.convolution_filter(x, filt)     Linear filtering via convolution. 
filters.filtertools.recursive_filter(x, ar_coeff)   Autoregressive, or recursive, filtering. 
filters.filtertools.miso_lfilter(ar, ma, x)         use nd convolution to merge inputs, 
filters.filtertools.fftconvolve3(in1[, in2, ...])   Convolve two N-dimensional arrays using FFT. 
filters.filtertools.fftconvolveinv(in1, in2)        Convolve two N-dimensional arrays using FFT. 
seasonal.seasonal_decompose(x[, model, ...])        Seasonal decomposition using moving averages 

#TSA Tools
tsatools.add_trend(x[, trend, prepend, ...])        Adds a trend and/or constant to an array. 
tsatools.detrend(x[, order, axis])                  Detrend an array with a trend of given order along axis 0 or 1 
tsatools.lagmat(x, maxlag[, trim, original, ...])   Create 2d array of lags 
tsatools.lagmat2ds(x, maxlag0[, maxlagex, ...])     Generate lagmatrix for 2d array, columns arranged by variables 

#VARMA Process
varma_process.VarmaPoly(ar[, ma])           class to keep track of Varma polynomial format 

#Interpolation
interp.denton.dentonm(indicator, benchmark) Modified Denton's method to convert low-frequency to high-frequency data. 

#Unit Root Tests - bad for time series data 
statsmodels.stats.diagnostic.unitroot_adf(x, maxlag=None, trendorder=0, autolag='AIC', store=False)
statsmodels.tsa.stattools.adfuller(x, maxlag=None, regression='c', autolag='AIC', store=False, regresults=False)    
        The Augmented Dickey-Fuller test can be used to test for a unit root in a univariate process in the presence of serial correlation.    
        H0:there is a unit root    
        returns (,p-value, , , , , )
        
#Time Series Plots
tsaplots.plot_acf(x[, ax, lags, alpha, ...])    Plot the autocorrelation function 
tsaplots.plot_pacf(x[, ax, lags, alpha, ...])   Plot the partial autocorrelation function 
tsaplots.month_plot(x[, dates, ylabel, ax])     Seasonal plot of monthly data 
tsaplots.quarter_plot(x[, dates, ylabel, ax])   Seasonal plot of quarterly data 

##ACF(auto corelation) and PACF(partial corelation) plot 

#ACF plot:  a bar chart of the coefficients of correlation between a time series and lags of itself
#Note that corelation at lag-k propagates to lag-k+1 and higher lags  

#PACF plot : plot of the partial correlation coefficients between the series and lags of itself. 
#The partial autocorrelation at lag k is the difference between the actual correlation at lag k 
#and the expected correlation due to the propagation of correlation at lag k-1. 

#For example if the PACF plot has a significant spike only at lag 1, 
#meaning that all the higher-order autocorrelations are effectively explained 
#by the lag-1 autocorrelation


#CHECK https://onlinecourses.science.psu.edu/stat510
##Detecting
Detecting seasonality
    Seasonality (or periodicity) can usually be assessed from an autocorrelation plot
    seasonality in a timeseries refers to predictable and recurring trends and patterns over a period of time
    ACF shows an oscillation(with period S), indicative of a seasonal series
    Seasonal differencing is defined as a difference between a value 
    and a value with lag that is a multiple of S.
    Note, do this at first to get a series , then do Non-seasonal differencing if required to make it stationary
    check https://onlinecourses.science.psu.edu/stat510/node/67
    
Detecting stationarity
      Stationarity can be assessed from a run sequence plot. 
      The run sequence plot should show constant location(mean) and scale(not increasing/decreasing mean)
      It can also be detected from an autocorrelation plot. 
      Specifically, non-stationarity is often indicated by an autocorrelation plot with very slow decay.
      Take first difference or more to get the data stationary(ie d= Non-seasonal differencing)

      
      
##Autoregressive Moving Average (ARMA, (p,q)
#p is the order of the autoregressive part, linear combination of past time series values
#q is the order of the moving average part, linear combination of the past white noise terms.
#white noise is a random signal having equal intensity at different frequencies, giving it a constant power spectral density

#Finding appropriate values of p and q in the ARMA(p,q) model can be facilitated 
#by plotting the partial autocorrelation functions for an estimate of p, 
#and likewise using the autocorrelation functions for an estimate of q. 

#Note , Autoregressive integrated moving average generalization of ARMA 
#ARIMA(p,d,q), d is the degree of differencing (the number of times the data have had past values subtracted), 
#here d is Non-seasonal differencing 

#Plot autocorrelation function(acf) for model identification.
#Shape                                   
#        Indicated Model
Exponential, decaying to zero           
        Autoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model. 
Alternating positive and negative, decaying to zero
        Autoregressive model. Use the partial autocorrelation plot to help identify the order. 
One or more spikes, rest are essentially zero
        Moving average model, order identified by where plot becomes zero. 
Decay, starting after a few lags
        Mixed autoregressive and moving average (ARMA) model. 
All zero or close to zero
        Data are essentially random. 
High values at fixed intervals
        Include seasonal autoregressive term. 
No decay to zero
        Series is not stationary. 
        
      
Specifically, for an AR(1) process, 
the sample autocorrelation function should have an exponentially 
decreasing appearance. 
However, higher-order AR processes are often a mixture of 
exponentially decreasing and damped sinusoidal components.

For higher-order autoregressive processes, 
the partial autocorrelation of an AR(p) process becomes zero at lag p + 1 
and greater, 
This is usually determined by placing a 95% confidence interval on the sample partial autocorrelation plot 
If the software program does not generate the confidence band, 
it is approximately ± 2 /sqrt(N), with N denoting the sample size.

The autocorrelation function of a MA(q) process becomes zero at lag q + 1 
and greater, by placing the 95% confidence interval 
for the sample autocorrelation function on the sample autocorrelation plot. 

#Another option for finding (p,q)
Note create ARMA(1,1); ARMA(1,2), … , ARMA(3,3), … , ARMA(5,5). 
To compare these 25 models, use the AIC, lowest AIC gives p, and q 
Use stattools.arma_order_select_ic, x13.x13_arima_select_order


The data may follow an ARIMA(p,d,0) model 
if the ACF and PACF plots of the differenced data show the following patterns: 
    the ACF is exponentially decaying or sinusoidal;
    there is a significant spike at lag p in PACF, but none beyond lag p.
The data may follow an ARIMA(0,d,q) model 
if the ACF and PACF plots of the differenced data show the following patterns: 
    the PACF is exponentially decaying or sinusoidal;
    there is a significant spike at lag q in ACF, but none beyond lag q.

    
#acf
statsmodels.tsa.stattools.acf(x, unbiased=False, nlags=40, qstat=False, fft=False, alpha=None, missing='none')[source]
    Autocorrelation function for 1d arrays.
    Parameters:
    x : array
        Time series data
    unbiased : bool
        If True, then denominators for autocovariance are n-k, otherwise n
    nlags: int, optional
        Number of lags to return autocorrelation for.
    qstat : bool, optional
        If True, returns the Ljung-Box q statistic for each autocorrelation coefficient. 
    fft : bool, optional
        If True, computes the ACF via FFT.
    alpha : scalar, optional
        If a number is given, the confidence intervals for the given level are returned. For instance if alpha=.05, 95 % confidence intervals are returned where the standard deviation is computed according to Bartlett's formula.
    missing : str, optional
        A string in ['none', 'raise', 'conservative', 'drop'] specifying how the NaNs are to be treated.
    Returns:
    acf : array
        autocorrelation function
    confint : array, optional
        Confidence intervals for the ACF. Returned if confint is not None.
    qstat : array, optional
        The Ljung-Box Q-Statistic. Returned if q_stat is True.
    pvalues : array, optional
        The p-values associated with the Q-statistics. Returned if q_stat is True.
        H0: The data are independently distributed 
            i.e. the correlations is 0
   
##Example with AR(p) 

from __future__ import print_function
import statsmodels.api as sm
import numpy as np
import pandas as pd

data = sm.datasets.sunspots.load()

from datetime import datetime
dates = sm.tsa.datetools.dates_from_range('1700', length=len(data.endog))
endog = pd.Series(data.endog, index=dates)

#AR(p)
ar_model = sm.tsa.AR(endog, freq='A')
pandas_ar_res = ar_model.fit(maxlag=9, method='mle', disp=-1)

#Out-of-sample prediction
pred = pandas_ar_res.predict(start='2005', end='2015')
>>> print(pred)
2005-12-31    20.003298
2006-12-31    24.703996
2007-12-31    20.026133
2008-12-31    23.473641
2009-12-31    30.858566
                ...    
2011-12-31    87.024635
2012-12-31    91.321196
2013-12-31    79.921585
2014-12-31    60.799495
2015-12-31    40.374852
Freq: A-DEC, dtype: float64

#Using explicit dates
ar_model = sm.tsa.AR(data.endog, dates=dates, freq='A')
ar_res = ar_model.fit(maxlag=9, method='mle', disp=-1)
pred = ar_res.predict(start='2005', end='2015')
>>> print(pred)
[ 20.00329845  24.70399631  20.02613267  23.47364059  30.8585664
  61.33541408  87.02463499  91.32119576  79.92158511  60.79949541
  40.37485169]

>>> print(ar_res.data.predict_dates)
DatetimeIndex(['2005-12-31', '2006-12-31', '2007-12-31', '2008-12-31',
               '2009-12-31', '2010-12-31', '2011-12-31', '2012-12-31',
               '2013-12-31', '2014-12-31', '2015-12-31'],
              dtype='datetime64[ns]', freq='A-DEC')

   
##Example with ARMA(p,q)
%matplotlib inline
from __future__ import print_function
import numpy as np
from scipy import stats
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

from statsmodels.graphics.api import qqplot

print(sm.datasets.sunspots.NOTE)

dta = sm.datasets.sunspots.load_pandas().data

dta.index = pd.Index(sm.tsa.datetools.dates_from_range('1700', '2008'))
del dta["YEAR"]

dta.plot(figsize=(12,8));
 
#plot acf, partial acf 
fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40, ax=ax1)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(dta, lags=40, ax=ax2)
 
#p=2, q=0 
arma_mod20 = sm.tsa.ARMA(dta, (2,0)).fit(disp=False)
>>> print(arma_mod20.params)
const                49.659542
ar.L1.SUNACTIVITY     1.390656
ar.L2.SUNACTIVITY    -0.688571
dtype: float64

#p=3, q=0 
arma_mod30 = sm.tsa.ARMA(dta, (3,0)).fit(disp=False)
>>> print(arma_mod20.aic, arma_mod20.bic, arma_mod20.hqic)
2622.636338065809 2637.56970317 2628.60672591

>>> print(arma_mod30.params)
const                49.749936
ar.L1.SUNACTIVITY     1.300810
ar.L2.SUNACTIVITY    -0.508093
ar.L3.SUNACTIVITY    -0.129650
dtype: float64

>>> print(arma_mod30.aic, arma_mod30.bic, arma_mod30.hqic)
2619.4036286964474 2638.07033508 2626.8666135

#•Does our model obey the theory?
>>> sm.stats.durbin_watson(arma_mod30.resid.values)
1.9564807635787604

#residual plot 
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111)
ax = arma_mod30.resid.plot(ax=ax);
 
#residula test 
resid = arma_mod30.resid
>>> stats.normaltest(resid)
NormaltestResult(statistic=49.845019661107585, pvalue=1.5006917858823576e-11)
#qqplot 
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111)
fig = qqplot(resid, line='q', ax=ax, fit=True)
 
#plot acf, pcf of residual 
fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(resid.values.squeeze(), lags=40, ax=ax1)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(resid, lags=40, ax=ax2)
 
#get ACF 
r,q,p = sm.tsa.acf(resid.values.squeeze(), qstat=True)
data = np.c_[range(1,41), r[1:], q, p]
table = pd.DataFrame(data, columns=['lag', "AC", "Q", "Prob(>Q)"])
>>> print(table.set_index('lag')) #H0: The data are independently distributed
            AC          Q  Prob(>Q)
lag                                
1.0   0.009179   0.026287  0.871202
2.0   0.041793   0.573041  0.750872
3.0  -0.001335   0.573600  0.902448
4.0   0.136089   6.408927  0.170620
5.0   0.092468   9.111841  0.104685
...        ...        ...       ...
36.0 -0.119329  91.248909  0.000001
37.0 -0.036665  91.723876  0.000002
38.0 -0.046193  92.480525  0.000002  <-- reject H0
39.0 -0.017768  92.592893  0.000003
40.0 -0.006220  92.606716  0.000005
[40 rows x 3 columns]

#•This indicates a lack of fit.
#•In-sample dynamic prediction. How good does our model do?
predict_sunspots = arma_mod30.predict('1990', '2012', dynamic=True)
>>> print(predict_sunspots)
1990-12-31    167.047417
1991-12-31    140.993002
1992-12-31     94.859112
1993-12-31     46.860896
1994-12-31     11.242577
                 ...    
2008-12-31     41.963810
2009-12-31     46.869285
2010-12-31     51.423261
2011-12-31     54.399720
2012-12-31     55.321692
Freq: A-DEC, dtype: float64

#plot predict 
fig, ax = plt.subplots(figsize=(12, 8))
ax = dta.ix['1950':].plot(ax=ax)
fig = arma_mod30.plot_predict('1990', '2012', dynamic=True, ax=ax, plot_insample=False)
 

def mean_forecast_err(y, yhat):
    return y.sub(yhat).mean()

>>> mean_forecast_err(dta.SUNACTIVITY, predict_sunspots)
5.6369602158434047

#Exercise: Can you obtain a better fit for the Sunspots model? 
#(Hint: sm.tsa.AR has a method select_order)





 
 
 



###statsmodel  - sm.tsa - Generate Simulated ARMA process
##ARMA Process
arima_process.ArmaProcess(ar, ma[, nobs])       Represent an ARMA process for given lag-polynomials 

arima_process.ar2arma(ar_des, p, q[, n, ...])   find arma approximation to ar process 
arima_process.arma2ar(ar, ma[, nobs])           get the AR representation of an ARMA process 
arima_process.arma2ma(ar, ma[, nobs])           get the impulse response function (MA representation) for ARMA process 

arima_process.arma_acf(ar, ma[, nobs])          theoretical autocorrelation function of an ARMA process 
arima_process.arma_acovf(ar, ma[, nobs])        theoretical autocovariance function of ARMA process 

arima_process.arma_generate_sample(ar, ma, ...) Generate a random sample of an ARMA process 
arima_process.arma_impulse_response(ar, ma)     get the impulse response function (MA representation) for ARMA process 
arima_process.arma_pacf(ar, ma[, nobs])         partial autocorrelation function of an ARMA process 
arima_process.arma_periodogram(ar, ma[, ...])   periodogram for ARMA process given by lag-polynomials ar and ma 

arima_process.deconvolve(num, den[, n])         Deconvolves divisor out of signal, division of polynomials for n terms 
arima_process.index2lpol(coeffs, index)         expand coefficients to lag poly 
arima_process.lpol2index(ar)                    remove zeros from lagpolynomial, squeezed representation with index 
arima_process.lpol_fiar(d[, n])                 AR representation of fractional integration 
arima_process.lpol_fima(d[, n])                 MA representation of fractional integration 
arima_process.lpol_sdiff(s)                     return coefficients for seasonal difference (1-L^s) 
sandbox.tsa.fftarma.ArmaFft(ar, ma, n)          fft tools for arma processes 


class statsmodels.tsa.arima_process.ArmaProcess(ar, ma, nobs=100)
    Represent an ARMA process for given lag-polynomials
    It does not do any estimation or statistical analysis.
    both the AR and MA components should include the coefficient on the zero-lag. 
    This is typically 1. 
    Further, due to the conventions used in signal processing 
    used in signal.lfilter  vs. conventions in statistics for ARMA processes, 
    the AR paramters should have the opposite sign of what you might expect
    ar : array_like, 1d
        Coefficient for autoregressive lag polynomial, including zero lag. 
    ma : array_like, 1d
        Coefficient for moving-average lag polynomial, including zero lag
    nobs : int, optional
        Length of simulated time series. 
    #Methods 
    acf([nobs])                             theoretical autocorrelation function of an ARMA process 
    acovf([nobs])                           theoretical autocovariance function of ARMA process 
    arma2ar([nobs])  
    arma2ma([nobs])  
    from_coeffs(arcoefs, macoefs[, nobs])   Create ArmaProcess instance from coefficients of the lag-polynomials 
    from_estimation(model_results[, nobs])  Create ArmaProcess instance from ARMA estimation results 
    generate_sample([nsample, scale, distrvs, ...]) generate ARMA samples 
    impulse_response([nobs])                        get the impulse response function (MA representation) for ARMA process 
    invertroots([retnew])                           make MA polynomial invertible by inverting roots inside unit circle 
    pacf([nobs])                                    partial autocorrelation function of an ARMA process 
    periodogram([nobs])                             periodogram for ARMA process given by lag-polynomials ar and ma 
    #Attributes
    arroots                     Roots of autoregressive lag-polynomial 
    isinvertible                Arma process is invertible if MA roots are outside unit circle 
    isstationary                Arma process is stationary if AR roots are outside unit circle 
    maroots                     Roots of moving average lag-polynomial 



#generate Sample 
ArmaProcessInstance.generate_sample([nsample, scale, distrvs, ...]) 
    generate ARMA samples 
ArmaProcess.generate_sample(nsample=100, scale=1.0, distrvs=None, axis=0, burnin=0)
    generate ARMA samples
    nsample : int or tuple of ints
              If nsample is an integer, then this creates a 1d timeseries of length size. 
              If nsample is a tuple, then the timeseries is along axis(axis=0, means columnwise)
              All other axis have independent arma samples.
    scale : float, standard deviation of noise
    distrvs : function, random number generator,default: np.random.randn 
    burnin : integer (default: 0)
        to reduce the effect of initial conditions, 
        burnin observations at the beginning of the sample are dropped

#Example - Simulated ARMA(4,1)
from statsmodels.tsa.arima_process import arma_generate_sample, ArmaProcess


np.random.seed(1234)
# include zero-th lag
arparams = np.array([1, .75, -.65, -.55, .9]) #p =4, means five coeffcients inc const term 
maparams = np.array([1, .65])   #q = 1, means 2 terms including constant 


# Let's make sure this model is estimable.
arma_t = ArmaProcess(arparams, maparams)
arma_t.isinvertible
arma_t.isstationary


# * What does this mean?

fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111)
ax.plot(arma_t.generate_sample(nsample=50));
plt.show()

#For analysis purpose 
import pandas as pd
dates = sm.tsa.datetools.dates_from_range('1980m1', length=nobs)
y = pd.TimeSeries(arma_t, index=dates)
arma_mod = sm.tsa.ARMA(y, order=(2,2)) #For analysis purpose 
arma_res = arma_mod.fit(trend='nc', disp=-1)


#with another value 
arparams = np.array([1, .35, -.15, .55, .1])
maparams = np.array([1, .65])
arma_t = ArmaProcess(arparams, maparams)
arma_t.isstationary


arma_rvs = arma_t.generate_sample(nsample=500, burnin=250, scale=2.5)

#Plot PACF and ACF 
#For mixed ARMA processes the Autocorrelation function is a mixture of exponentials 
#and damped sine waves after (q-p) lags.
#The partial autocorrelation function is a mixture of exponentials 
#and dampened sine waves after (p-q) lags.

fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(arma_rvs, lags=40, ax=ax1)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(arma_rvs, lags=40, ax=ax2)
plt.show()


#For analysis purpose 
arma11 = sm.tsa.ARMA(arma_rvs, (1,1)).fit()
resid = arma11.resid

#q The Ljung-Box Q-Statistic
#H0: any of a group of autocorrelations of a time series are zero, values are random and independent over time.
#Q-Statistic's p value
r,q,p = sm.tsa.acf(resid, qstat=True)
data = np.c_[range(1,41), r[1:], q, p]
table = pd.DataFrame(data, columns=['lag', "AC", "Q", "Prob(>Q)"])
>>> print(table.set_index('lag')) #<0.05, reject H0, not good fit 
            AC           Q      Prob(>Q)
lag                                     
1.0   0.254921   32.687669  1.082216e-08
2.0  -0.172416   47.670733  4.450737e-11
3.0  -0.420945  137.159383  1.548473e-29
4.0  -0.046875  138.271291  6.617736e-29
5.0   0.103240  143.675896  2.958739e-29
...        ...         ...           ...
36.0  0.142724  231.734107  1.923091e-30
37.0  0.095768  236.706149  5.937808e-31
38.0 -0.084744  240.607793  2.890898e-31
39.0 -0.150126  252.878971  3.963021e-33
40.0 -0.083767  256.707729  1.996181e-33
[40 rows x 3 columns]


#2nd Try 
arma41 = sm.tsa.ARMA(arma_rvs, (4,1)).fit()
resid = arma41.resid
r,q,p = sm.tsa.acf(resid, qstat=True)
data = np.c_[range(1,41), r[1:], q, p]
table = pd.DataFrame(data, columns=['lag', "AC", "Q", "Prob(>Q)"])
>>> print(table.set_index('lag'))#H0: The data are independently distributed
            AC          Q  Prob(>Q)
lag                                
1.0  -0.007889   0.031302  0.859569
2.0   0.004132   0.039906  0.980245
3.0   0.018103   0.205415  0.976710
4.0  -0.006760   0.228538  0.993948
5.0   0.018120   0.395024  0.995466
...        ...        ...       ...
36.0  0.041271  21.358847  0.974774
37.0  0.078704  24.716879  0.938948
38.0 -0.029729  25.197056  0.944895
39.0 -0.078397  28.543388  0.891179
40.0 -0.014466  28.657578  0.909268
[40 rows x 3 columns]


###statsmodel - sm.tsa - Datetime utils - sm.tsa.datetools.

def date_parser(timestr, parserinfo=None, **kwargs):
    Uses dateutil.parser.parse, but also handles monthly dates of the form
    1999m4, 1999:m4, 1999:mIV, 1999mIV and the same for quarterly data
    with q instead of m. It is not case sensitive. The default for annual
    data is the end of the year, which also differs from dateutil.
def date_range_str(start, end=None, length=None):
    Returns a list of abbreviated date strings.
    Parameters
    ----------
    start : str
        The first abbreviated date, for instance, '1965q1' or '1965m1'
    end : str, optional
        The last abbreviated date if length is None.
    length : int, optional
        The length of the returned array of end is None.
    Returns
    -------
    date_range : list
        List of strings
def dates_from_str(dates):
    Turns a sequence of date strings and returns a list of datetime.
    Parameters
    ----------
    dates : array-like
        A sequence of abbreviated dates as string. For instance,
        '1996m1' or '1996Q1'. The datetime dates are at the end of the
        period.
    Returns
    -------
    date_list : array
        A list of datetime types.  
def dates_from_range(start, end=None, length=None):
    Turns a sequence of date strings and returns a list of datetime.
    Parameters
    ----------
    start : str
        The first abbreviated date, for instance, '1965q1' or '1965m1'
    end : str, optional
        The last abbreviated date if length is None.
    length : int, optional
        The length of the returned array of end is None.
    Examples
    --------
    >>> import statsmodels.api as sm
    >>> dates = sm.tsa.datetools.date_range('1960m1', length=nobs)
    Returns
    -------
    date_list : array
        A list of datetime types.
    
dateutil.parser.parse(timestr, parserinfo=None, **kwargs)[source] 
    From module dateutil 
    Parse a string in one of the supported formats, using the parserinfo parameters.
    Parameters:
        •timestr – A string containing a date/time stamp.
        •parserinfo – A parserinfo object containing parameters for the parser. 
            If None, the default arguments to the parserinfo constructor are used.
    The **kwargs parameter takes the few below keyword arguments:
        •dayfirst – Whether to interpret the first value in an ambiguous 3-integer date 
                (e.g. 01/05/09) as the day (True) or month (False). 
                If yearfirst is set to True, this distinguishes between YDM and YMD. 
                If set to None, this value is retrieved from the current parserinfo object 
                (which itself defaults to False).
        •yearfirst – Whether to interpret the first value in an ambiguous 3-integer date 
                (e.g. 01/05/09) as the year. 
                If True, the first number is taken to be the year, 
                otherwise the last number is taken to be the year. 
                If this is set to None, the value is retrieved from the current parserinfo object 
                (which itself defaults to False).
        •fuzzy – Whether to allow fuzzy parsing, allowing for string like 'Today is January 1, 2047 at 8:21:00AM'.
        •fuzzy_with_tokens – 
        If True, fuzzy is automatically set to True, and the parser will return a tuple where the first element is the parsed datetime.datetime datetimestamp and the second element is a tuple containing the portions of the string which were ignored:
        >>> from dateutil.parser import parse
        >>> parse("Today is January 1, 2047 at 8:21:00AM", fuzzy_with_tokens=True)
        (datetime.datetime(2047, 1, 1, 8, 21), (u'Today is ', u' ', u'at '))

 
   



###statsmodel  -  sm.tsa. - selection of orders  


## selection of ARMA or ARIMA orders 
stattools.arma_order_select_ic(y[, max_ar, ...])    Returns information criteria for many ARMA models 
x13.x13_arima_select_order(endog[, ...])            Perform automatic seaonal ARIMA order identification using x12/x13 ARIMA. 
x13.x13_arima_analysis(endog[, maxorder, ...])      Perform x13-arima analysis for monthly or quarterly data. 
ar_model.AR.select_order(maxlag, ic, trend='c', method='mle') finds the AR order 

#ARIMA with seasonal component ie (p,d,q)x(P,D,Q) 
#for seasonal is (P,D,Q)
x13_arima_select_order(endog, maxorder=(2, 1), maxdiff=(2, 1), diff=None, 
                exog=None, log=None, outlier=True, trading=False, forecast_years=None, start=None, freq=None, print_stdout=False, x12path=None, prefer_x13=True)
    Perform automatic seaonal ARIMA order identification using x12/x13 ARIMA
    endog : array-like, pandas.Series
        The series to model. 
        It is best to use a pandas object with a DatetimeIndex or PeriodIndex. 
        However, you can pass an array-like object. 
        If your object does not have a dates index then start and freq are not optional.
    maxorder : tuple
        The maximum order of the regular and seasonal ARMA polynomials to examine during the model identification. 
        The order for the regular polynomial must be greater than zero and no larger than 4. 
        The order for the seasonal polynomial may be 1 or 2.
    maxdiff : tuple
        The maximum orders for regular and seasonal differencing in the automatic differencing procedure. 
        Acceptable inputs for regular differencing are 1 and 2. 
        The maximum order for seasonal differencing is 1. 
        If diff is specified then maxdiff should be None
    Returns:results : Bunch
        A bunch object that has the following attributes:
        •order : tuple The regular order
        •sorder : tuple The seasonal order
        •include_mean : bool Whether to include a mean or not
        •results : str The full results from the X12/X13 analysis
        •stdout : str The captured stdout from the X12/X13 analysis
 
 
x13_arima_analysis(endog, maxorder=(2, 1), maxdiff=(2, 1), 
                diff=None, exog=None, log=None, outlier=True, trading=False, forecast_years=None, retspec=False, speconly=False, start=None, freq=None, print_stdout=False, x12path=None, prefer_x13=True)
    Perform x13-arima analysis for monthly or quarterly data.
    Returns:res : Bunch
        A bunch object with the following attributes:
        •results : str The full output from the X12/X13 run.
        •seasadj : pandas.Series The final seasonally adjusted endog
        •trend : pandas.Series The trend-cycle component of endog
        •irregular : pandas.Series The final irregular component of endog
        •stdout : str The captured stdout produced by x12/x13.
        •spec : str, optional Returned if retspec is True. 
                The only thing returned if speconly is True.
         
     
#AR(p)
#Create a instance at first 
class statsmodels.tsa.ar_model.AR(endog, dates=None, freq=None, missing='none')
    select_order(maxlag, ic, trend='c', method='mle')
        Select the lag order according to the information criterion.
        maxlag : int, The highest lag length tried. 
        ic : str {'aic','bic','hqic','t-stat'},
            Criterion used for selecting the optimal lag length.
        trend : str {'c','nc'}
            Whether to include a constant or not. 
            'c' - include constant. 
            'nc' - no constant.
        Returns: bestlag : int,Best lag according to IC.
    
#ARMA(p,q)
statsmodels.tsa.stattools.arma_order_select_ic(y, max_ar=4, max_ma=2, ic='bic', trend='c', model_kw={}, fit_kw={})
    y : array-like
        Time-series data
    max_ar : int
        Maximum number of AR lags to use. Default 4.
    max_ma : int
        Maximum number of MA lags to use. Default 2.
    ic : str, list
        Information criteria to report. Either a single string or a list of different criteria is possible.
    Returns: Results object
         Each ic is an attribute with a DataFrame for the results.
         The AR order used is the row index. 
         The ma order used is the column index. 
         The minimum orders are available as ic_min_order.
 
 
#Example 
from statsmodels.tsa.arima_process import arma_generate_sample
import statsmodels.api as sm
import numpy as np

#ARMA(2,2)
arparams = np.array([.75, -.25]) #p=2 
maparams = np.array([.65, .35]) #q = 2  
arparams = np.r_[1, -arparams] #+ const term 
maparam = np.r_[1, maparams] #+ const term
nobs = 250
np.random.seed(2014)
y = arma_generate_sample(arparams, maparams, nobs)
res = sm.tsa.arma_order_select_ic(y, ic=['aic', 'bic'], trend='nc')
res.aic_min_order #(p,q) = (1,2)
res.bic_min_order #(1,2)
>>> res
{'aic': 
            0           1           2
0         NaN  552.734225  484.296878
1  562.109243  485.519797  480.328585
2  507.045813  482.910658  481.919260
3  484.039960  482.148680  483.863790
4  481.884948  483.837738  485.837566, 
'aic_min_order': (1, 2), 
'bic':
            0           1           2
0         NaN  559.777147  494.861261
1  569.152164  496.084180  494.414429
2  517.610196  496.996502  499.526565
3  498.125803  499.755985  504.992555
4  499.492252  504.966503  510.487793, 
'bic_min_order': (1, 2)}

#x13
#For analysis purpose 
import pandas as pd
dates = sm.tsa.datetools.dates_from_range('1980m1', length=nobs)
df = pd.DataFrame(y, index=dates)
#Requires x13as.exe in PATH 
#download from https://www.census.gov/srd/www/winx13/winx13_down.html
res = statsmodels.tsa.x13.x13_arima_select_order(df)
>>> res.order
(0, 0, 2)
>>> res.sorder
(0, 0, 0)






###statsmodel  - sm.tsa  - Decomposing trend and seasonality 
statsmodels.tsa.seasonal.seasonal_decompose(x, model='additive', filt=None, freq=None, two_sided=True)[source]
    Seasonal decomposition using moving averages
    The additive model is Y[t] = T[t] + S[t] + e[t]
    The multiplicative model is Y[t] = T[t] * S[t] * e[t]
    Parameters:
    x : array-like
        Time series
    model : str {'additive', 'multiplicative'}
        Type of seasonal component. Abbreviations are accepted.
    filt : array-like
        The filter coefficients for filtering out the seasonal component. 
        The concrete moving average method used in filtering is determined by two_sided.
    freq : int, optional
        Frequency of the series. Must be used if x is not a pandas object. 
        Overrides default periodicity of x 
        if x is a pandas object with a timeseries index.
            #freqstr      Seasonal period    
                 # of datapoints for aggregation
            A              1                 aggregate yearly 
            Q              4                 aggregate yearly 
            M              12                aggregate yearly
            W              52                aggregate yearly
            D              7                 aggregate weekly 
            B              5                 aggregate weekly 
            H              24                aggregate daily 
    two_sided : bool
        The moving average method used in filtering. 
        If True (default), a centered moving average is computed using the filt. 
        If False, the filter coefficients are for past values only.
    Returns:
    results : obj
        A object with 'seasonal', 'trend', and 'resid' attributes.
        has .plot() method 
        
##How to get 'freq' value 
#For Pandas Timeseries , is it inferred automatically

#convert index to DateTimeIndex:
#Note Index must have 'freq'(must not be None) which used by decompose
#df contains 'divida' column 
df.reset_index(inplace=True)
df['Date'] = pd.to_datetime(df['Date'])
df = df.set_index('Date')
s=sm.tsa.seasonal_decompose(df.divida)


#but if inference fails or if not pandas obeject or any other fre 
#then calculate 'fre' manually 
import pandas as pd
import statsmodels.api as sm

centrumGalerie = pd.read_csv('data/Centrum-Galerie-Belegung.csv', #collected parking lot occupancy of a shopping mall called 
         names=['Datum', 'Belegung'],
         index_col=['Datum'],
         parse_dates=True)
centrumGalerie.Belegung.plot()

>>> centrumGalerie.Belegung.index #note freq=None 
DatetimeIndex(['2014-04-13 01:45:00', '2014-04-13 02:00:00',
               '2014-04-13 02:15:00', '2014-04-13 02:30:00',
               '2014-04-13 02:45:00', '2014-04-13 03:00:00',
               '2014-04-13 03:15:00', '2014-04-13 03:30:00',
               '2014-04-13 03:45:00', '2014-04-13 04:00:00',
               ...
               '2015-06-17 16:00:00', '2015-06-17 16:15:00',
               '2015-06-17 16:30:00', '2015-06-17 16:45:00',
               '2015-06-17 17:00:00', '2015-06-17 17:15:00',
               '2015-06-17 17:30:00', '2015-06-17 17:45:00',
               '2015-06-17 18:00:00', '2015-06-17 18:15:00'],
              dtype='datetime64[ns]', name='Datum', length=40710, freq=None)
              
#option -1 
df = centrumGalerie.copy()
df.index.freq = pd.tseries.frequencies.to_offset('15T')
#note 'seasonal_decompose' uses index.inferred_freq, which can not be set via above technique 
#use below 
df.index= pd.date_range(start=df.index[0], periods=df.index.size, freq='15T')
>>> res = sm.tsa.seasonal_decompose(df.Belegung)
ValueError: freq T not understood. Please report if you think this is in error.
       

#data is stored with 15min resolution 
#to see a weekly seasonality, so freq is 
#freq = ((24h*60min)/15min)*7days
decompfreq = int(24*60/15*7) #must be int 

res = sm.tsa.seasonal_decompose(df.Belegung,freq=decompfreq)
res.plot()
plt.show()
#because of freq, many values in original time series would contain Nan 
T = res.trend.dropna()   
S = res.seasonal.dropna()   
new_s = res.resid.dropna()   
ori_s = res.observed[T.index.values] #res.observed.asfreq('W') would not make it as same freq of T

#recreate the Series and check for error 
pred_s = T + S + new_s  #for additive, for multiplicative, T*S*new_s 
pred_s.dropna(inplace=True)
RMSE = np.sqrt( np.sum(   (ori_s -pred_s)**2 )/len(pred_s) )

#with freq=1 , note there is no Seasonal and resid as there is nothing to aggregate
res = sm.tsa.seasonal_decompose(df.Belegung,freq=1)
T = res.trend.dropna()   #== df.Belegung
S = res.seasonal.dropna()    #0 
new_s = res.resid.dropna()   #0
ori_s = res.observed[T.index.values]

###statsmodel -  sm.tsa - Filters
#Filters are used for removing cycles, seasonals etc 
#from data to get pure 'trend'
#these trends are used for Time Series estimation


##Time Series Filters
filters.bk_filter.bkfilter(X[, low, high, K])       Baxter-King bandpass filter 
filters.hp_filter.hpfilter(X[, lamb])               Hodrick-Prescott filter 
filters.cf_filter.cffilter(X[, low, high, drift])   Christiano Fitzgerald asymmetric, random walk filter 



#Example 
statsmodels.tsa.filters.bk_filter.bkfilter(X, low=6, high=32, K=12)
    Parameters:
    X : array-like
        A 1 or 2d ndarray. 
        If 2d, variables are assumed to be in columns.
    low : float
        Minimum period for oscillations, ie., Baxter and King suggest 
        that the Burns-Mitchell U.S. business cycle has 6 for quarterly data 
        and 1.5 for annual data.
    high : float
        Maximum period for oscillations 
        BK suggest that the U.S. business cycle has 32 for quarterly data 
        and 8 for annual data.
    K : int
        Lead-lag length of the filter. 
        Baxter and King propose a truncation length of 12 for quarterly data 
        and 3 for annual data.
    Returns:
    Y : array
        Cyclical component of X
 
#Example 
import statsmodels.api as sm
import pandas as pd
dta = sm.datasets.macrodata.load_pandas().data
index = pd.DatetimeIndex(start='1959Q1', end='2009Q4', freq='Q')
dta.set_index(index, inplace=True)
cycles = sm.tsa.filters.bkfilter(dta[['realinv']], 6, 24, 12)
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
cycles.plot(ax=ax, style=['r--', 'b-'])
plt.show()

#Example 
statsmodels.tsa.filters.hp_filter.hpfilter(X, lamb=1600)
    Hodrick-Prescott filter
    Parameters:
    X : array-like
        The 1d ndarray timeseries to filter of length (nobs,) or (nobs,1)
    lamb : float
        The Hodrick-Prescott smoothing parameter. 
        A value of 1600 is suggested for quarterly data. 
        Ravn and Uhlig suggest using a value of 6.25 (1600/4**4) for annual data 
        and 129600 (1600*3**4) for monthly data.
    Returns:
    cycle : array
        The estimated cycle in the data given lamb.
    trend : array
        The estimated trend in the data given lamb.
 
#Example 
import statsmodels.api as sm
import pandas as pd
dta = sm.datasets.macrodata.load_pandas().data
index = pd.DatetimeIndex(start='1959Q1', end='2009Q4', freq='Q')
dta.set_index(index, inplace=True)
cycle, trend = sm.tsa.filters.hpfilter(dta.realgdp, 1600)
gdp_decomp = dta[['realgdp']]
gdp_decomp["cycle"] = cycle
gdp_decomp["trend"] = trend

import matplotlib.pyplot as plt
fig, ax = plt.subplots()
gdp_decomp[["realgdp", "trend"]]["2000-03-31":].plot(ax=ax,fontsize=16);
plt.show()

#Example 
statsmodels.tsa.filters.cf_filter.cffilter(X, low=6, high=32, drift=True)
    Christiano Fitzgerald asymmetric, random walk filter
    Parameters:
        X : array-like
        1 or 2d array to filter. If 2d, variables are assumed to be in columns.
    low : float
        Minimum period of oscillations. 
        Features below low periodicity are filtered out. 
        Default is 6 for quarterly data, giving a 1.5 year periodicity.
    high : float
        Maximum period of oscillations. 
        Features above high periodicity are filtered out. 
        Default is 32 for quarterly data, giving an 8 year periodicity.
    drift : bool
        Whether or not to remove a trend from the data. 
        The trend is estimated as np.arange(nobs)*(X[-1] - X[0])/(len(X)-1)
    Returns:
    cycle : array
        The features of X between periodicities given by low and high
    trend : array
        The trend in the data with the cycles removed.
 
#Example 
import statsmodels.api as sm
import pandas as pd
dta = sm.datasets.macrodata.load_pandas().data
index = pd.DatetimeIndex(start='1959Q1', end='2009Q4', freq='Q')
dta.set_index(index, inplace=True)
cf_cycles, cf_trend = sm.tsa.filters.cffilter(dta[["infl", "unemp"]])

import matplotlib.pyplot as plt
fig, ax = plt.subplots()
cf_cycles.plot(ax=ax, style=['r--', 'b-'])
plt.show()

##Other filters 
filters.filtertools.convolution_filter(x, filt)     Linear filtering via convolution. 
filters.filtertools.recursive_filter(x, ar_coeff)   Autoregressive, or recursive, filtering 
tsatools.add_trend(x[, trend, prepend, ...])        Adds a trend and/or constant to an array. 
                                                    trend : str {'c','t','ct','ctt'}
                                                        'c' add constant only 
                                                        't' add trend only 
                                                        'ct' add constant and linear trend 
                                                        'ctt' add constant and linear and quadratic trend.
                                                    Returns columns as ['ctt','ct','c'] whenever applicable

tsatools.detrend(x[, order, axis])                  Detrend an array with a trend of given order along axis 0 or 1 
                                                    order : int
                                                        specifies the polynomial order of the trend, 
                                                        zero is constant, one is linear trend, 
                                                        two is quadratic trend
                                                    Returns:
                                                    detrended data series : ndarray
                                                        The detrended series is the residual of the linear regression of the data 
                                                        on the trend of given order.
 






###statsmodel  - sm.tsa  - Model fitting and estimation
##Estimation - statsmodels.tsa.api

ar_model.AR(endog[, dates, freq, missing])      Autoregressive AR(p) model 
ar_model.ARResults(model, params[, ...])        Class to hold results from fitting an AR model. 


arima_model.ARMA(endog, order[, exog, ...])     Autoregressive Moving Average ARMA(p,q) Model 
arima_model.ARMAResults(model, params[, ...])   Class to hold results from fitting an ARMA model. 

arima_model.ARIMA(endog, order[, exog, ...])    Autoregressive Integrated Moving Average ARIMA(p,d,q) Model 
arima_model.ARIMAResults(model, params[, ...])  


     
class statsmodels.tsa.ar_model.AR(endog, dates=None, freq=None, missing='none')
    Autoregressive AR(p) model
    Parameters:
    endog : array-like
        1-d endogenous response variable. The independent variable.
    dates : array-like of datetime, optional
        An array-like object of datetime objects. 
        If a pandas object is given for endog or exog, 
        it is assumed to have a DateIndex.
    freq : str, optional
        The frequency of the time-series. 
        A Pandas offset or 'B', 'D', 'W', 'M', 'A', or 'Q'. 
        This is optional if dates are given.
    missing : str
        Available options are 'none', 'drop', and 'raise'. 
        If 'none', no nan checking is done. 
        If 'drop', any observations with nans are dropped. 
        If 'raise', an error is raised. Default is 'none.'
    #Methods 
    fit([maxlag, method, ic, trend, ...])            Fit the unconditional maximum likelihood of an AR(p) process. 
    from_formula(formula, data[, subset, drop_cols]) Create a Model from a formula and dataframe. 
    hessian(params)                                  Returns numerical hessian for now. 
    loglike(params)                                  The loglikelihood of an AR(p) process 
    predict(params[, start, end, dynamic])           Returns in-sample and out-of-sample prediction. 
    score(params)                                    Return the gradient of the loglikelihood at params. 
    select_order(maxlag, ic[, trend, method])        Select the lag order according to the information criterion. 
     
     
 

class statsmodels.tsa.arima_model.ARIMA(endog, order, exog=None, dates=None, freq=None, missing='none')
    Autoregressive Integrated Moving Average ARIMA(p,d,q) Model
    endog : array-like.
        The endogenous variable.
    order : iterable, (p,d,q) order of the model
    exog : array-like, optional
            An optional arry of exogenous variables. 
            This should not include a constant or trend. 
            You can specify this in the fit method.
    dates : array-like of datetime, optional
        An array-like object of datetime objects. 
        If a pandas object is given for endog or exog, it is assumed to have a DateIndex.
    freq : str, optional
           The frequency of the time-series. 
           A Pandas offset or 'B', 'D', 'W', 'M', 'A', or 'Q'. 
           This is optional if dates are given.
 
#Attributes
endog_names  
exog_names 
#Methods of the ARIMA( similar exists for ARMA etc) 
fit([start_params, trend, method, ...]) 
    Fits ARIMA(p,d,q) model by exact maximum likelihood via Kalman filter. 
    method : str {'css-mle','mle','css'}
        This is the loglikelihood to maximize. If 'css-mle', the conditional sum of squares 
        likelihood is maximized and its values are used as starting values for the computation 
        of the exact likelihood via the Kalman filter. If 'mle', the exact likelihood is 
        maximized via the Kalman Filter. If 'css' the conditional sum of squares likelihood is 
        maximized. 
    trend : str {'c','nc'}
        Whether to include a constant or not. 
        'c' includes constant, 
        'nc' no constant.
    solver : str or None, optional
        The default is 'lbfgs' (limited memory Broyden-Fletcher-Goldfarb-Shanno). 
        Other choices are 'bfgs', 'newton' (Newton-Raphson), 'nm' (Nelder-Mead), 
        'cg' - (conjugate gradient), 'ncg' (non-conjugate gradient), and 'powell'. 
        By default, the limited memory BFGS uses m=12 to approximate the Hessian, 
        projected gradient tolerance of 1e-8 and factr = 1e2. 
        You can change these by using kwargs.
    
from_formula(formula, data[, subset])   Create a Model from a formula and dataframe. 
geterrors(params)                       Get the errors of the ARMA process. 
hessian(params)                         Compute the Hessian at params, 
information(params)                     Fisher information matrix of model 
loglike(params[, set_sigma2])           Compute the log-likelihood for ARMA(p,q) model 
loglike_css(params[, set_sigma2])       Conditional Sum of Squares likelihood function. 
loglike_kalman(params[, set_sigma2])    Compute exact loglikelihood for ARMA(p,q) model by the Kalman Filter. 
score(params)                           Compute the score function at params. 
predict(params[, start, end, exog, typ, dynamic]) ARIMA model in-sample and out-of-sample prediction 

 
##Methods of Result from ARIMA.fit(..)
#AR, ARMA, VAR have similar Result type and methods 
aic()  
arfreq()                                    Returns the frequency of the AR roots. 
arparams()  
arroots()  
bic()  
bse()  
conf_int([alpha, cols, method])             Returns the confidence interval of the fitted parameters. 
cov_params()  
f_test(r_matrix[, cov_p, scale, invcov])    Compute the F-test for a joint linear hypothesis. 
fittedvalues()  
forecast([steps, exog, alpha])              Out-of-sample forecasts 
hqic()  
llf()  
load(fname)                                 load a pickle, (class method) 
mafreq()                                    Returns the frequency of the MA roots. 
maparams()  
maroots()  
normalized_cov_params()  
pvalues()  
remove_data()                                   remove data arrays, all nobs arrays from result and model 
resid()  
save(fname[, remove_data])                      save a pickle of this instance 
summary([alpha])                                Summarize the Model 
summary2([title, alpha, float_format])          Experimental summary function for ARIMA Results 
t_test(r_matrix[, cov_p, scale, use_t])         Compute a t-test for a each linear hypothesis of the form Rb = q 
tvalues()                                       Return the t-statistic for a given parameter estimate. 
wald_test(r_matrix[, cov_p, scale, invcov, ...])Compute a Wald-test for a joint linear hypothesis
                                                Used for to check whether smaller(nested to larger) model fits significantly compared to larger model
                                                H0: two/more coefficients of interest from larger model are simultaneously equal to zero
                                  
plot_predict([start, end, exog, dynamic, ...])  
    Plot forecasts 
ARIMAResults.predict(start=None, end=None, exog=None, typ='linear', dynamic=False)[source]
    ARIMA model in-sample and out-of-sample prediction
    Parameters:
    start : int, str, or datetime
        Zero-indexed observation number at which to start forecasting, 
        ie., the first forecast is start. 
        Can also be a date string to parse or a datetime type.
    end : int, str, or datetime
        Zero-indexed observation number at which to end forecasting, 
        ie., the first forecast is start. 
        Can also be a date string to parse or a datetime type. 
        However, if the dates index does not have a fixed frequency, 
        end must be an integer index if you want out of sample prediction.
    exog : array-like, optional
        If the model is an ARMAX and out-of-sample forecasting is requested, 
        exog must be given. 
        Note that you'll need to pass k_ar additional lags 
        for any exogenous variables. 
        E.g., if you fit an ARMAX(2, q) model 
        and want to predict 5 steps, you need 7 observations to do this.
    dynamic : bool, optional
        The dynamic keyword affects in-sample prediction. 
        If dynamic is False, then the in-sample lagged values are used for prediction. 
        If dynamic is True, then in-sample forecasts are used in place of lagged dependent variables. 
        The first forecasted value is start.
    typ : str {'linear', 'levels'}
        •'linear' : Linear prediction in terms of the differenced endogenous variables.
        •'levels' : Predict the levels of the original endogenous variables.
    Returns:
    predict : array
        The predicted values.
        
##testing for non stationary 
statsmodels.tsa.stattools.adfuller(x, maxlag=None, regression='c', autolag='AIC', store=False, regresults=False)[source]
    a unit root test tests whether a time series variable is non-stationary and possesses a unit root. 
    H0: presence of a unit root or nonstionary
    Parameters:
    x : array_like, 1d
        data series
    maxlag : int
        Maximum lag which is included in test, default 12*(nobs/100)^{1/4}
    regression : {'c','ct','ctt','nc'}
        Constant and trend order to include in regression
        •'c' : constant only (default)
        •'ct' : constant and trend
        •'ctt' : constant, and linear and quadratic trend
        •'nc' : no constant, no trend
    autolag : {'AIC', 'BIC', 't-stat', None}
        •if None, then maxlag lags are used
        •if 'AIC' (default) or 'BIC', then the number of lags is chosen to minimize the corresponding information criterion
        •'t-stat' based choice of maxlag. Starts with maxlag and drops a lag until the t-statistic on the last lag length is significant using a 5%-sized test
    store : bool
        If True, then a result instance is returned additionally to the adf statistic. Default is False
    regresults : bool, optional
        If True, the full regression results are returned. Default is False
    Returns:
    adf : float
        Test statistic
    pvalue : float
        MacKinnon's approximate p-value based on MacKinnon (1994, 2010)
    usedlag : int
        Number of lags used
    nobs : int
        Number of observations used for the ADF regression and calculation of the critical values
    critical values : dict
        Critical values for the test statistic at the 1 %, 5 %, and 10 % levels. Based on MacKinnon (2010)
    icbest : float
        The maximized information criterion if autolag is not None.
    resstore : ResultStore, optional
        A dummy class with results attached as attributes
                                                 
                                                 
                                                 
##Example - AR - sunspots data 


from __future__ import print_function
import statsmodels.api as sm
import numpy as np
import pandas as pd

data = sm.datasets.sunspots.load()
from datetime import datetime
dates = sm.tsa.datetools.dates_from_range('1700', length=len(data.endog))

#Make a pandas TimeSeries or DataFrame
endog = pd.DataFrame(data.endog, index=dates)

#AR model 
ar_model = sm.tsa.AR(endog, freq='A')
pandas_ar_res = ar_model.fit(maxlag=9, method='mle', disp=-1)

#Out-of-sample prediction

pred = pandas_ar_res.predict(start='2005', end='2015')
>>> print(pred)
2005-12-31    20.003302
2006-12-31    24.703999
2007-12-31    20.026145
2008-12-31    23.473645
2009-12-31    30.858584
2010-12-31    61.335464
2011-12-31    87.024706
2012-12-31    91.321256
2013-12-31    79.921607
2014-12-31    60.799490
2015-12-31    40.374843
Freq: A-DEC, dtype: float64

#Using explicit dates
ar_model = sm.tsa.AR(data.endog, dates=dates, freq='A')
ar_res = ar_model.fit(maxlag=9, method='mle', disp=-1)
pred = ar_res.predict(start='2005', end='2015')
>>> print(pred)
[ 20.0033  24.704   20.0261  23.4736  30.8586  61.3355  87.0247  91.3213
  79.9216  60.7995  40.3748]


##Example - ARIMA
#https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
%matplotlib inline
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 15, 6

#Load data 
data = pd.read_csv('data/AirPassengers.csv')
>>> print(data.head())
     Month  #Passengers
0  1949-01          112
1  1949-02          118
2  1949-03          132
3  1949-04          129
4  1949-05          121


#Parse dates 
dateparse = lambda x: pd.to_datetime(x, format='%Y-%m')
data = pd.read_csv('data/AirPassengers.csv', parse_dates=['Month'], index_col='Month',date_parser=dateparse)
>>> print(data.head())
            #Passengers
Month
1949-01-01          112
1949-02-01          118
1949-03-01          132
1949-04-01          129
1949-05-01          121
>>> data.index
DatetimeIndex(['1949-01-01', '1949-02-01', '1949-03-01', '1949-04-01',
               '1949-05-01', '1949-06-01', '1949-07-01', '1949-08-01',
               '1949-09-01', '1949-10-01',
               ...
               '1960-03-01', '1960-04-01', '1960-05-01', '1960-06-01',
               '1960-07-01', '1960-08-01', '1960-09-01', '1960-10-01',
               '1960-11-01', '1960-12-01'],
              dtype='datetime64[ns]', name='Month', length=144, freq=None)
              
ts = data['#Passengers']
#Selecting 
#1. Specific the index as a string constant:
#ERROR: data['1949-01-01']
data.loc['1949-01-01']
ts['1949-01-01']
#2. Import the datetime library and use 'datetime' function:
from datetime import datetime
#ERROR data[datetime(1949,1,1)]
data.loc[datetime(1949,1,1)]
ts[datetime(1949,1,1)]
#1. Specify the entire range:
data['1949-01-01':'1949-05-01']
ts['1949-01-01':'1949-05-01']
#2. Use ':' if one of the indices is at ends:
data[:'1949-05-01']
ts[:'1949-05-01']
#year
data['1949']
ts['1949']

##Checking stationary 
#constant mean
#constant variance
#an autocovariance that does not depend on time
#run plot 
plt.plot(data)  #clearly shows overall increasing trend 
#or 
plt.plot(ts)
#Or 
data.plot(kind='line')
plt.show()

#further checking of stationary 
#rolling statistics plots - ever increasing 
#and  Dickey-Fuller test - adfuller
#H0: TS is non-stationary

from statsmodels.tsa.stattools import adfuller
def test_stationarity(timeseries):    
    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12, center=False).mean()
    rolstd = timeseries.rolling(window=12, center=False).std()
    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)
    #Perform Dickey-Fuller test:
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print(dfoutput)
    

>>> test_stationarity(ts)  
#variation in standard deviation is small, mean is clearly increasing with time 
#p-value>0.05, hence accept H0(=TS is non-stationary)
Results of Dickey-Fuller Test:
Test Statistic                   0.815369
p-value                          0.991880
#Lags Used                      13.000000
Number of Observations Used    130.000000
Critical Value (1%)             -3.481682
Critical Value (10%)            -2.578770
Critical Value (5%)             -2.884042
dtype: float64


##How to make a Time Series Stationary?
#Reasons for nonstionary 
1. Trend – varying mean over time. 
    In this case, average mean increasing over time 
2. Seasonality – variations at specific time-frames. 
    eg people might have a tendency to buy cars in a particular month 
    because of pay increment or festivals.

##Eliminating Trend 
#First option for reducing increasing trend  - Tranformation 
#eg log, sqrt or cube root to dampen the trend 
#for decreasing trend, exp, square, cube etc 
ts_log = np.log(ts)
plt.plot(ts_log)
plt.show()

#Further options are 
#Aggregation – taking average for a time period like monthly/weekly averages
#Smoothing – taking rolling averages
#Polynomial Fitting – fit a regression model

#Smoothing - Moving average   
moving_avg = ts_log.rolling(window=12, center=False).mean()
plt.plot(ts_log)
plt.plot(moving_avg, color='red')
plt.show()

#Smoothing -  Differencing 
ts_log_moving_avg_diff = ts_log - moving_avg
ts_log_moving_avg_diff.dropna(inplace=True) #remove any NAN 
>>> test_stationarity(ts_log_moving_avg_diff)
#very near to stationary 
Results of Dickey-Fuller Test:
Test Statistic                  -3.162908
p-value                          0.022235
#Lags Used                      13.000000
Number of Observations Used    119.000000
Critical Value (1%)             -3.486535
Critical Value (10%)            -2.579896
Critical Value (5%)             -2.886151
dtype: float64

#Note, we have taken rolling average with window=12 , valid for any yearly data 
#but for random datetimeIndex, eg for stocks data,
#better option is 'weighted moving average'
expwighted_avg = ts_log.ewm(halflife=12).mean()
plt.plot(ts_log)
plt.plot(expwighted_avg, color='red')
ts_log_ewma_diff = ts_log - expwighted_avg
>>> test_stationarity(ts_log_ewma_diff)
Results of Dickey-Fuller Test:
Test Statistic                  -3.601262
p-value                          0.005737
#Lags Used                      13.000000
Number of Observations Used    130.000000
Critical Value (1%)             -3.481682
Critical Value (10%)            -2.578770
Critical Value (5%)             -2.884042
dtype: float64



##Eliminating both Trend and Seasonality
#Differencing – taking the differece with a particular time lag
#Decomposition – modeling both trend and seasonality and removing them from the model.

##Eliminating both Trend and Seasonality - Differencing
ts_log_diff = ts_log - ts_log.shift(1)
#or 
ts_log_diff = ts_log.diff(periods=1) #Periods to shift for forming difference

ts_log_diff.dropna(inplace=True)
>>> test_stationarity(ts_log_diff)
Results of Dickey-Fuller Test:
Test Statistic                  -2.717131
p-value                          0.071121
#Lags Used                      14.000000
Number of Observations Used    128.000000
Critical Value (1%)             -3.482501
Critical Value (10%)            -2.578960
Critical Value (5%)             -2.884398
dtype: float64

#Note 2nd differencing is extremly rare , DOn't use 
ts_log_diff2 = ts_log_diff - ts_log_diff.shift(1)
#or 
ts_log_diff2 = ts_log_diff.diff(1) #Note, it is not ts_log.diff(periods=2), which is diff with every 2nd period 

ts_log_diff2.dropna(inplace=True)
>>> test_stationarity(ts_log_diff2)
Results of Dickey-Fuller Test:
Test Statistic                -8.196629e+00
p-value                        7.419305e-13
#Lags Used                     1.300000e+01
Number of Observations Used    1.280000e+02
Critical Value (1%)           -3.482501e+00
Critical Value (10%)          -2.578960e+00
Critical Value (5%)           -2.884398e+00
dtype: float64


##Eliminating both Trend and Seasonality - Decomposing
#In this approach, both trend and seasonality are modeled separately 
#and the remaining part of the series is returned


from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(ts_log)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid  #we need to work with this series further 

plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.tight_layout()
plt.show()
#Get stats 
ts_log_decompose = residual
ts_log_decompose.dropna(inplace=True)
>>> test_stationarity(ts_log_decompose)
Results of Dickey-Fuller Test:
Test Statistic                -6.332387e+00
p-value                        2.885059e-08
#Lags Used                     9.000000e+00
Number of Observations Used    1.220000e+02
Critical Value (1%)           -3.485122e+00
Critical Value (10%)          -2.579569e+00
Critical Value (5%)           -2.885538e+00
dtype: float64


##Forecasting a Time Series
#Use differencingfor making stationary 

#ACF and PACF plots:
#p – The lag value where the PACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case p=2.
#q – The lag value where the ACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case q=2.


#below line must, must not contain NaN 
ts_log_diff.dropna(inplace=True)

fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
sm.graphics.tsa.plot_acf(ts_log_diff, lags=40, ax=ax1,alpha=0.05) #confidence level 95%
ax2 = fig.add_subplot(212)
sm.graphics.tsa.plot_pacf(ts_log_diff, lags=40, ax=ax2,alpha=0.05)
plt.show()

#option-2 
from statsmodels.tsa.stattools import acf, pacf
#other option 
lag_acf = acf(ts_log_diff, nlags=20)
lag_pacf = pacf(ts_log_diff, nlags=20, method='ols')

#Plot ACF: 
plt.subplot(121) 
plt.plot(lag_acf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.title('Autocorrelation Function')

#Plot PACF:
plt.subplot(122)
plt.plot(lag_pacf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.title('Partial Autocorrelation Function')
plt.tight_layout()



#Fit now 
#since 1st order differntiation is done, hence d= 1
from statsmodels.tsa.arima_model import ARIMA

#AR Model
model = ARIMA(ts_log, order=(2, 1, 0))  
results_AR = model.fit(disp=-1)   #disp:If True, convergence information is output.
plt.plot(ts_log_diff)
plt.plot(results_AR.fittedvalues, color='red')
plt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_diff)**2))

#MA Model
model = ARIMA(ts_log, order=(0, 1, 2))  
results_MA = model.fit(disp=-1)  
plt.plot(ts_log_diff)
plt.plot(results_MA.fittedvalues, color='red')
plt.title('RSS: %.4f'% sum((results_MA.fittedvalues-ts_log_diff)**2))

#ARIMA 
model = ARIMA(ts_log, order=(2, 1, 2))  
results_ARIMA = model.fit(disp=-1)  
plt.plot(ts_log_diff)
plt.plot(results_ARIMA.fittedvalues, color='red')
plt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-ts_log_diff)**2))

plt.show()


##Taking it back to original scale
predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)
print(predictions_ARIMA_diff.head())

#Notice that these start from '1949-02-01' and not the first month. 
#This is because we took a lag by 1  for differencing 

#To undo, first determine the cumulative sum at index 
predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()
#and then add it to the base number. 
predictions_ARIMA_log = pd.Series(ts_log.ix[0], index=ts_log.index)
predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)
#and then anti-log 
predictions_ARIMA = np.exp(predictions_ARIMA_log)
plt.plot(ts)
plt.plot(predictions_ARIMA)
plt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts)**2)/len(ts)))





##Example - ARIMAX Model 
#ARIMA with with additional explanatory variables 
#Note Multivariate cause 'causality' (check VAR), called "Granger-causes"

#if a signal X1 "Granger-causes" (or "G-causes") a signal X2, 
#then past values of X1 should contain information that helps predict X2 
#above and beyond the information contained in past values of X2 alone.

import pandas as pd
import matplotlib.pyplot as plt
df=pd.read_csv('data/salesdata2.csv')
>>> df.head()
   Month   Marketing       Sales
0      1  107.284347  123.819229
1      2  214.523447  267.318463
2      3  327.159934  482.107206
3      4  437.384597  762.756909
4      5  518.389038  997.734857

#plot 
df[['Marketing','Sales']].plot() #ever increasing, non-stationary 
plt.show()

#formal nonstationary test , H0: non-stationary 
import statsmodels.api as sm
print(sm.tsa.stattools.adfuller(df['Marketing'])) # >0.05, accept H0 
print(sm.tsa.stattools.adfuller(df['Sales'])) # >0.05, accept H0 


#Naive OLS - produces strong multicollinearity 
df['const']=1
model1=sm.OLS(endog=df['Sales'],exog=df[['Marketing','const']])
results1=model1.fit()
print(results1.summary())

#differenced time-series regression - multicollinearity  gone 
df['diffS']=df['Sales'].diff()
df['diffM']=df['Marketing'].diff()
df.dropna(inplace=True)
df[['diffS','diffM' ]].plot(kind='line') #stationary 
plt.show()

model2=sm.OLS(endog=df['diffS'].dropna(),exog=df[['diffM','const']].dropna())
results2=model2.fit()
print(results2.summary())

##Test for causality 
statsmodels.tsa.stattools.grangercausalitytests(x, maxlag, addconst=True, verbose=True)[source]
    H0: the time series in the second column, x2, 
    does NOT Granger cause the time series in the first column, x1. 
    Grange causality means that past values of x2 have a statistically 
    significant effect on the current value of x1, 
    taking past values of x1 into account as regressors.
    Parameters:
    x : array, 2d, (nobs,2)
        data for test whether the time series in the second column 
        Granger causes the time series in the first column
    maxlag : integer
        the Granger causality test results are calculated for all lags up to maxlag
    Returns:
    results : dictionary
        all test results, dictionary keys are the number of lags. 
        For each lag the values are a tuple, 
        with the first element a dictionary with teststatistic, pvalues, degrees of freedom,
        the second element are the OLS estimation results for the restricted model, 
        the unrestricted model and the restriction (contrast) matrix 
        for the parameter f_test.

#for lag=1 , H0: Marketing does not cause Sales, <0.05 reject H0 
>>> print(sm.tsa.stattools.grangercausalitytests(df[['Sales','Marketing']].dropna(),1))
Granger Causality
number of lags (no zero) 1
ssr based F test:         F=33.4561 , p=0.0000  , df_denom=71, df_num=1
ssr based chi2 test:   chi2=34.8698 , p=0.0000  , df=1
likelihood ratio test: chi2=28.5705 , p=0.0000  , df=1
parameter F test:         F=33.4561 , p=0.0000  , df_denom=71, df_num=1
{1: ({'ssr_chi2test': (34.86978075633554, 3.5250989086317312e-09, 1), 'lrtest':
(28.570467812774496, 9.034970552654862e-08, 1), 'ssr_ftest': (33.45614099594356,
 1.8100367149360244e-07, 71.0, 1), 'params_ftest': (33.456140995944004, 1.810036
7149357443e-07, 71.0, 1)}, [<statsmodels.regression.linear_model.RegressionResul
tsWrapper object at 0x0000001510F56198>, <statsmodels.regression.linear_model.Re
gressionResultsWrapper object at 0x0000001510F56B38>, array([[0., 1., 0.]])])}

#reverse , H0: sales does not cause Marketing , >0.05, accept H0 
>>> print(sm.tsa.stattools.grangercausalitytests(df[['Marketing','Sales']].dropn
a(),1))

Granger Causality
number of lags (no zero) 1
ssr based F test:         F=0.3057  , p=0.5821  , df_denom=71, df_num=1
ssr based chi2 test:   chi2=0.3186  , p=0.5724  , df=1
likelihood ratio test: chi2=0.3179  , p=0.5729  , df=1
parameter F test:         F=0.3057  , p=0.5821  , df_denom=71, df_num=1
{1: ({'ssr_chi2test': (0.3186090371140572, 0.5724447641509534, 1), 'lrtest': (0.
31792510953391684, 0.5728572543075492, 1), 'ssr_ftest': (0.3056924545283522, 0.5
82072249688844, 71.0, 1), 'params_ftest': (0.30569245452810706, 0.58207224968899
58, 71.0, 1)}, [<statsmodels.regression.linear_model.RegressionResultsWrapper ob
ject at 0x0000001510F56710>, <statsmodels.regression.linear_model.RegressionResu
ltsWrapper object at 0x0000001510F56780>, array([[0., 1., 0.]])])}


#ARIMAX , Sales = y, x= Marketing 

#pandas object must have DatetimeIndex, so convert to ndarray and mention freq
model3=sm.tsa.ARIMA(endog=df['Sales'].values,exog=df[['diffM']].values,order=[1,1,0], freq='M')
results3 = model3.fit(disp=False)
>> results3.summary()
                             ARIMA Model Results
==============================================================================
Dep. Variable:                    D.y   No. Observations:                   65
Model:                 ARIMA(1, 1, 0)   Log Likelihood                -588.110
Method:                       css-mle   S.D. of innovations           2053.061
Date:                Tue, 30 Jan 2018   AIC                           1184.220
Time:                        20:11:53   BIC                           1192.917
Sample:                             1   HQIC                          1187.652

==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const        451.8275    286.842      1.575      0.120    -110.373    1014.028
x1            -2.9538      2.083     -1.418      0.161      -7.036       1.128
ar.L1.D.y     -0.4590      0.119     -3.854      0.000      -0.692      -0.226
                                    Roots
=============================================================================
                 Real           Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1           -2.1786           +0.0000j            2.1786            0.5000
-----------------------------------------------------------------------------

#Plot orignal and fitted values 
df['lagS'] = df['diffS'].shift()
df.dropna(inplace=True)
plt.plot(df['lagS'])  
plt.plot(results3.fittedvalues, color='red')
plt.title('RSS: %.4f'% sum((results3.fittedvalues-df['lagS'])**2))

plt.show()














###statsmodel - sm.tsa - Vector autoregression (VAR) 
#Captures the linear interdependencies among multiple time series(multivariate/ independent variables)
 
#VAR models generalize the univariate autoregressive model (AR model) by allowing 
#for more than one independent variables  

#each variable has an equation explaining its evolution based on its own lags 
#and the lags of the other model variables
     
#Vector Autogressive Processes (VAR)
vector_ar.var_model.VAR(endog[, dates, ...])      Fit VAR(p) process and do lag order selection 
vector_ar.var_model.VARResults(endog, ...[, ...]) Estimate VAR(p) process with fixed number of lags 
vector_ar.dynamic.DynamicVAR(data[, ...])         Estimates time-varying vector autoregression (VAR(p)) 

vector_ar.var_model.VARProcess(coefs, ...[, ...])   Class represents a known VAR(p) process 
vector_ar.irf.IRAnalysis(model[, P, ...])           Impulse response analysis class. 
vector_ar.var_model.FEVD(model[, P, periods])       Compute and plot Forecast error variance decomposition and asymptotic 

  
#Example 
from  statsmodels.tsa.api  import * 
import statsmodels.api as sm

# some example data
import pandas
mdata = sm.datasets.macrodata.load_pandas().data

# prepare the dates index
dates = mdata[['year', 'quarter']].astype(int).astype(str)
quarterly = dates["year"] + "Q" + dates["quarter"]

#dates_from_str(dates)- convert sequence like '1996m1' or '1996Q1' to dates

from statsmodels.tsa.base.datetools import dates_from_str
quarterly = dates_from_str(quarterly)

mdata = mdata[['realgdp','realcons','realinv']]
mdata.index = pandas.DatetimeIndex(quarterly)
#to remove seasonality, take log and then 1st order diff - creates a stationary series 
data = np.log(mdata).diff().dropna()  #create 1st order difference 

# make a VAR model
model = VAR(data) #passed time series must be  stationary

#To actually do the estimation, call the fit method with the desired lag order
>>> results = model.fit(2) #lag=2

>>> results.summary() #L1= lag 1, etc 

  Summary of Regression Results
==================================
Model:                         VAR
Method:                        OLS
Date:           Fri, 08, Jul, 2011
Time:                     11:30:22
--------------------------------------------------------------------
No. of Equations:         3.00000    BIC:                   -27.5830
Nobs:                     200.000    HQIC:                  -27.7892
Log likelihood:           1962.57    FPE:                7.42129e-13
AIC:                     -27.9293    Det(Omega_mle):     6.69358e-13
--------------------------------------------------------------------
Results for equation realgdp
==============================================================================
                 coefficient       std. error           t-stat            prob
------------------------------------------------------------------------------
const               0.001527         0.001119            1.365           0.174
L1.realgdp         -0.279435         0.169663           -1.647           0.101
L1.realcons         0.675016         0.131285            5.142           0.000
L1.realinv          0.033219         0.026194            1.268           0.206
L2.realgdp          0.008221         0.173522            0.047           0.962
L2.realcons         0.290458         0.145904            1.991           0.048
L2.realinv         -0.007321         0.025786           -0.284           0.777
==============================================================================

Results for equation realcons
==============================================================================
                 coefficient       std. error           t-stat            prob
------------------------------------------------------------------------------
const               0.005460         0.000969            5.634           0.000
L1.realgdp         -0.100468         0.146924           -0.684           0.495
L1.realcons         0.268640         0.113690            2.363           0.019
L1.realinv          0.025739         0.022683            1.135           0.258
L2.realgdp         -0.123174         0.150267           -0.820           0.413
L2.realcons         0.232499         0.126350            1.840           0.067
L2.realinv          0.023504         0.022330            1.053           0.294
==============================================================================

Results for equation realinv
==============================================================================
                 coefficient       std. error           t-stat            prob
------------------------------------------------------------------------------
const              -0.023903         0.005863           -4.077           0.000
L1.realgdp         -1.970974         0.888892           -2.217           0.028
L1.realcons         4.414162         0.687825            6.418           0.000
L1.realinv          0.225479         0.137234            1.643           0.102
L2.realgdp          0.380786         0.909114            0.419           0.676
L2.realcons         0.800281         0.764416            1.047           0.296
L2.realinv         -0.124079         0.135098           -0.918           0.360
==============================================================================

Correlation matrix of residuals
             realgdp  realcons   realinv
realgdp     1.000000  0.603316  0.750722
realcons    0.603316  1.000000  0.131951
realinv     0.750722  0.131951  1.000000

#still corelated 

##Plotting on VARResults 
plot()                                      Plot input time series 
plot_acorr([nlags, linewidth])              Plot theoretical autocorrelation function 
plot_forecast(steps[, alpha, plot_stderr])  Plot forecast 
plot_sample_acorr([nlags, linewidth])       Plot theoretical autocorrelation function 
plotsim([steps])                            Plot a simulation from the VAR(p) process for the desired number of steps 


#Plot input time series
results.plot()
#Plotting time series autocorrelation function:
results.plot_acorr()

#Lag order selection
#Choice of lag order can be a difficult problem. 
>>> model.select_order(15) #Returns:  selections : dict {info_crit -> selected_order}
                 VAR Order Selection
======================================================
            aic          bic          fpe         hqic
------------------------------------------------------
0        -27.70       -27.65    9.358e-13       -27.68
1        -28.02      -27.82*    6.745e-13      -27.94*
2        -28.03       -27.66    6.732e-13       -27.88
3       -28.04*       -27.52   6.651e-13*       -27.83

#When calling the fit function, one can pass a maximum number of lags 
#and the order criterion to use for order selection:
results = model.fit(maxlags=15, ic='aic')

##few imp attributes of VARResults
k_ar : int,Order of VAR process
k_trend : int
llf
model
names
neqs : int,Number of variables (equations)
nobs : int
n_totobs : int
params

>>> results.k_ar
3
>>> results.k_trend
1

##Forecasting
lag_order = results.k_ar  #p=k_ar of VAR(p), in results, it also called k 
#Produce linear minimum MSE forecasts for desired number of steps ahead, using prior values y
results.forecast(data.values[-lag_order:], 5) #y,steps 
results.plot_forecast(steps=15)

#other forcasting 
VARResults.forecast_interval(y, steps, alpha=0.05)
    Construct forecast interval estimates assuming the y are Gaussian
    for desired number of steps ahead, using prior values y
    Returns:
        (lower, mid, upper) : (ndarray, ndarray, ndarray) 
VARResults.forecast_cov(steps=1)
    Compute forecast covariance matrices for desired number of steps
    Parameters:
    steps : int 
    Returns:
    covs : ndarray (steps x k x k) 



#Impulse Response Analysis
##Impulse responses are of interest in econometric studies: 
#they are the estimated responses to a unit impulse(step function) in one of the variables
#and impact to other variables 
irf = results.irf(periods=10)
#methods on statsmodels.tsa.vector_ar.irf.IRAnalysis
irf([periods, var_decomp, var_order])        Analyze impulse responses to shocks in system 
irf_errband_mc([orth, repl, T, signif, ...]) Compute Monte Carlo integrated error bands assuming normally 
irf_resim([orth, repl, T, seed, burn, cum])  Simulates impulse response function, returning an array  
plot([orth, impulse, response, signif, ...])     
    Plot impulse responses 
plot_cum_effects([orth, impulse, response, ...]) 
    Plot cumulative impulse response functions 
    orth : bool, default False
        Compute orthogonalized impulse responses
    impulse : string or int
        variable providing the impulse
    response : string or int
        variable affected by the impulse
    signif : float (0 < signif < 1)
        Significance level for error bars, defaults to 95% CI
    subplot_params : dict
        To pass to subplot plotting funcions. 
        Example: if fonts are too big, pass {'fontsize' : 8} 
    plot_params : dict
    plot_stderr: bool, default True
        Plot standard impulse response error bands

#Draw 'variable providing the impulse' to 'variable affected by the impulse'
#plot with error bands 
irf.plot(orth=False)
#Note the plot function is flexible and can plot only variables of interest if so desired:
irf.plot(impulse='realgdp')
#The cumulative effects \Psi_n = \sum_{i=0}^n \Phi_i can be plotted with the long run effects as follows:
irf.plot_cum_effects(orth=False)


##Forecast Error Variance Decomposition (FEVD)
#Forecast errors of component j on k in an i-step ahead forecast 
#can be decomposed using the orthogonalized impulse responses

#The variance decomposition indicates the amount of information each variable 
#contributes to the other variables in the autoregression
fevd = results.fevd(periods=5)
#Methods on statsmodels.tsa.vector_ar.var_model.FEVD
cov()                    Compute asymptotic standard errors 
plot([periods, figsize]) Plot graphical display of FEVD 
summary()                Summary

#Example 
>>> fevd.summary() 
#ie 0th time, unit impulse on one variable, 
#then till 4th time, how unit value is distribute on other variables 
FEVD for realgdp
      realgdp  realcons   realinv
0    1.000000  0.000000  0.000000
1    0.864889  0.129253  0.005858
2    0.816725  0.177898  0.005378
3    0.793647  0.197590  0.008763
4    0.777279  0.208127  0.014594

FEVD for realcons
      realgdp  realcons   realinv
0    0.359877  0.640123  0.000000
1    0.358767  0.635420  0.005813
2    0.348044  0.645138  0.006817
3    0.319913  0.653609  0.026478
4    0.317407  0.652180  0.030414

FEVD for realinv
      realgdp  realcons   realinv
0    0.577021  0.152783  0.270196
1    0.488158  0.293622  0.218220
2    0.478727  0.314398  0.206874
3    0.477182  0.315564  0.207254
4    0.466741  0.324135  0.209124

#or plot 
results.fevd(20).plot()


#Statistical test - Granger causality
#To find  whether a variable or group of variables is 'causal'
#(ie acting as a cause) for another variable

#In the context of VAR models, 
#one can say that a set of variables are Granger-causal  within one of the variables

#The VARResults object has the test_causality method for performing 
#either a Wald (\chi^2) test or an F-test.

#if a signal X1 "Granger-causes" (or "G-causes") a signal X2, 
#then past values of X1 should contain information that helps predict X2 
#above and beyond the information contained in past values of X2 alone.


#H0: ['realinv', 'realcons'] is not causal with realgdp
#here reject H0
>>> results.test_causality('realgdp', ['realinv', 'realcons'], kind='f') #kind : {'f', 'wald'}
Granger causality f-test
=============================================================
   Test statistic   Critical Value          p-value        df
-------------------------------------------------------------
         6.999888         2.114554            0.000  (6, 567)
=============================================================
H_0: ['realinv', 'realcons'] do not Granger-cause realgdp
Conclusion: reject H_0 at 5.00% significance level

{'conclusion': 'reject',
 'crit_value': 2.1145543864562706,
 'df': (6, 567),
 'pvalue': 3.3805963773886478e-07,
 'signif': 0.05,
 'statistic': 6.9998875522543473}
 
 
##Test white noise assumption- ACF plot with error bounds 
#Sample (Y) autocorrelations are compared with the standard 2 / \sqrt(T) bounds
>>> results.test_whiteness(nlags=10, plot=True, linewidth=8)
FAIL: Some autocorrelations exceed 0.1418 bound. See plot
>>> plt.show()

##Test assumption of normal-distributed errors using Jarque-Bera-style omnibus Chi^2 test
#H0: normal 
>>> results.test_normality()
Normality skew/kurtosis Chi^2-test
=======================================================
   Test statistic   Critical Value          p-value  df
-------------------------------------------------------
        17.991533        12.591587            0.006   6
=======================================================
H_0: data generated by normally-distributed process
Conclusion: reject H_0 at 5.00% significance level
{'conclusion': 'reject', 'statistic': 17.991532823743086, 'pvalue': 0.0062533898
6501885, 'signif': 0.05, 'crit_value': 12.591587243743977, 'df': 6}








###statsmodel - sm.tsa  - Dynamic Vector Autoregressions


#To estimate a moving-window regression on time series data 
#for the purposes of making forecasts throughout the data sample. 

#For example, we may wish to produce the series of 2-step-ahead forecasts produced 
#by a VAR(p) model estimated at each point in time.

np.random.seed(1)
import pandas.util.testing as ptest
ptest.N = 500
data = ptest.makeTimeDataFrame().cumsum(0) #columnwise 
>>> data.head()
                   A         B         C         D
2000-01-03  1.624345 -1.719394 -0.153236  1.301225
2000-01-04  1.012589 -1.662273 -2.585745  0.988833
2000-01-05  0.484417 -2.461821 -2.077760  0.717604
2000-01-06 -0.588551 -2.753416 -2.401793  2.580517
2000-01-07  0.276856 -3.012398 -3.912869  1.937644

>>> var = DynamicVAR(data, lag_order=2, window_type='expanding') # {'expanding', 'rolling'}
#returns as pandas.Panel 
#Methods of DynamicVAR are 
T()                             Number of time periods in results 
coefs()                         Return dynamic regression coefficients as WidePanel 
equations()  
forecast([steps])               Produce dynamic forecast 
plot_forecast([steps, figsize]) Plot h-step ahead forecasts against actual realizations of time series. 
r2()                            Returns the r-squared values. 
resid() 


#The estimated coefficients for the dynamic model are returned 
#as a pandas.WidePanel object - stored as 3-dimensional array
#dimensions are items,major_axis,minor_axis for axis=0,1,2

#for example, all of the model coefficients by equation or by date

>>> var.coefs
<class 'pandas.core.panel.WidePanel'>
Dimensions: 9 (items) x 489 (major) x 4 (minor)
Items: L1.A to intercept
Major axis: 2000-01-18 00:00:00 to 2001-11-30 00:00:00
Minor axis: A to D

# all estimated coefficients for equation A
>>> var.coefs.minor_xs('A').info()
Index: 489 entries , 2000-01-18 00:00:00 to 2001-11-30 00:00:00
Data columns:
L1.A         489  non-null values
L1.B         489  non-null values
L1.C         489  non-null values
L1.D         489  non-null values
L2.A         489  non-null values
L2.B         489  non-null values
L2.C         489  non-null values
L2.D         489  non-null values
intercept    489  non-null values
dtype: float64(9)

# coefficients on 11/30/2001
>>> var.coefs.major_xs(datetime(2001, 11, 30)).T
             A              B              C              D
L1.A         0.9567         -0.07389       0.0588         -0.02848
L1.B         -0.00839       0.9757         -0.004945      0.005938
L1.C         -0.01824       0.1214         0.8875         0.01431
L1.D         0.09964        0.02951        0.05275        1.037
L2.A         0.02481        0.07542        -0.04409       0.06073
L2.B         0.006359       0.01413        0.02667        0.004795
L2.C         0.02207        -0.1087        0.08282        -0.01921
L2.D         -0.08795       -0.04297       -0.06505       -0.06814
intercept    0.07778        -0.283         -0.1009        -0.6426


#Dynamic forecasts for a given number of steps ahead can be produced 
#using the forecast function and return a pandas.DataFrame object:
>>> var.forecast(steps=2)
                       A              B              C              D
<snip>
2001-11-23 00:00:00    -6.661         43.18          33.43          -23.71
2001-11-26 00:00:00    -5.942         43.58          34.04          -22.13
2001-11-27 00:00:00    -6.666         43.64          33.99          -22.85
2001-11-28 00:00:00    -6.521         44.2           35.34          -24.29
2001-11-29 00:00:00    -6.432         43.92          34.85          -26.68
2001-11-30 00:00:00    -5.445         41.98          34.87          -25.94


#The forecasts can be visualized using plot_forecast:
>>> var.plot_forecast(2)







###statsmodel - sm.tsa - State Space Methods  
#http://www.statsmodels.org/dev/statespace.html 

#Linear Gaussian state space model is technique involving 
#state equation, Observation equation involving states and initial states 

#state space methods offer a unified approach to a wide range of
#models and techniques: dynamic regression, ARIMA, UC models,
#latent variable models, spline-fitting and many ad-hoc filters

#state space model is linear and Gaussian
#therefore properties and results of multivariate normal distribution apply;
• state vector evolves as a VAR(1) process;
• system matrices usually contain unknown parameters;
• estimation has therefore two aspects:
    ◦ measuring the unobservable state (prediction, filtering and smoothing);
    ◦ estimation of unknown parameters (maximum likelihood estimation);


##Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX)
#Multivariate 
sarimax.SARIMAX(endog[, exog, order, ...])        Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors 
sarimax.SARIMAXResults(model, params, ...[, ...]) Class to hold results from fitting an SARIMAX model. 

# Load the statsmodels api
import statsmodels.api as sm

# Load your dataset
df  = pd.read_csv('your/dataset/here.csv')

# We could fit an AR(2) model, described above
mod_ar2 = sm.tsa.SARIMAX(endog=df.COLUMN1, order=(2,0,0))
# Note that mod_ar2 is an instance of the SARIMAX class

#Can give Multivariate(many exog) dependency of endog - X portion of ARIMAX 
mod_ar2 = sm.tsa.SARIMAX(endog=df.COLUMN1, exog=df['COL2','COL3'], order=(2,0,0))
 

# Fit the model via maximum likelihood
res_ar2 = mod_ar2.fit()
# Note that res_ar2 is an instance of the SARIMAXResults class

# Show the summary of results
print(res_ar2.summary())

# We could also fit a more complicated model with seasonal components.
# As an example, here is an SARIMA(1,1,1) x (0,1,1,4):
mod_sarimax = sm.tsa.SARIMAX(endog, order=(1,1,1),
                             seasonal_order=(0,1,1,4))
res_sarimax = mod_sarimax.fit()

# Show the summary of results
print(res_sarimax.summary())

    
##details
class statsmodels.tsa.statespace.sarimax.SARIMAX(endog, exog=None, 
            order=(1, 0, 0), seasonal_order=(0, 0, 0, 0), 
            trend=None, measurement_error=False, time_varying_regression=False, 
            mle_regression=True, simple_differencing=False, 
            enforce_stationarity=True, enforce_invertibility=True, 
            hamilton_representation=False, **kwargs)
    endog : array_like
        The observed time-series process y
    exog : array_like, optional
        Array of exogenous regressors, shaped nobs x k.
    order : iterable or iterable of iterables, optional
        The (p,d,q) order of the model 
    seasonal_order : iterable, optional
        The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, 
        differences, MA parameters, and periodicity
        s is an integer giving the periodicity (number of periods in season), 
        often it is 4 for quarterly data or 12 for monthly data    
    trend : str{'n','c','t','ct'} or iterable, optional
        Parameter controlling the deterministic trend polynomial A(t). 
        Can be specified as a string where 'c' indicates a constant 
        (i.e. a degree zero component of the trend polynomial), 
        't' indicates a linear trend with time, 
        and 'ct' is both. 
        Can also be specified as an iterable defining the polynomial as in numpy.poly1d, 
        where [1,1,0,1] would denote a + bt + ct^3
    measurement_error : boolean, optional
        Whether or not to assume the endogenous observations endog were measured 
        with error. Default is False.
    time_varying_regression : boolean, optional
        Used when an explanatory variables, exog, are provided provided 
        to select whether or not coefficients on the exogenous regressors 
        are allowed to vary over time. Default is False.
    mle_regression : boolean, optional
        Whether or not to use estimate the regression coefficients 
        for the exogenous variables as part of maximum likelihood estimation 
        or through the Kalman filter (i.e. recursive least squares). 
        If time_varying_regression is True, this must be set to False. 
        Default is True.
    #Attributes
    polynomial_ar (array)       Array containing autoregressive lag polynomial coefficients, ordered from lowest degree to highest. Initialized with ones, unless a coefficient is constrained to be zero (in which case it is zero). 
    polynomial_ma (array)       Array containing moving average lag polynomial coefficients, ordered from lowest degree to highest. Initialized with ones, unless a coefficient is constrained to be zero (in which case it is zero). 
    polynomial_seasonal_ar      (array) Array containing seasonal moving average lag polynomial coefficients, ordered from lowest degree to highest. Initialized with ones, unless a coefficient is constrained to be zero (in which case it is zero). 
    polynomial_seasonal_ma      (array) Array containing seasonal moving average lag polynomial coefficients, ordered from lowest degree to highest. Initialized with ones, unless a coefficient is constrained to be zero (in which case it is zero). 
    polynomial_trend            (array) Array containing trend polynomial coefficients, ordered from lowest degree to highest. Initialized with ones, unless a coefficient is constrained to be zero (in which case it is zero). 
    k_ar (int)                  Highest autoregressive order in the model, zero-indexed. 
    k_ar_params (int)           Number of autoregressive parameters to be estimated. 
    k_diff (int)                Order of intergration. 
    k_ma (int)                  Highest moving average order in the model, zero-indexed. 
    k_ma_params (int)           Number of moving average parameters to be estimated. 
    seasonal_periods            (int) Number of periods in a season. 
    k_seasonal_ar               (int) Highest seasonal autoregressive order in the model, zero-indexed. 
    k_seasonal_ar_params        (int) Number of seasonal autoregressive parameters to be estimated. 
    k_seasonal_diff             (int) Order of seasonal intergration. 
    k_seasonal_ma (int)         Highest seasonal moving average order in the model, zero-indexed. 
    k_seasonal_ma_params        (int) Number of seasonal moving average parameters to be estimated. 
    k_trend (int)               Order of the trend polynomial plus one (i.e. the constant polynomial would have k_trend=1). 
    k_exog (int)                Number of exogenous regressors. 
    update(params[, transformed, complex_step])      Update the parameters of the model 
    fit([start_params, transformed, cov_type, ...])  Fits the model by maximum likelihood via Kalman filter. 
    hessian(params, *args, **kwargs)                 Hessian matrix of the likelihood function, evaluated at the given 
    impulse_responses(params[, steps, impulse, ...]) Impulse response function 
#few Imp Methods 
SARIMAX.fit(start_params=None, transformed=True, cov_type='opg', 
        cov_kwds=None, method='lbfgs', maxiter=50, full_output=1, 
        disp=5, callback=None, return_params=False, optim_score=None, 
        optim_complex_step=None, optim_hessian=None, **kwargs)
    Fits the model by maximum likelihood via Kalman filter.
    Parameters:
    start_params : array_like, optional
        Initial guess of the solution for the loglikelihood maximization. 
        If None, the default is given by Model.start_params.
    transformed : boolean, optional
        Whether or not start_params is already transformed. Default is True.
    cov_type : str, optional
        The cov_type keyword governs the method for calculating 
        the covariance matrix of parameter estimates. Can be one of:
        •'opg' for the outer product of gradient estimator
        •'oim' for the observed information matrix estimator, calculated using the method of Harvey (1989)
        •'approx' for the observed information matrix estimator, calculated using a numerical approximation of the Hessian matrix.
        •'robust' for an approximate (quasi-maximum likelihood) covariance matrix that may be valid even in the presense of some misspecifications. Intermediate calculations use the 'oim' method.
        •'robust_approx' is the same as 'robust' except that the intermediate calculations use the 'approx' method.
        •'none' for no covariance matrix calculation.
    cov_kwds : dict or None, optional
        A dictionary of arguments affecting covariance matrix computation.
        opg, oim, approx, robust, robust_approx
        •'approx_complex_step' : boolean, optional - If True, numerical approximations are computed using complex-step methods. If False, numerical approximations are computed using finite difference methods. Default is True.
        •'approx_centered' : boolean, optional - If True, numerical approximations computed using finite difference methods use a centered approximation. Default is False.
    method : str, optional
        The method determines which solver from scipy.optimize is used, and it can be chosen from among the following strings:
        •'newton' for Newton-Raphson, 'nm' for Nelder-Mead
        •'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)
        •'lbfgs' for limited-memory BFGS with optional box constraints
        •'powell' for modified Powell's method
        •'cg' for conjugate gradient
        •'ncg' for Newton-conjugate gradient
        •'basinhopping' for global basin-hopping solver
        The explicit arguments in fit are passed to the solver, with the exception of the basin-hopping solver. Each solver has several optional arguments that are not the same across solvers. See the notes section below (or scipy.optimize) for the available arguments and for the list of explicit arguments that the basin-hopping solver supports.
    maxiter : int, optional
        The maximum number of iterations to perform.
    full_output : boolean, optional
        Set to True to have all available output in the Results object's mle_retvals attribute. The output is dependent on the solver. See LikelihoodModelResults notes section for more information.
    disp : boolean, optional
        Set to True to print convergence messages.
    callback : callable callback(xk), optional
        Called after each iteration, as callback(xk), where xk is the current parameter vector.
    return_params : boolean, optional
        Whether or not to return only the array of maximizing parameters. Default is False.
    optim_score : {'harvey', 'approx'} or None, optional
        The method by which the score vector is calculated. 'harvey' uses the method from Harvey (1989), 'approx' uses either finite difference or complex step differentiation depending upon the value of optim_complex_step, and None uses the built-in gradient approximation of the optimizer. Default is None. This keyword is only relevant if the optimization method uses the score.
    optim_complex_step : bool, optional
        Whether or not to use complex step differentiation when approximating the score; if False, finite difference approximation is used. Default is True. This keyword is only relevant if optim_score is set to 'harvey' or 'approx'.
    optim_hessian : {'opg','oim','approx'}, optional
        The method by which the Hessian is numerically approximated. 'opg' uses outer product of gradients, 'oim' uses the information matrix formula from Harvey (1989), and 'approx' uses numerical approximation. This keyword is only relevant if the optimization method uses the Hessian matrix.
    **kwargs
        Additional keyword arguments to pass to the optimizer.
 
 
class statsmodels.tsa.statespace.sarimax.SARIMAXResults(model, params, filter_results, cov_type='opg', **kwargs)[source]
    Class to hold results from fitting an SARIMAX model.
    aic()       (float) Akaike Information Criterion 
    arfreq()    (array) Frequency of the roots of the reduced form autoregressive 
    arparams()  (array) Autoregressive parameters actually estimated in the model. 
    arroots()   (array) Roots of the reduced form autoregressive lag polynomial 
    bic()       (float) Bayes Information Criterion 
    bse()  
    conf_int([alpha, cols, method])         Returns the confidence interval of the fitted parameters. 
    cov_params([r_matrix, column, scale, cov_p, ...])   Returns the variance/covariance matrix. 
    cov_params_approx() (array)             The variance / covariance matrix. Computed using the numerical 
    cov_params_oim() (array)                The variance / covariance matrix. Computed using the method 
    cov_params_opg() (array)                The variance / covariance matrix. Computed using the outer 
    cov_params_robust() (array)             The QMLE variance / covariance matrix. Alias for 
    cov_params_robust_approx() (array)      The QMLE variance / covariance matrix. Computed using the 
    cov_params_robust_oim() (array)         The QMLE variance / covariance matrix. Computed using the 
    f_test(r_matrix[, cov_p, scale, invcov]) Compute the F-test for a joint linear hypothesis. 
    fittedvalues() (array)                  The predicted values of the model. An (nobs x k_endog) array. 
    forecast([steps])                       Out-of-sample forecasts 
    get_forecast([steps])                   Out-of-sample forecasts 
    get_prediction([start, end, dynamic, exog])     In-sample prediction and out-of-sample forecasting 
    hqic() (float)                              Hannan-Quinn Information Criterion 
    impulse_responses([steps, impulse, ...])    Impulse response function 
    initialize(model, params, **kwd)  
    llf()               (float) The value of the log-likelihood function evaluated at params. 
    llf_obs()           float) The value of the log-likelihood function evaluated at params. 
    load(fname)         load a pickle, (class method) 
    loglikelihood_burn()    (float) The number of observations during which the likelihood is not 
    mafreq() (array)        Frequency of the roots of the reduced form moving average 
    maparams()              (array) Moving average parameters actually estimated in the model. 
    maroots()               (array) Roots of the reduced form moving average lag polynomial 
    normalized_cov_params()  
    plot_diagnostics([variable, lags, fig, figsize])    Diagnostic plots for standardized residuals of one endogenous variable 
    predict([start, end, dynamic])      In-sample prediction and out-of-sample forecasting 
    pvalues() (array)                   The p-values associated with the z-statistics of the 
    remove_data()               remove data arrays, all nobs arrays from result and model 
    resid() (array)             The model residuals. An (nobs x k_endog) array. 
    save(fname[, remove_data])  save a pickle of this instance 
    simulate(nsimulations[, measurement_shocks, ...]) Simulate a new time series following the state space model 
    summary([alpha, start])     Summarize the Model 
    t_test(r_matrix[, cov_p, scale, use_t])     Compute a t-test for a each linear hypothesis of the form Rb = q 
    test_heteroskedasticity(method[, ...])      Test for heteroskedasticity of standardized residuals 
    test_normality(method)                      Test for normality of standardized residuals. 
    test_serial_correlation(method[, lags])     Ljung-box test for no serial correlation of standardized residuals 
    tvalues()                                   Return the t-statistic for a given parameter estimate. 
    wald_test(r_matrix[, cov_p, scale, invcov, ...]) Compute a Wald-test for a joint linear hypothesis. 
    wald_test_terms([skip_single, ...])         Compute a sequence of Wald tests for terms over multiple columns 
    zvalues() (array)                   The z-statistics for the coefficients. 


SARIMAXResults.simulate(nsimulations, measurement_shocks=None, 
        state_shocks=None, initial_state=None)
    Simulate a new time series following the state space model
    Parameters:
    nsimulations : int
        The number of observations to simulate. 
        If the model is time-invariant this can be any number. 
        If the model is time-varying, 
        then this number must be less than or equal to the number
    Returns:
    simulated_obs : array
        An (nsimulations x k_endog) array of simulated observations.
 
 
SARIMAXResults.summary(alpha=0.05, start=None)[source]
    Summarize the Model
    Parameters:
    alpha : float, optional
        Significance level for the confidence intervals. Default is 0.05.
 
 
 
SARIMAXResults.predict(start=None, end=None, dynamic=False, **kwargs)
    In-sample prediction and out-of-sample forecasting
    Parameters:
    start : int, str, or datetime, optional
        Zero-indexed observation number at which to start forecasting, 
        i.e., the first forecast is start. 
        Can also be a date string to parse or a datetime type. 
        Default is the the zeroth observation.
    end : int, str, or datetime, optional
        Zero-indexed observation number at which to end forecasting, 
        i.e., the last forecast is end. 
        Can also be a date string to parse or a datetime type. 
        However, if the dates index does not have a fixed frequency, 
        end must be an integer index if you want out of sample prediction. 
        Default is the last observation in the sample.
    dynamic : boolean, int, str, or datetime, optional
        Integer offset relative to start at which to begin dynamic prediction
    Returns statsmodels.tsa.statespace.mlemodel.PredictionResults
    Have attributes 
    ['conf_int', 'df', 'dist', 'dist_args', 'link', 'linpred', 'model', 
    'predicted_mean', 'prediction_results', 'row_labels', 'se_mean', 'se_obs', 'summary_frame',
    't_test', 'tvalues', 'var_pred_mean', 'var_resid']
    
    
    
SARIMAXResults.impulse_responses(steps=1, impulse=0, 
            orthogonalized=False, cumulative=False, **kwargs)
    Impulse response function
    Parameters:
    steps : int, optional
        The number of steps for which impulse responses are calculated. 
        Default is 1. Note that the initial impulse is not counted as a step, so if steps=1, the output will have 2 entries.
    impulse : int or array_like
        If an integer, the state innovation to pulse; 
        must be between 0 and k_posdef-1.
        Alternatively, a custom impulse vector may be provided; 
        must be shaped k_posdef x 1.
        k_posdef is the same as in the state space model.
    orthogonalized : boolean, optional
        Whether or not to perform impulse using orthogonalized innovations. 
        Note that this will also affect custum impulse vectors. Default is False.
    cumulative : boolean, optional
        Whether or not to return cumulative impulse responses. D
        efault is False.
    Returns:
        impulse_responses : array
            Responses for each endogenous variable due to the impulse 
            given by the impulse argument. A (steps + 1 x k_endog) array.
 
 
##Regression diagnostics
MLEResults.test_normality(method)[source]
    Test for normality of standardized residuals.
    H0: normal 
    Null hypothesis is normality.
    Parameters:
        method : string {'jarquebera'} or None
 
MLEResults.test_heteroskedasticity(method, alternative='two-sided', use_f=True)[source]
    The null hypothesis is of no heteroskedasticity. 
    That means different things depending on which alternative is selected:
    •Increasing: Null hypothesis is that the variance is not increasing throughout the sample; that the sum-of-squares in the later subsample is not greater than the sum-of-squares in the earlier subsample.
    •Decreasing: Null hypothesis is that the variance is not decreasing throughout the sample; that the sum-of-squares in the earlier subsample is not greater than the sum-of-squares in the later subsample.
    •Two-sided: Null hypothesis is that the variance is not changing throughout the sample. Both that the sum-of-squares in the earlier subsample is not greater than the sum-of-squares in the later subsample and that the sum-of-squares in the later subsample is not greater than the sum-of-squares in the earlier subsample.
    Parameters:
    method : string {'breakvar'} or None
    alternative : string, 'increasing', 'decreasing' or 'two-sided'
    use_f : boolean, optional
        Whether or not to compare against the asymptotic distribution (chi-squared) or the approximate small-sample distribution (F). Default is True (i.e. default is to compare against an F distribution).
    Returns:
    output : array
        An array with (test_statistic, pvalue) for each endogenous variable. 
        The array is then sized (k_endog, 2). 


MLEResults.test_serial_correlation(method, lags=None)[source]
    Ljung-box test for no serial correlation of standardized residuals
    Null hypothesis is no serial correlation.
    Parameters:
        method : string {'ljungbox','boxpierece'} or None
        lags : None, int or array_like
            If lags is an integer then this is taken to be the largest lag 
            that is included, 
            the test result is reported for all smaller lag length. 
            If lags is a list or array, 
            then all lags are included up to the largest lag in the list, 
            however only the tests for the lags in the list are reported. 
            If lags is None, then the default maxlag is 12*(nobs/100)^{1/4}
    Returns:
    output : array
        An array with (test_statistic, pvalue) for each endogenous variable 
        and each lag. The array is then sized (k_endog, 2, lags). 
    
 
SARIMAXResults.plot_diagnostics(variable=0, lags=10, fig=None, figsize=None)
    Diagnostic plots for standardized residuals of one endogenous variable
    Parameters:
    variable : integer, optional
        Index of the endogenous variable for which the diagnostic plots should be created. 
        Default is 0.
    lags : integer, optional
        Number of lags to include in the correlogram. Default is 10.
    fig : Matplotlib Figure instance, optional
        If given, subplots are created in this figure instead of in a new figure. 
        Note that the 2x2 grid will be created in the provided figure 
        using fig.add_subplot().
    figsize : tuple, optional
        If a figure is created, this argument allows specifying a size. 
        The tuple is (width, height).

 
 
 
##Example of SARIMAX 
 

%matplotlib inline
import numpy as np
import pandas as pd
from scipy.stats import norm
import statsmodels.api as sm
import matplotlib.pyplot as plt
from datetime import datetime
import requests
from io import BytesIO


##Examples - ARIMA Example 1: Arima
#1.ARIMA(1,1,1) model on the U.S. Wholesale Price Index (WPI) dataset.

# Dataset
wpi1 = requests.get('http://www.stata-press.com/data/r12/wpi1.dta').content
data = pd.read_stata(BytesIO(wpi1))
data.index = data.t
>>> data.head()
                  wpi          t    ln_wpi
t
1960-01-01  30.700001 1960-01-01  3.424263
1960-04-01  30.799999 1960-04-01  3.427515
1960-07-01  30.700001 1960-07-01  3.424263
1960-10-01  30.700001 1960-10-01  3.424263
1961-01-01  30.799999 1961-01-01  3.427515

# Fit the model
#order =(AR specification, Integration order, MA specification). 
#The integration order must be an integer,
#In a pure ARMA model where the underlying data is already stationary, Integration order would be 0).
mod = sm.tsa.statespace.SARIMAX(data['wpi'], trend='c', order=(1,1,1))
res = mod.fit(disp=False)
>>> print(res.summary())

  Statespace Model Results                           
==============================================================================
Dep. Variable:                    wpi   No. Observations:                  124
Model:               SARIMAX(1, 1, 1)   Log Likelihood                -134.983
Date:                Tue, 28 Feb 2017   AIC                            277.965
Time:                        21:34:51   BIC                            289.246
Sample:                    01-01-1960   HQIC                           282.548
                         - 10-01-1990                                         
Covariance Type:                  opg                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
intercept      0.1050      0.068      1.546      0.122      -0.028       0.238
ar.L1          0.8740      0.054     16.178      0.000       0.768       0.980
ma.L1         -0.4206      0.100     -4.191      0.000      -0.617      -0.224
sigma2         0.5226      0.053      9.926      0.000       0.419       0.626
===================================================================================
Ljung-Box (Q):                       36.96   Jarque-Bera (JB):                10.15
Prob(Q):                              0.61   Prob(JB):                         0.01
Heteroskedasticity (H):              17.00   Skew:                             0.27
Prob(H) (two-sided):                  0.00   Kurtosis:                         4.30
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).

#Thus the maximum likelihood estimates imply that for the process above, we have:
Δy(t) =0.1050+0.8740Δy(t−1) −0.4206ϵ(t−1) +ϵ(t)

    
##ARIMA Example 2: Arima with additive seasonal effects  
#2.Variation of example 1 which adds an MA(4) term to the ARIMA(1,1,1) specification to allow for an additive seasonal effect.
#Before estimating the dataset, graphs showing:
1.The time series (in logs) - non stationary 
2.The first difference of the time series (in logs) - looks stationary 
3.The autocorrelation function - for q
4.The partial autocorrelation function for p 

# Dataset
data = pd.read_stata(BytesIO(wpi1))
data.index = data.t
data['ln_wpi'] = np.log(data['wpi'])
data['D.ln_wpi'] = data['ln_wpi'].diff()

# Graph data
fig, axes = plt.subplots(1, 2, figsize=(15,4))

# Levels
axes[0].plot(data.index._mpl_repr(), data['wpi'], '-')
axes[0].set(title='US Wholesale Price Index')

# Log difference
axes[1].plot(data.index._mpl_repr(), data['D.ln_wpi'], '-')
axes[1].hlines(0, data.index[0], data.index[-1], 'r')
axes[1].set(title='US Wholesale Price Index - difference of logs');

# Graph data
fig, axes = plt.subplots(1, 2, figsize=(15,4))
#first row is NaN because of diff, remove those 
fig = sm.graphics.tsa.plot_acf(data.ix[1:, 'D.ln_wpi'], lags=40, ax=axes[0])
fig = sm.graphics.tsa.plot_pacf(data.ix[1:, 'D.ln_wpi'], lags=40, ax=axes[1])
   
#to specify an ARIMA(1,1,4) process, we would use:
mod = sm.tsa.statespace.SARIMAX(data['wpi'], trend='c', order=(1,1,4))
#here 4 is a maximum degree of the lag polynomial, 
#it implies that all polynomial terms up to that degree are included

#To have a polynomial that has terms for the 1st and 4th degrees, 
#but leaves out the 2nd and 3rd terms.
ar = 1          # this is the maximum degree specification
ma = (1,0,0,1)  # this is the lag polynomial specification
mod = sm.tsa.statespace.SARIMAX(data['wpi'], trend='c', order=(ar,1,ma)))


# Fit the model
mod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,4))
res = mod.fit(disp=False, maxiter=200)
>>> print(res.summary())
                           Statespace Model Results
==============================================================================
Dep. Variable:                 ln_wpi   No. Observations:                  124
Model:               SARIMAX(1, 1, 4)   Log Likelihood                 386.359
Date:                Wed, 31 Jan 2018   AIC                           -758.718
Time:                        05:52:49   BIC                           -738.976
Sample:                    01-01-1960   HQIC                          -750.698
                         - 10-01-1990
Covariance Type:                  opg
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
intercept      0.0026      0.002      1.545      0.122      -0.001       0.006
ar.L1          0.7751      0.106      7.322      0.000       0.568       0.983
ma.L1         -0.3848      0.134     -2.878      0.004      -0.647      -0.123
ma.L2         -0.0524      0.100     -0.524      0.601      -0.249       0.144
ma.L3          0.0675      0.071      0.955      0.340      -0.071       0.206
ma.L4          0.3019      0.125      2.406      0.016       0.056       0.548
sigma2         0.0001   1.07e-05     10.091      0.000    8.72e-05       0.000
================================================================================
===
Ljung-Box (Q):                       30.77   Jarque-Bera (JB):                42.13
Prob(Q):                              0.85   Prob(JB):                         0.00
Heteroskedasticity (H):               2.51   Skew:                             0.41
Prob(H) (two-sided):                  0.00   Kurtosis:                         5.75
================================================================================

 
    
##ARIMA Example 3: Airline Model 
#3.ARIMA(2,1,0) x (1,1,0,12) model of monthly airline data. 
#This example allows a multiplicative seasonal effect
# Dataset
air2 = requests.get('http://www.stata-press.com/data/r12/air2.dta').content
data = pd.read_stata(BytesIO(air2))
data.index = pd.date_range(start=datetime(data.time[0], 1, 1), periods=len(data), freq='MS')
data['lnair'] = np.log(data['air'])  

#Fit the model
#simple_differencing=True, then the time series provided as endog is literatlly differenced and 
#an ARMA model is fit to the resulting new time series. 
#This implies that a number of initial periods are lost to the differencing process, 
#The default is simple_differencing=False, 
#in which case the integration component is implemented as part of the state space formulation, 
#and all of the original data can be used in estimation.

#seasonal - 12 , means yearly 
mod = sm.tsa.statespace.SARIMAX(data['lnair'], order=(2,1,0), seasonal_order=(1,1,0,12), simple_differencing=True)
res = mod.fit(disp=False)
>>> print(res.summary())

                                 Statespace Model Results                                 
==========================================================================================
Dep. Variable:                       D.DS12.lnair   No. Observations:                  131
Model:             SARIMAX(2, 0, 0)x(1, 0, 0, 12)   Log Likelihood                 240.821
Date:                            Tue, 28 Feb 2017   AIC                           -473.643
Time:                                    21:34:54   BIC                           -462.142
Sample:                                02-01-1950   HQIC                          -468.970
                                     - 12-01-1960                                         
Covariance Type:                              opg                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
ar.L1         -0.4057      0.080     -5.045      0.000      -0.563      -0.248
ar.L2         -0.0799      0.099     -0.809      0.419      -0.274       0.114
ar.S.L12      -0.4723      0.072     -6.592      0.000      -0.613      -0.332
sigma2         0.0014      0.000      8.403      0.000       0.001       0.002
===================================================================================
Ljung-Box (Q):                       49.89   Jarque-Bera (JB):                 0.72
Prob(Q):                              0.14   Prob(JB):                         0.70
Heteroskedasticity (H):               0.54   Skew:                             0.14
Prob(H) (two-sided):                  0.04   Kurtosis:                         3.23
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).

    
    
##ARIMA Example 4: ARMAX (Friedman)
#demonstrates the use of explanatory variables (the X part of ARMAX). 
#When exogenous regressors are included, 
#the SARIMAX module uses the concept of "regression with SARIMA errors

# Dataset
friedman2 = requests.get('http://www.stata-press.com/data/r12/friedman2.dta').content
data = pd.read_stata(BytesIO(friedman2))
data.index = data.time

# Variables
endog = data.ix['1959':'1981', 'consump']
>>> endog
time
1959-01-01     310.399994

exog = sm.add_constant(data.ix['1959':'1981', 'm2'])
>>> exog
            const           m2
time
1959-01-01    1.0   289.149994


# Fit the model- endog with two explanatory variables via exog 
mod = sm.tsa.statespace.SARIMAX(endog, exog, order=(1,0,1))
res = mod.fit(disp=False)
>>> print(res.summary())

  Statespace Model Results                           
==============================================================================
Dep. Variable:                consump   No. Observations:                   92
Model:               SARIMAX(1, 0, 1)   Log Likelihood                -340.508
Date:                Tue, 28 Feb 2017   AIC                            691.015
Time:                        21:34:55   BIC                            703.624
Sample:                    01-01-1959   HQIC                           696.105
                         - 10-01-1981                                         
Covariance Type:                  opg                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const        -36.0692     56.618     -0.637      0.524    -147.038      74.900
m2             1.1220      0.036     30.836      0.000       1.051       1.193
ar.L1          0.9348      0.041     22.718      0.000       0.854       1.015
ma.L1          0.3091      0.089      3.488      0.000       0.135       0.483
sigma2        93.2540     10.888      8.565      0.000      71.913     114.595
===================================================================================
Ljung-Box (Q):                       38.72   Jarque-Bera (JB):                23.49
Prob(Q):                              0.53   Prob(JB):                         0.00
Heteroskedasticity (H):              22.51   Skew:                             0.17
Prob(H) (two-sided):                  0.00   Kurtosis:                         5.45
===================================================================================

##Out of sample predictions
#new estimate using data that excludes the last few observations 

# Dataset
raw = pd.read_stata(BytesIO(friedman2))
raw.index = raw.time
data = raw.ix[:'1981']  #upto 1981

true_y = raw.ix['1982':'1991', 'consump']

# Variables
endog = data.ix['1959':, 'consump']  #from 1959 to 1981 
exog = sm.add_constant(data.ix['1959':, 'm2'])
nobs = endog.shape[0]

# Fit the model  - data from 1959 to 1978 
mod = sm.tsa.statespace.SARIMAX(endog.ix[:'1978-01-01'], exog=exog.ix[:'1978-01-01'], order=(1,0,1))
fit_res = mod.fit(disp=False)
>>> print(fit_res.summary())
                           Statespace Model Results                           
==============================================================================
Dep. Variable:                consump   No. Observations:                   77
Model:               SARIMAX(1, 0, 1)   Log Likelihood                -243.316
Date:                Tue, 28 Feb 2017   AIC                            496.633
Time:                        21:34:55   BIC                            508.352
Sample:                    01-01-1959   HQIC                           501.320
                         - 01-01-1978                                         
Covariance Type:                  opg                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.6776     18.491      0.037      0.971     -35.565      36.920
m2             1.0379      0.021     50.330      0.000       0.997       1.078
ar.L1          0.8775      0.059     14.859      0.000       0.762       0.993
ma.L1          0.2771      0.108      2.572      0.010       0.066       0.488
sigma2        31.6977      4.683      6.769      0.000      22.519      40.876
===================================================================================
Ljung-Box (Q):                       46.77   Jarque-Bera (JB):                 6.05
Prob(Q):                              0.21   Prob(JB):                         0.05
Heteroskedasticity (H):               6.09   Skew:                             0.57
Prob(H) (two-sided):                  0.00   Kurtosis:                         3.76
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).


#fit_res - from 1959 to 1978 
#res - from 1959 to 1981 
mod = sm.tsa.statespace.SARIMAX(endog, exog=exog, order=(1,0,1))

#Kalman filtering,  linear quadratic estimation (LQE), (but nonlinear version also exists)
#In the prediction step, the Kalman filter produces estimates of the current state variables, 
#along with their uncertainties. 
#Once the outcome of the next measurement (necessarily corrupted with some amount of error, including random noise) is observed, 
#these estimates are updated using a weighted average, 
#with more weight being given to estimates with higher certainty. 
#The algorithm is recursive. 
#It can run in real time, using only the present input measurements 
#and the previously calculated state and its uncertainty matrix; 

res = mod.filter(fit_res.params) #based on fit_res.params, get this model Result 


# In-sample one-step-ahead predictions
#One-step-ahead prediction uses the true values of the endogenous values 
#at each step to predict the next in-sample value. 
predict = res.get_prediction() #statsmodels.tsa.statespace.mlemodel.PredictionResults
predict_ci = predict.conf_int()

>>> predict.summary_frame()
consump            mean    mean_se  mean_ci_lower  mean_ci_upper
1959-01-01   300.782333  14.680043     272.009978     329.554687
1959-04-01   314.700037   5.810863     303.310955     326.089118
1959-07-01   318.334225   5.642810     307.274521     329.393928

#predictionResult has below methods 
'conf_int', 'df', 'dist', 'dist_args', 'link', 'linpred', 'model', 
'predicted_man', 'prediction_results', 'row_labels', 
'se_mean', 'se_obs', 'summary_frame','t_test', 'tvalues', 
'var_pred_mean', 'var_resid'


#note 
#fit_res - from 1959 to 1978 
#res - from 1959 to 1981 

#Dynamic predictions use one-step-ahead prediction up to some point in the dataset 
#(specified by the dynamic argument); 
#after that, the previous predicted endogenous values are used 
#in place of the true endogenous values for each new predicted element.

#The dynamic argument is specified to be an offset relative to the start argument. If start is not specified, it is assumed to be 0.
#perform dynamic prediction starting in the first quarter of 1978.
predict_dy = res.get_prediction(dynamic='1978-01-01')
predict_dy_ci = predict_dy.conf_int()
>>> predict_dy.summary_frame()
consump            mean    mean_se  mean_ci_lower  mean_ci_upper
1959-01-01   300.782333  14.680043     272.009978     329.554687
1959-04-01   314.700037   5.810863     303.310955     326.089118
1959-07-01   318.334225   5.642810     307.274521     329.393928


#Difference in prediction performance 
#Notice that up to the point where dynamic prediction begins (1978:Q1), 
#the two are the same.

fig, ax = plt.subplots(figsize=(9,4))
npre = 4
ax.set(title='Personal consumption', xlabel='Date', ylabel='Billions of dollars')

# Plot data points
data.ix['1977-07-01':, 'consump'].plot(ax=ax, style='o', label='Observed')

# Plot predictions
predict.predicted_mean.ix['1977-07-01':].plot(ax=ax, style='r--', label='One-step-ahead forecast')
ci = predict_ci.ix['1977-07-01':]
ax.fill_between(ci.index, ci.ix[:,0], ci.ix[:,1], color='r', alpha=0.1)
predict_dy.predicted_mean.ix['1977-07-01':].plot(ax=ax, style='g', label='Dynamic forecast (1978)')
ci = predict_dy_ci.ix['1977-07-01':]
ax.fill_between(ci.index, ci.ix[:,0], ci.ix[:,1], color='g', alpha=0.1)

legend = ax.legend(loc='lower right')


# Prediction error
#one-step-ahead prediction is considerably better.

fig, ax = plt.subplots(figsize=(9,4))
npre = 4
ax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')

# In-sample one-step-ahead predictions and 95% confidence intervals
predict_error = predict.predicted_mean - endog
predict_error.ix['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')
ci = predict_ci.ix['1977-10-01':].copy()
ci.iloc[:,0] -= endog.loc['1977-10-01':]
ci.iloc[:,1] -= endog.loc['1977-10-01':]
ax.fill_between(ci.index, ci.ix[:,0], ci.ix[:,1], alpha=0.1)

# Dynamic predictions and 95% confidence intervals
predict_dy_error = predict_dy.predicted_mean - endog
predict_dy_error.ix['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')
ci = predict_dy_ci.ix['1977-10-01':].copy()
ci.iloc[:,0] -= endog.loc['1977-10-01':]
ci.iloc[:,1] -= endog.loc['1977-10-01':]
ax.fill_between(ci.index, ci.ix[:,0], ci.ix[:,1], color='r', alpha=0.1)

legend = ax.legend(loc='lower left');
legend.get_frame().set_facecolor('w')


#outof sample predictions 
true_y = raw.ix['1982':'1991', 'consump']
X = sm.add_constant(raw.ix['1982':'1991', 'm2'])
start = len(endog)  #insample till this point 
howmany = len(true_y)
p_out = res.get_prediction(start=start, end=start+howmany-1, exog=X)
predicted_out = p_out.summary_frame().copy()
predicted_out.index = true_y.index 
predicted_out['y_true']= true_y 
predicted_out['error']= predicted_out['y_true'] - predicted_out['mean']

#plot 
fig, axes = plt.subplots(2, 1)
predicted_out[ [ 'y_true', 'mean', 'mean_ci_lower', 'mean_ci_upper' ] ].plot(kind='line', ax=axes[0])
axes[0].set_title("Predicted vs True values", x=0)
predicted_out['error'].plot(kind='line', ax= axes[1])
axes[1].set_title("Error", x=0)
plt.show()



##Statsspace mdeols - Unobserved Components
#These are also known as structural time series models,
#and decompose a (univariate) time series into trend, seasonal, cyclical, and irregular components.
structural.UnobservedComponents(endog[, ...])   Univariate unobserved components time series model 
structural.UnobservedComponentsResults(...)     Class to hold results from fitting an unobserved components model. 


class statsmodels.tsa.statespace.structural.UnobservedComponents(endog, 
        level=False, trend=False, seasonal=None, cycle=False, autoregressive=None, 
        exog=None, irregular=False, stochastic_level=False, 
        stochastic_trend=False, stochastic_seasonal=True, 
        stochastic_cycle=False, damped_cycle=False, 
        cycle_period_bounds=None, mle_regression=True, **kwargs)
    level : bool or string, optional
        Whether or not to include a level component
    trend : bool, optional
        Whether or not to include a trend component. Default is False. 
        If True, level must also be True.
    seasonal : int or None, optional
        The period of the seasonal component, if any. Default is None.
    cycle : bool, optional
        Whether or not to include a cycle component. Default is False.
    autoregressive : int or None, optional
        The order of the autoregressive component. Default is None.
    exog : array_like or None, optional
        Exogenous variables.
    irregular : bool, optional
        Whether or not to include an irregular component. Default is False.
    stochastic_level : bool, optional
        Whether or not any level component is stochastic. Default is False.
    stochastic_trend : bool, optional
        Whether or not any trend component is stochastic. Default is False.
    stochastic_seasonal : bool, optional
        Whether or not any seasonal component is stochastic. Default is False.
    stochastic_cycle : bool, optional
        Whether or not any cycle component is stochastic. Default is False.
    damped_cycle : bool, optional
        Whether or not the cycle component is damped. Default is False.
    cycle_period_bounds : tuple, optional
        A tuple with lower and upper allowed bounds for the period of the cycle
#Details 
    y_t = mu_t + gamma_t + c_t + epsilon_t
#Where 
mu_t - trend including level (result in UnobservedComponentsResults.level , .trend attributes)
    where the level is a generalization of the intercept term 
    that can dynamically vary across time, 
    and the trend is a generalization of the time-trend such 
    that the slope can dynamically vary across time.
    For both elements (level and trend)
        •The element is included vs excluded (if the trend is included, there must also be a level included).
        •The element is deterministic vs stochastic (i.e. whether or not the variance on the error term is confined to be zero or not)
    The level/trend components can be specified using the boolean 
    keyword arguments level, stochastic_level, trend, etc., 
    or all at once as a string argument to level. 
    #Model name                         Full string syntax                  Abbreviated syntax
    No trend                            'irregular'                         'ntrend' 
    Fixed intercept                     'fixed intercept'   
    Deterministic constant              'deterministic constant'            'dconstant' 
    Local level                         'local level'                       'llevel' 
    Random walk                         'random walk'                       'rwalk' 
    Fixed slope                         'fixed slope'   
    Deterministic trend                 'deterministic trend'               'dtrend' 
    Local linear deterministic trend    'local linear deterministic trend'  'lldtrend' 
    Random walk with drift              'random walk with drift'            'rwdrift' 
    Local linear trend                  'local linear trend'                'lltrend' 
    Smooth trend                        'smooth trend'                      'strend' 
    Random trend                        'random trend'                      'rtrend' 
gamma_t - Seasonal (result in UnobservedComponentsResults.seasonal )
    The periodicity (number of seasons) is s, 
c_t - Cycle (result in UnobservedComponentsResults.cycle )
    The cyclical component is intended to capture cyclical effects at time frames 
    much longer than captured by the seasonal component. 
    For example, in economics the cyclical term is often intended to capture 
    the business cycle, and is then expected to have a period between '1.5 and 12 years'
    Could be stochastic_cycle and/or damped_cycle
epsilon_t - Irregular
    The irregular components are independent and identically distributed (iid)
Autoregressive Irregular - (result in UnobservedComponentsResults.autoregressive)
    For  white noise irregular component, specify via autoregressive 
Regression effects - (result in UnobservedComponentsResults.regression_coefficients)
    For Exogenous regressors via exog argument 
    
#Few Methods 
aic()                           (float) Akaike Information Criterion 
bic()                           (float) Bayes Information Criterion 
bse()  
conf_int([alpha, cols, method]) Returns the confidence interval of the fitted parameters. 

forecast([steps])               Out-of-sample forecasts 
get_forecast([steps])           Out-of-sample forecasts 
get_prediction([start, end, dynamic, exog])         In-sample prediction and out-of-sample forecasting 
predict([start, end, dynamic])                      In-sample prediction and out-of-sample forecasting 
impulse_responses([steps, impulse, ...])            Impulse response function 

plot_components([which, alpha, observed, ...])      Plot the estimated components of the model. 
plot_diagnostics([variable, lags, fig, figsize])    Diagnostic plots for standardized residuals of one endogenous variable 
save(fname[, remove_data])                          save a pickle of this instance 
load(fname)                                         load a pickle, (class method) 
simulate(nsimulations[, measurement_shocks, ...])   Simulate a new time series following the state space model 
summary([alpha, start])                             Summarize the Model 

t_test(r_matrix[, cov_p, scale, use_t])             Compute a t-test for a each linear hypothesis of the form Rb = q 
test_heteroskedasticity(method[, ...])              Test for heteroskedasticity of standardized residuals 
test_normality(method)                              Test for normality of standardized residuals. 
test_serial_correlation(method[, lags])             Ljung-box test for no serial correlation of standardized residuals 
wald_test(r_matrix[, cov_p, scale, invcov, ...])    Compute a Wald-test for a joint linear hypothesis. 

    
UnobservedComponentsResults.plot_components(which=None, alpha=0.05, observed=True, level=True, trend=True, seasonal=True, cycle=True, autoregressive=True, legend_loc='upper right', fig=None, figsize=None)[source]
    Plot the estimated components of the model.
    which : {'filtered', 'smoothed'}, or None, optional
        Type of state estimate to plot. 
        Default is 'smoothed' if smoothed results are available otherwise 'filtered'.
    alpha : float, optional
        The confidence intervals for the components are (1 - alpha) %
    Draws below 
    0.Observed series against predicted series
    1.Level
    2.Trend
    3.Seasonal
    4.Cycle
    5.Autoregressive


##Example 
http://www.statsmodels.org/dev/examples/notebooks/generated/statespace_cycles.html
#Trends and cycles in unemployment - Three methods 
%matplotlib inline

import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt

from pandas_datareader.data import DataReader
endog = DataReader('UNRATE', 'fred', start='1954-01-01')

#First Methods - Hodrick-Prescott (HP) filter
#specify the parameter λ=129600  because the unemployment rate is observed monthly.
#quarterly  - 1600, annual - 6.25, monthly - 129600 

hp_cycle, hp_trend = sm.tsa.filters.hpfilter(endog, lamb=129600)




##2nd method - Unobserved components and ARIMA model (UC-ARIMA)
#where the trend is modeled as a random walk 
#and the irregular is modeled with an ARIMA model 
mod_ucarima = sm.tsa.UnobservedComponents(endog, 'rwalk', autoregressive=4)
# Here the powell method is used, since it achieves a
# higher loglikelihood than the default L-BFGS method
res_ucarima = mod_ucarima.fit(method='powell', disp=False)
#check for level and AR irregular
res_ucarima.level, res_ucarima.autoregressive
res_ucarima.plot_components()
plt.show()

print(res_ucarima.summary())
                       Unobserved Components Results                         
==============================================================================
Dep. Variable:                 UNRATE   No. Observations:                  757
Model:                    random walk   Log Likelihood                 236.446
                              + AR(4)   AIC                           -460.892
Date:                Tue, 28 Feb 2017   BIC                           -433.116
Time:                        21:34:27   HQIC                          -450.194
Sample:                    01-01-1954                                         
                         - 01-01-2017                                         
Covariance Type:                  opg                                         
================================================================================
                   coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------
sigma2.level     0.0176      0.003      6.179      0.000       0.012       0.023
sigma2.ar        0.0112      0.003      3.533      0.000       0.005       0.017
ar.L1            1.0418      0.066     15.677      0.000       0.912       1.172
ar.L2            0.4618      0.106      4.358      0.000       0.254       0.669
ar.L3           -0.3254      0.125     -2.605      0.009      -0.570      -0.081
ar.L4           -0.1904      0.077     -2.484      0.013      -0.341      -0.040
===================================================================================
Ljung-Box (Q):                       76.62   Jarque-Bera (JB):                38.64
Prob(Q):                              0.00   Prob(JB):                         0.00
Heteroskedasticity (H):               0.51   Skew:                             0.23
Prob(H) (two-sided):                  0.00   Kurtosis:                         4.01
===================================================================================



##3rd Method - Unobserved components with stochastic cycle (UC)
#where the cycle is modeled explicitly.
mod_uc = sm.tsa.UnobservedComponents(
    endog, 'rwalk',
    cycle=True, stochastic_cycle=True, damped_cycle=True,
)
# Here the powell method gets close to the optimum
res_uc = mod_uc.fit(method='powell', disp=False)
# but to get to the highest loglikelihood we do a
# second round using the L-BFGS method.
res_uc = mod_uc.fit(res_uc.params, disp=False)

#Check 
res_uc.level, res_uc.cycle
res_uc.plot_components()
plt.show()
print(res_uc.summary())
                           Unobserved Components Results                            
=====================================================================================
Dep. Variable:                        UNRATE   No. Observations:                  757
Model:                           random walk   Log Likelihood                 204.432
                   + damped stochastic cycle   AIC                           -400.864
Date:                       Tue, 28 Feb 2017   BIC                           -382.347
Time:                               21:34:28   HQIC                          -393.732
Sample:                           01-01-1954                                         
                                - 01-01-2017                                         
Covariance Type:                         opg                                         
===================================================================================
                      coef    std err          z      P>|z|      [0.025      0.975]
-----------------------------------------------------------------------------------
sigma2.level        0.0102      0.004      2.341      0.019       0.002       0.019
sigma2.cycle        0.0214      0.004      4.911      0.000       0.013       0.030
frequency.cycle     0.0657      0.005     13.175      0.000       0.056       0.076
damping.cycle       0.9912      0.004    266.360      0.000       0.984       0.999
===================================================================================
Ljung-Box (Q):                      148.61   Jarque-Bera (JB):                79.57
Prob(Q):                              0.00   Prob(JB):                         0.00
Heteroskedasticity (H):               0.49   Skew:                             0.45
Prob(H) (two-sided):                  0.00   Kurtosis:                         4.31
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).

##2nd Example 
#http://www.statsmodels.org/dev/examples/notebooks/generated/statespace_structural_harvey_jaeger.html
#Data
#Following Harvey and Jaeger, we will consider the following time series:
•US real GNP, "output", (GNPC96)
•US GNP implicit price deflator, "prices", (GNPDEF)
•US monetary base, "money", (AMBSL)

#All data series considered here are taken 
#from Federal Reserve Economic Data (FRED). 
# Datasets
from pandas_datareader.data import DataReader

# Get the raw data
start = '1948-01'
end = '2008-01'
us_gnp = DataReader('GNPC96', 'fred', start=start, end=end)
us_gnp_deflator = DataReader('GNPDEF', 'fred', start=start, end=end)
us_monetary_base = DataReader('AMBSL', 'fred', start=start, end=end).resample('QS').mean()
recessions = DataReader('USRECQ', 'fred', start=start, end=end).resample('QS').last().values[:,0]

# Construct the dataframe after log transforming 
dta = pd.concat(map(np.log, (us_gnp, us_gnp_deflator, us_monetary_base)), axis=1)
dta.columns = ['US GNP','US Prices','US monetary base']
dates = dta.index._mpl_repr()


# Plot the data
ax = dta.plot(figsize=(13,3))
ylim = ax.get_ylim()
ax.xaxis.grid()
ax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);

# Model specifications

# Unrestricted model, using string specification
unrestricted_model = {
    'level': 'local linear trend', 'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True
}

# Unrestricted model, setting components directly
# This is an equivalent, but less convenient, way to specify a
# local linear trend model with a stochastic damped cycle:
# unrestricted_model = {
#     'irregular': True, 'level': True, 'stochastic_level': True, 'trend': True, 'stochastic_trend': True,
#     'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True
# }

# The restricted model forces a smooth trend
restricted_model = {
    'level': 'smooth trend', 'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True
}

# Restricted model, setting components directly
# This is an equivalent, but less convenient, way to specify a
# smooth trend model with a stochastic damped cycle. Notice
# that the difference from the local linear trend model is that
# `stochastic_level=False` here.
# unrestricted_model = {
#     'irregular': True, 'level': True, 'stochastic_level': False, 'trend': True, 'stochastic_trend': True,
#     'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True
# }

#We now fit the following models:
#    Output, unrestricted model
#    Prices, unrestricted model
#    Prices, restricted model
#    Money, unrestricted model
#    Money, restricted model

# Output
output_mod = sm.tsa.UnobservedComponents(dta['US GNP'], **unrestricted_model)
output_res = output_mod.fit(method='powell', disp=False)

# Prices
prices_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **unrestricted_model)
prices_res = prices_mod.fit(method='powell', disp=False)

prices_restricted_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **restricted_model)
prices_restricted_res = prices_restricted_mod.fit(method='powell', disp=False)

# Money
money_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **unrestricted_model)
money_res = money_mod.fit(method='powell', disp=False)

money_restricted_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **restricted_model)
money_restricted_res = money_restricted_mod.fit(method='powell', disp=False)

#check one 
print(output_res.summary())

                            Unobserved Components Results                            
=====================================================================================
Dep. Variable:                        US GNP   No. Observations:                  241
Model:                    local linear trend   Log Likelihood                 770.164
                   + damped stochastic cycle   AIC                          -1528.328
Date:                       Tue, 28 Feb 2017   BIC                          -1507.420
Time:                               21:35:01   HQIC                         -1519.905
Sample:                           01-01-1948                                         
                                - 01-01-2008                                         
Covariance Type:                         opg                                         
====================================================================================
                       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------------
sigma2.irregular  4.843e-08   7.26e-06      0.007      0.995   -1.42e-05    1.43e-05
sigma2.level      7.352e-06   4.93e-05      0.149      0.882   -8.93e-05       0.000
sigma2.trend      3.007e-06   1.43e-06      2.102      0.036    2.03e-07    5.81e-06
sigma2.cycle      3.834e-05   2.55e-05      1.503      0.133   -1.17e-05    8.83e-05
frequency.cycle      0.4546      0.049      9.285      0.000       0.359       0.551
damping.cycle        0.8639      0.043     20.232      0.000       0.780       0.948
===================================================================================
Ljung-Box (Q):                       40.32   Jarque-Bera (JB):                 9.86
Prob(Q):                              0.46   Prob(JB):                         0.01
Heteroskedasticity (H):               0.27   Skew:                            -0.04
Prob(H) (two-sided):                  0.00   Kurtosis:                         4.00
===================================================================================

#plot_components 
fig = output_res.plot_components(legend_loc='lower right', figsize=(15, 9));



#Harvey and Jaeger summarize the models in another way 
#to highlight the relative importances of the trend and cyclical components

# Create Table I
table_i = np.zeros((5,6))

start = dta.index[0]
end = dta.index[-1]
time_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)
models = [
    ('US GNP', time_range, 'None'),
    ('US Prices', time_range, 'None'),
    ('US Prices', time_range, r'$\sigma_\eta^2 = 0$'),
    ('US monetary base', time_range, 'None'),
    ('US monetary base', time_range, r'$\sigma_\eta^2 = 0$'),
]
index = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])
parameter_symbols = [
    r'$\sigma_\zeta^2$', r'$\sigma_\eta^2$', r'$\sigma_\kappa^2$', r'$\rho$',
    r'$2 \pi / \lambda_c$', r'$\sigma_\varepsilon^2$',
]

i = 0
for res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):
    if res.model.stochastic_level:
        (sigma_irregular, sigma_level, sigma_trend,
         sigma_cycle, frequency_cycle, damping_cycle) = res.params
    else:
        (sigma_irregular, sigma_level,
         sigma_cycle, frequency_cycle, damping_cycle) = res.params
        sigma_trend = np.nan
    period_cycle = 2 * np.pi / frequency_cycle
    
    table_i[i, :] = [
        sigma_level*1e7, sigma_trend*1e7,
        sigma_cycle*1e7, damping_cycle, period_cycle,
        sigma_irregular*1e7
    ]
    i += 1
    
pd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')
table_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)
table_i


##State space Model - Vector Autoregressive Moving-Average with eXogenous regressors (VARMAX)
# multivariate statespace model.

varmax.VARMAX(endog[, exog, order, trend, ...]) Vector Autoregressive Moving Average with eXogenous regressors model 
varmax.VARMAXResults(model, params, ...[, ...]) Class to hold results from fitting an VARMAX model. 

##SOP 
# Load the statsmodels api
import statsmodels.api as sm

# Load your (multivariate) dataset
endog = pd.read_csv('your/dataset/here.csv')

# Fit a local level model
mod_var1 = sm.tsa.VARMAX(endog, order=(1,0))
# Note that mod_var1 is an instance of the VARMAX class

# Fit the model via maximum likelihood
res_var1 = mod_var1.fit()
# Note that res_var1 is an instance of the VARMAXResults class

# Show the summary of results
print(res_var1.summary())

# Construct impulse responses
irfs = res_ll.impulse_responses(steps=10)

##Example
%matplotlib inline

import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt

dta = sm.datasets.webuse('lutkepohl2', 'http://www.stata-press.com/data/r12/')
dta.index = dta.qtr
endog = dta.ix['1960-04-01':'1978-10-01', ['dln_inv', 'dln_inc', 'dln_consump']]

#The VARMAX class in Statsmodels allows estimation of VAR, VMA, and VARMA models 
#(through the order argument), 
#optionally with a constant term (via the trend argument). 
#Exogenous regressors may also be included (by the exog argument), 
#class allows measurement error  (via the measurement_error argument) 

#Example 1: VARX(2) , two endogenous variables and an exogenous series
exog = endog['dln_consump']
mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(2,0), trend='nc', exog=exog)
res = mod.fit(maxiter=1000, disp=False)
>>> print(res.summary())

                             Statespace Model Results                             
==================================================================================
Dep. Variable:     ['dln_inv', 'dln_inc']   No. Observations:                   75
Model:                            VARX(2)   Log Likelihood                 348.269
Date:                    Tue, 28 Feb 2017   AIC                           -670.537
Time:                            21:35:16   BIC                           -640.410
Sample:                        04-01-1960   HQIC                          -658.508
                             - 10-01-1978                                         
Covariance Type:                      opg                                         
===================================================================================
Ljung-Box (Q):                59.41, 42.46   Jarque-Bera (JB):         16.41, 13.03
Prob(Q):                        0.02, 0.37   Prob(JB):                   0.00, 0.00
Heteroskedasticity (H):         0.46, 1.03   Skew:                      0.05, -0.64
Prob(H) (two-sided):            0.06, 0.95   Kurtosis:                   5.29, 4.59
                            Results for equation dln_inv                            
====================================================================================
                       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------------
L1.dln_inv          -0.2773      0.088     -3.149      0.002      -0.450      -0.105
L1.dln_inc           0.3374      0.620      0.544      0.586      -0.877       1.552
L2.dln_inv          -0.1160      0.157     -0.740      0.459      -0.423       0.191
L2.dln_inc           0.3971      0.387      1.025      0.305      -0.362       1.156
beta.dln_consump     0.5448      0.752      0.724      0.469      -0.929       2.019
                            Results for equation dln_inc                            
====================================================================================
                       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------------
L1.dln_inv           0.0335      0.042      0.794      0.427      -0.049       0.116
L1.dln_inc           0.0962      0.133      0.726      0.468      -0.164       0.356
L2.dln_inv           0.0516      0.051      1.017      0.309      -0.048       0.151
L2.dln_inc           0.2734      0.169      1.622      0.105      -0.057       0.604
beta.dln_consump     0.4818      0.198      2.427      0.015       0.093       0.871
                                  Error covariance matrix                                   
============================================================================================
                               coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------------
sqrt.var.dln_inv             0.0441      0.003     14.164      0.000       0.038       0.050
sqrt.cov.dln_inv.dln_inc     0.0013      0.002      0.549      0.583      -0.003       0.006
sqrt.var.dln_inc            -0.0127      0.001    -12.420      0.000      -0.015      -0.011
============================================================================================

#plot the impulse response functions of the endogenous variables.
ax = res.impulse_responses(10, orthogonalized=True).plot(figsize=(13,3))
ax.set(xlabel='t', title='Responses to a shock to `dln_inv`');

#Example 2: VMA(2)
#where the innovations to the process are uncorrelated.
#and  include the constant term.

mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(0,2), error_cov_type='diagonal')
res = mod.fit(maxiter=1000, disp=False)
>>> print(res.summary())

                             Statespace Model Results                             
==================================================================================
Dep. Variable:     ['dln_inv', 'dln_inc']   No. Observations:                   75
Model:                             VMA(2)   Log Likelihood                 353.887
                              + intercept   AIC                           -683.775
Date:                    Tue, 28 Feb 2017   BIC                           -655.965
Time:                            21:35:20   HQIC                          -672.670
Sample:                        04-01-1960                                         
                             - 10-01-1978                                         
Covariance Type:                      opg                                         
===================================================================================
Ljung-Box (Q):                68.49, 39.14   Jarque-Bera (JB):         12.79, 13.08
Prob(Q):                        0.00, 0.51   Prob(JB):                   0.00, 0.00
Heteroskedasticity (H):         0.44, 0.81   Skew:                      0.06, -0.48
Prob(H) (two-sided):            0.04, 0.60   Kurtosis:                   5.02, 4.81
                           Results for equation dln_inv                          
=================================================================================
                    coef    std err          z      P>|z|      [0.025      0.975]
---------------------------------------------------------------------------------
const             0.0182      0.005      3.800      0.000       0.009       0.028
L1.e(dln_inv)    -0.2572      0.106     -2.430      0.015      -0.465      -0.050
L1.e(dln_inc)     0.5135      0.633      0.812      0.417      -0.726       1.753
L2.e(dln_inv)     0.0295      0.149      0.198      0.843      -0.263       0.322
L2.e(dln_inc)     0.1819      0.475      0.383      0.702      -0.749       1.113
                           Results for equation dln_inc                          
=================================================================================
                    coef    std err          z      P>|z|      [0.025      0.975]
---------------------------------------------------------------------------------
const             0.0207      0.002     13.087      0.000       0.018       0.024
L1.e(dln_inv)     0.0487      0.042      1.173      0.241      -0.033       0.130
L1.e(dln_inc)    -0.0789      0.139     -0.568      0.570      -0.351       0.193
L2.e(dln_inv)     0.0176      0.042      0.415      0.678      -0.066       0.101
L2.e(dln_inc)     0.1290      0.153      0.844      0.399      -0.170       0.428
                             Error covariance matrix                              
==================================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
----------------------------------------------------------------------------------
sigma2.dln_inv     0.0020      0.000      7.351      0.000       0.001       0.003
sigma2.dln_inc     0.0001   2.33e-05      5.822      0.000       9e-05       0.000
==================================================================================

#Caution: VARMA(p,q) specifications
#Although the model allows estimating VARMA(p,q) specifications, 
#these models are not identified without additional restrictions 
#on the representation matrices, which are not built-in. 
#For this reason, a warning is issued when these models are specified 

mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))
res = mod.fit(maxiter=1000, disp=False)
>>> print(res.summary())

/private/tmp/statsmodels/statsmodels/tsa/statespace/varmax.py:153: EstimationWarning: Estimation of VARMA(p,q) models is not generically robust, due especially to identification issues.
  EstimationWarning)

                             Statespace Model Results                             
==================================================================================
Dep. Variable:     ['dln_inv', 'dln_inc']   No. Observations:                   75
Model:                         VARMA(1,1)   Log Likelihood                 354.291
                              + intercept   AIC                           -682.583
Date:                    Tue, 28 Feb 2017   BIC                           -652.455
Time:                            21:35:24   HQIC                          -670.553
Sample:                        04-01-1960                                         
                             - 10-01-1978                                         
Covariance Type:                      opg                                         
===================================================================================
Ljung-Box (Q):                69.15, 40.99   Jarque-Bera (JB):         11.03, 18.14
Prob(Q):                        0.00, 0.43   Prob(JB):                   0.00, 0.00
Heteroskedasticity (H):         0.45, 0.78   Skew:                      0.01, -0.52
Prob(H) (two-sided):            0.05, 0.54   Kurtosis:                   4.88, 5.17
                           Results for equation dln_inv                          
=================================================================================
                    coef    std err          z      P>|z|      [0.025      0.975]
---------------------------------------------------------------------------------
const             0.0103      0.064      0.161      0.872      -0.115       0.135
L1.dln_inv       -0.0059      0.685     -0.009      0.993      -1.349       1.337
L1.dln_inc        0.3915      2.708      0.145      0.885      -4.916       5.699
L1.e(dln_inv)    -0.2446      0.696     -0.352      0.725      -1.608       1.119
L1.e(dln_inc)     0.1210      2.963      0.041      0.967      -5.686       5.928
                           Results for equation dln_inc                          
=================================================================================
                    coef    std err          z      P>|z|      [0.025      0.975]
---------------------------------------------------------------------------------
const             0.0164      0.026      0.619      0.536      -0.036       0.068
L1.dln_inv       -0.0318      0.273     -0.116      0.907      -0.568       0.504
L1.dln_inc        0.2372      1.079      0.220      0.826      -1.878       2.352
L1.e(dln_inv)     0.0872      0.280      0.312      0.755      -0.461       0.636
L1.e(dln_inc)    -0.2367      1.115     -0.212      0.832      -2.422       1.949
                                  Error covariance matrix                                   
============================================================================================
                               coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------------
sqrt.var.dln_inv             0.0449      0.003     14.510      0.000       0.039       0.051
sqrt.cov.dln_inv.dln_inc     0.0017      0.003      0.654      0.513      -0.003       0.007
sqrt.var.dln_inc             0.0116      0.001     11.775      0.000       0.010       0.013
============================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).



##States Space Model - Dynamic Factor Models
#Factor models generally try to find a small number of unobserved "factors" 
#that influence a subtantial portion of the variation in a larger number of observed variables, 
 

#Dynamic factor models explicitly model the transition dynamics of the unobserved factors, 
#and so are often applied to time-series data.

dynamic_factor.DynamicFactor(endog, ...[, ...]) Dynamic factor model 
dynamic_factor.DynamicFactorResults(model, ...) Class to hold results from fitting an DynamicFactor model. 


class statsmodels.tsa.statespace.dynamic_factor.DynamicFactor(endog, k_factors, 
        factor_order, exog=None, error_order=0, 
        error_var=False, error_cov_type='diagonal', enforce_stationarity=True, **kwargs)[source]
    Parameters:
    endog : array_like
        The observed time-series process y
    exog : array_like, optional
        Array of exogenous regressors for the observation equation, shaped nobs x k_exog.
    k_factors : int
        The number of unobserved factors.
        To exclude factors completely, set k_factors = 0.
    factor_order : int
        The order of the vector autoregression followed by the factors.
        this is the number of lags to include in the factor evolution equation, 
        To have static factors, set factor_order = 0.
    error_cov_type : {'scalar', 'diagonal', 'unstructured'}, optional
        The structure of the covariance matrix of the observation error term, where 'unstructured' puts no restrictions on the matrix, 'diagonal' requires it to be any diagonal matrix (uncorrelated errors), and 'scalar' requires it to be a scalar times the identity matrix. Default is 'diagonal'.
    error_order : int, optional
        The order of the vector autoregression followed 
        by the observation error component. 
        Default is None, corresponding to white noise errors.
    error_var : boolean, optional
        Whether or not to model the errors jointly via a vector autoregression,VAR(error_order) 
        rather than as individual autoregressions, AR(error_order)
        Has no effect unless error_order is set. Default is False.
#Methods of Result 
coefficients_of_determination()
    Coefficients of determination 
    A k_endog x k_factors array, where coefficients_of_determination[i, j] 
    represents the R^2 value from a regression of factor j and a constant 
    on endogenous variable i.
plot_coefficients_of_determination([...])           Plot the coefficients of determination 
plot_diagnostics([variable, lags, fig, figsize])    Diagnostic plots for standardized residuals of one endogenous variable 
predict([start, end, dynamic])                      In-sample prediction and out-of-sample forecasting 
fittedvalues() (array)                              The predicted values of the model. An (nobs x k_endog) array. 
forecast([steps])               Out-of-sample forecasts 
get_forecast([steps])           Out-of-sample forecasts 
get_prediction([start, end, dynamic, exog])         In-sample prediction and out-of-sample forecasting 
hqic() (float)                  Hannan-Quinn Information Criterion 
impulse_responses([steps, impulse, ...])            Impulse response function 
save(fname[, remove_data])                          save a pickle of this instance 
simulate(nsimulations[, measurement_shocks, ...])   Simulate a new time series following the state space model 
summary([alpha, start, separate_params])            Summarize the Model 
t_test(r_matrix[, cov_p, scale, use_t])             Compute a t-test for a each linear hypothesis of the form Rb = q 
test_heteroskedasticity(method[, ...])              Test for heteroskedasticity of standardized residuals 
test_normality(method)                              Test for normality of standardized residuals. 
test_serial_correlation(method[, lags])             Ljung-box test for no serial correlation of standardized residuals 
wald_test(r_matrix[, cov_p, scale, invcov, ...])    Compute a Wald-test for a joint linear hypothesis. 
     



##SOP
# Load the statsmodels api
import statsmodels.api as sm

# Load your dataset
endog = pd.read_csv('your/dataset/here.csv')

# Fit a local level model
mod_dfm = sm.tsa.DynamicFactor(endog, k_factors=1, factor_order=2)
# Note that mod_dfm is an instance of the DynamicFactor class

# Fit the model via maximum likelihood
res_dfm = mod_dfm.fit()
# Note that res_dfm is an instance of the DynamicFactorResults class

# Show the summary of results
print(res_ll.summary())

# Show a plot of the r^2 values from regressions of
# individual estimated factors on endogenous variables.
fig_dfm = res_ll.plot_coefficients_of_determination()


##Example - Unobserved factor in macro economic data 
#http://www.statsmodels.org/dev/examples/notebooks/generated/statespace_dfm_coincident.html

#Below data is used to find one unobserved factor which influences these four 
#(hint:reccesion)
    Industrial production (IPMAN)
    Real aggregate income (excluding transfer payments) (W875RX1)
    Manufacturing and trade sales (CMRMTSPL)
    Employees on non-farm payrolls (PAYEMS)

#In all cases, the data is at the monthly frequency 
#and has been seasonally adjusted; the time-frame considered is 1972 - 2005.

%matplotlib inline

import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt

np.set_printoptions(precision=4, suppress=True, linewidth=120)

from pandas_datareader.data import DataReader

# Get the datasets from FRED
start = '1979-01-01'
end = '2014-12-01'
indprod = DataReader('IPMAN', 'fred', start=start, end=end)
income = DataReader('W875RX1', 'fred', start=start, end=end)
sales = DataReader('CMRMTSPL', 'fred', start=start, end=end)
emp = DataReader('PAYEMS', 'fred', start=start, end=end)
#Some error might exists in original data, check orginal paper on how to circumvent 
dta = pd.concat((indprod, income, sales, emp), axis=1)
dta.columns = ['indprod', 'income', 'sales', 'emp']

dta.ix[:, 'indprod':'emp'].plot(subplots=True, layout=(2, 2), figsize=(15, 6));

#Stock and Watson (1991) report that for their datasets, 
#they could not reject the null hypothesis of a unit root in each series 
#(so the series are integrated), 
#but they did not find strong evidence that the series were co-integrated.

#As a result, they suggest estimating the model using the first differences 
#(of the logs) of the variables, demeaned and standardized.

# Create log-differenced series
dta['dln_indprod'] = (np.log(dta.indprod)).diff() * 100
dta['dln_income'] = (np.log(dta.income)).diff() * 100
dta['dln_sales'] = (np.log(dta.sales)).diff() * 100
dta['dln_emp'] = (np.log(dta.emp)).diff() * 100

# De-mean and standardize
dta['std_indprod'] = (dta['dln_indprod'] - dta['dln_indprod'].mean()) / dta['dln_indprod'].std()
dta['std_income'] = (dta['dln_income'] - dta['dln_income'].mean()) / dta['dln_income'].std()
dta['std_sales'] = (dta['dln_sales'] - dta['dln_sales'].mean()) / dta['dln_sales'].std()
dta['std_emp'] = (dta['dln_emp'] - dta['dln_emp'].mean()) / dta['dln_emp'].std()


#following specification:
#    k_factors = 1 - (there is 1 unobserved factor)
#    factor_order = 2 - (it follows an AR(2) process)
#    error_var = False - (the errors evolve as independent AR processes rather than jointly as a VAR - note that this is the default option, so it is not specified below)
#    error_order = 2 - (the errors are autocorrelated of order 2: i.e. AR(2) processes)
#    error_cov_type = 'diagonal' - (the innovations are uncorrelated; this is again the default)


#Multivariate models can have a relatively large number of parameters, 
#Hence use 2 step optimizations - Powell and then BFGS
# Get the endogenous data
endog = dta.ix['1979-02-01':, 'std_indprod':'std_emp']

# Create the model
#Four y series are jointly modeled 
mod = sm.tsa.DynamicFactor(endog, k_factors=1, factor_order=2, error_order=2)
initial_res = mod.fit(method='powell', disp=False)
res = mod.fit(initial_res.params, disp=False)

#separate_params=True: Summary would print separately 
#Results for equation for each y variable, involving fi, i=1...k_factors 
#Results for factor equation for each factor, fi, of k_factors involving AR(factor_order) terms 
#Results for error equation for each y variable involving  AR(error_order) terms 
#Error covariance , sigma2 for each y variable 

#separate_params=False : print all together 
>>> print(res.summary(separate_params=False))

                                             Statespace Model Results                                            
=================================================================================================================
Dep. Variable:     ['std_indprod', 'std_income', 'std_sales', 'std_emp']   No. Observations:                  431
Model:                                 DynamicFactor(factors=1, order=2)   Log Likelihood               -2046.077
                                                          + AR(2) errors   AIC                           4128.154
Date:                                                   Tue, 28 Feb 2017   BIC                           4201.344
Time:                                                           21:34:46   HQIC                          4157.052
Sample:                                                       02-01-1979                                         
                                                            - 12-01-2014                                         
Covariance Type:                                                     opg                                         
====================================================================================================
                                       coef    std err          z      P>|z|      [0.025      0.975]
----------------------------------------------------------------------------------------------------
loading.f1.std_indprod              -0.8749      0.022    -39.083      0.000      -0.919      -0.831
loading.f1.std_income               -0.2675      0.046     -5.873      0.000      -0.357      -0.178
loading.f1.std_sales                -0.4953      0.026    -19.362      0.000      -0.545      -0.445
loading.f1.std_emp                  -0.2951      0.030     -9.675      0.000      -0.355      -0.235
sigma2.std_indprod                   0.0009      0.001      0.982      0.326      -0.001       0.003
sigma2.std_income                    0.8937      0.029     31.233      0.000       0.838       0.950
sigma2.std_sales                     0.5880      0.034     17.393      0.000       0.522       0.654
sigma2.std_emp                       0.3677      0.015     24.828      0.000       0.339       0.397
L1.f1.f1                             0.2433      0.038      6.454      0.000       0.169       0.317
L2.f1.f1                             0.2938      0.043      6.872      0.000       0.210       0.378
L1.e(std_indprod).e(std_indprod)    -1.1532      0.014    -83.507      0.000      -1.180      -1.126
L2.e(std_indprod).e(std_indprod)    -0.9851      0.014    -70.299      0.000      -1.013      -0.958
L1.e(std_income).e(std_income)      -0.1878      0.022     -8.370      0.000      -0.232      -0.144
L2.e(std_income).e(std_income)      -0.0863      0.047     -1.831      0.067      -0.179       0.006
L1.e(std_sales).e(std_sales)        -0.4151      0.042     -9.794      0.000      -0.498      -0.332
L2.e(std_sales).e(std_sales)        -0.1730      0.049     -3.522      0.000      -0.269      -0.077
L1.e(std_emp).e(std_emp)             0.2935      0.032      9.053      0.000       0.230       0.357
L2.e(std_emp).e(std_emp)             0.4882      0.028     17.232      0.000       0.433       0.544
========================================================================================================
Ljung-Box (Q):          50.91, 28.81, 53.61, 61.18   Jarque-Bera (JB):   15.75, 12973.87, 14.41, 3133.84
Prob(Q):                    0.12, 0.91, 0.07, 0.02   Prob(JB):                    0.00, 0.00, 0.00, 0.00
Heteroskedasticity (H):     0.95, 4.21, 0.47, 0.35   Skew:                      -0.06, -1.20, 0.10, 0.82
Prob(H) (two-sided):        0.74, 0.00, 0.00, 0.00   Kurtosis:                  3.93, 29.77, 3.87, 16.11
========================================================================================================




##Estimated factors
#While it can be useful to plot the unobserved factors, 
#it is less useful here than one might think for two reasons:
#    The sign-related identification issue described above.
#    Since the data was differenced, the estimated factor explains the variation in the differenced data, not the original data.


#With these reservations, the unobserved factor is plotted below, 
#along with the NBER indicators for US recessions(= our unobserved factor)

fig, ax = plt.subplots(figsize=(13,3))

# Plot the factor
dates = endog.index._mpl_repr()
#res.factors : Estimates of unobserved factors for each data point 
#factors has four data points - filtered, smoothed - nobs x k_factors 
#filtered_cov, smoothed_cov are covariance matrix 

In general, #any large variations in factor must translate to all endog's large variations 
ax.plot(dates, res.factors.filtered[0], label='Factor')
ax.legend()

# Retrieve and also plot the NBER recession indicators
rec = DataReader('USREC', 'fred', start=start, end=end)
ylim = ax.get_ylim()
ax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);
plt.show()

#R^2 value of fitting of each y with Factors 
>>> res.coefficients_of_determination
array([[0.9613],     #R^2 of fitting of factor1 with 'std_indprod'
       [0.0706],     #R^2 of fitting of factor1 with 'std_income'
       [0.3092],     #R^2 of fitting of factor1 with 'std_sales'
       [0.3507]])    #R^2 of fitting of factor1 with 'std_emp'
res.plot_coefficients_of_determination() #display above 
plt.show()


##Coincident Index
# the goal of this model was to create an interpretable series 
#which could be used to understand the current status of the macroeconomy. 

#This is what the coincident index is designed to do. 
#In essense, what is done is to reconstruct the mean of the (differenced) factor.

#We will compare it to the coincident index on published 
#by the Federal Reserve Bank of Philadelphia (USPHCI on FRED).

usphci = DataReader('USPHCI', 'fred', start='1979-01-01', end='2014-12-01')['USPHCI']
usphci.plot(figsize=(13,3));
plt.show(block=False)

dusphci = usphci.diff()[1:].values
def compute_coincident_index(mod, res):
    # Estimate W(1)
    spec = res.specification
    design = mod.ssm['design']
    transition = mod.ssm['transition']
    ss_kalman_gain = res.filter_results.kalman_gain[:,:,-1]
    k_states = ss_kalman_gain.shape[0]

    W1 = np.linalg.inv(np.eye(k_states) - np.dot(
        np.eye(k_states) - np.dot(ss_kalman_gain, design),
        transition
    )).dot(ss_kalman_gain)[0]

    # Compute the factor mean vector
    factor_mean = np.dot(W1, dta.ix['1972-02-01':, 'dln_indprod':'dln_emp'].mean())
    
    # Normalize the factors
    factor = res.factors.filtered[0]
    factor *= np.std(usphci.diff()[1:]) / np.std(factor)

    # Compute the coincident index
    coincident_index = np.zeros(mod.nobs+1)
    # The initial value is arbitrary; here it is set to
    # facilitate comparison
    coincident_index[0] = usphci.iloc[0] * factor_mean / dusphci.mean()
    for t in range(0, mod.nobs):
        coincident_index[t+1] = coincident_index[t] + factor[t] + factor_mean
    
    # Attach dates
    coincident_index = pd.Series(coincident_index, index=dta.index).iloc[1:]
    
    # Normalize to use the same base year as USPHCI
    coincident_index *= (usphci.ix['1992-07-01'] / coincident_index.ix['1992-07-01'])
    
    return coincident_index

#plot the calculated coincident index 
#along with the US recessions and the comparison coincident index USPHCI.

fig, ax = plt.subplots(figsize=(13,3))

# Compute the index
coincident_index = compute_coincident_index(mod, res)

# Plot the factor
dates = endog.index._mpl_repr()
ax.plot(dates, coincident_index, label='Coincident index')
ax.plot(usphci.index._mpl_repr(), usphci, label='USPHCI')
ax.legend(loc='lower right')

# Retrieve and also plot the NBER recession indicators
ylim = ax.get_ylim()
ax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);
plt.show()



   
    
    


###statsmodel - Generalized method of moments - statsmodels.sandbox.regression.gmm.
# non-parametric 
#This is sandbox module, means not fully tested. Use 'linearmodules' module

#The method requires that a certain number of moment conditions were specified for the model. 
#These moment conditions are functions of the model parameters and the data, 
#such that their expectation is zero at the true values of the parameters. 
E[ z * (y - f(X, beta)] = 0


#The GMM method then minimizes a certain norm of the sample averages of the moment conditions.

#The GMM estimators are known to be consistent, asymptotically normal, and efficient 
GMM(endog, exog, instrument[, k_moms, ...])     Class for estimation by Generalized Method of Moments 
GMMResults(*args, **kwds)                       just a storage class right now 
IV2SLS(endog, exog[, instrument])               Instrumental variables estimation using Two-Stage Least-Squares (2SLS) 
IVGMM(endog, exog, instrument[, k_moms, ...])   Basic class for instrumental variables estimation using GMM 
IVGMMResults(*args, **kwds) 
LinearIVGMM(endog, exog, instrument[, ...])     class for linear instrumental variables models estimated with GMM 
NonlinearIVGMM(endog, exog, instrument, ...)    Class for non-linear instrumental variables estimation wusing GMM 

##Instrumental Variable GMM regression 

exogenous = independent variables ,causally independent from other variables in the system
endogenous = dependent variables,  causal relations with the independent variables
An instrument is a variable that does not itself belong in the explanatory equation 
but is correlated with the endogenous explanatory variables

These models make use of instruments  which are correlated 
with the endogenous variable(Y and regressors) but not with the model error. 
yi=x1iβ1+x2iβ2+ϵi
x2i=z1iδ+z2iγ+νi

x1i is a set of k1 regressors that are exogenous(independent) 
while x2i is a set of k2 regressors (k=k1+k2)
that are endogenous in the sense that Cov(x2i,ϵi)≠0. 
In total there are The k regressors in the model. 

The p2 element vector z2i are instruments that explain x2i but not yi. 

x1i and z1i are the same since variables are also valid to use 
when projecting the endogenous variables. 

In total there are p=p1+p2=k1+p2 variables available to use 
when projecting the endogenous regressors.

Total DataFrame length = k1 U k2 U p2 
and x1i(=z1i) and z2i are truely exogenous


#In statsmodel 
yi=x1iβ1+x2iβ2+ϵi
x2i=z1iδ+z2iγ+νi
endog          yi in the model
exog           x1i and x2i(included endogenous), k regressors in the model.
instruments    z1i(=x1i,included exogenous) + z2i(excluded exogenous) in the model, 
               
#Example 
griliches76_data['const'] = 1
endog = griliches76_data['lw']
exog = griliches76_data[[ 's', 'iq', 'const','expr', 'tenure', 'rns',
                       'smsa', 'D_67', 'D_68', 'D_69', 'D_70',
                       'D_71', 'D_73']])
instrument = griliches76_data[['const', 'expr', 'tenure', 'rns', 'smsa', 
                       'D_67', 'D_68', 'D_69', 'D_70', 'D_71',
                       'D_73', 'med', 'kww', 'age', 'mrt']])

#means - |, -, & from set  
yi = 'lw'
x1i = exog & instrument = 'const','expr', 'tenure', 'rns','smsa', 'D_67', 'D_68', 'D_69', 'D_70','D_71', 'D_73'
x2i = exog - instrument = 's', 'iq'
z1i = x1i =  'const','expr', 'tenure', 'rns','smsa', 'D_67', 'D_68', 'D_69', 'D_70','D_71', 'D_73'
z2i = instrument - exog = 'med', 'kww', 'age', 'mrt'
#in linearmodels(another module) 
dependent = yi = 'lw'
exog = x1i = z1i = 'const','expr', 'tenure', 'rns','smsa', 'D_67', 'D_68', 'D_69', 'D_70','D_71', 'D_73'
endog = x2i = 's', 'iq'
instruments = z2i = 'med', 'kww', 'age', 'mrt'

##class 
class statsmodels.sandbox.regression.gmm.LinearIVGMM(endog, exog, instrument, k_moms=None, k_params=None, missing='none', **kwds)[source]
    The model is assumed to have the following moment condition
    E( z * (y - x*beta)) = 0
class statsmodels.sandbox.regression.gmm.NonlinearIVGMM(endog, exog, instrument, func, **kwds)[source]
    Class for non-linear instrumental variables estimation using GMM
    The model is assumed to have the following moment condition
        E[ z * (y - f(X, beta)] = 0
    Where y is the dependent endogenous variable, 
    x are the explanatory variables and z are the instruments.
    f is a nonlinear function. 
    Parameters:	
    endog : array_like
        dependent endogenous variable
    exog : array_like
        explanatory, right hand side variables, 
        including explanatory variables that are endogenous.
    instruments : array_like
        Instrumental variables, 
        variables that are exogenous to the error in the linear model 
        containing both included and excluded exogenous variables
    func : callable
        function for the mean or conditional expectation of the endogenous variable.
        The function will be called with parameters 
        and the array of explanatory, right hand side variables, 
        func(params, exog) should return predicted value based on exog and params 
        Note params shape comes from fit's start_params
        if fit_start=None, then below is used 
        np.zeros(exog.shape[1])
NonlinearIVGMM.fit(start_params=None, maxiter=10, inv_weights=None, 
        weights_method='cov', wargs=(), has_optimal_weights=True, 
        optim_method='bfgs', optim_args=None)
    Estimate parameters using GMM and return GMMResults
    inv_weights
        for IV subclasses inv_weights = z'z where z are the instruments, 
        otherwise an identity matrix is used.    
    weights_method : string, defines method for robust
        Options here are similar to statsmodels.stats.robust_covariance 
        default is heteroscedasticity consistent, HC0
        currently available methods are
            cov : HC0, optionally with degrees of freedom correction
            hac :
            iid : untested, only for Z*u case, IV cases with u as error indep of Z
            ac : not available yet
            cluster : not connected yet
            others from robust_covariance
    wargs` : tuple or dict,
        required and optional arguments for weights_method
            centered : bool, indicates whether moments are centered for the calculation of the weights and covariance matrix, applies to all weight_methods
            ddof : int degrees of freedom correction, applies currently only to cov
            maxlag : int number of lags to include in HAC calculation , applies only to hac
            others not yet, e.g. groups for cluster robust
        
    
class statsmodels.sandbox.regression.gmm.IVGMMResults(*args, **kwds)
    Attributes
        bse_ 	                standard error of the parameter estimates
    Methods
        bse() 	
        calc_cov_params(moms, gradmoms[, weights, ...]) 	calculate covariance of parameter estimates
        compare_j(other) 	                                overidentification test for comparing two nested gmm estimates
        conf_int([alpha, cols, method]) 	                Returns the confidence interval of the fitted parameters.
        cov_params([r_matrix, column, scale, cov_p, ...]) 	Returns the variance/covariance matrix.
        f_test(r_matrix[, cov_p, scale, invcov]) 	        Compute the F-test for a joint linear hypothesis.
        fittedvalues() 	
        get_bse(**kwds) 	                      standard error of the parameter estimates with options
        initialize(model, params, **kwd) 	
        jtest() 	                              overidentification test
        jval() 	
        llf() 	
        load(fname) 	                          load a pickle, (class method)
        normalized_cov_params() 	
        predict([exog, transform]) 	               Call self.model.predict with self.params as the first argument.
        pvalues() 	
        q() 	
        remove_data() 	                            remove data arrays, all nobs arrays from result and model
        resid() 	
        save(fname[, remove_data]) 	                save a pickle of this instance
        ssr() 	
        summary([yname, xname, title, alpha]) 	    Summarize the Regression Results
        t_test(r_matrix[, cov_p, scale, use_t]) 	Compute a t-test for a each linear hypothesis of the form Rb = q
        tvalues() 	                                Return the t-statistic for a given parameter estimate.
        wald_test(r_matrix[, cov_p, scale, invcov, ...]) 	Compute a Wald-test for a joint linear hypothesis.
        wald_test_terms([skip_single, ...]) 	    Compute a sequence of Wald tests for terms over multiple columns



##Example -Euler equations in rational expectation models
# solving E[z *(y- f(params,x))] = 0
#where y = 1, f = beta*(1+R_at_t+1)(c_at_t+1/c_at_t)**(-gamma)
#params = [beta, gamma]
#beta is the discount factor of the agent, 
#and gamma is the coefficient reflecting constant relative risk aversion. 
#R is a rate of return on assets or interest rate, c is consumption


import numpy as np
import pandas as pd

from statsmodels.sandbox.regression import gmm
dta = pd.read_csv('data/consumption.csv', parse_dates=[0])
>>> print (dta.iloc[:5])
                  qtr       r       c
0 1947-01-01 00:00:00  0.0038  1017.2
1 1947-04-01 00:00:00  0.0038  1034.0
2 1947-07-01 00:00:00  0.0066  1037.5
3 1947-10-01 00:00:00  0.0085  1037.7
4 1948-01-01 00:00:00  0.0097  1042.6


dta['c_growth'] = dta['c'] / dta['c'].shift(1)
dta['c_growth_lag1'] = dta['c_growth'].shift(1)
dta['r_lag1'] = dta['r'].shift(1)
dta['r_lag2'] = dta['r'].shift(2)
dta['r_forw1'] = dta['r'].shift(-1)
dta['c_lag1'] = dta['c'].shift(1)
dta['c_forw1'] = dta['c'].shift(-1)
dta['const'] = 1

dta_clean = dta.dropna()


#yi=x1iβ1+x2iβ2+ϵi
#x2i=z1iδ+z2iγ+νi
#yi = coming in later section
#x1i = exog & instrument = Null 
#x2i = exog - instrument = 'r_forw1', 'c_forw1', 'c'
#z1i = x1i = Null 
#z2i = instrument - exog = 'r_lag1', 'r_lag2', 'c_growth', 'c_growth_lag1','const'

endog_df = dta_clean[['r_forw1', 'c_forw1', 'c']]
exog_df = dta_clean[['r_forw1', 'c_forw1', 'c']]
instrument_df = dta_clean[['r_lag1', 'r_lag2', 'c_growth', 'c_growth_lag1','const']]
endog, exog, instrument  = list(map(np.asarray, [endog_df, exog_df, instrument_df]))



##Version-1 , use endog = 0 
#but manually taking  1- f(x,params) in function 
def func1(params, exog): #starts_param= [1,1] =[beta, gama]
    beta, gamma = params
    r_forw1, c_forw1, c = exog.T  # 'r_forw1', 'c_forw1', 'c'
    # moment condition without instrument    
    err = 1 - beta * (1 + r_forw1) * np.power(c_forw1 / c, -gamma)
    return -err
    
    
endog1 = np.zeros(exog.shape[0])    
#k_moms = number of moment conditions, 
#if None then it is set equal to the number of columns of instruments
mod1 = gmm.NonlinearIVGMM(endog1, exog, instrument, func1, k_moms=4)
w0inv = np.dot(instrument.T, instrument) / len(endog1)
res1 = mod1.fit([1,-1], maxiter=2, inv_weights=w0inv) 
#output 
Optimization terminated successfully.
         Current function value: 0.000539
         Iterations: 6
         Function evaluations: 23
         Gradient evaluations: 23
Optimization terminated successfully.
         Current function value: 0.152609
         Iterations: 6
         Function evaluations: 11
         Gradient evaluations: 11
#Here params are beta, gamma , renamed to discount, CRRA
>>> print(res1.summary(yname='Euler Eq', xname=['discount', 'CRRA']))
                            NonlinearIVGMM Results                            
==============================================================================
Dep. Variable:               Euler Eq   Hansen J:                        36.47
Model:                 NonlinearIVGMM   Prob (Hansen J):              1.20e-08
Method:                           GMM                                         
Date:                Wed, 25 Dec 2013                                         
Time:                        20:08:33                                         
No. Observations:                 239                                         
==============================================================================
                 coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
discount       0.8977      0.017     52.860      0.000         0.864     0.931
CRRA          -6.7953      2.050     -3.315      0.001       -10.813    -2.778
==============================================================================

##overidentification test
#overidentification - When the number of moment conditions is greater 
#than the dimension of the params vector 
#Over-identification allows us to check 
#whether the model's moment conditions match the data well or not.
#H0: model is valid ie  instruments are valid for this model
#rejecting H0 means , correctly specify instruments or model 
>>> res1.jtest()
(36.510482051328154, 1.1799118334705946e-08, 2) #(stats, p-value, df)
#Note df = where q - p, measures the degree of overidentification
#q = no of moments equation, p= no of paramters 



#Use a HAC robust standard error, that is robust to heteroscedasticity as well as autocorrelation.
#define these options through weights_method='hac', wargs={'maxlag':4}, which uses the Newey, West standard errors based on 4 lags.

res1_hac4_2s = mod1.fit([1, -1], maxiter=2, inv_weights=w0inv, weights_method='hac', wargs={'maxlag':4})
>>> print(res1_hac4_2s.summary(yname='Euler Eq', xname=['discount', 'CRRA']))
Optimization terminated successfully.
         Current function value: 0.000539
         Iterations: 6
         Function evaluations: 23
         Gradient evaluations: 23
Optimization terminated successfully.
         Current function value: 0.055395
         Iterations: 5
         Function evaluations: 10
         Gradient evaluations: 10
                            NonlinearIVGMM Results                            
==============================================================================
Dep. Variable:               Euler Eq   Hansen J:                        13.24
Model:                 NonlinearIVGMM   Prob (Hansen J):               0.00133
Method:                           GMM                                         
Date:                Wed, 25 Dec 2013                                         
Time:                        20:08:33                                         
No. Observations:                 239                                         
==============================================================================
                 coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
discount       0.9213      0.014     67.281      0.000         0.894     0.948
CRRA          -4.1136      1.507     -2.729      0.006        -7.068    -1.160
==============================================================================

##version 2, endog = 1, 
#then func2 returns predicted portion of the equation 

def func2(params, exog):
    beta, gamma = params
    #endog, exog = args
    r_forw1, c_forw1, c = exog.T  # unwrap iterable (ndarray)    
    # 2nd part of moment condition without instrument    
    predicted = beta * (1. + r_forw1) * np.power(c_forw1 / c, -gamma)
    return predicted
    
endog2 = np.ones(exog.shape[0])    
mod2 = gmm.NonlinearIVGMM(endog2, exog, instrument, func2, k_moms=4)
w0inv = np.dot(instrument.T, instrument) / len(endog2)  
res2_hac4_2s = mod2.fit([1,-1], maxiter=2, inv_weights=w0inv, weights_method='hac', wargs={'maxlag':4})
>>> print(res2_hac4_2s.summary(yname='Euler Eq', xname=['discount', 'CRRA']))
                            NonlinearIVGMM Results                            
==============================================================================
Dep. Variable:               Euler Eq   Hansen J:                        13.24
Model:                 NonlinearIVGMM   Prob (Hansen J):               0.00133
Method:                           GMM                                         
Date:                Wed, 25 Dec 2013                                         
Time:                        20:08:33                                         
No. Observations:                 239                                         
==============================================================================
                 coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
discount       0.9213      0.014     67.281      0.000         0.894     0.948
CRRA          -4.1136      1.507     -2.729      0.006        -7.068    -1.160
==============================================================================

##Compare
>>> res1_hac4_2s.params
array([ 0.92129628, -4.11361243])
>>> res2_hac4_2s.params
array([ 0.92129628, -4.11361243])
>>> res1_hac4_2s.params - res2_hac4_2s.params, np.max(np.abs(res1_hac4_2s.params - res2_hac4_2s.params))
(array([  3.27948779e-12,   3.71550790e-10]), 3.7155079013473369e-10)

#using centered = False as a weights argument:
res_ = mod2.fit([1,-1], maxiter=2, inv_weights=w0inv, weights_method='hac', 
                wargs={'maxlag':4, 'centered':False}, optim_args={'disp':0})
print(res_.params)
print(res_.bse)
[ 0.92045263 -4.22337303]
[ 0.01345577  1.47341411]




##Example - Poisson: Maximum Likelihood and General Method of Moments
#for docvis of docvisits data 
#Note Poisson model defines E[z*(y−exp(x*beta)]=0
#Hence in terms of GMM f(params, x) = exp(x'*params), where params= beta 

import numpy as np
import pandas as pd
from statsmodels.sandbox.regression import gmm
dta2 = pd.read_csv('data/docvisits.csv')
>>> print(dta2.iloc[:5])
   docvis  age     income  female  black  hispanic  married  physlim  private  \
0       0  3.9  30.000000       1      0         1        1        0        1   
1       1  4.7  10.000000       0      0         1        1        0        1   
2      15  2.7  27.000000       1      0         0        0        0        1   
3       0  3.0  11.250000       0      0         0        1        0        0   
4       2  5.4  76.330002       1      0         0        1        0        1   
   chronic  
0        0  
1        0  
2        1  
3        0  
4        0  

dta_clean2 = dta2
dta_clean2['const'] = 1
dta_clean2['income'] *= 0.1  # rescale to have values into similar ranges

#yi=x1iβ1+x2iβ2+ϵi
#x2i=z1iδ+z2iγ+νi
#yi = docvis
#x1i = exog & instrument = 'private', 'chronic', 'female', 'const'
#x2i = exog - instrument = 'income'
#z1i = x1i = 'private', 'chronic', 'female', 'const'
#z2i = instrument - exog = 'age', 'black','hispanic'

endog_df = dta_clean2[['docvis']]
exog_names = ['private', 'chronic', 'female', 'income', 'const']
exog_df = dta_clean2[exog_names]
instrument_df = dta_clean2[['private', 'chronic', 'female', 'age', 'black',              
                           'hispanic', 'const']]
endog, exog, instrument  = map(np.asarray, [endog_df, exog_df, instrument_df])
endog = np.squeeze(endog)


##Maximum Likelihood Estimation MLE
#Poisson is a standard maximum likelihood estimator 
#that assumes that observations are independent, 
#and that explanatory variables are exogenous (i.e. without measurement or endogeneity error)

from statsmodels.discrete.discrete_model import Poisson
res_poisson = Poisson(endog, exog).fit()
>>> print(res_poisson.summary(yname='docvisits', xname=exog_names))
Optimization terminated successfully.
         Current function value: 4.193914
         Iterations 12
                          Poisson Regression Results                          
==============================================================================
Dep. Variable:              docvisits   No. Observations:                 4412
Model:                        Poisson   Df Residuals:                     4407
Method:                           MLE   Df Model:                            4
Date:                Wed, 25 Dec 2013   Pseudo R-squ.:                  0.1930
Time:                        14:50:25   Log-Likelihood:                -18504.
converged:                       True   LL-Null:                       -22930.
                                        LLR p-value:                     0.000
==============================================================================
                 coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
private        0.7987      0.028     28.813      0.000         0.744     0.853
chronic        1.0919      0.016     69.112      0.000         1.061     1.123
female         0.4925      0.016     30.770      0.000         0.461     0.524
income         0.0356      0.002     14.750      0.000         0.031     0.040
const         -0.2297      0.029     -8.004      0.000        -0.286    -0.173
==============================================================================

##Version-1 : GMM with instruments = exog , no new Z2i

def moment_exponential(params, exog, exp=True): #params.shape = from fit's starts_param
    # moment condition without instrument
    if exp:
        predicted = np.exp(np.dot(exog, params))
    else:
        predicted = np.dot(exog, params)    
    #try to avoid runaway optimizers that cannot handle nans
    if np.isnan(predicted).any():
        raise ValueError('nans in predicted')
    return predicted

mod_ex = gmm.NonlinearIVGMM(endog, exog, exog, moment_exponential)
w0exinv = np.dot(exog.T, exog) / len(endog)
#start_params.shape = exog.shape[1] ie # of columns in exog 
start_params = 0.1 + np.zeros(exog.shape[1])
res_ex0 = mod_ex.fit(start_params, maxiter=0, inv_weights=w0exinv, optim_method='nm')
res_ex = mod_ex.fit(res_ex0.params, maxiter=0, inv_weights=w0exinv, optim_method='bfgs')

>>> print(res_ex.summary(yname='docvisits', xname=exog_names))
                            NonlinearIVGMM Results                            
==============================================================================
Dep. Variable:              docvisits   Hansen J:                    2.193e-10
Model:                 NonlinearIVGMM   Prob (Hansen J):                   nan
Method:                           GMM                                         
Date:                Wed, 25 Dec 2013                                         
Time:                        14:50:25                                         
No. Observations:                4412                                         
==============================================================================
                 coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
private        0.7987      0.109      7.328      0.000         0.585     1.012
chronic        1.0919      0.056     19.501      0.000         0.982     1.202
female         0.4925      0.059      8.415      0.000         0.378     0.607
income         0.0356      0.011      3.286      0.001         0.014     0.057
const         -0.2297      0.111     -2.072      0.038        -0.447    -0.012
==============================================================================

#Compare 
print(res_poisson.params)
print(res_ex.params)
[ 0.79866538  1.09186511  0.49254807  0.03557013 -0.22972634]
[ 0.79866522  1.09186505  0.49254798  0.03557012 -0.22972605]

>>> print(res_ex.params - res_poisson.params)
[ -1.60226566e-07  -5.64773353e-08  -9.39717644e-08  -4.27707619e-09
   2.90754947e-07]

#Standard deviation 
print(res_poisson.bse)
print(res_ex.bse)
print(res_ex.bse - res_poisson.bse)
[ 0.02771898  0.01579852  0.01600728  0.0024115   0.02870217]
[ 0.10898908  0.05598878  0.05852984  0.01082366  0.11086067]
[ 0.08127009  0.04019027  0.04252255  0.00841216  0.0821585 ]


##Homoscedasticity, Heteroscedasticity and Overdispersion
#In the Poisson model the variance is equal to the mean. 
#The Maximum Likelihood Estimator in discrete.Poisson imposes this assumption. 
#In contrast GMM with heteroscedasticity robust standard errors does not impose 
#any assumption on the variance of the error term. 

#In between these two extrems are overdispersed Poisson or exponential models 
#that assume that the variance is the same as in the Poisson Model 
#however can have a scale factor larger than one.

#However, the assumption on the variance do not affect the parameter estimates in this case, but will affect the estimated covariance of the parameter estimates. We can consistently estimate the mean or conditional expectation of the endogenous variable, however other properties of the conditional distribution of the endogenous variable depend on the assumption of correctly specified higher moments.
#The scale, that is the variance scale factor, based on the MLE of the Poisson model is one.
>>> res_poisson.scale
1.0

#However, We can estimate the scale and standard deviation 
#based on the residuals of the Poisson MLE estimation, 
#which is in this case 3.6 
#and indicates that there is considerable overdispersion

fittedvalues = res_poisson.predict(linear=False)  #attribute is currently linear prediction :(
resid_pearson = res_poisson.resid / np.sqrt(fittedvalues)
scale = (resid_pearson**2).mean()
>>> scale, np.sqrt(scale), resid_pearson.std()
(12.947923548090147, 3.5983223240963484, 3.5983077409797635)

#In GMM, we can replicate OLS results with GMM 
#if we specify 'iid' as the weights method. 
#This assumes that the variance of error term is uncorrelated or independent of the explanatory variables, 
#but it also assumes that this variance is constant. 
#The latter is, however, not the case for the Poisson/Exponential model, so it does not replicate the MLE standard errors.
res_ex_iid = mod_ex.fit(res_ex.params, maxiter=0, inv_weights=w0exinv,
                    weights_method='iid',
                    optim_method='bfgs', optim_args={'disp':False})
print(res_ex_iid.params)
print(res_ex_iid.bse)
[ 0.79866522  1.09186505  0.49254798  0.03557012 -0.22972605]
[ 0.15455798  0.06488262  0.06341369  0.00791076  0.17186891]


##Version-2 Endogeneity and Instrumental Variables
#note income as a explanatory variable in exog, which might have "measurement errors".
#If income is correlated with the residual, then using Poisson, 
#the maximum likelihood estimator, 
#and GMM without instruments will report biased estimates.

mod = gmm.NonlinearIVGMM(endog, exog, instrument, moment_exponential)
w0inv = np.dot(instrument.T, instrument) / len(endog)
start_params = 0.1 + np.zeros(exog.shape[1])
res0 = mod.fit(start_params, maxiter=0, inv_weights=w0inv, optim_method='nm')
res = mod.fit(res0.params, maxiter=2, inv_weights=w0inv, optim_method='bfgs')   
>>> print(res.params)
[ 0.53532318  1.09015077  0.66367986  0.1428607  -0.59840238]
>>> print(res.summary(yname='docvisits', xname=exog_names))
                            NonlinearIVGMM Results                            
==============================================================================
Dep. Variable:              docvisits   Hansen J:                        9.529
Model:                 NonlinearIVGMM   Prob (Hansen J):               0.00853
Method:                           GMM                                         
Date:                Wed, 25 Dec 2013                                         
Time:                        14:50:26                                         
No. Observations:                4412                                         
==============================================================================
                 coef    std err          z      P>|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
private        0.5353      0.160      3.348      0.001         0.222     0.849
chronic        1.0902      0.062     17.650      0.000         0.969     1.211
female         0.6637      0.096      6.915      0.000         0.476     0.852
income         0.1429      0.027      5.261      0.000         0.090     0.196
const         -0.5984      0.138     -4.323      0.000        -0.870    -0.327
==============================================================================

#The effect of income is now 0.1429 which is much larger 
#than the effect estimated using MLE, which was 0.0356

#Note below 
Prob (Hansen J):               0.00853
#H0: model is valid , < 0.05, this model is not valid 
#Overidentification: When the number of moment conditions is greater 
#than the dimension of the params vector 









###Scikit- Machine learning - Introduction 
##http://scikit-learn.org/stable/tutorial/basic/tutorial.html

# a learning problem considers a set of n samples of data 
#and then tries to predict properties of unknown data. 

#•supervised learning - to predict target based on given data 
#Two types
#classification: Given, samples belong to two or more classes 
#                predict the class of new data based on given classification of training labelled data  
#regression:  To predict target value of new data by fitting training data 

#•unsupervised learning- Without any target value . 
#Examples to find groups of input data in clustering , 
#to find probability density or 
#to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization 

#Split input data into Training set and testing set
#training set on which we learn data properties 
#and  testing set on which we test these properties.

## Labelled and unlabelled data 
#Labelled data have a label or tag for each row of input data 
#Example:  to classify some patients in two categories: healty and sick patients. 
#labels are : healthy or sick.
label      gender  age 
healthy    m       18 
healthy    f       29 
healthy    f       34 
healthy    m       21 
... 
sick       m       68 
sick       f       74 
sick       m       65

#Unlabelled data could be for example  the new patients arriving to the hospital
gender  age
f       65
m       21
...
m       23
f       18
f       75

Based on these labelled patients, you could try to classify the labelled patients. 
This is known as supervised learning.

For only unlabelled data, you can try to define clusters of patients, 
i.e. groups of patients showing similarities using some clustering method 
for example,  we could find that two clusters, age<35 and age>60, 
This is called unsupervised learning.

For semi-supervised learning, we use both labelled and unlabelled data together 
to categorize  patients. 
You can use unlabelled data to build clusters and the few labelled data points 
to decide which clusters represent healthy and sick patients.

##Definition of kernel density estimation (KDE)
A non-parametric way to estimate the probability density function of a random variable. 
Kernel density estimation is a fundamental data smoothing problem 
where inferences about the population are made, based on a finite data sample
Estimation of density for a point process where window functions (kernels) 
are convolved with data

##Definition of multiclass and multilabel 
#All classifiers in scikit-learn do multiclass classification out-of-the-box. 
#OR use sklearn.multiclass module to experiment with different multiclass strategies.
 
•binary classification - target label maps to only two class 

•Multiclass classification means a classification task with more than two classes; 
e.g., classify a set of images of fruits which may be oranges, apples, or pears. 
Multiclass classification makes the assumption that each sample is assigned to one and only one label: 
a fruit can be either an apple or a pear but not both at the same time.

Other than inherently multiclass classifier,  There are strategies 
for reducing the problem of multiclass classification to multiple binary classification
    One-vs.-one
        In the one-vs.-one (OvO) reduction, one trains K (K − 1) / 2 binary classifiers 
        for a K-way multiclass problem; each receives the samples of a pair of classes 
        from the original training set, and must learn to distinguish these two classes. 
        At prediction time, a voting scheme is applied: 
        all K (K − 1) / 2 classifiers are applied to an unseen sample 
        and the class that got the highest number of "+1" predictions gets predicted by the combined classifier
        Like OvR, OvO suffers from ambiguities in that some regions of its input space may receive the same number of votes
    One-vs.-rest
        The one-vs.-rest(or one-vs.-all, OvA or OvR, one-against-all, OAA) strategy involves 
        training a single classifier per class, with the samples of that class 
        as positive samples and all other samples as negatives. 
        This strategy requires the base classifiers to produce a real-valued confidence score 
        for its decision, rather than just a class label;
        Making decisions means applying all classifiers to an unseen sample x 
        and predicting the label k for which the corresponding classifier reports the highest confidence score:

•Multilabel classification assigns to each sample a set of target labels. 
This can be thought as predicting properties of a data-point that are not mutually exclusive, 
A text might be about any of religion, politics, finance or education at the same time 
or none of these.

•Multioutput regression assigns each sample a set of target values. 
This can be thought of as predicting several properties for each data-point, 
such as wind direction and magnitude at a certain location.

•Multioutput-multiclass classification called multi-task classification 
means that a single estimator has to handle several joint classification tasks. 
The output format is a 2d numpy array or sparse matrix.
Multi-task classification is similar to the multi-output classification task 
with different model formulations. 

The set of labels can be different for each output variable. 
For instance, a sample could be assigned 'pear' for an output variable 
that takes possible values in a finite set of species such as 'pear', 'apple'; 
and 'blue' or 'green' for a second output variable that takes possible values 
in a finite set of colors such as 'green', 'red', 'blue', 'yellow'...

This means that any classifiers handling multi-output multiclass or multi-task classification tasks, 
support the multi-label classification task as a special case. 

#At present, no metric in sklearn.metrics supports the multioutput-multiclass classification task.

#Note scikit only accepts label and feature with ndarray/sci.parse with float32/float64 type 
#Use transformation convert other type data to numerical value 

#label or Target 
binary/multiclass:Accepts numerical class/target value ,No need to transform to one hot encoding 
    One way to transform string label to numerical , use LabelEncoder 
    LabelEncoder transforms to 0...NoOf_classes
    For binary string, you can use LabelBinarizer to transform to 0/1
multilabel:
    Use LabelBinarizer to transform multiclass to one hot encoded multilabel 
    Use MultiLabelBinarizer() to convert multilabel string/numeric categorical to multilabel 

#
Inherently multiclass: 
    ◦sklearn.naive_bayes.BernoulliNB
    ◦sklearn.tree.DecisionTreeClassifier
    ◦sklearn.tree.ExtraTreeClassifier
    ◦sklearn.ensemble.ExtraTreesClassifier
    ◦sklearn.naive_bayes.GaussianNB
    ◦sklearn.neighbors.KNeighborsClassifier
    ◦sklearn.semi_supervised.LabelPropagation
    ◦sklearn.semi_supervised.LabelSpreading
    ◦sklearn.discriminant_analysis.LinearDiscriminantAnalysis
    ◦sklearn.svm.LinearSVC (setting multi_class='crammer_singer')
    ◦sklearn.linear_model.LogisticRegression (setting multi_class='multinomial')
    ◦sklearn.linear_model.LogisticRegressionCV (setting multi_class='multinomial')
    ◦sklearn.neural_network.MLPClassifier
    ◦sklearn.neighbors.NearestCentroid
    ◦sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
    ◦sklearn.neighbors.RadiusNeighborsClassifier
    ◦sklearn.ensemble.RandomForestClassifier
    ◦sklearn.linear_model.RidgeClassifier
    ◦sklearn.linear_model.RidgeClassifierCV

•Multiclass as One-Vs-One: 
    ◦sklearn.svm.NuSVC
    ◦sklearn.svm.SVC.
    ◦sklearn.gaussian_process.GaussianProcessClassifier (setting multi_class = 'one_vs_one')

•Multiclass as One-Vs-All: 
    ◦sklearn.ensemble.GradientBoostingClassifier
    ◦sklearn.gaussian_process.GaussianProcessClassifier (setting multi_class = 'one_vs_rest')
    ◦sklearn.svm.LinearSVC (setting multi_class='ovr')
    ◦sklearn.linear_model.LogisticRegression (setting multi_class='ovr')
    ◦sklearn.linear_model.LogisticRegressionCV (setting multi_class='ovr')
    ◦sklearn.linear_model.SGDClassifier
    ◦sklearn.linear_model.Perceptron
    ◦sklearn.linear_model.PassiveAggressiveClassifier

•Support multilabel: 
    ◦sklearn.tree.DecisionTreeClassifier
    ◦sklearn.tree.ExtraTreeClassifier
    ◦sklearn.ensemble.ExtraTreesClassifier
    ◦sklearn.neighbors.KNeighborsClassifier
    ◦sklearn.neural_network.MLPClassifier
    ◦sklearn.neighbors.RadiusNeighborsClassifier
    ◦sklearn.ensemble.RandomForestClassifier
    ◦sklearn.linear_model.RidgeClassifierCV

•Support multiclass-multioutput: 
    ◦sklearn.tree.DecisionTreeClassifier
    ◦sklearn.tree.ExtraTreeClassifier
    ◦sklearn.ensemble.ExtraTreesClassifier
    ◦sklearn.neighbors.KNeighborsClassifier
    ◦sklearn.neighbors.RadiusNeighborsClassifier
    ◦sklearn.ensemble.RandomForestClassifier


 

        

###SciKit - documented Examples 
#userguide 
http://scikit-learn.org/stable/user_guide.html

##Check these minimum examples 
#D:\Desktop\PPT\python\machineLearning\scikit-learn-master\examples

Isotonic Regression
    http://scikit-learn.org/stable/auto_examples/plot_isotonic_regression.html#sphx-glr-auto-examples-plot-isotonic-regression-py

Pipelining: chaining a PCA and a logistic regression
    http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html#sphx-glr-auto-examples-plot-digits-pipe-py

Selecting dimensionality reduction with Pipeline and GridSearchCV
    http://scikit-learn.org/stable/auto_examples/plot_compare_reduction.html#sphx-glr-auto-examples-plot-compare-reduction-py

Imputing missing values before building an estimator
    http://scikit-learn.org/stable/auto_examples/missing_values.html#sphx-glr-auto-examples-missing-values-py

Multilabel classification
    http://scikit-learn.org/stable/auto_examples/plot_multilabel.html#sphx-glr-auto-examples-plot-multilabel-py



Classifier comparison
    http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py



Comparison of the K-Means and MiniBatchKMeans clustering algorithms
    http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py


The Iris Dataset
    http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html#sphx-glr-auto-examples-datasets-plot-iris-dataset-py

PCA example with Iris Data-set
    http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py

Decision Tree Regression with AdaBoost
    http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py


Comparing random forests and the multi-output meta estimator
    http://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_regression_multioutput.html#sphx-glr-auto-examples-ensemble-plot-random-forest-regression-multioutput-py


Gradient Boosting regression
    http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py

Multi-class AdaBoosted Decision Trees
    http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-multiclass-py



Pipeline Anova SVM
    http://scikit-learn.org/stable/auto_examples/feature_selection/feature_selection_pipeline.html#sphx-glr-auto-examples-feature-selection-feature-selection-pipeline-py

Cross-validation on diabetes Dataset Exercise
    http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#sphx-glr-auto-examples-exercises-plot-cv-diabetes-py


Comparison of F-test and mutual information
    http://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html#sphx-glr-auto-examples-feature-s     eection-plot-f-test-vs-mi-py

Ordinary Least Squares and Ridge Regression Variance
    http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py


Logistic function
    http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py

Linear Regression Example
    http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py

Logistic Regression 3-class Classifier
    http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py


Bayesian Ridge Regression
    http://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py

Underfitting vs. Overfitting
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py

Train error vs Test error
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_train_error_vs_test_error.html#sphx-glr-auto-examples-model-selection-plot-train-error-vs-test-error-py

Confusion matrix
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py

Sample pipeline for text feature extraction and evaluation
    http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py

Precision-Recall
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py

Receiver Operating Characteristic (ROC)
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py

Plotting Learning Curves
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py

Nearest Neighbors regression
    http://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py

Nearest Neighbors Classification
    http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py

Using FunctionTransformer to select columns
    http://scikit-learn.org/stable/auto_examples/preprocessing/plot_function_transformer.html#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py

Robust Scaling on Toy Data
    http://scikit-learn.org/stable/auto_examples/preprocessing/plot_robust_scaling.html#sphx-glr-auto-examples-preprocessing-plot-robust-scaling-py

Support Vector Regression (SVR) using linear and non-linear kernels
    http://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py

SVM Margins Example
    http://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html#sphx-glr-auto-examples-svm-plot-svm-margin-py

SVM-Kernels
    http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html#sphx-glr-auto-examples-svm-plot-svm-kernels-py

Non-linear SVM
    http://scikit-learn.org/stable/auto_examples/svm/plot_svm_nonlinear.html#sphx-glr-auto-examples-svm-plot-svm-nonlinear-py

Lasso and Elastic Net for Sparse Signals
    http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py

Clustering text documents using k-means
    http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py
Decision Tree Regression
    http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py






##List of all examples 

http://scikit-learn.org/stable/auto_examples/index.html#general-examples
http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html#sphx-glr-auto-examples-plot-cv-predict-py
http://scikit-learn.org/stable/auto_examples/plot_isotonic_regression.html#sphx-glr-auto-examples-plot-isotonic-regression-py
http://scikit-learn.org/stable/auto_examples/feature_stacker.html#sphx-glr-auto-examples-feature-stacker-py
http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html#sphx-glr-auto-examples-plot-digits-pipe-py
http://scikit-learn.org/stable/auto_examples/plot_compare_reduction.html#sphx-glr-auto-examples-plot-compare-reduction-py
http://scikit-learn.org/stable/auto_examples/missing_values.html#sphx-glr-auto-examples-missing-values-py
http://scikit-learn.org/stable/auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py
http://scikit-learn.org/stable/auto_examples/plot_multilabel.html#sphx-glr-auto-examples-plot-multilabel-py
http://scikit-learn.org/stable/auto_examples/plot_johnson_lindenstrauss_bound.html#sphx-glr-auto-examples-plot-johnson-lindenstrauss-bound-py
http://scikit-learn.org/stable/auto_examples/plot_kernel_ridge_regression.html#sphx-glr-auto-examples-plot-kernel-ridge-regression-py
http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py
http://scikit-learn.org/stable/auto_examples/plot_kernel_approximation.html#sphx-glr-auto-examples-plot-kernel-approximation-py



http://scikit-learn.org/stable/auto_examples/index.html#examples-based-on-real-world-datasets
http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-topics-extraction-with-nmf-lda-py
http://scikit-learn.org/stable/auto_examples/applications/plot_outlier_detection_housing.html#sphx-glr-auto-examples-applications-plot-outlier-detection-housing-py
http://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py
http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html#sphx-glr-auto-examples-applications-face-recognition-py
http://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html#sphx-glr-auto-examples-applications-plot-stock-market-py
http://scikit-learn.org/stable/auto_examples/applications/plot_model_complexity_influence.html#sphx-glr-auto-examples-applications-plot-model-complexity-influence-py
http://scikit-learn.org/stable/auto_examples/applications/plot_species_distribution_modeling.html#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py
http://scikit-learn.org/stable/auto_examples/applications/wikipedia_principal_eigenvector.html#sphx-glr-auto-examples-applications-wikipedia-principal-eigenvector-py
http://scikit-learn.org/stable/auto_examples/applications/svm_gui.html#sphx-glr-auto-examples-applications-svm-gui-py
http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py
http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py


http://scikit-learn.org/stable/auto_examples/index.html#biclustering
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster.bicluster
http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_coclustering.html#sphx-glr-auto-examples-bicluster-plot-spectral-coclustering-py
http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html#sphx-glr-auto-examples-bicluster-plot-spectral-biclustering-py
http://scikit-learn.org/stable/auto_examples/bicluster/bicluster_newsgroups.html#sphx-glr-auto-examples-bicluster-bicluster-newsgroups-py


http://scikit-learn.org/stable/auto_examples/index.html#calibration
http://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html#sphx-glr-auto-examples-calibration-plot-compare-calibration-py
http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py
http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration.html#sphx-glr-auto-examples-calibration-plot-calibration-py
http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_multiclass.html#sphx-glr-auto-examples-calibration-plot-calibration-multiclass-py


http://scikit-learn.org/stable/auto_examples/index.html#classification
http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py
http://scikit-learn.org/stable/auto_examples/classification/plot_lda.html#sphx-glr-auto-examples-classification-plot-lda-py
http://scikit-learn.org/stable/auto_examples/classification/plot_classification_probability.html#sphx-glr-auto-examples-classification-plot-classification-probability-py
http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py
http://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py


http://scikit-learn.org/stable/auto_examples/index.html#clustering
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster
http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_agglomeration.html#sphx-glr-auto-examples-cluster-plot-digits-agglomeration-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_face_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-face-ward-segmentation-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html#sphx-glr-auto-examples-cluster-plot-cluster-iris-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_face_segmentation.html#sphx-glr-auto-examples-cluster-plot-face-segmentation-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html#sphx-glr-auto-examples-cluster-plot-segmentation-toy-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_face_compress.html#sphx-glr-auto-examples-cluster-plot-face-compress-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-metrics-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_birch_vs_minibatchkmeans.html#sphx-glr-auto-examples-cluster-plot-birch-vs-minibatchkmeans-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_stability_low_dim_dense.html#sphx-glr-auto-examples-cluster-plot-kmeans-stability-low-dim-dense-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py
http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py


http://scikit-learn.org/stable/auto_examples/index.html#covariance-estimation
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.covariance
http://scikit-learn.org/stable/auto_examples/covariance/plot_lw_vs_oas.html#sphx-glr-auto-examples-covariance-plot-lw-vs-oas-py
http://scikit-learn.org/stable/auto_examples/covariance/plot_sparse_cov.html#sphx-glr-auto-examples-covariance-plot-sparse-cov-py
http://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py
http://scikit-learn.org/stable/auto_examples/covariance/plot_outlier_detection.html#sphx-glr-auto-examples-covariance-plot-outlier-detection-py
http://scikit-learn.org/stable/auto_examples/covariance/plot_mahalanobis_distances.html#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py
http://scikit-learn.org/stable/auto_examples/covariance/plot_robust_vs_empirical_covariance.html#sphx-glr-auto-examples-covariance-plot-robust-vs-empirical-covariance-py


http://scikit-learn.org/stable/auto_examples/index.html#cross-decomposition
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_decomposition
http://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_compare_cross_decomposition.html#sphx-glr-auto-examples-cross-decomposition-plot-compare-cross-decomposition-py


http://scikit-learn.org/stable/auto_examples/index.html#dataset-examples
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets
http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html#sphx-glr-auto-examples-datasets-plot-digits-last-image-py
http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html#sphx-glr-auto-examples-datasets-plot-iris-dataset-py
http://scikit-learn.org/stable/auto_examples/datasets/plot_random_dataset.html#sphx-glr-auto-examples-datasets-plot-random-dataset-py
http://scikit-learn.org/stable/auto_examples/datasets/plot_random_multilabel_dataset.html#sphx-glr-auto-examples-datasets-plot-random-multilabel-dataset-py


http://scikit-learn.org/stable/auto_examples/index.html#decomposition
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition
http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html#sphx-glr-auto-examples-decomposition-plot-incremental-pca-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_3d.html#sphx-glr-auto-examples-decomposition-plot-pca-3d-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_sparse_coding.html#sphx-glr-auto-examples-decomposition-plot-sparse-coding-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py
http://scikit-learn.org/stable/auto_examples/decomposition/plot_image_denoising.html#sphx-glr-auto-examples-decomposition-plot-image-denoising-py


http://scikit-learn.org/stable/auto_examples/index.html#ensemble-methods
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble
http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html#sphx-glr-auto-examples-ensemble-plot-isolation-forest-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html#sphx-glr-auto-examples-ensemble-plot-voting-decision-regions-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_regression_multioutput.html#sphx-glr-auto-examples-ensemble-plot-random-forest-regression-multioutput-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_probas.html#sphx-glr-auto-examples-ensemble-plot-voting-probas-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regularization-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html#sphx-glr-auto-examples-ensemble-plot-ensemble-oob-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_embedding.html#sphx-glr-auto-examples-ensemble-plot-random-forest-embedding-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html#sphx-glr-auto-examples-ensemble-plot-partial-dependence-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html#sphx-glr-auto-examples-ensemble-plot-adaboost-hastie-10-2-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-multiclass-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py
http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py


http://scikit-learn.org/stable/auto_examples/index.html#tutorial-exercises
http://scikit-learn.org/stable/auto_examples/exercises/digits_classification_exercise.html#sphx-glr-auto-examples-exercises-digits-classification-exercise-py
http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_digits.html#sphx-glr-auto-examples-exercises-plot-cv-digits-py
http://scikit-learn.org/stable/auto_examples/exercises/plot_iris_exercise.html#sphx-glr-auto-examples-exercises-plot-iris-exercise-py
http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#sphx-glr-auto-examples-exercises-plot-cv-diabetes-py


http://scikit-learn.org/stable/auto_examples/index.html#feature-selection
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection
http://scikit-learn.org/stable/auto_examples/feature_selection/feature_selection_pipeline.html#sphx-glr-auto-examples-feature-selection-feature-selection-pipeline-py
http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py
http://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html#sphx-glr-auto-examples-feature-selection-plot-f-test-vs-mi-py
http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py
http://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_boston.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-boston-py
http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-py
http://scikit-learn.org/stable/auto_examples/feature_selection/plot_permutation_test_for_classification.html#sphx-glr-auto-examples-feature-selection-plot-permutation-test-for-classification-py


http://scikit-learn.org/stable/auto_examples/index.html#gaussian-process-for-machine-learning
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.gaussian_process
http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc_xor.html#sphx-glr-auto-examples-gaussian-process-plot-gpc-xor-py
http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc_iris.html#sphx-glr-auto-examples-gaussian-process-plot-gpc-iris-py
http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_compare_gpr_krr.html#sphx-glr-auto-examples-gaussian-process-plot-compare-gpr-krr-py
http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-co2-py
http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_prior_posterior.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-prior-posterior-py
http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc_isoprobability.html#sphx-glr-auto-examples-gaussian-process-plot-gpc-isoprobability-py
http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc.html#sphx-glr-auto-examples-gaussian-process-plot-gpc-py
http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-noisy-py
http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-noisy-targets-py


http://scikit-learn.org/stable/auto_examples/index.html#generalized-linear-models
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model
http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html#sphx-glr-auto-examples-linear-model-plot-lasso-lars-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html#sphx-glr-auto-examples-linear-model-plot-ridge-path-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_path.html#sphx-glr-auto-examples-linear-model-plot-logistic-path-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_separating_hyperplane.html#sphx-glr-auto-examples-linear-model-plot-sgd-separating-hyperplane-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html#sphx-glr-auto-examples-linear-model-plot-sgd-loss-functions-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_coeffs.html#sphx-glr-auto-examples-linear-model-plot-ridge-coeffs-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html#sphx-glr-auto-examples-linear-model-plot-polynomial-interpolation-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_weighted_samples.html#sphx-glr-auto-examples-linear-model-plot-sgd-weighted-samples-py
http://scikit-learn.org/stable/auto_examples/linear_model/lasso_dense_vs_sparse_data.html#sphx-glr-auto-examples-linear-model-lasso-dense-vs-sparse-data-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_3d.html#sphx-glr-auto-examples-linear-model-plot-ols-3d-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html#sphx-glr-auto-examples-linear-model-plot-multi-task-lasso-support-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_comparison.html#sphx-glr-auto-examples-linear-model-plot-sgd-comparison-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html#sphx-glr-auto-examples-linear-model-plot-ransac-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_huber_vs_ridge.html#sphx-glr-auto-examples-linear-model-plot-huber-vs-ridge-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_penalties.html#sphx-glr-auto-examples-linear-model-plot-sgd-penalties-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_ard.html#sphx-glr-auto-examples-linear-model-plot-ard-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_omp.html#sphx-glr-auto-examples-linear-model-plot-omp-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_iris.html#sphx-glr-auto-examples-linear-model-plot-sgd-iris-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_theilsen.html#sphx-glr-auto-examples-linear-model-plot-theilsen-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html#sphx-glr-auto-examples-linear-model-plot-logistic-multinomial-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_robust_fit.html#sphx-glr-auto-examples-linear-model-plot-robust-fit-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py
http://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_recovery.html#sphx-glr-auto-examples-linear-model-plot-sparse-recovery-py


http://scikit-learn.org/stable/auto_examples/index.html#manifold-learning
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold
http://scikit-learn.org/stable/auto_examples/manifold/plot_swissroll.html#sphx-glr-auto-examples-manifold-plot-swissroll-py
http://scikit-learn.org/stable/auto_examples/manifold/plot_mds.html#sphx-glr-auto-examples-manifold-plot-mds-py
http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py
http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py
http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py


http://scikit-learn.org/stable/auto_examples/index.html#gaussian-mixture-models
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.mixture
http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html#sphx-glr-auto-examples-mixture-plot-gmm-pdf-py
http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html#sphx-glr-auto-examples-mixture-plot-gmm-py
http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html#sphx-glr-auto-examples-mixture-plot-gmm-selection-py
http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py
http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_sin.html#sphx-glr-auto-examples-mixture-plot-gmm-sin-py
http://scikit-learn.org/stable/auto_examples/mixture/plot_concentration_prior.html#sphx-glr-auto-examples-mixture-plot-concentration-prior-py


http://scikit-learn.org/stable/auto_examples/index.html#model-selection
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection
http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py
http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py
http://scikit-learn.org/stable/auto_examples/model_selection/plot_train_error_vs_test_error.html#sphx-glr-auto-examples-model-selection-plot-train-error-vs-test-error-py
http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py
http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html#sphx-glr-auto-examples-model-selection-grid-search-digits-py
http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py
http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html#sphx-glr-auto-examples-model-selection-randomized-search-py
http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py
http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py
http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py
http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py
http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py


http://scikit-learn.org/stable/auto_examples/index.html#nearest-neighbors
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors
http://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py
http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py
http://scikit-learn.org/stable/auto_examples/neighbors/plot_nearest_centroid.html#sphx-glr-auto-examples-neighbors-plot-nearest-centroid-py
http://scikit-learn.org/stable/auto_examples/neighbors/plot_digits_kde_sampling.html#sphx-glr-auto-examples-neighbors-plot-digits-kde-sampling-py
http://scikit-learn.org/stable/auto_examples/neighbors/plot_species_kde.html#sphx-glr-auto-examples-neighbors-plot-species-kde-py
http://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html#sphx-glr-auto-examples-neighbors-plot-kde-1d-py
http://scikit-learn.org/stable/auto_examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.html#sphx-glr-auto-examples-neighbors-plot-approximate-nearest-neighbors-hyperparameters-py
http://scikit-learn.org/stable/auto_examples/neighbors/plot_approximate_nearest_neighbors_scalability.html#sphx-glr-auto-examples-neighbors-plot-approximate-nearest-neighbors-scalability-py


http://scikit-learn.org/stable/auto_examples/index.html#neural-networks
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neural_network
http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html#sphx-glr-auto-examples-neural-networks-plot-mnist-filters-py
http://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py
http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py
http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py


http://scikit-learn.org/stable/auto_examples/index.html#preprocessing
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing
http://scikit-learn.org/stable/auto_examples/preprocessing/plot_function_transformer.html#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py
http://scikit-learn.org/stable/auto_examples/preprocessing/plot_robust_scaling.html#sphx-glr-auto-examples-preprocessing-plot-robust-scaling-py


http://scikit-learn.org/stable/auto_examples/index.html#semi-supervised-classification
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.semi_supervised
http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_structure.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-structure-py
http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-digits-py
http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_versus_svm_iris.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-versus-svm-iris-py
http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits_active_learning.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-digits-active-learning-py


http://scikit-learn.org/stable/auto_examples/index.html#support-vector-machines
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm
http://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py
http://scikit-learn.org/stable/auto_examples/svm/plot_svm_nonlinear.html#sphx-glr-auto-examples-svm-plot-svm-nonlinear-py
http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-py
http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py
http://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py
http://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html#sphx-glr-auto-examples-svm-plot-custom-kernel-py
http://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html#sphx-glr-auto-examples-svm-plot-weighted-samples-py
http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html#sphx-glr-auto-examples-svm-plot-svm-kernels-py
http://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html#sphx-glr-auto-examples-svm-plot-svm-margin-py
http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py
http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py
http://scikit-learn.org/stable/auto_examples/svm/plot_svm_scale_c.html#sphx-glr-auto-examples-svm-plot-svm-scale-c-py
http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py


http://scikit-learn.org/stable/auto_examples/index.html#working-with-text-documents
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text
http://scikit-learn.org/stable/auto_examples/text/hashing_vs_dict_vectorizer.html#sphx-glr-auto-examples-text-hashing-vs-dict-vectorizer-py
http://scikit-learn.org/stable/auto_examples/text/mlcomp_sparse_document_classification.html#sphx-glr-auto-examples-text-mlcomp-sparse-document-classification-py
http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py
http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py


http://scikit-learn.org/stable/auto_examples/index.html#decision-trees
http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree
http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py
http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py
http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html#sphx-glr-auto-examples-tree-plot-iris-py
http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py




###Scikit - Sample dataSet and dataset API - use prefix - sklearn.datasets

import sklearn.datasets


##Toy datasets
load_boston([return_X_y])           Load and return the boston house-prices dataset (regression). 
load_iris([return_X_y])             Load and return the iris dataset (classification). 
load_diabetes([return_X_y])         Load and return the diabetes dataset (regression). 
load_digits([n_class, return_X_y])  Load and return the digits dataset (classification). 
load_linnerud([return_X_y])         Load and return the linnerud dataset (multivariate regression). 

#Example 
from sklearn.datasets import load_boston
boston = load_boston()
>>> print(boston.data.shape)  #ndarray 
(506, 13)

##Sample images -  based on the uint8 
load_sample_images()            Load sample images for image manipulation. 
load_sample_image(image_name)   Load the numpy array of a single sample image 


from sklearn.datasets import load_sample_image
# Load the Summer Palace photo
china = load_sample_image("china.jpg")

# Convert to floats instead of the default 8 bits integer coding. 
#float data (need to be in the range [0-1]) for imshow 
china = np.array(china, dtype=np.float64) / 255
plt.figure(1)
plt.clf()
ax = plt.axes([0, 0, 1, 1])
plt.axis('off')
plt.title('Original image (96,615 colors)')
plt.imshow(china)


##Generators for classification and clustering - multiclass (target= only one label, many classes)
make_classification(n_samples=100, n_features=20, n_informative=2, 
        n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, 
        flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, 
        shuffle=True, random_state=None)


from sklearn import datasets
X, y = datasets.make_classification(n_samples=100000, n_features=20,
                                    n_informative=2, n_redundant=2)


##Generators for classification and clustering - multilabel multiclass (target=many labels, many classes)
make_multilabel_classification(n_samples=100, n_features=20, 
        n_classes=5, n_labels=2, length=50, allow_unlabeled=True, sparse=False, 
        return_indicator='dense', return_distributions=False, random_state=None)

from sklearn.datasets import make_multilabel_classification
X, Y = make_multilabel_classification(n_classes=2, n_labels=1,
                                      allow_unlabeled=True,
                                      random_state=1)


##Generators for regression                                     
make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, 
        tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)                                 
                                      
X, y = make_regression(n_samples=200, n_features=5000, random_state=0)


##Generators for manifold learning
make_s_curve([n_samples, noise, random_state])      Generate an S curve dataset. 
make_swiss_roll([n_samples, noise, random_state])   Generate a swiss roll dataset. 

##Generators for decomposition
make_low_rank_matrix([n_samples, ...])          Generate a mostly low rank matrix with bell-shaped singular values 
make_sparse_coded_signal(n_samples, ...[, ...]) Generate a signal as a sparse combination of dictionary elements. 
make_spd_matrix(n_dim[, random_state])          Generate a random symmetric, positive-definite matrix. 
make_sparse_spd_matrix([dim, alpha, ...])       Generate a sparse symmetric definite positive matrix. 
                                    
       
##Datasets in svmlight / libsvm format
#each line takes the form <label> <feature-id>:<feature-value> <feature-id>:<feature-value> .... 
#scipy sparse CSR matrices are used for X and numpy arrays are used for y.
#CSR:Compressed Sparse Row format
#CSR:Arithmatic operations and [row,colum] at index and slicing possible 
#CSR: has many mathematical methods eg sin, cos, or convert to ndarray via .toarray()/.todense() and use np.methods

from sklearn.datasets import load_svmlight_file
X_train, y_train = load_svmlight_file("/path/to/train_dataset.txt")

#load two (or more) datasets at once:
# X_train and X_test are guaranteed to have the same number of features
X_train, y_train, X_test, y_test = load_svmlight_files( ("/path/to/train_dataset.txt", "/path/to/test_dataset.txt"))
                                                    
## Loading from external datasets(use other libs) 
•pandas.io                          provides tools to read data from common formats including CSV, Excel, JSON and SQL
•scipy.io                           specializes in binary formats often used in scientific computing context such as .mat and .arff
•sklearn.datasets.load_files        for directories of text files where the name of each directory is the name of each category 
                                    and each file inside of each directory corresponds to one sample from that category


##check others - http://scikit-learn.org/stable/datasets/index.html#datasets

      
##Ref: sample databases
datasets.clear_data_home([data_home])               Delete all the content of the data home cache. 
datasets.get_data_home([data_home])                 Return the path of the scikit-learn data dir. 
datasets.fetch_20newsgroups([data_home, ...])       Load the filenames and data from the 20 newsgroups dataset. 
datasets.fetch_20newsgroups_vectorized([...])       Load the 20 newsgroups dataset and transform it into tf-idf vectors. 
datasets.load_boston([return_X_y])                  Load and return the boston house-prices dataset (regression). 
datasets.load_breast_cancer([return_X_y])           Load and return the breast cancer wisconsin dataset (classification). 
datasets.load_diabetes([return_X_y])                Load and return the diabetes dataset (regression). 
datasets.load_digits([n_class, return_X_y])         Load and return the digits dataset (classification). 
datasets.load_files(container_path[, ...])          Load text files with categories as subfolder names. 
datasets.load_iris([return_X_y])                    Load and return the iris dataset (classification). 
datasets.fetch_lfw_pairs([subset, ...])             Loader for the Labeled Faces in the Wild (LFW) pairs dataset 
datasets.fetch_lfw_people([data_home, ...])         Loader for the Labeled Faces in the Wild (LFW) people dataset 
datasets.load_linnerud([return_X_y])                Load and return the linnerud dataset (multivariate regression). 
datasets.mldata_filename(dataname)                  Convert a raw name for a data set in a mldata.org filename. 
datasets.fetch_mldata(dataname[, ...])              Fetch an mldata.org data set 
datasets.fetch_olivetti_faces([data_home, ...])     Loader for the Olivetti faces data-set from AT&T. 
datasets.fetch_california_housing([...])            Loader for the California housing dataset from StatLib. 
datasets.fetch_covtype([data_home, ...])            Load the covertype dataset, downloading it if necessary. 
datasets.fetch_kddcup99([subset, shuffle, ...])     Load and return the kddcup 99 dataset (classification). 
datasets.fetch_rcv1([data_home, subset, ...])       Load the RCV1 multilabel dataset, downloading it if necessary. 
datasets.load_mlcomp(name_or_id[, set_, ...])       Load a datasets as downloaded from http://mlcomp.org 
datasets.load_sample_image(image_name)              Load the numpy array of a single sample image 
datasets.load_sample_images()                       Load sample images for image manipulation. 
datasets.fetch_species_distributions([...])         Loader for species distribution dataset from Phillips et. 
datasets.load_svmlight_file(f[, n_features, ...])   Load datasets in the svmlight / libsvm format into sparse CSR matrix 
datasets.load_svmlight_files(files[, ...])          Load dataset from multiple files in SVMlight format 
datasets.dump_svmlight_file(X, y, f[, ...])         Dump the dataset in svmlight / libsvm file format. 

##Ref: Samples generator
datasets.make_blobs([n_samples, n_features, ...])   Generate isotropic Gaussian blobs for clustering. 
datasets.make_classification([n_samples, ...])      Generate a random n-class classification problem. 
datasets.make_circles([n_samples, shuffle, ...])    Make a large circle containing a smaller circle in 2d. 
datasets.make_friedman1([n_samples, ...])           Generate the 'Friedman #1' regression problem 
datasets.make_friedman2([n_samples, noise, ...])    Generate the 'Friedman #2' regression problem 
datasets.make_friedman3([n_samples, noise, ...])    Generate the 'Friedman #3' regression problem 
datasets.make_gaussian_quantiles([mean, ...])       Generate isotropic Gaussian and label samples by quantile 
datasets.make_hastie_10_2([n_samples, ...])         Generates data for binary classification used in Hastie et al. 
datasets.make_low_rank_matrix([n_samples, ...])     Generate a mostly low rank matrix with bell-shaped singular values 
datasets.make_moons([n_samples, shuffle, ...])      Make two interleaving half circles 
datasets.make_multilabel_classification([...])      Generate a random multilabel classification problem. 
datasets.make_regression([n_samples, ...])          Generate a random regression problem. 
datasets.make_s_curve([n_samples, noise, ...])      Generate an S curve dataset. 
datasets.make_sparse_coded_signal(n_samples, ...)   Generate a signal as a sparse combination of dictionary elements. 
datasets.make_sparse_spd_matrix([dim, ...])         Generate a sparse symmetric definite positive matrix. 
datasets.make_sparse_uncorrelated([...])            Generate a random regression problem with sparse uncorrelated design 
datasets.make_spd_matrix(n_dim[, random_state])     Generate a random symmetric, positive-definite matrix. 
datasets.make_swiss_roll([n_samples, noise, ...])   Generate a swiss roll dataset. 
datasets.make_biclusters(shape, n_clusters)         Generate an array with constant block diagonal structure for biclustering. 
datasets.make_checkerboard(shape, n_clusters)       Generate an array with block checkerboard structure for biclustering. 






###Scikit - Steps of ML 
MAINSTEP : Choose the righ estimator 
0. Split data into training and test data from sklearn.feature_extraction
1. Use Feature Extraction(for text/image data)
1.1 Features selection , dimensionality reduction from sklearn.feature_selection
2. Preprocessing steps(eg normalizer) from sklearn.pre_processing
3. Using cross validation to select best parameter of the model from training data 
    CV  predict and scoring (sklearn.model_selection)
    Model specific CV 
    Heper parameter tuning - GridSearch 
4. Compare result with Dummy classifier/regressor to understand imporvement 
5. Evaluate model performance on test data (sklearn.metric)
6. Combining various steps  - Use Pipeline or FeatureUnion from sklearn.model_selection 
7. Only for classifier - probablity calibration if you want prob score of X



##Scikit - Steps of ML - Label/target and feature matrix data format 

#Note in scikit , X can be ndarray or scipy.parse
#(prefeably Compressed Sparse Rows and Compressed Sparse Columns, others would be converted to this)
X : {array-like, sparse matrix}, shape [n_samples, n_features]

#to work with pandas , convert pandas DF to ndArray 
dataset_array = dataset.values
#or 
mat = dataset.as_matrix()

#for example for Clustering 
# Convert DataFrame to matrix
mat = dataset.as_matrix()
# Using sklearn
km = sklearn.cluster.KMeans(n_clusters=5)
km.fit(mat)
# Get cluster assignment labels
labels = km.labels_
# Format results as a DataFrame
results = pandas.DataFrame([dataset.index,labels]).T


#Note : Use sklearn-pandas for better compatibility for each column 
#Generally
1. if all columns are of float32/float64(homogeneousely typed), then its OK 
   Else,for  heterogeneously typed, convert them explicitly as dtype would be 'object'
>>> df
   float_col  int_col str_col
0        0.1        1       a
1        0.2        2       b
2        0.2        6     NaN
3       10.1        8       c
4        NaN       -1       a

>>> df.values[:,:-1]
array([[0.1, 1],
       [0.2, 2],
       [0.2, 6],
       [10.1, 8],
       [nan, -1]], dtype=object)

>>> df.values[:,:-1].astype(float32)
array([[  0.1       ,   1.        ],
       [  0.2       ,   2.        ],
       [  0.2       ,   6.        ],
       [ 10.10000038,   8.        ],
       [         nan,  -1.        ]], dtype=float32)
       
2. scikit  takes float32/float64 for feature value and label value
   Use transformers to change each feature 
   Note: some transformers expect a 1D input (the label-oriented ones) 
         while some others, like OneHotEncoder or Imputer, 
         expect 2D, with the shape [n_samples, n_features].
   For categorical input 
     (prefix-Label - takes 1D)
     (without prefix Label - takes 2D)   
     with string multiclass/binary y and x 
        Use LabelBinarizer() to convert to 0/1 for binary OR one hot encoding(becomes multilabel for y) for multiclass
        Use LabelEncoder() to convert to 0...No_Of_classes
     with number multiclass/binary 
       Scikit takes directly for y 
       for X,       
        Use LabelEncoder() to convert to 0...No_Of_classes
        Use LabelBinarizer() to convert to 0/1 for binary OR one hot encoding(becomes multilabel) for multiclass
        Use OneHotEncoder() to convert to one hot encoding
     Use MultiLabelBinarizer() to convert from string/numeric multilabel to proper format 
   For float/int, it better to normalize by StandardScaler()(Not Normalizer() as Normalizer is meant with l1/l2 norms)
   For sparse data, use MaxAbsScaler()
   Data with many outliers, use RobustScaler
   To convert a numeric feature to binary 0/1, use Binarizer  
   For text feature, convert using TfidfVectorizer (=CountVectorizer + TfidfTransformer)
   Use sklearn-pandas to transform each column 
   
   
##Scikit - Steps of ML - General structure of scikit class 

##Regression/classifiation has fit() method which returns self 

fit(X, y)      
    Fit the model according to the given training data.
    X : {array-like, sparse matrix}, shape = [n_samples, n_features]
        Training vector, where n_samples in the number of samples and n_features is the number of features.
    y : array-like, shape = [n_samples]
        Target vector relative to X
 
##then use Regression/classifiation's predict() to predict testing data 
predict(X)
    X : {array-like, sparse matrix}, shape = [n_samples, n_features]
    Returns:
        C : array, shape = [n_samples]

##Other methods of Regression/classifiation
get_params([deep])              Get parameters for this estimator. 
score(X, y[, sample_weight])    Returns score based on the estimator, 
                                More is better 

##Regression/classifiation class has many attributes(suffix _), 
#below is minimal for regression 
coef_ : array, Estimated coefficients 
intercept_ : array, Independent term in the linear model.


##For transformational class(Extraction, selection, preprocessing)
#X : {array-like, sparse matrix}, shape [n_samples, n_features]

#call fit() first 
fit(X[, y]) 
#then call transform()
transform(X[, y]) 
#or call 
fit_transform(X[, y]) 

#To use the model for futher transformation of new input 
transform(new_X[, y]) 

#for inverset transform - note one hot encoding is inversed with on hot encoding labels
inverse_transform(transformed)

#Selction(best one) along with Extraction 
restrict(selection.get_support()) 

#other methods 
get_params([deep]) 
inverse_transform(X[, dict_type]) Transform array or sparse matrix X back to feature mappings. 
get_feature_names()               To get the feature names 
restrict(support, indices=False)    Restrict the features to those in support using feature selection.


##Scikit - Norms
L1 norm = min of Sum( |yi-f(x) |), robust, unstable or possible many solutions, only efficient for sparse matrix 
L2 norm = min of Sum ( (yi-f(x))**2), called least square, not robust, but one stable solution 




###Scikit - MAINSTEP - Choosing the right estimator
#check  http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
#Different estimators are better suited for different types of data and different problems

#algorithm could be 
1. Get samples of size > 50 
2. for predicting a category 
    2.1 for labeled data (classification)
        2.1.1 for < 100K samples
                2.1.1.1 linear SVC (C-Support Vector Classification) - sklearn.svm.SVC
                        based on Support Vector Method 
                        If SVC is not working 
                        .1 if the data is text 
                            use Naive Bayes - sklearn.naive_bayes                         
                        .2 if data is not text 
                            use Nearest Neighbors - sklearn.neighbors
                            If not working, use Ensemble methods
                            eg averaging methods -  Bagging methods (sklearn.ensemble.BaggingClassifier), Forests of randomized trees(sklearn.ensemble.RandomForestClassifier)
                            boosting methods - AdaBoost, sklearn.ensemble.AdaBoostClassifier, Gradient Tree Boosting - sklearn.ensemble.GradientBoostingClassifier        
        2.1.2 for >100K samples 
            Use Stochastic gradient descent(SGD) classifier sklearn.linear_model.SGDClassifier
            If not working, use kernel approximation, sklearn.kernel_approximation 
    2.2 for unlabelled data (clustering)
        2.2.1 for number of categories known 
            2.2.1.1 for samples <10K
                    Use K means - sklearn.cluster.KMeans
                    if not working 
                        use Spectral clustering, sklearn.cluster.SpectralClustering
                        or use Gaussian mixture models, sklearn.mixture.GMM
            2.2.1.2 for samples > 10K 
                    use mini batch kmeans - sklearn.cluster.MiniBatchKMeans
        2.2.2 for number of categories un-known
            2.2.2.1 for samples < 10K 
                    Use meanShift - sklearn.cluster.MeanShift
                    or Gaussian mixture model with variational inference , VBGMM, sklearn.mixture.VBGMM
            2.2.2.2 for samples > 10K 
                  Create subset with samples < 10K and use above 
    
3. for predicting a quantity - regression 
    3.1 for samples < 100K
        3.1.1 for few features should be important 
            Use Lasso, sklearn.linear_model.Lasso
            or ElasticNet, sklearn.linear_model.ElasticNet
        3.1.2 for all features should be important 
            Use Ridge Regression, sklearn.linear_model.Ridge
            or  Support Vector Regression using Support vector machines , sklearn.svm.SVR(kernel='linear')
            if not working 
                use sklearn.svm.SVR(kernel='rbf')
                or Ensemble methods 
                for example  avergaing methods - bagging sklearn.ensemble.BaggingRegressor
                or random forests, sklearn.ensemble.RandomForestRegressor
                or Boosing methods - sklearn.ensemble.GradientBoostingRegressor
    3.2 for samples size > 100K 
        Use  Stochastic Gradient Descent , sklearn.linear_model.SGDRegressor
4. for just visualizing data
    Use randomized Proncipal component analysis(PCA), sklearn.decomposition.PCA
    4.1 If PCA not working and samples <10K 
        Use isomap , sklearn.manifold.Isomap
        or Spectral embedding , sklearn.manifold.SpectralEmbedding
        If above not working 
            use Locally linear embedding , LLE - sklearn.manifold.locally_linear_embedding 
    4.2 If PCA not working and samples > 10K 
        use kernel approximation, sklearn.kernel_approximation 

        














###Scikit - STEP0: Split data into training and test data 

##Use - sklearn.model_selection.train_test_split
sklearn.model_selection.train_test_split(*arrays, **options)[source]
    *arrays : sequence of indexables with same length / shape[0]
        Allowed inputs are lists, numpy arrays, scipy-sparse matrices 
        or pandas dataframes.
    test_size, train_size : float, int, None, optional
        If float, should be between 0.0 and 1.0 and represent the proportion 
        of the dataset to include in the test split. 
        If int, represents the absolute number of test samples. 
        If None, the value is set to the complement of the train size. 
        By default, the value is set to 0.25. 
    random_state : int, RandomState instance or None, optional (default=None)
    shuffle : boolean, optional (default=True)
    stratify : array-like or None (default is None)
        If not None, data is split in a stratified fashion, 
        using this as the class labels.

#Example 
import numpy as np
from sklearn.model_selection import train_test_split

X, y = np.arange(10).reshape((5, 2)), range(5)  #5 samples with two features 
>>> X
array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7],
       [8, 9]])
>>> list(y)
[0, 1, 2, 3, 4]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

>>> X_train
array([[4, 5],
       [0, 1],
       [6, 7]])
>>> y_train
[2, 0, 3]
>>> X_test
array([[2, 3],
       [8, 9]])
>>> y_test
[1, 4]

##sklearn.model_selection.train_test_split uses sklearn.model_selection.ShuffleSplit
#To use low level - sklearn.model_selection.ShuffleSplit

from sklearn.model_selection import ShuffleSplit
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) # index = [0,1,2,3]
y = np.array([1, 2, 1, 2])

>>> rs = ShuffleSplit(n_splits=3, test_size=.25, random_state=0)  #n_splits : Number of re-shuffling & splitting iterations.
>>> rs.get_n_splits(X)
3
>>> print(rs)
ShuffleSplit(n_splits=3, random_state=0, test_size=0.25, train_size=None)

for train_index, test_index in rs.split(X):    #Generate indices to split data into training and test set.
    print("TRAIN:", train_index, "TEST:", test_index)

TRAIN: [3 1 0] TEST: [2]        #one set of splitting, get the indices and from them get the actual data 
TRAIN: [2 1 3] TEST: [0]
TRAIN: [0 2 1] TEST: [3]


rs = ShuffleSplit(n_splits=3, train_size=0.5, test_size=.25, random_state=0)
for train_index, test_index in rs.split(X):
    print("TRAIN:", train_index, "TEST:", test_index)

TRAIN: [3 1] TEST: [2]
TRAIN: [2 1] TEST: [0]
TRAIN: [0 2] TEST: [3]



##Scikit - sklearn-utils 
sklearn.utils.check_random_state(seed)                      Turn seed into a np.random.RandomState instance 
sklearn.utils.estimator_checks.check_estimator(Estimator)   Check if estimator adheres to scikit-learn conventions. 
sklearn.utils.resample(*arrays, **options)                  Resample arrays or sparse matrices in a consistent way 
sklearn.utils.shuffle(*arrays, **options)                   Alias to resample(*arrays, replace=False) to do random permutations of the collections.         

#Example 
>>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])
>>> y = np.array([0, 1, 2])

>>> from scipy.sparse import coo_matrix
>>> X_sparse = coo_matrix(X)

>>> from sklearn.utils import shuffle
#can take multiple arrays as input 
>>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)
>>> X
array([[ 0.,  0.],
       [ 2.,  1.],
       [ 1.,  0.]])

>>> X_sparse                   
<3x2 sparse matrix of type '<... 'numpy.float64'>'
    with 3 stored elements in Compressed Sparse Row format>

>>> X_sparse.toarray()
array([[ 0.,  0.],
       [ 2.,  1.],
       [ 1.,  0.]])

>>> y
array([2, 1, 0])

#only take two 
>>> shuffle(y, n_samples=2, random_state=0)
array([0, 1])



###Scikit - STEP1.0: Feature Extraction  -  sklearn.feature_extraction 

##Scikit - Feature Extraction - Loading features from dicts -  DictVectorizer 

DictVectorizer (dtype=<type 'numpy.float64'>, separator='=', sparse=True, sort=True)

#can be used to convert feature arrays represented as lists of standard Python dict objects 
#to the NumPy/SciPy representation 
#for pandas DF, use df.to_dict('records') to get dict 
measurements = [
    {'city': 'Dubai', 'temperature': 33.},
    {'city': 'London', 'temperature': 12.},
    {'city': 'San Fransisco', 'temperature': 18.},
    ]

from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer()
transformed = vec.fit_transform(measurements) #note category feature is transformed into one-hot-Encoder
>>> transformed.toarray()
array([[  1.,   0.,   0.,  33.],
       [  0.,   1.,   0.,  12.],
       [  0.,   0.,   1.,  18.]])

>>> vec.get_feature_names()
['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']

>>> vec.inverse_transform(transformed)
[{'temperature': 33.0, 'city=Dubai': 1.0}, {'temperature': 12.0, 'city=London':
1.0}, {'temperature': 18.0, 'city=San Fransisco': 1.0}]

#Combining selection and extraction 
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_selection import SelectKBest, chi2
v = DictVectorizer()
D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]
X = v.fit_transform(D)
>>> X.todense()         #['bar', 'baz', 'foo']
matrix([[2., 0., 1.],
        [0., 1., 3.]])
        
support = SelectKBest(chi2, k=2).fit(X, [0, 1]) #only selct 2 best, .fit(X,y), y= target values to be fitted 
>>> v.get_feature_names()
['bar', 'baz', 'foo']
#get_support(indices=False)-Get a mask, or integer index, of the features selected
>>> v.restrict(support.get_support()) 
DictVectorizer(dtype=..., separator='=', sort=True,sparse=True)
>>> v.get_feature_names()
['bar', 'foo']
>>> v.transform(D).toarray()
array([[2., 1.],
       [0., 3.]])
>>> v.inverse_transform(v.transform(D))
[{'foo': 1.0, 'bar': 2.0}, {'foo': 3.0}]



##Scikit - Feature Extraction - Feature hashing -  FeatureHasher 
#- high speed , low memory but has no 'inverse_transform' method

FeatureHasher(n_features=1048576, input_type='dict', dtype=<type 'numpy.float64'>, non_negative=False)

#If non_negative=True is passed to the constructor, the absolute value is taken. 
#This undoes some of the hash collision handling, 
#but allows the output to be passed to estimators 
#like sklearn.naive_bayes.MultinomialNB or sklearn.feature_selection.chi2 feature selectors 
#that expect non-negative inputs.

#FeatureHasher accepts either mappings (input_type)
#(like Python's dict and its variants in the collections module)- 'dict'
#(feature, value) pairs - 'pair', or strings - 'strings'
#for pandas DF, use df.to_dict('records') to get dict 
#feature_name should be a string, while value should be a number. 
#In the case of 'string', a value of 1 is implied. 
#The feature_name is hashed to find the appropriate column for the feature

#Example 

>>> from sklearn.feature_extraction import FeatureHasher
>>> h = FeatureHasher(n_features=10)
>>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]
>>> f = h.fit_transform(D)
>>> f.toarray()                     #category is transformed into Encoder
array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],
       [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])



       
##Scikit - Text feature extraction - Introduction
#Uses scipy.sparse  
Bag of Words or 'Bag of n-grams' representation means 
    •tokenizing strings and giving an integer id for each possible token, 
     for instance by using white-spaces and punctuation as token separators.
    •counting the occurrences of tokens in each document.
    •normalizing and weighting with diminishing importance tokens  
     that occur in the majority of samples / documents.

features and samples are defined as follows:
    •each individual token occurrence frequency (normalized or not) 
     is treated as a feature.
    •the vector of all the token frequencies for a given document 
     is considered a multivariate sample.

Vectorization: A corpus of documents can thus be represented by a matrix 
    with one row per document 
    and one column per token (e.g. word) occurring in the corpus.




##Scikit - Text feature extraction -  CountVectorizer
#implements both tokenization and occurrence counting in a single class

sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8', 
        decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, 
        tokenizer=None, stop_words=None, token_pattern='(?u)\b\w\w+\b', 
        ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, 
        vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)  
    input : string {'filename', 'file', 'content'}
            filename or file-like object can be sequence of those 
    max_df : float in range [0.0, 1.0] or int, default=1.0
            When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).
    min_df : float in range [0.0, 1.0] or int, default=1
            When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold
    encoding : string, 'utf-8' by default.
        If bytes or files are given to analyze, this encoding is used to decode.
    decode_error : {'strict', 'ignore', 'replace'}
    strip_accents : {'ascii', 'unicode', None}
        Remove accents during the preprocessing step. 
    analyzer : string, {'word', 'char', 'char_wb'} or callable
        Whether the feature should be made of word or character n-grams. 
        Option 'char_wb' creates character n-grams only from text inside word boundaries; 
        n-grams at the edges of words are padded with space.
    preprocessor : callable or None (default)
        Override the preprocessing (string transformation) stage 
    tokenizer : callable or None (default)
        Override the string tokenization step 
    ngram_range : tuple (min_n, max_n)
        The lower and upper boundary of the range of n-values for different n-grams 
    stop_words : string {'english'}, list, or None (default)
        If 'english', a built-in stop word list for English is used.
        If a list, that list is assumed to contain stop words, 
        all of which will be removed from the resulting tokens(for analyzer == 'word')
        If None, no stop words will be used. 
        max_df can be set to a value in the range [0.7, 1.0) to automatically 
        detect and filter stop words based on intra corpus document frequency of terms.
    lowercase : boolean, True by default
        Convert all characters to lowercase before tokenizing.
    token_pattern : string
        Regular expression denoting what constitutes a 'token', 
        only used if analyzer == 'word'. 
        The default regexp select tokens of 2 or more alphanumeric characters 
        (punctuation is completely ignored and always treated as a token separator).
    max_features : int or None, default=None
        If not None, build a vocabulary that only consider the top max_features 
        ordered by term frequency across the corpus.
    vocabulary : Mapping or iterable, optional
        Either a Mapping (e.g., a dict) where keys are terms and values are indices(0..max)
        in the feature matrix, or an iterable over terms. 
        If not given, a vocabulary is determined from the input documents. 
    binary : boolean, default=False
        If True, all non zero counts are set to 1. 
        This is useful for discrete probabilistic models that model binary events rather than integer counts.

        
#Example 
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df=1)

corpus = [
        'This is the first document.',
        'This is the second second document.',
        'And the third one.',
        'Is this the first document?',
    ]
X = vectorizer.fit_transform(corpus)
>>> X                              
<4x9 sparse matrix of type '<... 'numpy.int64'>'
    with 19 stored elements in Compressed Sparse ... format>


>>> vectorizer.inverse_transform(X)
[array(['document', 'first', 'the', 'is', 'this'], dtype='<U8'), array(['second'
, 'document', 'the', 'is', 'this'], dtype='<U8'), array(['one', 'third', 'and',
'the'], dtype='<U8'), array(['document', 'first', 'the', 'is', 'this'], dtype='<
U8')]
>>> vectorizer.get_feature_names()
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']

>>> X.toarray()           
array([[0, 1, 1, 1, 0, 0, 1, 0, 1],
       [0, 1, 0, 1, 0, 2, 1, 0, 1],
       [1, 0, 0, 0, 1, 0, 1, 1, 0],
       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)

#Classifier needs float value, use the tf–idf transform.
#or can use TfidfVectorizer = CountVectorizer + TfidfTransformer, directly 
transformer = TfidfTransformer()
>>> transformer.fit_transform(X).toarray()
array([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,
        0.        , 0.35872874, 0.        , 0.43877674],
       [0.        , 0.27230147, 0.        , 0.27230147, 0.        ,
        0.85322574, 0.22262429, 0.        , 0.27230147],
       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,
        0.        , 0.28847675, 0.55280532, 0.        ],
       [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,
        0.        , 0.35872874, 0.        , 0.43877674]])

#The mapping from feature name to column index 
#is stored in the vocabulary_ attribute of the vectorizer
>>> vectorizer.vocabulary_.get('document')
1

#words that were not seen in the training corpus will be completely ignored 
#in future calls to the transform method:
>>> vectorizer.transform(['Something completely new.']).toarray()
array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)



##CountVectorizer - Advanced 
#Note that in the previous corpus, the first and the last documents have 
#exactly the same words hence are encoded in equal vectors. 
#we lose the information that the last document is an interrogative form.

#To preserve some of the local ordering information 
#we can extract 2-grams of words (ie pair of words)
#in addition to the 1-grams (individual words):
bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\b\w+\b', min_df=1)


#The vocabulary extracted by this vectorizer is hence much bigger 
#and can now resolve ambiguities encoded in local positioning patterns:
X_2 = bigram_vectorizer.fit_transform(corpus).toarray()
>>> X_2
array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],
       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],
       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],
       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)

>>> bigram_vectorizer.get_feature_names()
['and', 'and the', 'document', 'first', 'first document', 'is', 'is the', 
'is this', 'one', 'second', 'second document', 'second second', 'the', 
'the first', 'the second', 'the third', 'third', 'third one', 'this', 'this is', 
'this the']

##Using n-grams (n pairs of words or chars )

#For example , with a corpus of two documents: ['words', 'wprds']. 
#The second document contains a misspelling of the word 'words'.
 
#A simple bag of words representation would consider these two as very distinct document
#A character 2-gram representation, would find the documents matching in 4 out of 8 features, 
#which may help the preferred classifier decide better

#'char_wb' analyzer is used, which creates n-grams only from characters 
#inside word boundaries (padded with space on each side). 

ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2), min_df=1)
counts = ngram_vectorizer.fit_transform(['words', 'wprds'])
>>> ngram_vectorizer.get_feature_names() == (
    [' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'])
True
>>> counts.toarray().astype(int)
array([[1, 1, 1, 0, 1, 1, 1, 0],
       [1, 1, 0, 1, 1, 1, 0, 1]])


#The 'char' analyzer, alternatively, creates n-grams that span across words:
ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5), min_df=1)
>>> ngram_vectorizer.fit_transform(['jumpy fox'])
<1x4 sparse matrix of type '<... 'numpy.int64'>'
   with 4 stored elements in Compressed Sparse ... format>
>>> ngram_vectorizer.get_feature_names() == (
    [' fox ', ' jump', 'jumpy', 'umpy '])
True

>>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5), min_df=1)
>>> ngram_vectorizer.fit_transform(['jumpy fox'])
<1x5 sparse matrix of type '<... 'numpy.int64'>'
    with 5 stored elements in Compressed Sparse ... format>
>>> ngram_vectorizer.get_feature_names() == (
    ['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'])
    
    





##Scikit - Text feature extraction -  Tf–idf term weighting - TfidfTransformer
#Transform a count matrix to a normalized tf or tf-idf representation
#does not have 'inverse_transform'

#In a large text corpus, some words (e.g. 'the', 'a', 'is' in English) 
#carrying very little meaningful information about the actual contents of the document

#In order to re-weight the count features into floating point values suitable 
#for usage by a classifier , use the tf–idf transform.

#Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency

sklearn.feature_extraction.text.TfidfTransformer(norm='l2', use_idf=True, 
   smooth_idf=True, sublinear_tf=False)

#Example 
from sklearn.feature_extraction.text import TfidfTransformer
transformer = TfidfTransformer()

#an example with the following counts. 
#The first term is present 100% of the time hence not very interesting. 
#The two other features only in less than 50% of the time 
#hence probably more representative of the content of the documents:

counts = [  [3, 0, 1],
            [2, 0, 0],
            [3, 0, 0],
            [4, 0, 0],
            [3, 2, 0],
            [3, 0, 2]]
    
tfidf = transformer.fit_transform(counts)
>>> tfidf                         
<6x3 sparse matrix of type '<... 'numpy.float64'>'
    with 9 stored elements in Compressed Sparse ... format>

>>> tfidf.toarray()                        
array([[ 0.85...,  0.  ...,  0.52...],
       [ 1.  ...,  0.  ...,  0.  ...],
       [ 1.  ...,  0.  ...,  0.  ...],
       [ 1.  ...,  0.  ...,  0.  ...],
       [ 0.55...,  0.83...,  0.  ...],
       [ 0.63...,  0.  ...,  0.77...]])


#Each row is normalized to have unit euclidean norm. 
#The weights of each feature computed by the fit method call are stored in a model attribute:
>>> transformer.idf_                       
array([ 1. ...,  2.25...,  1.84...])




##Scikit - Text feature extraction -  TfidfVectorizer 
#combines CountVectorizer and TfidfTransformer
#Convert a collection of raw documents to a matrix of TF-IDF features.

TfidfVectorizer(input=u'content', encoding=u'utf-8', decode_error=u'strict', strip_accents=None,
     lowercase=True, preprocessor=None, tokenizer=None, analyzer=u'word', stop_words=None, 
     token_pattern=u'(?u)\b\w\w+\b', ngram_range=(1, 1), max_df=1.0, min_df=1, 
     max_features=None, vocabulary=None, binary=False, dtype=<type 'numpy.int64'>, 
     norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False)

#usage 
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(min_df=1)
corpus = [
        'This is the first document.',
        'This is the second second document.',
        'And the third one.',
        'Is this the first document?',
    ]
    
>>> X = vectorizer.fit_transform(corpus)
<4x9 sparse matrix of type '<... 'numpy.float64'>'
    with 19 stored elements in Compressed Sparse ... format>



    
    

##Scikit -  Text feature extraction -  Vectorizing with the hashing  -  HashingVectorizer
#combining sklearn.feature_extraction.FeatureHasher class and CountVectorizer.

HashingVectorizer(input=u'content', encoding=u'utf-8', decode_error=u'strict', strip_accents=None, 
    lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=u'(?u)\b\w\w+\b', 
    ngram_range=(1, 1), analyzer=u'word', n_features=1048576, binary=False, norm=u'l2', 
    non_negative=False, dtype=<type 'numpy.float64'>)
    
##Performing out-of-core scaling with HashingVectorizer
# HashingVectorizer has the ability to perform out-of-core scaling. 
#This means that we can learn from data that does not fit into the computer's main memory

#Usage 
from sklearn.feature_extraction.text import HashingVectorizer
hv = HashingVectorizer(n_features=10)
corpus = [
        'This is the first document.',
        'This is the second second document.',
        'And the third one.',
        'Is this the first document?',
    ]
>>> X= hv.fit_transform(corpus)
<4x10 sparse matrix of type '<... 'numpy.float64'>'
    with 16 stored elements in Compressed Sparse ... format>

    



##Scikit - Image feature extraction- Patch extraction - extract_patches_2d or PatchExtractor
#extracts patches from an image stored as a two-dimensional array, 
#or three-dimensional with color information along the third axis. 

extract_patches_2d(image, patch_size, max_patches=None, random_state=None)
PatchExtractor(patch_size=None, max_patches=None, random_state=None)

#For rebuilding an image from all its patches, use reconstruct_from_patches_2d. 
reconstruct_from_patches_2d(patches, image_size)


#PatchExtractor class supports multiple images as input. 
#It is implemented as an estimator, so it can be used in pipelines
#there is no inverse_transform() 

#X : array, shape = (n_samples, image_height, image_width) or
#(n_samples, image_height, image_width, n_channels) 


five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)
patches = image.PatchExtractor((2, 2)).transform(five_images)
>>> patches.shape
(45, 2, 2, 3)


##Using extract_patches_2d
sklearn.feature_extraction.image.extract_patches_2d(image, patch_size, max_patches=None, random_state=None)[source]
    Reshape a 2D image into a collection of patches
    Parameters:
    image : array, shape = (image_height, image_width) or
        (image_height, image_width, n_channels) The original image data. For color images, the last dimension specifies the channel: a RGB image would have n_channels=3.
    patch_size : tuple of ints (patch_height, patch_width)
        the dimensions of one patch
    max_patches : integer or float, optional default is None
        The maximum number of patches to extract. If max_patches is a float between 0 and 1, it is taken to be a proportion of the total number of patches.
    random_state : int, RandomState instance or None, optional (default=None)
        Pseudo number generator state used for random sampling to use if max_patches is not None. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
    Returns:
    patches : array, shape = (n_patches, patch_height, patch_width) or
        (n_patches, patch_height, patch_width, n_channels) 
        The collection of patches extracted from the image, where n_patches is either max_patches or the total number of patches that can be extracted.

sklearn.feature_extraction.image.reconstruct_from_patches_2d(patches, image_size)[source]
    Reconstruct the image from all of its patches.
    Patches are assumed to overlap and the image is constructed by filling 
    in the patches from left to right, top to bottom, 
    averaging the overlapping regions.
    Parameters:
    patches : array, shape = (n_patches, patch_height, patch_width) or
        (n_patches, patch_height, patch_width, n_channels) The complete set of patches. If the patches contain colour information, channels are indexed along the last dimension: RGB patches would have n_channels=3.
    image_size : tuple of ints (image_height, image_width) or
        (image_height, image_width, n_channels) the size of the image that will be reconstructed
    Returns:
    image : array, shape = image_size
        the reconstructed image



#For example generate a 4x4 pixel picture with 3 color channels (e.g. in RGB format):

import numpy as np
from sklearn.feature_extraction import image

one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))
>>> one_image[:, :, 0]  # R channel of a fake RGB picture
array([[ 0,  3,  6,  9],
       [12, 15, 18, 21],
       [24, 27, 30, 33],
       [36, 39, 42, 45]])

patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,random_state=0)
>>> patches.shape
(2, 2, 2, 3)
>>> patches[:, :, :, 0]
array([[[ 0,  3],
        [12, 15]],

       [[15, 18],
        [27, 30]]])
>>> patches = image.extract_patches_2d(one_image, (2, 2))
>>> patches.shape
(9, 2, 2, 3)
>>> patches[4, :, :, 0]
array([[15, 18],
       [27, 30]])


#to reconstruct the original image from the patches 
#by averaging on overlapping areas:
reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))
np.testing.assert_array_equal(one_image, reconstructed)






##Scikit - Image feature extraction- Connectivity graph of an image - img_to_graph 

#Ward clustering (Hierarchical clustering) can cluster together only neighboring pixels of an image, 
#thus forming contiguous patches:

#img_to_graph returns such a matrix from a 2D or 3D image. 
#grid_to_graph build a connectivity matrix for images given the shape of these image.  

feature_extraction.image.img_to_graph(img[, ...])   
    Graph of the pixel-to-pixel gradient connections 
feature_extraction.image.grid_to_graph(n_x, n_y)    
    Graph of the pixel-to-pixel connections 

     
#Complete example 
http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#example-model-selection-grid-search-text-feature-extraction-py
http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html#example-model-selection-grid-search-digits-py




###Scikit - STEP1.1: Features(number of columns in X) selection /dimensionality reduction 
#to improve estimators' accuracy scores 
#or to boost their performance on very high-dimensional datasets

##Feature selection as part of a pipeline
#use a sklearn.pipeline.Pipeline:

clf = Pipeline([
  ('feature_selection', SelectFromModel(LinearSVC(penalty="l1"))),
  ('classification', RandomForestClassifier())
])
clf.fit(X, y)

#In general the methods are 
fit(X[, y])             Learn empirical variances from X. 
fit_transform(X[, y])   Fit to data, then transform it. 
get_params([deep])      Get parameters for this estimator. 
get_support([indices])  Get a mask, or integer index, of the features selected 
                        Parameters:
                        indices : boolean (default False)
                            If True, the return value will be an array of integers, rather than a boolean mask.
                        Returns:
                        support : array
                            An index that selects the retained features from a feature vector. 
                            If indices is False, this is a boolean array of shape [# input features], 
                            in which an element is True iff its corresponding feature is selected for retention. 
                            If indices is True, this is an integer array of shape [# output features] 
                            whose values are indices into the input feature vector.
inverse_transform(X)    Reverse the transformation operation 
                        Returns X_r : array of shape [n_samples, n_original_features]
                            X with columns of zeros inserted where features would have been removed by transform.
set_params(**params)    Set the parameters of this estimator. 
transform(X)            Reduce X to the selected features. 



##Scikit - Features selection - Removing features with low variance 
#sklearn.feature_selection.VarianceThreshold 
sklearn.feature_selection.VarianceThreshold(threshold=0.0)
#removes all features whose variance doesn't meet some threshold. 

#By default, it removes all zero-variance features, 
#i.e. features that have the same value in all samples.

>>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]
>>> selector = VarianceThreshold()
>>> selector.fit_transform(X)
array([[2, 0],
       [1, 4],
       [1, 1]])
>>> selector.inverse_transform(selector.fit_transform(X)) #note the last column is zero 
array([[0, 2, 0, 0],
       [0, 1, 4, 0],
       [0, 1, 1, 0]])

#example -  suppose that we have a dataset with boolean features
#and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. 
#Boolean features are Bernoulli random variables, and the variance is p(1 - p)
#use  threshold .8 * (1 - .8):


from sklearn.feature_selection import VarianceThreshold
X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
sel.fit_transform(X)
#VarianceThreshold has removed the first column, 
#which has a probability p = 5/6 > .8 of containing a zero.
array([[0, 1],
       [1, 0],
       [0, 0],
       [1, 1],
       [1, 0],
       [1, 1]])










##Scikit - Features selection  -  Univariate feature selection functions 
#these functions can be used in Univariate feature selection

sklearn.feature_selection.mutual_info_regression(X, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)[source]
    Estimate mutual information for a continuous target variable.
sklearn.feature_selection.mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)[source]
    Estimate mutual information for a discrete target variable.
    Mutual information between two random variables is a non-negative value, 
    which measures the dependency between the variables. 
    It is equal to zero if and only if two random variables are independent, 
    and higher values mean higher dependency.
    The function relies on nonparametric methods based on entropy estimation 
    from k-nearest neighbors distances 
    Note it determines Estimated mutual information between each feature and the target
    Hence remove those features with zero value
    Parameters:
    X : array_like or sparse matrix, shape (n_samples, n_features)
        Feature matrix.
    y : array_like, shape (n_samples,)
        Target vector.
    discrete_features : {'auto', bool, array_like}, default 'auto'
        If bool, then determines whether to consider all features discrete or continuous. 
        If array, then it should be either a boolean mask with shape (n_features,) 
        or array with indices of discrete features. 
        If 'auto', it is assigned to False for dense X and to True for sparse X.
    n_neighbors : int, default 3
        Number of neighbors to use for MI estimation for continuous variables, 
        Higher values reduce variance of the estimation, but could introduce a bias.
    Returns:
    mi : ndarray, shape (n_features,)
        Estimated mutual information between each feature and the target.

sklearn.feature_selection.f_classif(X, y)[source]
    Compute the ANOVA F-value for the provided sample.
    X : {array-like, sparse matrix} shape = [n_samples, n_features]
        The set of regressors that will be tested sequentially.
    y : array of shape(n_samples)
        The data matrix.
    Returns:
    F : array, shape = [n_features,]
        The set of F values.
    pval : array, shape = [n_features,]
         p-values < 0.05  are significants 
         and more near to zero are more significants 
 
sklearn.feature_selection.f_regression(X, y, center=True)
    Univariate linear regression tests.
    Linear model for testing the individual effect of each of many regressors
    X : {array-like, sparse matrix} shape = (n_samples, n_features)
        The set of regressors that will be tested sequentially.
    y : array of shape(n_samples).
        The data matrix
    center : True, bool,
        If true, X and y will be centered.
    Returns:
    F : array, shape=(n_features,)
        F values of features.
    pval : array, shape=(n_features,)
        p-values of F-scores
         p-values < 0.05  are significants 
         and more near to zero are more significants 
 
sklearn.feature_selection.chi2(X, y)
    Compute chi-squared stats between each non-negative feature and class.
    This score can be used to select the n_features features 
    with the highest values for the test chi-squared statistic from X, 
    which must contain only non-negative features such as booleans or frequencies 
    (e.g., term counts in document classification), relative to the classes.
    Recall that the chi-square test measures dependence between stochastic variables, 
    so using this function 'weeds out' the features that are the most likely 
    to be independent of class and therefore irrelevant for classification.
    X : {array-like, sparse matrix}, shape = (n_samples, n_features_in)
        Sample vectors.
    y : array-like, shape = (n_samples,)
        Target vector (class labels).
    Returns:
    chi2 : array, shape = (n_features,)
        chi2 statistics of each feature.
    pval : array, shape = (n_features,)
        p-values of each feature.

##Scikit - Features selection - Univariate feature selection
#works by selecting the best features based on univariate statistical tests, 

•SelectKBest removes all but the k highest scoring features
    sklearn.feature_selection.SelectKBest(score_func=<function f_classif>, k=10)
    
•SelectPercentile removes all 
 but a user-specified highest scoring percentage of features
    sklearn.feature_selection.SelectPercentile(score_func=<function f_classif>, percentile=10)
    
•Using common univariate statistical tests for each feature: 
    false positive rate(FRP) SelectFpr, 
        Select the pvalues below alpha based on a FPR test.
        sklearn.feature_selection.SelectFpr(score_func=<function f_classif>, alpha=0.05)
        
    false discovery rate SelectFdr, 
        Select the p-values for an estimated false discovery rate, below alpha 
        sklearn.feature_selection.SelectFdr(score_func=<function f_classif>, alpha=0.05)
        
    family wise error SelectFwe.
        Select the p-values corresponding to Family-wise error rate, , below alpha 
        sklearn.feature_selection.SelectFwe(score_func=<function f_classif>, alpha=0.05)
        
•GenericUnivariateSelect : selection with a configurable strategy
    This allows to select the best univariate selection strategy 
    with hyper-parameter search estimator
    sklearn.feature_selection.GenericUnivariateSelect(score_func=<function f_classif>, mode='percentile', param=1e-05)
    mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}
    
#Above takes as input a scoring function that returns univariate p-values
•For regression:     f_regression, mutual_info_regression
•For classification: chi2, f_classif, mutual_info_classif
#Don't use a regression scoring function with a classification problem

#For sparse data (i.e. data represented as sparse matrices), 
#chi2, mutual_info_regression, mutual_info_classif willnot convert it to dense.


 
#Example - we can perform a chi^2 test to the samples to retrieve only the two best features 

from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
iris = load_iris()
X, y = iris.data, iris.target
>>> X.shape
(150, 4)
X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
>>> X_new.shape
(150, 2)





##Scikit - Features selection - Recursive feature elimination with external estimator 

sklearn.feature_selection.RFE(estimator, n_features_to_select=None, step=1, verbose=0)
    RFE is to select features by recursively considering smaller 
    and smaller sets of features
sklearn.feature_selection.RFECV(estimator, step=1, cv=None, scoring=None, verbose=0, n_jobs=1)
    RFECV performs RFE in a cross-validation loop to find the optimal number of features
    estimator : object
        A supervised learning estimator with a fit method 
        check from http://scikit-learn.org/stable/supervised_learning.html
    n_features_to_select : int or None (default=None)
        The number of features to select. If None, half of the features are selected.
    step : int or float, optional (default=1)
        If greater than or equal to 1, then step corresponds to the (integer) number 
        of features to remove at each iteration. 
        If within (0.0, 1.0), then step corresponds to the percentage (rounded down) 
        of features to remove at each iteration.
    cv : int, cross-validation generator or an iterable, optional
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are:
        •None, to use the default 3-fold cross-validation,
        •integer, to specify the number of folds.
        •An object to be used as a cross-validation generator
         check class names from 
         >>> import sklearn.model_selection
         >>> dir(sklearn.model_selection )
        •An iterable yielding train/test splits.
        For integer/None inputs, if y is binary or multiclass, 
        sklearn.model_selection.StratifiedKFold is used. 
        If the estimator is a classifier or if y is neither binary nor multiclass, 
        sklearn.model_selection.KFold is used.
    scoring : string, callable or None, optional, default: None
        A string from model evaluation documentation 
        or a scorer callable object / function with signature scorer(estimator, X, y)
        Check loss function from below and convert to scorer by sklearn.metrics.make_scorer
        >>> import sklearn.metrics
        >>> dir(sklearn.metrics)
        Note sklearn.metrics have many direct scorer functions with suffix _scorer
        That can be directly used 
#Methods
decision_function(X)  
fit(X, y)               Fit the RFE model and then the underlying estimator on the selected features. 
fit_transform(X[, y])   Fit to data, then transform it
                        Returns X_new : numpy array of shape [n_samples, n_features_new]
get_params([deep])      Get parameters for this estimator. 
get_support([indices])  Get a mask, or integer index, of the features selected 
inverse_transform(X)    Reverse the transformation operation 
                        Returns  X_r : array of shape [n_samples, n_original_features]
                        X with columns of zeros inserted where features would have been removed by transform.
predict(X)              Reduce X to the selected features and then predict using the underlying estimator. 
                        Returns The predicted target values.
predict_log_proba(X)  
predict_proba(X)  
score(X, y)             Reduce X to the selected features and then return the score of the underlying estimator. 
set_params(**params)    Set the parameters of this estimator. 
transform(X)            Reduce X to the selected features. 

#Exmaple 
from sklearn.datasets import make_friedman1
from sklearn.feature_selection import RFE
from sklearn.svm import SVR
X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)
estimator = SVR(kernel="linear")
selector = RFE(estimator, 5, step=1) #RFECV(estimator, step=1, cv=5)
#or 
selector = RFECV(estimator, step=1, cv=5)

>>> selector = selector.fit(X, y)
>>> selector.support_   #Get a mask  of the features selected
array([ True,  True,  True,  True,  True,
        False, False, False, False, False], dtype=bool)
>>> selector.ranking_  #array of shape [n_features],The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. 
array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])  #Selected (i.e., estimated best) features are assigned rank 1.
>>> selector.ranking_
>>> selector.score()
>>> X_r = selector.transform(X) #X_r : array of shape [n_samples, n_selected_features]





##Scikit - Features selection /dimensionality reduction - using SelectFromModel
# can be used along with any estimator that has a coef_ or feature_importances_ attribute after fitting
# The features are considered unimportant and removed, 
#if the corresponding coef_ or feature_importances_ values are below the provided threshold parameter

class sklearn.feature_selection.SelectFromModel(estimator, threshold=None, prefit=False, norm_order=1)[source]
    estimator : object
        The base estimator from which the transformer is built. 
        This can be both a fitted (if prefit is set to True) 
        or a non-fitted estimator. 
    threshold : string, float, optional default None
        The threshold value to use for feature selection. 
        Features whose importance is greater or equal are kept 
        while the others are discarded. 
        If 'median' (resp. 'mean'), then the threshold value is the median (resp. the mean) 
        of the feature importances. 
        A scaling factor (e.g., '1.25*mean') may also be used. 
        If None and if the estimator has a parameter penalty set to l1, 
        either explicitly or implicitly (e.g, Lasso), 
        the threshold used is 1e-5. Otherwise, 'mean' is used by default.
    prefit : bool, default False
        Whether a prefit model is expected to be passed into the constructor directly or not. 
        If True, transform must be called directly 
        and SelectFromModel cannot be used with cross_val_score, GridSearchCV and similar utilities 
        that clone the estimator. 
        Otherwise train the model using fit and then transform to do feature selection.
    norm_order : non-zero int, inf, -inf, default 1
        Order of the norm used to filter the vectors of coefficients 
        below threshold in the case where the coef_ attribute of the estimator 
        is of dimension 2.
 
#Methods
fit(X[, y])                 Fit the SelectFromModel meta-transformer. 
fit_transform(X[, y])       Fit to data, then transform it. 
get_params([deep])          Get parameters for this estimator. 
get_support([indices])      Get a mask, or integer index, of the features selected 
inverse_transform(X)        Reverse the transformation operation 
partial_fit(X[, y])         Fit the SelectFromModel meta-transformer only once. 
set_params(**params)        Set the parameters of this estimator. 
transform(X)                Reduce X to the selected features. 


##Scikit - SelectFromModel - L1-based feature selection
#Linear models penalized with the L1 norm have sparse solutions
#Many of their estimated coefficients are zero, based on which, features can be selected 
#linear_model.Lasso for regression, 
#linear_model.LogisticRegression and svm.LinearSVC for classification:

#With SVMs and logistic-regression, the parameter C controls the sparsity: 
#the smaller C the fewer features selected. 
#With Lasso, the higher the alpha parameter, the fewer features selected.


from sklearn.svm import LinearSVC
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
iris = load_iris()
X, y = iris.data, iris.target
>>> X.shape
(150, 4)
lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
model = SelectFromModel(lsvc, prefit=True) #lsvc has already fit() called 
X_new = model.transform(X)
>>> X_new.shape
(150, 3)


#There are some well-known limitations of L1-penalized models for regression and classification
#To mitigate this problem, use randomization techniques 

sklearn.linear_model.RandomizedLasso(alpha='aic', scaling=0.5, sample_fraction=0.75, n_resampling=200, selection_threshold=0.25, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.2204460492503131e-16, random_state=None, n_jobs=1, pre_dispatch='3*n_jobs', memory=Memory(cachedir=None))
    RandomizedLasso implements this strategy for regression settings, 
    using the Lasso    
sklearn.linear_model.RandomizedLogisticRegression(C=1, scaling=0.5, sample_fraction=0.75, n_resampling=200, selection_threshold=0.25, tol=0.001, fit_intercept=True, verbose=False, normalize=True, random_state=None, n_jobs=1, pre_dispatch='3*n_jobs', memory=Memory(cachedir=None))
    RandomizedLogisticRegression uses the logistic regression 
    and is suitable for classification tasks
#Methods 
fit(X, y)               Fit the model using X, y as training data. 
fit_transform(X[, y])   Fit to data, then transform it. 
get_params([deep])      Get parameters for this estimator. 
get_support([indices])  Return a mask, or list, of the features/indices selected. 
inverse_transform(X)    Transform a new matrix using the selected features 
set_params(**params)    Set the parameters of this estimator. 
transform(X)            Transform a new matrix using the selected features 
    
#Example 
>>> from sklearn.linear_model import RandomizedLasso
>>> randomized_lasso = RandomizedLasso()
>>> X_new = randomized_lasso.fit_transform(X[, y]) #has inverse_transform()

>>> from sklearn.linear_model import RandomizedLogisticRegression
>>> randomized_logistic = RandomizedLogisticRegression()
>>> X_new = randomized_logistic.fit_transform(X[, y])#has inverse_transform()


 



##Scikit - SelectFromModel - Tree-based feature selection

#Tree-based estimators (sklearn.tree and forest of trees in the sklearn.ensemble module) 
#can be used to discard irrelevant features 
>> import sklearn.tree
>> import sklearn.ensemble
>> dir(sklearn.tree)
>> dir(sklearn.ensemble)

#Example 
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
iris = load_iris()
X, y = iris.data, iris.target
>>> X.shape
(150, 4)
clf = ExtraTreesClassifier()
clf = clf.fit(X, y)
>>> clf.feature_importances_  
array([ 0.04...,  0.05...,  0.4...,  0.4...])
model = SelectFromModel(clf, prefit=True)
X_new = model.transform(X)
>>> X_new.shape               
(150, 2)














###Scikit - STEP2.0: Preprocessing  - sklearn.preprocessing: Preprocessing 
#Use class version if need to be used in Pipeline 

#Some transformers expect a 1-dimensional input (the label-oriented ones) 
#while some others, like OneHotEncoder or Imputer, 
#expect 2-dimensional input, with the shape [n_samples, n_features].
#While some(eg StandardScaler) works with both 1D and 2D

#Note a transformer transforms all features/columns simultaneousely 
#if different feature is required with different transformation, use scikit-pandas 


preprocessing.Binarizer([threshold, copy])          Binarize data (set feature values to 0 or 1) according to a threshold 
preprocessing.binarize(X[, threshold, copy])        Boolean thresholding of array-like or scipy.sparse matrix 

preprocessing.LabelBinarizer([neg_label, ...])      Binarize labels in a one-vs-all fashion 
preprocessing.label_binarize(y, classes[, ...])     Binarize labels in a one-vs-all fashion 

preprocessing.MaxAbsScaler([copy])                  Scale each feature by its maximum absolute value. 
preprocessing.maxabs_scale(X[, axis, copy])         Scale each feature to the [-1, 1] range without breaking the sparsity. 

preprocessing.MinMaxScaler([feature_range, copy])   Transforms features by scaling each feature to a given range. 
preprocessing.minmax_scale(X[, ...])                Transforms features by scaling each feature to a given range. 

preprocessing.Normalizer([norm, copy])              Normalize samples individually to unit norm. 
preprocessing.normalize(X[, norm, axis, ...])       Scale input vectors individually to unit norm (vector length). 

preprocessing.RobustScaler([with_centering, ...])   Scale features using statistics that are robust to outliers. 
preprocessing.robust_scale(X[, axis, ...])          Standardize a dataset along any axis 

preprocessing.StandardScaler([copy, ...])           Standardize features by removing the mean and scaling to unit variance 
preprocessing.scale(X[, axis, with_mean, ...])      Standardize a dataset along any axis 

preprocessing.FunctionTransformer([func, ...])      Constructs a transformer from an arbitrary callable. 
preprocessing.Imputer([missing_values, ...])        Imputation transformer for completing missing values. 
preprocessing.KernelCenterer                        Center a kernel matrix 
preprocessing.LabelEncoder                          Encode labels with value between 0 and n_classes-1. 
preprocessing.MultiLabelBinarizer([classes, ...])   Transform between iterable of iterables and a multilabel format 
preprocessing.OneHotEncoder([n_values, ...])        Encode categorical integer features using a one-hot aka one-of-K scheme. 
preprocessing.PolynomialFeatures([degree, ...])     Generate polynomial and interaction features. 

preprocessing.add_dummy_feature(X[, value])         Augment dataset with an additional dummy feature. 


##Scikit - Preprocessing - add intercept term 
sklearn.preprocessing.add_dummy_feature(X, value=1.0)
    Augment dataset with an additional dummy feature.
    This is useful for fitting an intercept term with implementations which cannot otherwise fit it directly.

#Example 
>>> from sklearn.preprocessing import add_dummy_feature
>>> add_dummy_feature([[0, 1], [1, 0]])
array([[ 1.,  0.,  1.],
       [ 1.,  1.,  0.]])



##Scikit - Preprocessing  - Standardization - preprocessing.scale()

#creating zero mean and unit variance of a feature(column) - preprocessing.scale()

#scale and StandardScaler also work with 1d arrays. 
#hence useful for scaling the target / response variables used for regression.

#many elements used in the objective function of a learning algorithm 
#(such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) 
#assume that all features are centered around zero and have variance in the same order

from sklearn import preprocessing
import numpy as np
X = np.array([[ 1., -1.,  2.],
    [ 2.,  0.,  0.],
    [ 0.,  1., -1.]])
    
#axis=0, columnwise/featurewise, axis=1, samplewise 
X_scaled = preprocessing.scale(X,axis=0, with_mean=True, with_std=True, copy=True)
>>> X_scaled                                          
array([[ 0.  ..., -1.22...,  1.33...],
       [ 1.22...,  0.  ..., -0.26...],
       [-1.22...,  1.22..., -1.06...]])

>>> X_scaled.mean(axis=0) #row varying ie columnwise 
array([ 0.,  0.,  0.])

>>> X_scaled.std(axis=0)
array([ 1.,  1.,  1.])


##Scikit - Preprocessing  - Standardization - StandardScaler

#Use  StandardScaler where old data can be retrieved 
sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)

#to disable either centering or scaling by either passing with_mean=False or with_std=False 

#has following methods, hence can be put into PipeLine 
fit_transform(X[, y])           Fit to data, then transform it. 
inverse_transform(X[, copy])    Scale back the data to the original representation 


#Example 
X = np.array([[ 1., -1.,  2.],
    [ 2.,  0.,  0.],
    [ 0.,  1., -1.]])
    
scaler = preprocessing.StandardScaler().fit(X)
>>> scaler
StandardScaler(copy=True, with_mean=True, with_std=True)

>>> scaler.mean_                    #can compute   mean                      
array([ 1. ...,  0. ...,  0.33...])

>>> scaler.scale_                   #can compute  scale                        
array([ 0.81...,  0.81...,  1.24...])

>>> scaler.transform(X)             #transform X now                      
array([[ 0.  ..., -1.22...,  1.33...],
       [ 1.22...,  0.  ..., -0.26...],
       [-1.22...,  1.22..., -1.06...]])


#or with new data to transform it the same way it did on the training set
>>> scaler.transform([[-1.,  1., 0.]])                
array([[-2.44...,  1.22..., -0.26...]])



##Scikit - Preprocessing  - Scaling features(columns) to a range
#X : {array-like, sparse matrix}, shape [n_samples, n_features], 

#Use minmax_scale and maxabs_scale 
sklearn.preprocessing.minmax_scale(X, feature_range=(0, 1), axis=0, copy=True)
sklearn.preprocessing.maxabs_scale(X, axis=0, copy=True)

#or Use below classes, has fit(), fit_transform(), transform(), inverse_transform()
#MinMaxScaler with default range  [0,1] or with explicit feature_range=(min, max)
sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)
#or MaxAbsScaler with range [-1, 1]
sklearn.preprocessing.MaxAbsScaler(copy=True)

#OR Scaling data with outliers - use robust_scale and RobustScaler 
#uses more robust estimates for the center and range of  data.
sklearn.preprocessing.robust_scale(X, axis=0, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)
class sklearn.preprocessing.RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)

#parse data 
#MaxAbsScaler and maxabs_scale can be used for scaling sparse data
#RobustScaler cannot be fitted to sparse inputs, but can use the transform method on sparse inputs.


#Example 
X_train = np.array([[ 1., -1.,  2.],
            [ 2.,  0.,  0.],
            [ 0.,  1., -1.]])

min_max_scaler = preprocessing.MinMaxScaler()
X_train_minmax = min_max_scaler.fit_transform(X_train)
>>> X_train_minmax
array([[ 0.5       ,  0.        ,  1.        ],
       [ 1.        ,  0.5       ,  0.33333333],
       [ 0.        ,  1.        ,  0.        ]])


#with new data set - each element is feature value 
X_test = np.array([[ -3., -1.,  4.]])
X_test_minmax = min_max_scaler.transform(X_test)
>>> X_test_minmax
array([[-1.5       ,  0.        ,  1.66666667]])







##Scikit - Preprocessing  - Whitening - PCA
#If a downstream model make some assumption on the linear independence of the features
#(for example in Support Vector Machines with the RBF kernel and the K-Means clustering algorithm)
#Use sklearn.decomposition.PCA or sklearn.decomposition.RandomizedPCA with whiten=True 
#to remove the linear correlation across features.
#Note whiten=True  creates uncorrelated outputs with unit component-wise variances.

class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)
    Uses full SVD 
class sklearn.decomposition.RandomizedPCA(n_components, copy=True, iterated_power=3, whiten=False, random_state=None)
    Using approx Singular Value Decomposition (randomizedSVD) of the data 
    
#sklearn.decomposition.PCA or sklearn.decomposition.RandomizedPCA 
transform(X)            Apply the dimensionality reduction on X. 
score(X[, y])           Return the average log-likelihood of all samples 
fit_transform(X[, y])   Fit the model with X and apply the dimensionality reduction on X. 
get_params([deep])      Get parameters for this estimator. 
inverse_transform(X)    Transform data  Transform data back to its original space

#usage 
import numpy as np
from sklearn.decomposition import PCA
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
pca = PCA(n_components=2, whiten=True)
pca.fit(X)
>>> print(pca.explained_variance_ratio_) 
[ 0.99244...  0.00755...]

#X_new: array-like, shape (n_samples, n_components)
#X : array-like, shape (n_samples, n_features)
X_new = pca.transform(X) 





##Scikit - Preprocessing  - Normalization 
#X : {array-like, sparse matrix}, shape [n_samples, n_features], 

##Normalization is the process of scaling individual samples to have unit norm. 
sklearn.preprocessing.normalize(X, norm='l2', axis=1, copy=True, return_norm=False)

#has fit(), fit_transform(), transform(), inverse_transform()
sklearn.preprocessing.Normalizer(norm='l2', copy=True)
    norm : 'l1', 'l2', or 'max', optional ('l2' by default)

#This process can be useful if you plan to use a quadratic form 
#such as the dot-product or any other kernel 
#to quantify the similarity of any pair of samples.

X = [[ 1., -1.,  2.],
    [ 2.,  0.,  0.],
    [ 0.,  1., -1.]]
X_normalized = preprocessing.normalize(X, norm='l2')
>>> X_normalized                                      
array([[ 0.40..., -0.40...,  0.81...],
       [ 1.  ...,  0.  ...,  0.  ...],
       [ 0.  ...,  0.70..., -0.70...]])

#OR 

normalizer = preprocessing.Normalizer().fit_transform(X)                          
array([[ 0.40..., -0.40...,  0.81...],
       [ 1.  ...,  0.  ...,  0.  ...],
       [ 0.  ...,  0.70..., -0.70...]])
#for Test data 
>>> normalizer.transform([[-1.,  1., 0.]])             
array([[-0.70...,  0.70...,  0.  ...]])



##Scikit - Preprocessing  - Feature binarization 

sklearn.preprocessing.binarize(X, threshold=0.0, copy=True)
sklearn.preprocessing.Binarizer(threshold=0.0, copy=True) #fit(), fit_transform(), transform(), inverse_transform()

#Feature binarization is the process of thresholding numerical features to get boolean values(0/1)

#This can be useful for downstream probabilistic estimators that make assumption 
#that the input data is distributed according to a multi-variate Bernoulli distribution.
#for exaample, sklearn.neural_network.BernoulliRBM

X = [[ 1., -1.,  2.],
    [ 2.,  0.,  0.],
    [ 0.,  1., -1.]]

binarizer = preprocessing.Binarizer().fit_transform(X)  
>>> array([[ 1.,  0.,  1.],
       [ 1.,  0.,  0.],
       [ 0.,  1.,  0.]])


#to adjust the threshold of the binarizer:
binarizer = preprocessing.Binarizer(threshold=1.1)
>>> binarizer.transform(X)
array([[ 0.,  0.,  1.],
       [ 1.,  0.,  0.],
       [ 0.,  0.,  0.]])
       
       
class sklearn.preprocessing.MultiLabelBinarizer(classes=None, sparse_output=False)[source]
    Transform a multilabel Y to  multilabel format of 0/1
    a list of sets or tuples is a very intuitive format for multilabel data,
    To use this in a ML process, converts to the  supported multilabel format: 
    a (samples x classes) binary matrix indicating the presence of a class label.
    Parameters:
    classes : array-like of shape [n_classes] (optional)
        Indicates an ordering for the class labels
    sparse_output : boolean (default: False),
        Set to true if output binary array is desired in CSR sparse format
 

#Example 
#Input : A set of labels (any orderable and hashable object) for each sample
#Output:array or CSR matrix, shape (n_samples, n_classes)
#A matrix such that y_indicator[i, j] = 1 iff classes_[j] is in y[i], and 0 otherwise.


>>> from sklearn.preprocessing import MultiLabelBinarizer
>>> mlb = MultiLabelBinarizer()
>>> mlb.fit_transform([(1, 2), (3,)])
array([[1, 1, 0],
       [0, 0, 1]])
>>> mlb.classes_
array([1, 2, 3])

>>> mlb.fit_transform([set(['sci-fi', 'thriller']), set(['comedy'])])
array([[0, 1, 1],
       [1, 0, 0]])
>>> list(mlb.classes_)
['comedy', 'sci-fi', 'thriller']

       
       
       
##Scikit - Preprocessing  - Encoding categorical features


#use OneHotEncoder
sklearn.preprocessing.OneHotEncoder(n_values='auto', 
        categorical_features='all', dtype=<type 'numpy.float64'>, sparse=True, handle_unknown='error')
    The input to this transformer should be a matrix of integers, 
    denoting the values taken on by categorical (discrete) features
    A one-hot encoding of y labels should use a LabelBinarizer instead.
    n_values : 'auto', int or array of ints
        Number of values per feature.
            •'auto' : determine value range from training data.
            •int : number of categorical values per feature.
            Each feature value should be in range(n_values)
            •array : n_values[i] is the number of categorical values in
            X[:, i]. Each feature value should be in range(n_values[i])
    categorical_features : 'all' or array of indices or mask
        Specify what features are treated as categorical.
            •'all' (default): All features are treated as categorical.
            •array of indices: Array of categorical feature indices.
            •mask: Array of length n_features and with dtype=bool.
            Non-categorical features are always stacked to the right of the matrix.

#Example 
>>> from sklearn.preprocessing import OneHotEncoder
>>> enc = OneHotEncoder()
>>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  #three samples, each sample - three features(feature=column) given 
OneHotEncoder(categorical_features='all', dtype=<... 'numpy.float64'>,
       handle_unknown='error', n_values='auto', sparse=True)
>>> enc.n_values_
array([2, 3, 4])
>>> enc.feature_indices_
array([0, 2, 5, 9])
>>> enc.transform([[0, 1, 1]]).toarray()  #one sample with three features 
array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])



class sklearn.preprocessing.LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)                
    neg_label : int (default: 0)
        Value with which negative labels must be encoded.
    pos_label : int (default: 1)
        Value with which positive labels must be encoded.

#Example                
>>> from sklearn import preprocessing
>>> lb = preprocessing.LabelBinarizer()
>>> lb.fit([1, 2, 6, 4, 2])  #combination of classes
LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)
>>> lb.classes_
array([1, 2, 4, 6])
>>> lb.transform([1, 6]) #input= classes 
array([[1, 0, 0, 0],
       [0, 0, 0, 1]])
#Binary targets transform to a column vector
>>> lb = preprocessing.LabelBinarizer()
>>> lb.fit_transform(['yes', 'no', 'no', 'yes'])
array([[1],
       [0],
       [0],
       [1]])
#Passing a 2D matrix for multilabel(target is simultaneous many labels) classification
>>> import numpy as np
>>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))  #combination of classes
LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)
>>> lb.classes_
array([0, 1, 2]) 
>>> lb.transform([0, 1, 2, 1]) #input= classes 
array([[1, 0, 0],
       [0, 1, 0],
       [0, 0, 1],
       [0, 1, 0]])

      
class sklearn.preprocessing.LabelEncoder
    Encode labels with value between 0 and n_classes-1.

#Example 
#LabelEncoder can be used to normalize labels.
>>> from sklearn import preprocessing
>>> le = preprocessing.LabelEncoder()
>>> le.fit([1, 2, 2, 6])
LabelEncoder()
>>> le.classes_
array([1, 2, 6])
>>> le.transform([1, 1, 2, 6]) 
array([0, 0, 1, 2]...)
>>> le.inverse_transform([0, 0, 1, 2])
array([1, 1, 2, 6])

#It can also be used to transform non-numerical labels 
#(as long as they are hashable and comparable) to numerical labels.
>>> le = preprocessing.LabelEncoder()
>>> le.fit(["paris", "paris", "tokyo", "amsterdam"])
LabelEncoder()
>>> list(le.classes_)
['amsterdam', 'paris', 'tokyo']
>>> le.transform(["tokyo", "tokyo", "paris"]) 
array([2, 2, 1]...)
>>> list(le.inverse_transform([2, 2, 1]))
['tokyo', 'tokyo', 'paris']

      
##Scikit - Preprocessing  - Imputation(assigning) of missing values 
#Use Imputer() - axis=0 means varying x ie along the row ie a column/feature 
sklearn.preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0, verbose=0, copy=True)
    missing_values : integer or 'NaN', optional (default='NaN')
        The placeholder for the missing values. 
        All occurrences of missing_values will be imputed. For missing values encoded as np.nan, use the string value 'NaN'.
    strategy : string, optional (default='mean')
        The imputation strategy.
            •If 'mean', then replace missing values using the mean along the axis.
            •If 'median', then replace missing values using the median along the axis.
            •If 'most_frequent', then replace missing using the most frequent value along the axis.
    axis : integer, optional (default=0)
        The axis along which to impute.
        •If axis=0, then impute along columns.
        •If axis=1, then impute along rows.

# Example - to replace missing values, encoded as np.nan, using the mean value of the columns (axis 0)
# that contain the missing values:

import numpy as np
from sklearn.preprocessing import Imputer
imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])
Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)

X = [[np.nan, 2], [6, np.nan], [7, 6]]
>>> print(imp.transform(X))      #OR use fit_transform(X[, y])                     
[[ 4.          2.        ]
 [ 6.          3.666...]
 [ 7.          6.        ]]


#supports sparse matrices:
import scipy.sparse as sp
X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])
imp = Imputer(missing_values=0, strategy='mean', axis=0)
>>> imp.fit(X)
Imputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)
X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])
>>> print(imp.transform(X_test))                      
[[ 4.          2.        ]
 [ 6.          3.666...]
 [ 7.          6.        ]]


 
 
##Scikit - Preprocessing  - Generating polynomial features 

sklearn.preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)
    Generate polynomial and interaction features.
    Generate a new feature matrix consisting of all polynomial combinations 
    of the features with degree less than or equal to the specified degree. 
    For example, if an input sample is two dimensional and of the form [a, b], 
    the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].


#Often it's useful to add complexity to the model by considering nonlinear features of the input data. 
#A simple and common method to use is polynomial features, 
#which can get features' high-order and interaction terms

#Note that polynomial features are used implicitily in kernel methods 
#(e.g., sklearn.svm.SVC, sklearn.decomposition.KernelPCA) 
#when using polynomial Kernel functions

#Example 
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
X = np.arange(6).reshape(3, 2)
>>> X                                                 
array([[0, 1],
       [2, 3],
       [4, 5]])
poly = PolynomialFeatures(2)
>>> poly.fit_transform(X)                             
array([[  1.,   0.,   1.,   0.,   0.,   1.],
       [  1.,   2.,   3.,   4.,   6.,   9.],
       [  1.,   4.,   5.,  16.,  20.,  25.]])



#only interaction terms among features are required, 
# Use interaction_only=True:[1, a, b, c, ab, bc, ca, abc ].

X = np.arange(9).reshape(3, 3)
>>> X                                                 
array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
poly = PolynomialFeatures(degree=3, interaction_only=True)
>>> poly.fit_transform(X)                             
array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],
       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],
       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])


       
       
##Scikit - Preprocessing  - Custom transformers - FunctionTransformer
sklearn.preprocessing.FunctionTransformer(func=None, inverse_func=None, validate=True, 
        accept_sparse=False, pass_y=False, kw_args=None, inv_kw_args=None)
        
#For example - to build a transformer 
#that applies a log transformation in a pipeline

import numpy as np
from sklearn.preprocessing import FunctionTransformer
transformer = FunctionTransformer(np.log1p, np.exp)
X = np.array([[0, 1], [2, 3]])
>>> transformer.transform(X)
array([[ 0.        ,  0.69314718],
       [ 1.09861229,  1.38629436]])
>>> transformer.inverse_transform(transformer.transform(X))
array([[1., 2.],
       [3., 4.]])
       
       

       
       

##Scikit - Preprocessing  - Center a kernel matrix       
sklearn.preprocessing.KernelCenterer    
    (i.e., normalize to have zero mean) 
       


##Scikit - nonlinear Preprocessing  - QuantileTransformer
#Like scalers, QuantileTransformer puts each feature into the same range or distribution

sklearn.preprocessing.quantile_transform(X, axis=0, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=False)
class sklearn.preprocessing.QuantileTransformer(n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=True)
    n_quantiles : int, optional (default=1000)
        Number of quantiles to be computed. It corresponds to the number of landmarks used to discretize the cumulative density function.
    output_distribution : str, optional (default='uniform')
        Marginal distribution for the transformed data.
        The choices are 'uniform' (default) or 'normal'.

#Example 
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
quantile_transformer = preprocessing.QuantileTransformer(random_state=0)
X_train_trans = quantile_transformer.fit_transform(X_train)
X_test_trans = quantile_transformer.transform(X_test)
>>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) 
array([ 4.3,  5.1,  5.8,  6.5,  7.9])

#This feature corresponds to the sepal length in cm. 
#Once the quantile transformation applied, 
#those landmarks approach closely the percentiles previously defined:
>>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])
... 
array([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])

#This can be confirmed on a independent testing set with similar remarks:
>>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])
... 
array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])
>>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])
... 
array([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])

#With normal 
>>> quantile_transformer = preprocessing.QuantileTransformer(
...     output_distribution='normal', random_state=0)
>>> X_trans = quantile_transformer.fit_transform(X)
>>> quantile_transformer.quantiles_ 
array([[ 4.3...,   2...,     1...,     0.1...],
       [ 4.31...,  2.02...,  1.01...,  0.1...],
       [ 4.32...,  2.05...,  1.02...,  0.1...],
       ...,
       [ 7.84...,  4.34...,  6.84...,  2.5...],
       [ 7.87...,  4.37...,  6.87...,  2.5...],
       [ 7.9...,   4.4...,   6.9...,   2.5...]])

    
##Scikit - Preprocessing combined with Regression -     TransformedTargetRegressor
#Better use Pipeline 
class sklearn.preprocessing.TransformedTargetRegressor(regressor=None, transformer=None, func=None, inverse_func=None, check_inverse=True)[source]
    Useful for applying a non-linear transformation in regression problems. 
    This transformation can be given as a Transformer such as the QuantileTransformer 
    or as a function and its inverse such as log and exp.
    The computation during fit is
        regressor.fit(X, func(y))
        or
        regressor.fit(X, transformer.transform(y))
    The computation during predict is
        inverse_func(regressor.predict(X))
        or
        transformer.inverse_transform(regressor.predict(X))
    Parameters:
    regressor : object, default=LinearRegression()
        Regressor object such as derived from RegressorMixin. 
        This regressor will automatically be cloned each time prior to fitting.
    transformer : object, default=None
        Estimator object such as derived from TransformerMixin. 
        Cannot be set at the same time as func and inverse_func. 
    func : function, optional
        Function to apply to y before passing to fit. 
    inverse_func : function, optional
        Function to apply to the prediction of the regressor. 
    check_inverse : bool, default=True
        Whether to check that transform followed by inverse_transform 
        or func followed by inverse_func leads to the original targets.
#Methods
fit(X, y[, sample_weight])   Fit the model according to the given training data. 
get_params([deep])           Get parameters for this estimator. 
predict(X)                   Predict using the base regressor, applying inverse. 
score(X, y[, sample_weight]) Returns the coefficient of determination R^2 of the prediction. 
set_params(**params) Set the parameters of this estimator. 

 
#Example 
>>> import numpy as np
>>> from sklearn.linear_model import LinearRegression
>>> from sklearn.preprocessing import TransformedTargetRegressor
>>> tt = TransformedTargetRegressor(regressor=LinearRegression(),
            func=np.log, inverse_func=np.exp)
>>> X = np.arange(4).reshape(-1, 1)
>>> y = np.exp(2 * X).ravel()
>>> tt.fit(X, y) 
TransformedTargetRegressor(...)
>>> tt.score(X, y)
1.0
>>> tt.regressor_.coef_
array([ 2.])

#Examples
import numpy as np
from sklearn.datasets import load_boston
from sklearn.preprocessing import (TransformedTargetRegressor,
                                   QuantileTransformer)
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
boston = load_boston()
X = boston.data
y = boston.target
transformer = QuantileTransformer(output_distribution='normal')
regressor = LinearRegression()
regr = TransformedTargetRegressor(regressor=regressor,transformer=transformer)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
regr.fit(X_train, y_train) 
>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))
R2 score: 0.67
>>> raw_target_regr = LinearRegression().fit(X_train, y_train)
>>> print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test, y_test)))
R2 score: 0.64























###Scikit - STEP3.0 : Using cross validation - predict and score 
#Overfitting - when estimator scores best in training, but failes in test data 
#Use CrossValidation to estimate scores or predicts such that overfitting is minimized 


##Generally, train_test_split is used to split data set 
#then Test data is used to check model performance 

import numpy as np
from sklearn import cross_validation
from sklearn import datasets
from sklearn import svm

iris = datasets.load_iris()
>>> iris.data.shape, iris.target.shape
((150, 4), (150,))
#test_size : float, int, or None (default is None)
#train_size : float, int, or None (default is None)
#float means % 
X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)

>>> X_train.shape, y_train.shape
((90, 4), (90,))
>>> X_test.shape, y_test.shape
((60, 4), (60,))

>>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
>>> clf.score(X_test, y_test)         #get test data score of fittment                   
0.96...

#or with preprocessing 
from sklearn import preprocessing
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)
scaler = preprocessing.StandardScaler().fit(X_train)
X_train_transformed = scaler.transform(X_train)
clf = svm.SVC(C=1).fit(X_train_transformed, y_train)
X_test_transformed = scaler.transform(X_test)
>>> clf.score(X_test_transformed, y_test)  
0.9333...


##However, train_test_split holds some data for Testing and that can not be used for training 
#This can be avoided by using Crossvalidation 

#For example, CrossValidation with k-fold strategy 
    the training set is split into k smaller sets 
    The following procedure is followed for each of the k 'folds':
        •A model is trained using k-1 of the folds as training data;
        •the resulting model is validated on the remaining part of the data 


##CV model strategy 
sklearn.model_selection.StratifiedKFold(n_splits=3, shuffle=False, random_state=None)
sklearn.model_selection.KFold(n_splits=3, shuffle=False, random_state=None)

#http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators
model_selection.KFold([n_splits, shuffle, ...])         K-Folds cross-validator 
model_selection.GroupKFold([n_splits])                  K-fold iterator variant with non-overlapping groups. 
model_selection.StratifiedKFold([n_splits, ...])        Stratified K-Folds cross-validator 
model_selection.LeaveOneGroupOut()                      Leave One Group Out cross-validator 
model_selection.LeavePGroupsOut(n_groups)               Leave P Group(s) Out cross-validator 
model_selection.LeaveOneOut()                           Leave-One-Out cross-validator 
model_selection.LeavePOut(p)                            Leave-P-Out cross-validator 
model_selection.ShuffleSplit([n_splits, ...])           Random permutation cross-validator 
model_selection.GroupShuffleSplit([...])                Shuffle-Group(s)-Out cross-validation iterator 
model_selection.StratifiedShuffleSplit([...])           Stratified ShuffleSplit cross-validator 
model_selection.PredefinedSplit(test_fold)              Predefined split cross-validator 
model_selection.TimeSeriesSplit([n_splits])             Time Series cross-validator 


##Converting loss to scorer function 
sklearn.metrics.make_scorer(score_func, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs)[source]
    Make a scorer from a performance metric or loss function.
    This factory function wraps scoring functions for use in GridSearchCV and cross_val_score. 
    It takes a score function, 
    such as accuracy_score, mean_squared_error, adjusted_rand_index 
    or average_precision 
    and returns a callable that scores an estimator's output.


##Methods to get CV results 
sklearn.model_selection.cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', method='predict')[source]
    Generate cross-validated estimates(predicted) for each input data point
    Returns:
        predictions : ndarray
sklearn.model_selection.cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')[source]
    Evaluate a score by cross-validation
    estimator : estimator object implementing 'fit'
        The object to use to fit the data.
    X : array-like
        The data to fit. Can be for example a list, or an array.
    y : array-like, optional, default: None
        The target variable to try to predict in the case of supervised learning.
    groups : array-like, with shape (n_samples,), optional
        Group labels for the samples used while splitting the dataset into train/test set.
    scoring : string, callable or None, optional, default: None
        A string from model evaluation  
        or a scorer callable object / function 
        with signature scorer(estimator, X, y).
        Check loss function from below and convert to scorer by make_scorer
        >>> dir(sklearn.metrics)
        Note sklearn.metrics have many direct scorer functions with suffix _scorer
        That can be directly used 
    cv : int, cross-validation generator or an iterable, optional
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are:
        •None, to use the default 3-fold cross validation,
        •integer, to specify the number of folds in a (Stratified)KFold,
        •An object to be used as a cross-validation generator.
        Check 
        >>> dir(sklearn.model_selection )
        •An iterable yielding train, test splits.
        For integer/None inputs, if the estimator is a classifier 
        and y is either binary or multiclass, StratifiedKFold is used. 
        In all other cases, KFold is used.
    n_jobs : integer, optional
        The number of CPUs to use to do the computation. -1 means 'all CPUs'.
    verbose : integer, optional
        The verbosity level.
    fit_params : dict, optional
        Parameters to pass to the fit method of the estimator.
    Returns:
    scores : array of float, shape=(len(list(cv)),)
        Array of scores of the estimator for each run of the cross validation.
     
#Examples
from sklearn import datasets, linear_model
from sklearn.model_selection import cross_val_predict,cross_val_score
from sklearn import datasets
diabetes = datasets.load_diabetes()
X = diabetes.data[:150]
y = diabetes.target[:150]
lasso = linear_model.Lasso()
y_pred = cross_val_predict(lasso, X, y)

#Or 
>>> print(cross_val_score(lasso, X, y))   #KFold uses 3 sets 
[ 0.33150734  0.08022311  0.03531764]



#Example for iris data 
#for example - to estimate the accuracy of SVC by splitting the data, 
#fitting a model and computing the score 5 consecutive times 
from sklearn import datasets
iris = datasets.load.iris()

from sklearn.model_selection import cross_val_score
clf = svm.SVC(kernel='linear', C=1)

from sklearn.model_selection import cross_val_predict
predicted = cross_val_predict(clf, iris.data, iris.target, cv=10)
>>> metrics.accuracy_score(iris.target, predicted) 
0.973...
#OR 
scores = cross_val_score(clf, iris.data, iris.target, cv=5)
>>> scores                                              
array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])

#mean score and the 95% CI the score estimate are hence given by
>>> print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
Accuracy: 0.98 (+/- 0.03)

#With new score function 
from sklearn import metrics
scores = cross_val_score(clf, iris.data, iris.target, cv=5, scoring='f1_macro')
>>> scores                                              
array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])

#with new CV strategy 
from sklearn.model_selection import ShuffleSplit
n_samples = iris.data.shape[0]
cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)
>>> cross_val_score(clf, iris.data, iris.target, cv=cv)                                              
array([ 0.97...,  0.97...,  1.        ])

#Can be used with pipelien 
from sklearn.pipeline import make_pipeline
clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))
>>> cross_val_score(clf, iris.data, iris.target, cv=cv)                                      
array([ 0.97...,  0.93...,  0.95...])




sklearn.model_selection.cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score='warn')[source]
    Evaluate metric or metrics by cross-validation and also record fit/score times.
    scoring : string, callable, list/tuple, dict or None, default: None
        A single string  or a callable 
        For evaluating multiple metrics, either give a list of (unique) strings 
        or a dict with names as keys and callables as values.
        Check loss function from below and convert to scorer by make_scorer
        >>> dir(sklearn.metrics)
        Note sklearn.metrics have many direct scorer functions with suffix _scorer
        That can be directly used 
        If None, the estimator's default scorer (if available) is used.
    Returns:
    scores : dict of float arrays of shape=(n_splits,)
        Array of scores of the estimator for each run of the cross validation.
        A dict of arrays containing the score/time arrays for each scorer is returned. 
        The possible keys for this dict are:
            test_score
                The score array for test scores on each cv split.
            train_score
                The score array for train scores on each cv split. This is available only if return_train_score parameter is True.
            fit_time
                The time for fitting the estimator on the train set for each cv split.
            score_time
                The time for scoring the estimator on the test set for each cv split. (Note time for scoring on the train set is not included even if return_train_score is set to True
        For single metric evaluation, 
        where the scoring parameter is a string, callable or None, 
        the keys will be - ['test_score', 'fit_time', 'score_time']
        And for multiple metric evaluation, the return value is a dict 
        with the following keys - 
        ['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']

       
#Example 
from sklearn import datasets, linear_model
from sklearn.model_selection import cross_validate
from sklearn.metrics.scorer import make_scorer
from sklearn.metrics import confusion_matrix
from sklearn.svm import LinearSVC
diabetes = datasets.load_diabetes()
X = diabetes.data[:150]
y = diabetes.target[:150]
lasso = linear_model.Lasso()

#Single metric evaluation using cross_validate
cv_results = cross_validate(lasso, X, y, return_train_score=False)
>>> sorted(cv_results.keys())                         
['fit_time', 'score_time', 'test_score']
>>> cv_results['test_score']    
array([ 0.33...,  0.08...,  0.03...])

#Multiple metric evaluation using cross_validate 
scores = cross_validate(lasso, X, y,
        scoring=('r2', 'neg_mean_squared_error'))
>>> print(scores['test_neg_mean_squared_error'])      
[-3635.5... -3573.3... -6114.7...]
>>> print(scores['train_r2'])                         
[ 0.28...  0.39...  0.22...]        
        

#The multiple metrics can be specified either as a list, 
#tuple or set of predefined scorer names:
from sklearn.model_selection import cross_validate
from sklearn.metrics import recall_score
scoring = ['precision_macro', 'recall_macro']
clf = svm.SVC(kernel='linear', C=1, random_state=0)
scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,
                        cv=5, return_train_score=False)
>>> sorted(scores.keys())
['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']
>>> scores['test_recall_macro']                       
array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])

#Or as a dict mapping scorer name to a predefined or custom scoring function:
from sklearn.metrics.scorer import make_scorer
scoring = {'prec_macro': 'precision_macro',
           'rec_micro': make_scorer(recall_score, average='macro')}
scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,
                        cv=5, return_train_score=True)
>>> sorted(scores.keys())                 
['fit_time', 'score_time', 'test_prec_macro', 'test_rec_micro',
 'train_prec_macro', 'train_rec_micro']
>>> scores['train_rec_micro']                         
array([ 0.97...,  0.97...,  0.99...,  0.98...,  0.98...])


## Cross validation of time series data
class sklearn.model_selection.TimeSeriesSplit(n_splits=3, max_train_size=None)

#Example of 3-split time series cross-validation on a dataset with 6 samples:
from sklearn.model_selection import TimeSeriesSplit

X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4, 5, 6])
tscv = TimeSeriesSplit(n_splits=3)
>>> print(tscv)  
TimeSeriesSplit(max_train_size=None, n_splits=3)
>>> for train, test in tscv.split(X):
...     print("%s %s" % (train, test))
[0 1 2] [3]
[0 1 2 3] [4]
[0 1 2 3 4] [5]







###Scikit - STEP3.0 : Using cross validation - Tuning parameters - Grid Search  and RandomizedSearchCV

#get parameters(hyperparameters) by searching a parameter space for the best Cross-validation score
#Example - C, kernel and gamma for Support Vector Classifier, 
#alpha for Lasso, etc.


#A search consists of:
•an estimator (regressor or classifier such as sklearn.svm.SVC());
•a parameter space;
•a method for searching or sampling candidates;
•a cross-validation scheme; and
•a score function.

class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score='raise', return_train_score='warn')[source]
    Exhaustive search over specified parameter values for an estimator.
class sklearn.model_selection.RandomizedSearchCV(estimator, param_distributions, n_iter=10, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score='raise', return_train_score='warn')[source]
    Randomized search on hyper parameters.
    In contrast to GridSearchCV, not all parameter values are tried out, 
    but rather a fixed number of parameter settings is sampled 
    from the specified distributions. 
    The number of parameter settings that are tried is given by n_iter
    estimator : estimator object.
        A object of that type is instantiated for each grid point
    param_distributions : dict
        Dictionary with parameters names (string) as keys and distributions 
        or lists of parameters to try. 
        Distributions must provide a rvs method for sampling 
        (such as those from scipy.stats.distributions). 
        If a list is given, it is sampled uniformly.
    n_iter : int, default=10
        Number of parameter settings that are sampled. 
        n_iter trades off runtime vs quality of the solution.
    scoring : string, callable, list/tuple, dict or None, default: None
        A single string or a callable to evaluate the predictions on the test set.
        For evaluating multiple metrics, either give a list of (unique) strings 
        or a dict with names as keys and callables as values.
    n_jobs : int, default=1
        Number of jobs to run in parallel.
    iid : boolean, default=True
        If True, the data is assumed to be identically distributed across the folds, 
        and the loss minimized is the total loss per sample, 
        and not the mean loss across the folds.
    cv : int, cross-validation generator or an iterable, optional
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are:
            •None, to use the default 3-fold cross validation,
            •integer, to specify the number of folds in a (Stratified)KFold,
            •An object to be used as a cross-validation generator
             check from 
             >>> dir(sklearn.model_selection )
            •An iterable yielding train, test splits.
        For integer/None inputs, if the estimator is a classifier 
        and y is either binary or multiclass, StratifiedKFold is used. 
        In all other cases, KFold is used.
    refit : boolean, or string, default=True
        Refit an estimator using the best found parameters on the whole dataset.
        For multiple metric evaluation, this needs to be a string 
        denoting the scorer is used to find the best parameters 
        for refitting the estimator at the end.
        The refitted estimator is made available at the best_estimator_ attribute 
        and permits using predict directly on this GridSearchCV instance.
    return_train_score : boolean, optional
        If False, the cv_results_ attribute will not include training scores.
#Attributes:
    cv_results_ : dict of numpy (masked) ndarrays
        A dict with keys as column headers and values as columns, 
        that can be imported into a pandas DataFrame.
        The key 'params' is used to store a list of parameter settings dicts 
        for all the parameter candidates.
        The mean_fit_time, std_fit_time, mean_score_time and std_score_time are all in seconds.
        For multi-metric evaluation, the scores for all the scorers are available 
        in the cv_results_ dict at the keys ending with 
        that scorer's name ('_<scorer_name>') instead of '_score'       
    best_estimator_ : estimator or dict
        Estimator that was chosen by the search, 
        i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False.
    best_score_ : float
        Mean cross-validated score of the best_estimator
    best_params_ : dict
        Parameter setting that gave the best results on the hold out data.
    best_index_ : int
        The index (of the cv_results_ arrays) which corresponds to the best candidate parameter setting.
    scorer_ : function or a dict
        Scorer function used on the held out data to choose the best parameters 
        for the model.
    n_splits_ : int
        The number of cross-validation splits (folds/iterations).
#Methods
    decision_function(X)    Call decision_function on the estimator with the best found parameters. 
    fit(X[, y, groups])     Run fit with all sets of parameters. 
    inverse_transform(Xt)   Call inverse_transform on the estimator with the best found params. 
    predict(X)              Call predict on the estimator with the best found parameters. 
    predict_log_proba(X)    Call predict_log_proba on the estimator with the best found parameters. 
    predict_proba(X)        Call predict_proba on the estimator with the best found parameters. 
    score(X[, y])           Returns the score on the given data, if the estimator has been refit. 
    set_params(**params)    Set the parameters of this estimator. 
    get_params([deep])      Get parameters for this estimator.
    transform(X)            Call transform on the estimator with the best found parameters. 

#GridSearchCV/RandomizedSearchCV -  Parallelism
#use the keyword n_jobs=-1.

#GridSearchCV/RandomizedSearchCV -  Robustness to failure 
#Some parameter settings may result in a failure to fit one or more folds 
#of the data, use error_score=0 (or =np.NaN)  


#GridSearchCV/RandomizedSearchCV - Specifying an objective metric
#By default, it uses sklearn.metrics.accuracy_score for classification 
#and sklearn.metrics.r2_score for regression
#note scoring=can be list of scorer method strings, 
#in this case, mention refit=Best_scoring_method_as_string

#Other scoring functions from sklearn.metrics 
#Check loss function from below and convert to scorer by make_scorer
>>> dir(sklearn.metrics)
#Note sklearn.metrics have many direct scorer functions with suffix _scorer
#That can be directly used 
        
#Scoring                Function                        Comment
#Classification     
'accuracy'              metrics.accuracy_score   
'average_precision'     metrics.average_precision_score   
'f1'                    metrics.f1_score                for binary targets 
'f1_micro'              metrics.f1_score                micro-averaged 
'f1_macro'              metrics.f1_score                macro-averaged 
'f1_weighted'           metrics.f1_score                weighted average 
'f1_samples'            metrics.f1_score                by multilabel sample 
'neg_log_loss'          metrics.log_loss r              equires predict_proba support 
'precision' etc.        metrics.precision_score         suffixes apply as with 'f1' 
'recall' etc.           metrics.recall_score            suffixes apply as with 'f1' 
'roc_auc'               metrics.roc_auc_score   
#Clustering     
'adjusted_rand_score'   metrics.adjusted_rand_score   
#Regression     
'neg_mean_absolute_error'   metrics.mean_absolute_error   
'neg_mean_squared_error'    metrics.mean_squared_error   
'neg_median_absolute_error' metrics.median_absolute_error   
'r2'                        metrics.r2_score 


    
    
    
#Examples
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
iris = datasets.load_iris()
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svc = svm.SVC()
grid_search = GridSearchCV(svc, parameters)
grid_search.fit(iris.data, iris.target)                  
#Estimator that was chosen by the search, 
#i.e. estimator which gave highest score (or smallest loss if specified)on the left out data
best_pipeline = grid_search.best_estimator_
best_pipeline.score(iris.data, iris.target) #0.9696969696969697
#best_params_ : Parameter setting that gave the best results on the hold out data.
>>> grid_search.best_params_
{'reduce_dim__n_components': 5, 'clf__C': 0.1}
#best_index_ : int,The index (of the cv_results_ arrays) which corresponds to the best candidate parameter setting.
grid_search.best_index_
#best_score_ : float,Mean cross-validated score of the best_estimator
grid_search.best_score_
#cv_results_: Full result grid of CV , time is in sec 
pd.DataFrame(grid_search.cv_results_)

>> sorted(clf.get_params().keys())
'cv', 'error_score', 'estimator', 'estimator__C', 'estimator__cache_size', 
'estmator__class_weight', 'estimator__coef0', 'estimator__decision_function_shape',
'estimator__degree', 'estimator__gamma', 'estimator__kernel', 
'estimator__max_ier', 'estimator__probability', 'estimator__random_state', 
'estimator__shrinking', 'estimator__tol', 'estimator__verbose', 
'fit_params', 'iid', 'n_jobs', 'paramgrid', 'pre_dispatch', 
'refit', 'return_train_score', 'scoring', 'verbose']

#Note estimator's params come from class definition , prefixed with estimator__
#in param_grid, metntion param directly without any prefix 
sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', 
        coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, 
        class_weight=None, verbose=False, max_iter=-1, 
        decision_function_shape=None, random_state=None)

#Multiple params grid 
param_grid = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
 ]
grid_search = GridSearchCV(svc, param_grid)
grid_search.fit(iris.data, iris.target)  
best_pipeline = grid_search.best_estimator_
best_pipeline.score(iris.data, iris.target) #0.98
>>> grid_search.best_params_
{'kernel': 'rbf', 'gamma': 0.001, 'C': 1000}

#With randomized , mention the distribution agains params
import scipy.stats 
param_distributions = {'C': scipy.stats.expon(scale=100), \
                        'gamma': scipy.stats.expon(scale=.1),\
                        'kernel': ['rbf'], \
                        'class_weight':['balanced', None]}

grid_search = RandomizedSearchCV(svc, param_distributions, n_iter=20)
grid_search.fit(iris.data, iris.target)  
best_pipeline = grid_search.best_estimator_
best_pipeline.score(iris.data, iris.target) #.9933333333333333
>>> grid_search.best_params_
{'kernel': 'rbf', 'gamma': 0.12833688422565423, 'C': 4.699896951847006, 'class_weight': 'balanced'}


##Another examples with RandomForestClassifier 
#the parameters are 
sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini', 
    max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, 
    max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, 
    bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, 
    warm_start=False, class_weight=None)

#Example 
import numpy as np
from time import time
from operator import itemgetter
from scipy.stats import randint as sp_randint

from sklearn.grid_search import GridSearchCV, RandomizedSearchCV
from sklearn.datasets import load_digits 
from sklearn.ensemble import RandomForestClassifier

# get some data
digits = load_digits()  # Load and return the digits dataset (classification)
X, y = digits.data, digits.target

# build a classifier
clf = RandomForestClassifier(n_estimators=20)


# Utility function to handle cv_results_
def report(results, n_top=3):
    for i in range(1, n_top + 1):
        candidates = np.flatnonzero(results['rank_test_score'] == i)
        for candidate in candidates:
            print("Model with rank: {0}".format(i))
            print("Mean validation score: {0:.3f} (std: {1:.3f})".format(
                  results['mean_test_score'][candidate],
                  results['std_test_score'][candidate]))
            print("Parameters: {0}".format(results['params'][candidate]))
            print("")


# specify parameters and distributions from RandomForestClassifier
param_dist = {"max_depth": [3, None],
              "max_features": sp_randint(1, 11),
              "min_samples_split": sp_randint(1, 11),
              "min_samples_leaf": sp_randint(1, 11),
              "bootstrap": [True, False],
              "criterion": ["gini", "entropy"]}

# run randomized search
n_iter_search = 20
random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
                                   n_iter=n_iter_search)

start = time()
random_search.fit(X, y)
print("RandomizedSearchCV took %.2f seconds for %d candidates"
      " parameter settings." % ((time() - start), n_iter_search))
report(random_search.cv_results_)

# use a full grid over all parameters
param_grid = {"max_depth": [3, None],
              "max_features": [1, 3, 10],
              "min_samples_split": [1, 3, 10],
              "min_samples_leaf": [1, 3, 10],
              "bootstrap": [True, False],
              "criterion": ["gini", "entropy"]}

# run grid search
grid_search = GridSearchCV(clf, param_grid=param_grid)
start = time()
grid_search.fit(X, y)

print("GridSearchCV took %.2f seconds for %d candidate parameter settings."
      % (time() - start, len(grid_search.cv_results_['params'])))
report(grid_search.cv_results_)




###Scikit - STEP3.0 : Using cross validation - Alternate to GridSearch - model specific, ensemble


##Option-1: Model specific cross-validation
#Some models can fit data for a range of values of some parameter almost as efficiently 
#as fitting the estimator for a single value of the parameter

#This feature can be leveraged to perform a more efficient cross-validation 
#used for model selection of this parameter.

#The most common parameter is strength of the regularizer

linear_model.ElasticNetCV([l1_ratio, eps, ...])         Elastic Net model with iterative fitting along a regularization path 
linear_model.LarsCV([fit_intercept, ...])               Cross-validated Least Angle Regression model 
linear_model.LassoCV([eps, n_alphas, ...])              Lasso linear model with iterative fitting along a regularization path 
linear_model.LassoLarsCV([fit_intercept, ...])          Cross-validated Lasso, using the LARS algorithm 
linear_model.LogisticRegressionCV([Cs, ...])            Logistic Regression CV (aka logit, MaxEnt) classifier. 
linear_model.MultiTaskElasticNetCV([...])               Multi-task L1/L2 ElasticNet with built-in cross-validation. 
linear_model.MultiTaskLassoCV([eps, ...])               Multi-task L1/L2 Lasso with built-in cross-validation. 
linear_model.OrthogonalMatchingPursuitCV([...])         Cross-validated Orthogonal Matching Pursuit model (OMP) 
linear_model.RidgeCV([alphas, ...])                     Ridge regression with built-in cross-validation. 
linear_model.RidgeClassifierCV([alphas, ...])           Ridge classifier with built-in cross-validation. 

##Option-2: Selecting based on Information Criterion eg aic, bic 
#optimal estimate of the regularization parameter 
#by Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC)
linear_model.LassoLarsIC([criterion, ...])              Lasso model fit with Lars using BIC or AIC for model selection 


## Usage of above models 

#All these model take below cv parameter 
cv : int, cross-validation generator or an iterable, optional
    Determines the cross-validation splitting strategy. Possible inputs for cv are:
    •None, to use the default 3-fold cross-validation,
    •integer, to specify the number of folds.
    •An object to be used as a cross-validation generator.
     check 
     >>> dir(sklearn.model_selection )
    •An iterable yielding train/test splits.
    For integer/None inputs, KFold is used.


#model.fit(): These models autmatically use CV to get the best parameter
#by iterative fitting along a regularization path


#Methods - Example - from ElasticNetCV
fit(X, y)                               Fit linear model with coordinate descent 
get_params([deep])                      Get parameters for this estimator. 
path(X, y[, l1_ratio, eps, n_alphas, …]) Compute  path with coordinate descent 
predict(X)                              Predict using the linear model 
score(X, y[, sample_weight])            Returns the coefficient of determination R^2 of the prediction. 
set_params(**params)                    Set the parameters of this estimator. 

#Attributes - Example - from LarsCV
coef_           : array, shape (n_features,) parameter vector 
intercept_      : float, independent term in decision function
coef_path_      : array, shape (n_features, n_alphas),the varying values of the coefficients along the path
alpha_          : float, the estimated regularization parameter alpha
alphas_         : array, shape (n_alphas,),the different values of alpha along the path
cv_alphas_      : array, shape (n_cv_alphas,), all the values of alpha along the path for the different folds
cv_mse_path_    : array, shape (n_folds, n_cv_alphas),the mean square error on left-out for each fold along the path (alpha values given by cv_alphas)
n_iter_         : array-like or int,the number of iterations run by Lars with the optimal alpha.

##for example -
#To plot , http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC
from sklearn import datasets

diabetes = datasets.load_diabetes()
X = diabetes.data
y = diabetes.target

rng = np.random.RandomState(42)
X = np.c_[X, rng.randn(X.shape[0], 14)]  # add some bad features, c_- column wise append

# normalize data as done by Lars to allow for comparison
X /= np.sqrt(np.sum(X ** 2, axis=0))

#LassoLarsIC: least angle regression with BIC/AIC criterion
model_bic = LassoLarsIC(criterion='bic')
model_bic.fit(X, y)
alpha_bic_ = model_bic.alpha_

model_aic = LassoLarsIC(criterion='aic')
model_aic.fit(X, y)
alpha_aic_ = model_aic.alpha_
#OR 
#LassoCV: coordinate descent
model = LassoCV(cv=20).fit(X, y)
>>> print(model.alpha_) 
0.19947279427
>>> print(model.intercept_) 
0.398882965428
>>> print(model.predict([[0, 0]])) 
[ 0.39888297]

#LassoLarsCV: least angle regression
model = LassoLarsCV(cv=20).fit(X, y)



##To compute various  path 
sklearn.linear_model.lars_path(X, y, Xy=None, Gram=None, 
            max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.2204460492503131e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)
    Compute Least Angle Regression or Lasso path using LARS algorithm 
    Returns:
        alphas : array, shape: [n_alphas + 1]
            Maximum of covariances (in absolute value) at each iteration. n_alphas is either max_iter, n_features or the number of nodes in the path with alpha >= alpha_min, whichever is smaller.
        active : array, shape [n_alphas]
            Indices of active variables at the end of the path.
        coefs : array, shape (n_features, n_alphas + 1)
            Coefficients along the path
        n_iter : int
            Number of iterations run. Returned only if return_n_iter is set to True.
sklearn.linear_model.lasso_path(X, y, eps=0.001, n_alphas=100, alphas=None, 
            precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)[source]
    Compute Lasso path with coordinate descent
    Returns:
        alphas : array, shape (n_alphas,)
            The alphas along the path where models are computed.
        coefs : array, shape (n_features, n_alphas) or (n_outputs, n_features, n_alphas)
            Coefficients along the path.
        dual_gaps : array, shape (n_alphas,)
            The dual gaps at the end of the optimization for each alpha.
        n_iters : array-like, shape (n_alphas,)
            The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha.
sklearn.linear_model.logistic_regression_path(X, y, pos_class=None, 
            Cs=10, fit_intercept=True, max_iter=100, tol=0.0001, verbose=0, solver='lbfgs', coef=None, class_weight=None, dual=False, penalty='l2', intercept_scaling=1.0, multi_class='ovr', random_state=None, check_input=True, max_squared_sum=None, sample_weight=None)
    Compute a Logistic Regression model for a list of regularization parameters.
    Returns:
        coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)
            List of coefficients for the Logistic Regression model. If fit_intercept is set to True then the second dimension will be n_features + 1, where the last item represents the intercept.
        Cs : ndarray
            Grid of Cs used for cross-validation.
        n_iter : array, shape (n_cs,)
            Actual number of iteration for each Cs.
#Example for lars_path
import numpy as np
import matplotlib.pyplot as plt

from sklearn import linear_model
from sklearn import datasets

diabetes = datasets.load_diabetes()
X = diabetes.data
y = diabetes.target

print("Computing regularization path using the LARS ...")
alphas, _, coefs = linear_model.lars_path(X, y, method='lasso', verbose=True)  #method : {'lar', 'lasso'}, 

xx = np.sum(np.abs(coefs.T), axis=1)
xx /= xx[-1]

plt.plot(xx, coefs.T)  #coefs : shape (n_features, n_alphas + 1),Coefficients along the path
ymin, ymax = plt.ylim()
plt.vlines(xx, ymin, ymax, linestyle='dashed')
plt.xlabel('|coef| / max|coef|')
plt.ylabel('Coefficients')
plt.title('LASSO Path')
plt.axis('tight')
plt.show()




##Option-3: Selecting Model when using - Out of Bag Estimates - ensemble 

#When using ensemble methods based upon bagging, 
#i.e. generating new training sets using sampling with replacement, 
#part of the training set remains unused. 
#For each classifier in the ensemble, 
#a different part of the training set is left out.

#This left out portion can be used to estimate the generalization error 
#without having to rely on a separate validation set

#This can be used for model selection

##Type-1:bagging: averaging methods, 
#the driving principle is to build several estimators independently 
#and then to average their predictions. 
#On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.

#Bagging meta-estimator
class sklearn.ensemble.BaggingRegressor(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1, random_state=None, verbose=0)
class sklearn.ensemble.BaggingRegressor(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1, random_state=None, verbose=0)
    Takes one base-estimator and then uses Averaging method to get the best predictions 
    max_samples and max_features control the size of the subsets 
#Random forest 
ensemble.RandomForestClassifier([...])              A random forest classifier. 
ensemble.RandomForestRegressor([...])               A random forest regressor. 
#Extremely Randomized Trees- advanced than RandomForest
ensemble.ExtraTreesClassifier([...])                An extra-trees classifier. 
ensemble.ExtraTreesRegressor([n_estimators, ...])   An extra-trees regressor. 

#The main parameters to adjust 
n_estimators: number of trees in the forest. 
    The larger the better, but also the longer it will take to compute
max_features: size of the random subsets of features to consider when splitting a node. 
    The lower the greater the reduction of variance, but also the greater the increase in bias. 
#Take max_features=n_features for regression problems, 
#and max_features=sqrt(n_features) for classification tasks 

#Converting forests of trees to multi-output problems 
#if Y is an array of size [n_samples, n_outputs]).

#Example of Bagging meta-estimator

>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> bagging = BaggingClassifier(KNeighborsClassifier(),
                             max_samples=0.5, max_features=0.5)



#Example of RandomForestClassifier
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.datasets import make_classification
>>>
>>> X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = RandomForestClassifier(max_depth=2, random_state=0)
>>> clf.fit(X, y)
#Often features do not contribute equally to predict the target response
#the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. 
#The feature importances (the higher, the more important the feature).
>>> print(clf.feature_importances_)
[ 0.17287856  0.80608704  0.01884792  0.00218648]
>>> print(clf.predict([[0, 0, 0, 0]]))
[1]
#Example of RandomForestRegressor 
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.datasets import make_regression
>>>
>>> X, y = make_regression(n_features=4, n_informative=2,
...                        random_state=0, shuffle=False)
>>> regr = RandomForestRegressor(max_depth=2, random_state=0)
>>> regr.fit(X, y)
#Often features do not contribute equally to predict the target response
#the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. 
#The feature importances (the higher, the more important the feature).
>>> print(regr.feature_importances_)
[ 0.17339552  0.81594114  0.          0.01066333]
>>> print(regr.predict([[0, 0, 0, 0]]))
[-2.50699856]

#Example of Extremely Randomized Trees-ExtraTreesClassifier
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.datasets import make_blobs
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.ensemble import ExtraTreesClassifier
>>> from sklearn.tree import DecisionTreeClassifier

>>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100,
...     random_state=0)

>>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
...     random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean()                             
0.97...

>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean()                             
0.999...

>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean() > 0.999
True



##Type- 2:bagging: boosting methods, 
#base estimators are built sequentially 
#and one tries to reduce the bias of the combined estimator. 
#The motivation is to combine several weak models to produce a powerful ensemble.
ensemble.GradientBoostingClassifier([loss, ...])    Gradient Boosting for classification. 
ensemble.GradientBoostingRegressor([loss, ...])     Gradient Boosting for regression 
ensemble.AdaBoostRegressor([ ...])                  AdaBoost for regression. 
ensemble.AdaBoostClassifier([ ...])                 AdaBoost for classification 

#For largeDataset
#to use RandomForestClassifier as an alternative to GradientBoostingClassifier .
#As GradientBoostingClassifier memory requirements would be huge 

#Methods - example from RandomForestClassifier
apply(X)                    Apply trees in the forest to X, return leaf indices. 
decision_path(X)            Return the decision path in the forest 
fit(X, y[, sample_weight])  Build a forest of trees from the training set (X, y). 
get_params([deep])          Get parameters for this estimator. 
predict(X)                  Predict class for X. 
predict_log_proba(X)        Predict class log-probabilities for X. 
predict_proba(X)            Predict class probabilities for X. 
score(X, y[, sample_weight]) Returns the mean accuracy on the given test data and labels. 
set_params(**params) Set the parameters of this estimator. 

#Imp attributes 
estimators_ : list of DecisionTreeClassifier
    The collection of fitted sub-estimators.
classes_ : array of shape = [n_classes] or a list of such arrays(Only for Classifiers)
    The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).
n_classes_ : int or list (Only for Classifiers)
    The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).
n_features_ : int
    The number of features when fit is performed.
n_outputs_ : int
    The number of outputs when fit is performed.
feature_importances_ : array of shape = [n_features]
    The feature importances (the higher, the more important the feature).
oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.


#Example of AdaBoostClassifier
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import AdaBoostClassifier

>>> iris = load_iris()
>>> clf = AdaBoostClassifier(n_estimators=100)
>>> scores = cross_val_score(clf, iris.data, iris.target)
>>> scores.mean()                             
0.9...

#Example of GradientBoostingRegressor
>>> import numpy as np
>>> from sklearn.metrics import mean_squared_error
>>> from sklearn.datasets import make_friedman1
>>> from sklearn.ensemble import GradientBoostingRegressor

>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)
>>> X_train, X_test = X[:200], X[200:]
>>> y_train, y_test = y[:200], y[200:]
>>> est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,
...     max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)
>>> mean_squared_error(y_test, est.predict(X_test))    
5.00...
# Fitting additional weak-learners - for GradientBoosting
#to add more estimators to an already fitted model.
>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees
>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est
>>> mean_squared_error(y_test, est.predict(X_test))    
3.84...


#Example of GradientBoostingClassifier
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier

>>> X, y = make_hastie_10_2(random_state=0)
>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X, y)

#Often features do not contribute equally to predict the target response
#the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. 
#The feature importances (the higher, the more important the feature).
>>> clf.feature_importances_  
array([ 0.11,  0.1 ,  0.11,  ...
#Partial dependence plots (PDP) show the dependence 
#between the target response and a set of 'target' features, 
#marginalizing over the values of all other features 

#One-way PDPs tell us about the interaction between the target response and the target feature 
#PDPs with two target features show the interactions among the two features
#Example - two one-way PDPs for the features 0 and 1 
#and a two-way PDP between the two features
>>> from sklearn.ensemble.partial_dependence import plot_partial_dependence
features = [0, 1, (0, 1)]
>>> fig, axs = plot_partial_dependence(clf, X, features) 

#For multi-class models, you need to set the class label 
#for which the PDPs should be created via the label argument:
>>> from sklearn.datasets import load_iris
>>> iris = load_iris()
>>> mc_clf = GradientBoostingClassifier(n_estimators=10,
...     max_depth=1).fit(iris.data, iris.target)
>>> features = [3, 2, (3, 2)]
>>> fig, axs = plot_partial_dependence(mc_clf, X, features, label=0) 

#If you need the raw values of the partial dependence function 
>>> from sklearn.ensemble.partial_dependence import partial_dependence
>>> pdp, axes = partial_dependence(clf, [0], X=X)
>>> pdp  
array([[ 2.46643157,  2.46643157, ...
>>> axes  
[array([-1.62497054, -1.59201391, ...


##Type-3: Voting Classifier 
#combine conceptually different machine learning classifiers and use a majority vote 
#or the average predicted probabilities (soft vote) to predict the class labels
sklearn.ensemble.VotingClassifier(estimators, voting='hard', weights=None, n_jobs=1, flatten_transform=None)
    Two subtypes are selected by 
    voting : str, {'hard', 'soft'} (default='hard')
        Majority Class Labels (Majority/Hard Voting)
            the predicted class label for a particular sample is the class label that represents the majority (mode) 
            of the class labels predicted by each individual classifier
        Weighted Average Probabilities (Soft Voting)
            soft voting returns the class label 
            as argmax of the sum of predicted probabilities.
#hard voting 
#E.g., if the prediction for a given sample is
•classifier 1 -> class 1
•classifier 2 -> class 1
•classifier 3 -> class 2

#the VotingClassifier (with voting='hard') would classify the sample as 'class 1' 
#based on the majority class label.

#In the cases of a tie, the VotingClassifier will select the class based 
#on the ascending sort order. E.g., in the following scenario
•classifier 1 -> class 2
•classifier 2 -> class 1

#the class label 1 will be assigned to the sample.
#Example 
>>> from sklearn import datasets
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.naive_bayes import GaussianNB
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.ensemble import VotingClassifier

>>> iris = datasets.load_iris()
>>> X, y = iris.data[:, 1:3], iris.target

>>> clf1 = LogisticRegression(random_state=1)
>>> clf2 = RandomForestClassifier(random_state=1)
>>> clf3 = GaussianNB()

>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')

>>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
...     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
Accuracy: 0.90 (+/- 0.05) [Logistic Regression]
Accuracy: 0.93 (+/- 0.05) [Random Forest]
Accuracy: 0.91 (+/- 0.04) [naive Bayes]
Accuracy: 0.95 (+/- 0.05) [Ensemble]

#soft voting 
#Example - let's assume we have 3 classifiers 
#and a 3-class classification problems where we assign equal weights to all classifiers w1=1, w2=1, w3=1.

classifier      class 1     class 2     class 3
classifier 1    w1 * 0.2    w1 * 0.5    w1 * 0.3 
classifier 2    w2 * 0.6    w2 * 0.3    w2 * 0.1 
classifier 3    w3 * 0.3    w3 * 0.4    w3 * 0.3 
weighted average 0.37       0.4         0.3 

#the predicted class label is 2, since it has the highest average probability.

>>> from sklearn import datasets
>>> from sklearn.tree import DecisionTreeClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> from sklearn.svm import SVC
>>> from itertools import product
>>> from sklearn.ensemble import VotingClassifier

>>> # Loading some example data
>>> iris = datasets.load_iris()
>>> X = iris.data[:, [0,2]]
>>> y = iris.target

>>> # Training classifiers
>>> clf1 = DecisionTreeClassifier(max_depth=4)
>>> clf2 = KNeighborsClassifier(n_neighbors=7)
>>> clf3 = SVC(kernel='rbf', probability=True)
>>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='soft', weights=[2,1,2])

>>> clf1 = clf1.fit(X,y)
>>> clf2 = clf2.fit(X,y)
>>> clf3 = clf3.fit(X,y)
>>> eclf = eclf.fit(X,y)
>>> print(eclf.predict(X))
[1 1 1 2 2 2]

#Using the VotingClassifier with GridSearch

>>> from sklearn.model_selection import GridSearchCV
>>> clf1 = LogisticRegression(random_state=1)
>>> clf2 = RandomForestClassifier(random_state=1)
>>> clf3 = GaussianNB()
>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')

>>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],}

>>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
>>> grid = grid.fit(iris.data, iris.target)


#to predict the class labels based on the predicted class-probabilities 
#(scikit-learn estimators in the VotingClassifier must support predict_proba method):
>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')
#OR weights can be provided for the individual classifiers:
>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', weights=[2,5,1])








###Scikit - STEP4.0 -  sanity check - DummyClassifier and DummyRegressor -

#When doing supervised learning, it does a simple sanity check consists of comparing 
#one's estimator against simple rules of thumb.
 
sklearn.dummy.DummyClassifier(strategy='stratified', random_state=None, constant=None)
    DummyClassifier implements below strategies for classification:
    •'stratified' generates random predictions by respecting the training set class distribution.
    •'most_frequent' always predicts the most frequent label in the training set.
    •'prior' always predicts the class that maximizes the class prior (like most_frequent`) and predict_proba returns the class prior.
    •'uniform' generates predictions uniformly at random.
    •'constant' always predicts a constant label that is provided by the user.

sklearn.dummy.DummyRegressor(strategy='mean', constant=None, quantile=None)
    DummyRegressor  implements below strategies:
    •mean always predicts the mean of the training targets.
    •median always predicts the median of the training targets.
    •quantile always predicts a user provided quantile of the training targets.
    •constant always predicts a constant value that is provided by the user.
    In all these strategies, the predict method completely ignores the input data.

##Example - 
from sklearn.datasets import load_iris
from sklearn.cross_validation import train_test_split
iris = load_iris()
X, y = iris.data, iris.target
y[y != 1] = -1
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

#compare the accuracy of SVC and most_frequent:
from sklearn.dummy import DummyClassifier
from sklearn.svm import SVC
clf = SVC(kernel='linear', C=1).fit(X_train, y_train)
>>> clf.score(X_test, y_test) 
0.63...
clf = DummyClassifier(strategy='most_frequent',random_state=0)
clf.fit(X_train, y_train)
>>> clf.score(X_test, y_test)  
0.57...

#SVC doesn't do much better than a dummy classifier.
# Now, let's change the kernel:
>>> clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)
>>> clf.score(X_test, y_test)  
0.97...









###Scikit - STEP5.0:  Evaluating model performance 
#3 options 
Estimator score method
    Estimators have a score method providing a default evaluation 
    criterion for the problem they are designed to solve       
Scoring parameter
    Model-evaluation tools using cross-validation use scoring parameter
Metric functions
    use sklearn.metrics
    
##Scikit - Model performance - Directly using sklearn.metric
#sklearn.metric also exposes a set of simple functions measuring a prediction error 
    •functions ending with _score return a value to maximize, 
     the higher the better.
    •functions ending with _error or _loss return a value to minimize, 
     the lower the better. 
     
#One such example 
sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)
sklearn.metrics.zero_one_loss(y_true, y_pred, normalize=True, sample_weight=None)
 
#Scoring                Function                    Comment
#Classification     
'accuracy'              metrics.accuracy_score   
'average_precision'     metrics.average_precision_score   
'f1'                    metrics.f1_score            for binary targets 
'f1_micro'              metrics.f1_score            micro-averaged 
'f1_macro'              metrics.f1_score            macro-averaged 
'f1_weighted'           metrics.f1_score            weighted average 
'f1_samples'            metrics.f1_score            by multilabel sample 
'log_loss'              metrics.log_loss            requires predict_proba support 
'precision' etc.        metrics.precision_score     suffixes apply as with 'f1' 
'recall' etc.           metrics.recall_score        suffixes apply as with 'f1' 
'roc_auc'               metrics.roc_auc_score   
#Clustering     
'adjusted_rand_score'   metrics.adjusted_rand_score   
#Regression     
'mean_absolute_error'   metrics.mean_absolute_error   
'mean_squared_error'    metrics.mean_squared_error   
'median_absolute_error' metrics.median_absolute_error   
'r2'                    metrics.r2_score   


#When converting _error or _loss functions  into a scorer object 
#Use make_scorer, and set the greater_is_better parameter to False (True by default)

#Many scoring function takes addl argument eg fbeta_score
#Use make_scorer() to pass those (with greater_is_better=True)
sklearn.metrics.make_scorer(score_func, greater_is_better=True, 
    needs_proba=False, needs_threshold=False, **kwargs)

#Example 


>>> import numpy as np
>>> from sklearn.metrics import accuracy_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)
0.5
>>> accuracy_score(y_true, y_pred, normalize=False)
2
>>> zero_one_loss(y_true, y_pred)
0.25
>>> zero_one_loss(y_true, y_pred, normalize=False)
1
#multilabel case with binary label indicators:
>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5
>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5




##Scikit - Model performance - The scoring parameter with GridSearchCV and cross_val_score
#Model selection and evaluation using tools using grid_search.GridSearchCV 
#and cross_val_score, take a 'scoring' parameter 

#scoring: can be string from above (methods with suffix _score)
#or list of methods for multiple scoring methods(GridSearchCV) 


#Example with cross_val_score
from sklearn import svm, datasets
from sklearn.model_selection import cross_val_score
iris = datasets.load_iris()
X, y = iris.data, iris.target
clf = svm.SVC(probability=True, random_state=0)
>>> cross_val_score(clf, X, y, scoring='neg_log_loss') #Returns: scores : array of float, shape=(len(list(cv)),)
                                                       #Array of scores of the estimator for each run of the cross validation.
 
array([-0.07..., -0.16..., -0.06...])


#Example with GridSearchCV and fbeta_scoremake_scorer
from sklearn.metrics import fbeta_score, make_scorer
ftwo_scorer = make_scorer(fbeta_score, beta=2)
from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)

#Using customer scorer 
import numpy as np
def my_custom_loss_func(ground_truth, predictions):
    diff = np.abs(ground_truth - predictions).max()
    return np.log(1 + diff)

# loss_func will negate the return value of my_custom_loss_func,
#  which will be np.log(2), 0.693, given the values for ground_truth
#  and predictions defined below.
loss  = make_scorer(my_custom_loss_func, greater_is_better=False)
score = make_scorer(my_custom_loss_func, greater_is_better=True)
ground_truth = [[1, 1]]
predictions  = [0, 1]
from sklearn.dummy import DummyClassifier
clf = DummyClassifier(strategy='most_frequent', random_state=0)
clf = clf.fit(ground_truth, predictions)
>>> loss(clf,ground_truth, predictions) 
-0.69...
>>> score(clf,ground_truth, predictions) 
0.69...



##Scikit - Model performance - Specific metrics for  binary classification 
sklearn.metrics.matthews_corrcoef(y_true, y_pred)               
    Compute the Matthews correlation coefficient (MCC) for binary classes 
    A coefficient of +1 represents a perfect prediction, 
    0 no better than random prediction 
    and −1 indicates total disagreement between prediction and observation
sklearn.metrics.precision_recall_curve(y_true, probas_pred)     
    Compute precision-recall pairs for different probability thresholds 
sklearn.metrics.roc_curve(y_true, y_score[, pos_label, ...])    
    Compute Receiver operating characteristic (ROC) 
    Parameters 
    y_true : array, shape = [n_samples]
        True binary labels in range {0, 1} or {-1, 1}. 
        If labels are not binary, pos_label should be explicitly given.
    y_score : array, shape = [n_samples]
        Target scores, can either be probability estimates of the positive class, 
        confidence values, or non-thresholded measure of decisions 
        (as returned by 'decision_function' on some classifiers).
    pos_label : int or str, default=None
        Label considered as positive and others are considered negative.
    Returns 
    fpr : array, shape = [>2]
        Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i].
    tpr : array, shape = [>2]
        Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i].

#Example 
import numpy as np
from sklearn import metrics
y = np.array([1, 1, 2, 2])
scores = np.array([0.1, 0.4, 0.35, 0.8])
fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)
>>> fpr
array([ 0. ,  0.5,  0.5,  1. ])
>>> tpr
array([ 0.5,  0.5,  1. ,  1. ])
>>> thresholds
array([ 0.8 ,  0.4 ,  0.35,  0.1 ])
#average_precision_score  : Area under the precision-recall curve
#roc_curve  : Compute Receiver operating characteristic (ROC)
>>> metrics.roc_auc_score(y, scores)  #->1 better 
0.75
>>> metrics.average_precision_score(y_true, y_scores)  #->1 better 
0.79...




##Scikit - Model performance - Specific metrics for binary and multiclass case
cohen_kappa_score(y1, y2[, labels, weights])        
    Cohen's kappa: a statistic that measures inter-annotator agreement, 
    Returns:kappa : float, number between -1 and 1. 
    The maximum value means complete agreement; zero 
    or lower means chance agreement
confusion_matrix(y_true, y_pred[, labels])          
    Compute confusion matrix to evaluate the accuracy of a classification 
    Returns: C : array, shape = [n_classes, n_classes]
    entry i, j in a confusion matrix is the number of observations actually in group i, 
    but predicted to be in group j. 
    Diagonal should be max and offdiagonal should be zero
    for binary: two rows and two columns
    that reports the number of true positives(TP), false negatives(FN), 
    and 2nd row as false positives(FP), and true negatives(TN)
    true positive (TP)eqv. with hit true negative (TN)eqv. with correct rejection
    false positive (FP)eqv. with false alarm, Type I error 
    false negative (FN)eqv. with miss, Type II error
    #Other metrics base TP,TN,FP,EN 
    #sensitivity, recall, hit rate, or true positive rate (TPR)
    TPR  = TP/P   = TP /(TP + FN )
    #specificity or true negative rate (TNR)
    TNR  = TN /N   = TN/(T + FP)
    #precision or positive predictive value (PPV)
    PPV  = TP/(TP+ FP)     
    #negative predictive value (NPV)
    NPV  = TN/(TN  + FN)  
    #miss rate or false negative rate (FNR)
    FNR  = FNP   = FN/(FN  + TP)     = 1 − TPR
    #fall-out or false positive rate (FPR)
    FPR  = FP N   = FP /(FP  + TN)    = 1 − TNR    
    #false discovery rate (FDR)
    F D R  = F P /(F P  + T P)    = 1 − P P V    
    #false omission rate (FOR)
    F O R  = F N /(F N  + T N)     = 1 − N P V    
    #accuracy (ACC)
    A C C  = (T P  + T N)/(P + N )   = (T P  + T N)/(  T P  + T N  + F P  + F N )      
    #F1 score is the harmonic mean of precision and sensitivity
    F1   = 2 *P P V  * T P R /(  P P V  + T P R)     = 2 T P/(   2 T P  + F P  + F N)

hinge_loss(y_true, pred_decision[, labels, ...])    
    Average hinge loss (non-regularized) , 
    Returns:loss : float, lower the better 
 

#Example - confusion martrix 
from sklearn.metrics import confusion_matrix
y_true = [2, 0, 2, 2, 0, 1]
y_pred = [0, 0, 2, 2, 0, 2]
>>> confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])

y_true = ["cat", "ant", "cat", "cat", "ant", "bird"]
y_pred = ["ant", "ant", "cat", "cat", "ant", "cat"]
>>> confusion_matrix(y_true, y_pred, labels=["ant", "bird", "cat"])
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])

 
##Scikit - Model performance - Specific metrics forbinary and multilabel (but not multiclass) problems:
average_precision_score(y_true, y_score[, ...])     Compute average precision (AP) from prediction scores 
roc_auc_score(y_true, y_score[, average, ...])      Compute Area Under the Curve (AUC) from prediction scores 

>>> import numpy as np
>>> from sklearn.metrics import roc_auc_score
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> roc_auc_score(y_true, y_scores)
0.75


##Scikit - Model performance - Specific metrics for multilabel case
accuracy_score(y_true, y_pred[, normalize, ...])    Accuracy classification score. 
classification_report(y_true, y_pred[, ...])        Build a text report showing the main classification metrics 
f1_score(y_true, y_pred[, labels, ...])             Compute the F1 score, also known as balanced F-score or F-measure 
fbeta_score(y_true, y_pred, beta[, labels, ...])    Compute the F-beta score 
hamming_loss(y_true, y_pred[, classes])             Compute the average Hamming loss. 
jaccard_similarity_score(y_true, y_pred[, ...])     Jaccard similarity coefficient score 
log_loss(y_true, y_pred[, eps, normalize, ...])     Log loss, aka logistic loss or cross-entropy loss. 
precision_recall_fscore_support(y_true, y_pred)     Compute precision, recall, F-measure and support for each class 
precision_score(y_true, y_pred[, labels, ...])      Compute the precision 
recall_score(y_true, y_pred[, labels, ...])         Compute the recall 
zero_one_loss(y_true, y_pred[, normalize, ...])     Zero-one classification loss. 

##accuracy_score
from sklearn.metrics import accuracy_score
y_pred = [0, 2, 1, 3]
y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)
0.5
>>> accuracy_score(y_true, y_pred, normalize=False)
2

#In the multilabel case with binary label indicators:
>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5

##Classification report- a text report showing the main classification metrics. 
from sklearn.metrics import classification_report
y_true = [0, 1, 2, 2, 0]
y_pred = [0, 0, 2, 1, 0]
target_names = ['class 0', 'class 1', 'class 2']
>>> print(classification_report(y_true, y_pred, target_names=target_names))
             precision    recall  f1-score   support

    class 0       0.67      1.00      0.80         2
    class 1       0.00      0.00      0.00         1
    class 2       1.00      0.50      0.67         2

avg / total       0.67      0.60      0.59         5

##Hamming loss - hamming_loss()
#computes the average Hamming loss or Hamming distance between two sets of samples
y_pred = [1, 2, 3, 4]
y_true = [2, 2, 3, 4]
>>> hamming_loss(y_true, y_pred)
0.25
#In the multilabel case with binary label indicators:
>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
0.75

##Jaccard similarity coefficient score -  jaccard_similarity_score()
#The Jaccard similarity coefficient of the i-th samples, 
#with a ground truth label set yi and predicted label set yi_hat, is defined as
#J(yi, yi_hat) = |yi INTERSECTION yi_hat|/|yi UNION yi_hat| where | | is set length 

#In binary and multiclass classification,
#the Jaccard similarity coefficient score is equal to the classification accuracy.

import numpy as np
from sklearn.metrics import jaccard_similarity_score
y_pred = [0, 2, 1, 3]
y_true = [0, 1, 2, 3]
>>> jaccard_similarity_score(y_true, y_pred)
0.5
>>> jaccard_similarity_score(y_true, y_pred, normalize=False)
2
#In the multilabel case with binary label indicators:
>>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.75

##Precision, recall and F-measures
# precision is the ability of the classifier not to label as positive a sample that is negative, 
#and recall is the ability of the classifier to find all the positive samples.

#binary classification:
from sklearn import metrics
y_pred = [0, 1, 0, 0]
y_true = [0, 1, 0, 1]
>>> metrics.precision_score(y_true, y_pred)
1.0
>>> metrics.recall_score(y_true, y_pred)
0.5
>>> metrics.f1_score(y_true, y_pred)  
0.66...
>>> metrics.fbeta_score(y_true, y_pred, beta=0.5)  
0.83...
>>> metrics.fbeta_score(y_true, y_pred, beta=1)  
0.66...
>>> metrics.fbeta_score(y_true, y_pred, beta=2) 
0.55...
>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)  
(array([ 0.66...,  1.        ]), array([ 1. ,  0.5]), array([ 0.71...,  0.83...]), array([2, 2]...))

#Example of precision_recall_curve
import numpy as np
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
precision, recall, threshold = precision_recall_curve(y_true, y_scores)
>>> precision  
array([ 0.66...,  0.5       ,  1.        ,  1.        ])
>>> recall
array([ 1. ,  0.5,  0.5,  0. ])
>>> threshold
array([ 0.35,  0.4 ,  0.8 ])
>>> average_precision_score(y_true, y_scores)  
0.79...

##Multiclass and multilabel classification
from sklearn import metrics
y_true = [0, 1, 2, 0, 1, 2]
y_pred = [0, 2, 1, 0, 0, 1]
>>> metrics.precision_score(y_true, y_pred, average='macro')  
0.22...
>>> metrics.recall_score(y_true, y_pred, average='micro')
... 
0.33...
>>> metrics.f1_score(y_true, y_pred, average='weighted')  
0.26...
>>> metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)  
0.23...
>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)
... 
(array([ 0.66...,  0.        ,  0.        ]), array([ 1.,  0.,  0.]), array([ 0.71...,  0.        ,  0.        ]), array([2, 2, 2]...))


#For multiclass classification with a 'negative class', 
#it is possible to exclude some labels:
>>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')
# excluding 0, no labels were correctly recalled
0.0

#labels not present in the data sample may be accounted for in macro-averaging.
>>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')
0.166...

##Log loss, also called logistic regression loss or cross-entropy loss, 
#log_loss function computes log loss given a list of ground-truth labels and a probability matrix, 
#as returned by an estimator's predict_proba method.
from sklearn.metrics import log_loss
y_true = [0, 0, 1, 1]
y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]
>>> log_loss(y_true, y_pred)    
0.1738...
#The first [.9, .1] in y_pred denotes 90% probability that the first sample has label 0. 
#The log loss is non-negative.




##Hinge loss - hinge_loss()
#computes the average distance between the model and the data using hinge loss, 
#a one-sided metric that considers only prediction errors. 
#(Hinge loss is used in maximal margin classifiers such as support vector machines.)

#decision_function and predict 
#The desion function outputs  on which side of the hyperplane 
#generated by the classifier we are (and how far we are away from it). 
#Based on that information, the estimator then label the examples with the corresponding label

#in LinearSVC classifier, then decision_function will give you scores for each class label 
#(not same as SVC) and predict will give the class with the best score.


import numpy as np
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
y = np.array([1, 1, 2, 2])
from sklearn.svm import SVC
clf = SVC()
clf.fit(X, y) 


>>> clf.decision_function(X)
array([[-1.00052254],
       [-1.00006594],
       [ 1.00029424],
       [ 1.00029424]])
       
clf.predict(X)
array([1, 1, 2, 2])


#binary class
from sklearn import svm
from sklearn.metrics import hinge_loss
X = [[0], [1]]
y = [-1, 1]
est = svm.LinearSVC(random_state=0)
>>> est.fit(X, y)
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0)
     
pred_decision = est.decision_function([[-2], [3], [0.5]])
>>> pred_decision  
array([-2.18...,  2.36...,  0.09...])
>>> hinge_loss([-1, 1, 1], pred_decision)  
0.3...


#multiclass problem:

X = np.array([[0], [1], [2], [3]])
Y = np.array([0, 1, 2, 3])
labels = np.array([0, 1, 2, 3])
est = svm.LinearSVC()
>>> est.fit(X, Y)
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)
>>> pred_decision = est.decision_function([[-1], [2], [3]])
>>> y_true = [0, 2, 3]
>>> hinge_loss(y_true, pred_decision, labels)  
0.56...

 

##Scikit - Model performance - Specific metrics Multilabel ranking metrics
#In multilabel learning, each sample can have any number of ground truth labels associated with it. 
#The goal is to give high scores and better rank to the ground truth labels.

metrics.coverage_error(y_true, y_score[, ...])      Coverage error measure 
metrics.label_ranking_average_precision_score(...)  Compute ranking-based average precision 
metrics.label_ranking_loss(y_true, y_score) Compute Ranking loss measure 



##Coverage error - coverage_error()
#computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. 

#This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. 
#The best value of this metrics is thus the average number of true labels

import numpy as np
from sklearn.metrics import coverage_error
y_true = np.array([[1, 0, 0], [0, 0, 1]])
y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> coverage_error(y_true, y_score)
2.5

##Label ranking average precision - label_ranking_average_precision_score()
#computes label ranking average precision 
#linked to to the average_precision_score()
#but is based on the notion of label ranking instead of precision and recall.

#it is the average over each ground truth label assigned to each sample, 
#of the ratio of true vs. total labels with lower score

#The obtained score is always strictly greater than 0, and the best value is 1

import numpy as np
from sklearn.metrics import label_ranking_average_precision_score
y_true = np.array([[1, 0, 0], [0, 0, 1]])
y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> label_ranking_average_precision_score(y_true, y_score) 
0.416...

##Ranking loss - label_ranking_loss()
#computes the ranking loss which averages over the samples the number of label pairs 
#that are incorrectly ordered, i.e. true labels have a lower score than false labels, 
#weighted by the the inverse number of false and true labels. 

#The lowest achievable ranking loss is zero

import numpy as np
from sklearn.metrics import label_ranking_loss
y_true = np.array([[1, 0, 0], [0, 0, 1]])
y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> label_ranking_loss(y_true, y_score) 
0.75...
# With the following prediction, we have perfect and minimal loss
>>> y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])
>>> label_ranking_loss(y_true, y_score)
0.0




##Scikit - Model performance - Specific metrics for Regression 
metrics.explained_variance_score(y_true, y_pred)        Explained variance regression score function 
metrics.mean_absolute_error(y_true, y_pred)             Mean absolute error regression loss 
metrics.mean_squared_error(y_true, y_pred[, ...])       Mean squared error regression loss 
metrics.median_absolute_error(y_true, y_pred)           Median absolute error regression loss 
metrics.r2_score(y_true, y_pred[, ...])                 R^2 (coefficient of determination) regression score function. 



##Explained variance score - explained_variance_score()
#computes the explained variance regression score

#The best possible score is 1.0, lower values are worse.

from sklearn.metrics import explained_variance_score
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
>>> explained_variance_score(y_true, y_pred)  
0.957...
y_true = [[0.5, 1], [-1, 1], [7, -6]]
y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> explained_variance_score(y_true, y_pred, multioutput='raw_values')
array([ 0.967...,  1.        ])
>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])
0.990...


##Mean absolute error - mean_absolute_error()
#computes mean absolute error, a risk metric corresponding to the expected value 
#of the absolute error loss or l1-norm loss

from sklearn.metrics import mean_absolute_error
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
>>> mean_absolute_error(y_true, y_pred)
0.5
y_true = [[0.5, 1], [-1, 1], [7, -6]]
y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_absolute_error(y_true, y_pred)
0.75
>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')
array([ 0.5,  1. ])
>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.849...


##Mean squared error - mean_squared_error()
#computes mean square error, a risk metric corresponding to the expected value of the 
#squared (quadratic) error loss or loss.

from sklearn.metrics import mean_squared_error
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
>>> mean_squared_error(y_true, y_pred)
0.375
y_true = [[0.5, 1], [-1, 1], [7, -6]]
y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_squared_error(y_true, y_pred)  
0.7083...

##Median absolute error - median_absolute_error()
# is robust to outliers. 
#calculated by taking the median of all absolute differences between the target and the prediction

from sklearn.metrics import median_absolute_error
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
>>> median_absolute_error(y_true, y_pred)
0.5

##Rsquare score, the coefficient of determination
#provides a measure of how well future samples are likely to be predicted by the model. 

#Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). 
#A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.

from sklearn.metrics import r2_score
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
>>> r2_score(y_true, y_pred)  
0.948...
y_true = [[0.5, 1], [-1, 1], [7, -6]]
y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> r2_score(y_true, y_pred, multioutput='variance_weighted')
0.938...
y_true = [[0.5, 1], [-1, 1], [7, -6]]
y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> r2_score(y_true, y_pred, multioutput='uniform_average')
0.936...
>>> r2_score(y_true, y_pred, multioutput='raw_values')
array([ 0.965...,  0.908...])
>>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7])
0.925...





##Scikit - Model performance - Specific metrics for Clustering performance evaluation
#labels_true -  ground truth class assignments
#labels_pred -  clustering algorithm assignments of the same samples

#There are two forms of evaluation:
#•supervised, which uses a ground truth class values for each sample.
#•unsupervised, which does not and measures the 'quality' of the model itself.


metrics.adjusted_mutual_info_score(...)             Adjusted Mutual Information between two clusterings. 
metrics.adjusted_rand_score(labels_true, ...)       Rand index adjusted for chance. 
metrics.calinski_harabaz_score(X, labels)           Compute the Calinski and Harabaz score. 
metrics.completeness_score(labels_true, ...)        Completeness metric of a cluster labeling given a ground truth. 
metrics.fowlkes_mallows_score(labels_true, ...)     Measure the similarity of two clusterings of a set of points. 
metrics.homogeneity_completeness_v_measure(...)     Compute the homogeneity and completeness and V-Measure scores at once. 
metrics.homogeneity_score(labels_true, ...)         Homogeneity metric of a cluster labeling given a ground truth. 
metrics.mutual_info_score(labels_true, ...)         Mutual Information between two clusterings. 
metrics.normalized_mutual_info_score(...)           Normalized Mutual Information between two clusterings. 
metrics.silhouette_score(X, labels[, ...])          Compute the mean Silhouette Coefficient of all samples. 
metrics.silhouette_samples(X, labels[, metric])     Compute the Silhouette Coefficient for each sample. 
metrics.v_measure_score(labels_true, labels_pred)   V-measure cluster labeling given a ground truth. 


##Adjusted Rand index - adjusted_rand_score()
#measures the similarity of the two assignments, ignoring permutations and with chance normalization:
#Bounded range [-1, 1]: negative values are bad (independent labelings), 
#similar clusterings have a positive ARI, 1.0 is the perfect match score.


from sklearn import metrics
labels_true = [0, 0, 0, 1, 1, 1]
labels_pred = [0, 0, 1, 1, 2, 2]

>>> metrics.adjusted_rand_score(labels_true, labels_pred)  
0.24...

#One can permute 0 and 1 in the predicted labels, rename 2 to 3, 
#and get the same score:
labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.adjusted_rand_score(labels_true, labels_pred)  
0.24...


#adjusted_rand_score is symmetric: swapping the argument does not change the score. 
#It can thus be used as a consensus measure:
>>> metrics.adjusted_rand_score(labels_pred, labels_true)  
0.24...



##Mutual Information based scores 
#mutual_info_score, adjusted_mutual_info_score and normalized_mutual_info_score
#measures the agreement of the two assignments, ignoring permutations. 

#Two different normalized versions of this measure are available, 
#Normalized Mutual Information(NMI) and Adjusted Mutual Information(AMI). 
#NMI is often used in the literature 
#while AMI was proposed more recently and is normalized against chance:

#Bounded range [0, 1]: Values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, values of exactly 0 indicate purely independent label assignments and a AMI of exactly 1 indicates that the two label assignments are equal (with or without permutation).
from sklearn import metrics
labels_true = [0, 0, 0, 1, 1, 1]
labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
0.22504...


#One can permute 0 and 1 in the predicted labels, rename 2 to 3 
#and get the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
0.22504...

#are symmetric: swapping the argument does not change the score. 
#Thus they can be used as a consensus measure:
>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  
0.22504...

#Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
1.0

>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)
1.0

#This is not true for mutual_info_score, which is therefore harder to judge:
>>> metrics.mutual_info_score(labels_true, labels_pred)  
0.69...


##Homogeneity, completeness and V-measure - 
#homogeneity_score , completeness_score, homogeneity_completeness_v_measure(), v_measure_score
#two desirable objectives for any cluster assignment:
    •homogeneity: each cluster contains only members of a single class.
    •completeness: all members of a given class are assigned to the same cluster.
#bounded below by 0.0 and above by 1.0 (higher is better):
from sklearn import metrics
labels_true = [0, 0, 0, 1, 1, 1]
labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.homogeneity_score(labels_true, labels_pred)  
0.66...
>>> metrics.completeness_score(labels_true, labels_pred) 
0.42...

#Their harmonic mean called V-measure is computed by v_measure_score:
>>> metrics.v_measure_score(labels_true, labels_pred)    
0.51...

#or use homogeneity_completeness_v_measure()
>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
(0.66..., 0.42..., 0.51...)



##Silhouette Coefficient - sklearn.metrics.silhouette_score() when ground truth labels are not known
    •a: The mean distance between a sample and all other points in the same class.
    •b: The mean distance between a sample and all other points in the next nearest cluster.
#Silhouette Coefficient is function of above 

#•The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. 
#Scores around zero indicate overlapping clusters.
#The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.
from sklearn import metrics
from sklearn.metrics import pairwise_distances
from sklearn import datasets
dataset = datasets.load_iris()
X = dataset.data
y = dataset.target


#it is applied to the results of a cluster analysis.
import numpy as np
from sklearn.cluster import KMeans
kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
labels = kmeans_model.labels_
>>> metrics.silhouette_score(X, labels, metric='euclidean')
                                                 
0.55...




##Scikit - Other metrics - Pairwise metrics, Affinities and Kernels
#The sklearn.metrics.pairwise submodule implements utilities 
#to evaluate pairwise distances or affinity of sets of samples.

##Distances , method with suffix _distances 
>>> dir(sklearn.metrics.pairwise)
>>> sklearn.metrics.pairwise.distance_metrics()

metrics.pairwise_distances_argmin(X, Y[, …]) 
    Compute minimum distances between one point and a set of points. 
metrics.pairwise_distances_argmin_min(X, Y) 
    Compute minimum distances between one point and a set of points. 

sklearn.metrics.pairwise.pairwise_distances(X, Y=None, metric='euclidean', n_jobs=1, **kwds)[source]
    Compute the distance matrix from a vector array X and optional Y.
    This method takes either a vector array or a distance matrix, 
    and returns a distance matrix. 
    If the input is a vector array, the distances are computed. 
    If the input is a distances matrix, it is returned instead.
    If Y is given (default is None), 
    then the returned matrix is the pairwise distance between the arrays from both X and Y.
    Parameters:
    X : array [n_samples_a, n_samples_a] if metric == 'precomputed', or, [n_samples_a, n_features] otherwise
        Array of pairwise distances between samples, or a feature array.
    Y : array [n_samples_b, n_features], optional
        An optional second feature array. Only allowed if metric != 'precomputed'.
    metric : string, or callable
        The metric to use when calculating distance between instances in a feature array. 
        Valid values for metric are:
            •From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']. These metrics support sparse matrix inputs.
            •From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs.
    n_jobs : int
        The number of jobs to use for the computation. 
        If -1 all CPUs are used. 
        If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.
    **kwds : optional keyword parameters
        Any further parameters are passed directly to the distance function.
       If using a scipy.spatial.distance metric, the parameters are still metric dependent. 
    Returns:
        D : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]
            A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of the given matrix X, if Y is None. 
            If Y is not None, then D_{i, j} is the distance between the ith array from X and the jth array from Y.
     
 
sklearn.metrics.pairwise.paired_distances(X, Y, metric='euclidean', **kwds)[source]
    Computes the paired distances between X and Y.
    Computes the distances between (X[0], Y[0]), (X[1], Y[1]), etc…
    Parameters:
    X : ndarray (n_samples, n_features)
        Array 1 for distance computation.
    Y : ndarray (n_samples, n_features)
        Array 2 for distance computation.
    metric : string or callable
        The metric to use when calculating distance between instances in a feature array. 
        If metric is a string, from  sklearn.metrics.pairwise.PAIRED_DISTANCES, 
        including 'euclidean', 'manhattan', or 'cosine'. 
        Alternatively, if metric is a callable function, 
        it is called on each pair of instances (rows) 
        and the resulting value recorded. 
        The callable should take two arrays from X as input 
        and return a value indicating the distance between them.
    Returns:
        distances : ndarray (n_samples, )
 
#Exmaple 
>>> from sklearn.metrics.pairwise import paired_distances
>>> X = [[0, 1], [1, 1]]
>>> Y = [[0, 1], [2, 1]]
>>> paired_distances(X, Y)
array([ 0.,  1.])

 
##Kernels 
#Kernels are measures of similarity(a function, s), i.e. s(a, b) > s(a, c) 
#if objects a and b are considered 'more similar' than objects a and c
#check various kernels 
metrics.pairwise.kernel_metrics()               Valid metrics for pairwise_kernels 


sklearn.metrics.pairwise.pairwise_kernels(X, Y=None, metric='linear', filter_params=False, n_jobs=1, **kwds)[source]
    Compute the kernel between arrays X and optional array Y.
    This method takes either a vector array or a kernel matrix, 
    and returns a kernel matrix. 
    If the input is a vector array, the kernels are computed. 
    If the input is a kernel matrix, it is returned instead.
    If Y is given (default is None), 
    then the returned matrix is the pairwise kernel between the arrays from both X and Y.
    Parameters:
    X : array [n_samples_a, n_samples_a] if metric == 'precomputed', or, [n_samples_a, n_features] otherwise
        Array of pairwise kernels between samples, or a feature array.
    Y : array [n_samples_b, n_features]
        A second feature array only if X has shape [n_samples_a, n_features].
    metric : string, or callable
        Valid values for metric are::['rbf', 'sigmoid', 'polynomial', 'poly', 'linear', 'cosine']
    filter_params : boolean
        Whether to filter invalid parameters or not.
    n_jobs : int
        The number of jobs to use for the computation. 
        If -1 all CPUs are used. 
        If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.
    **kwds : optional keyword parameters
        Any further parameters are passed directly to the kernel function.
    Returns:
    K : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]
        A kernel matrix K such that K_{i, j} is the kernel 
        between the ith and jth vectors of the given matrix X, if Y is None. 
        If Y is not None, then K_{i, j} is the kernel between the ith array from X and the jth array from Y.
     
 

## Chi-squared kernel
#The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. 
#It can be computed using chi2_kernel and then passed to an sklearn.svm.SVC with kernel="precomputed":
>>> from sklearn.svm import SVC
>>> from sklearn.metrics.pairwise import chi2_kernel
>>> X = [[0, 1], [1, 0], [.2, .8], [.7, .3]]
>>> y = [0, 1, 0, 1]
>>> K = chi2_kernel(X, gamma=.5)
>>> K                        
array([[ 1.        ,  0.36...,  0.89...,  0.58...],
       [ 0.36...,  1.        ,  0.51...,  0.83...],
       [ 0.89...,  0.51...,  1.        ,  0.77... ],
       [ 0.58...,  0.83...,  0.77... ,  1.        ]])

>>> svm = SVC(kernel='precomputed').fit(K, y)
>>> svm.predict(K)
array([0, 1, 0, 1])

#It can also be directly used as the kernel argument:
>>> svm = SVC(kernel=chi2_kernel).fit(X, y)
>>> svm.predict(X)
array([0, 1, 0, 1])







###Scikit - STEP6.0:  Combining various steps  - Use Pipeline 

sklearn.pipeline.make_pipeline(*steps)
    shorthand for constructing pipelines; 
    it takes a variable number of estimators and returns a pipeline, 
    filling in the names automatically
    Returns an instance of Pipeline 
#Example 
from sklearn.pipeline import make_pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import Binarizer
>>> make_pipeline(Binarizer(), MultinomialNB()) 
Pipeline(memory=None,
         steps=[('binarizer', Binarizer(copy=True, threshold=0.0)),
                ('multinomialnb', MultinomialNB(alpha=1.0,
                                                class_prior=None,
                                                fit_prior=True))])
>>> _.named_steps
{'binarizer': Binarizer(copy=True, threshold=0.0), 
'multinomialnb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)}


class sklearn.pipeline.Pipeline(steps, memory=None)
    The purpose of the pipeline is to assemble several steps that can be cross-validated 
    together while setting different parameters
    All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). 
    The last estimator may be any type (transformer, classifier, etc.)
    Calling fit on the pipeline is the same as calling fit on each estimator in turn,
    transform the input and pass it on to the next step. 
    The pipeline has all the methods that the last estimator in the pipeline has,
    i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. 
    If the last estimator is a transformer, again, so is the pipeline.    
    steps : list
        List of (name, transform) tuples 
        where name is userdefined string 
        where transform (implementing fit/transform) that are chained, 
        with the last object an estimator.
    memory : None, str or object with the joblib.Memory interface, optional
        Used to cache the fitted transformers of the pipeline. 
        By default, no caching is performed. 
        If a string is given, it is the path to the caching directory
        Caching the transformers is advantageous when fitting is time consuming.
    #Attributes:
    named_steps : bunch object, a dictionary with attribute access
        Read-only attribute to access any step parameter by user given name.
        Keys are step names and values are steps parameters.
    #Methods
    decision_function(X)
        Apply transforms, and decision_function of the final estimator
        Returns y_score : array-like, shape = [n_samples, n_classes]
    fit(X, y=None, **fit_params)             
        Fit the model
        Fit all the transforms one after the other and transform the data, 
        then fit the transformed data using the final estimator.
    fit_transform(X, y=None, **fit_params)    
        Fit the model and transform with the final estimator
        Fits all the transforms one after the other and transforms the data, 
        then uses fit_transform on transformed data with the final estimator.
    inverse_transform(X)
        Apply inverse transformations in reverse order
        All estimators in the pipeline must support inverse_transform
        Returns Xt : array-like, shape = [n_samples, n_features]
    fit_predict(X, y=None, **fit_params)     
        Applies fit_predict of last step in pipeline after transforms.
        Applies fit_transforms of a pipeline to the data, 
        followed by the fit_predict method of the final estimator in the pipeline. 
        Valid only if the final estimator implements fit_predict.
    predict(X)                    
        Apply transforms to the data, and predict with the final estimator, 
        return y_pred
    predict_log_proba(X)          
        Apply transforms, and predict_log_proba of the final estimator ,
        Returns: y_score of size [n_samples, n_classes]
    predict_proba(X)              
        Apply transforms, and predict_proba of the final estimator ,
        Returns: y_score of size [n_samples, n_classes]
    score(X, y=None)              
        Apply transforms, and score with the final estimator, 
        return float. Higher value better 
    set_params(param)             
        Set the parameters of this estimator.,
        Valid parameter keys can be listed with get_params().
    get_params(deep=True)         
        Get parameters for this estimator.
    #Note general input parameters 
    X : iterable
        Training data. Must fulfill input requirements of first step of the pipeline.
    y : iterable, default=None
        Training targets. Must fulfill label requirements for all steps of the pipeline.
    **fit_params : dict of string -> object
        Parameters passed to the fit method of each step, 
        eg: parameter 'p' for step with name 's' has key 's__p'.
        (Note double underscore)

##Example of Pipeline for selecting Feature and estimating(SVC) C-Support Vector Classification. 
#SVC: The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.
#SVC: The multiclass support is handled according to a one-vs-one scheme.
#C : float, optional (default=1.0)
#Penalty parameter C of the error term.

from sklearn import svm
from sklearn.datasets import samples_generator
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
from sklearn.pipeline import Pipeline
# generate some data to play with
X, y = samples_generator.make_classification(n_informative=5, n_redundant=0, random_state=42)
# ANOVA SVM-C
anova_filter = SelectKBest(f_regression, k=5)
clf = svm.SVC(kernel='linear')
anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])
# You can set the parameters using the names issued
# For instance, fit using a k of 10 in the SelectKBest
# and a parameter 'C' of the svm
anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)

prediction = anova_svm.predict(X)
>>> anova_svm.score(X, y)                        
0.829...
# getting the selected features chosen by anova_filter
>>> anova_svm.named_steps['anova'].get_support()... 
array([False, False,  True,  True, False, False, True,  True, False,
       True,  False,  True,  True, False, True,  False, True, True,
       False, False], dtype=bool)



##Example of Joint parameter selection with SVC 
#grid search over parameters of all estimators in the pipeline at once.

from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.decomposition import PCA

from sklearn.datasets import samples_generator
# generate some data to play with
X, y = samples_generator.make_classification(n_informative=5, n_redundant=0, random_state=42)

#Split the data 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

#Create estimators 
estimators = [('reduce_dim', PCA()), ('clf', SVC())]
pipe = Pipeline(estimators)
>>> pipe 
Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',n_components=None, random_state=None, svd_solver='auto', tol=0.0,whiten=False)), 
('clf', SVC(C=1.0, cache_size=200, class_weight=None,coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',kernel='rbf', max_iter=-1, probability=False, random_state=None,shrinking=True, tol=0.001, verbose=False))])


#The estimators of a pipeline are stored as a list in the steps attribute:
>>>> pipe.steps[0]
('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False))

#OR as a dict in named_steps:
>>> pipe.named_steps['reduce_dim']
PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)

#Get all parameters name of the Pipeline 
#includes 'self' and steps parameters 
>>> sorted(pipe.get_params().keys())
['clf', 'clf__C', 'clf__cache_size', 'clf__class_weight', 'clf__coef0', 'clf__de
cision_function_shape', 'clf__degree', 'clf__gamma', 'clf__kernel', 'clf__max_it
er', 'clf__probability', 'clf__random_state', 'clf__shrinking', 'clf__tol', 'clf
__verbose', 'memory', 'reduce_dim', 'reduce_dim__copy', 'reduce_dim__iterated_po
wer', 'reduce_dim__n_components', 'reduce_dim__random_state', 'reduce_dim__svd_s
olver', 'reduce_dim__tol', 'reduce_dim__whiten', 'steps']

#Parameters of the estimators in the pipeline can be accessed 
#using the <estimator_name>__<parameter> syntax:
pipe.set_params(clf__C=10) 

#Then fit and predict 
pipe.fit(X_train, y_train)
prediction = pipe.predict(X_test)
>>> pipe.score(X_test, y_test)     #0.9393939393939394

#for example: for doing grid searches over parameter range 
from sklearn.model_selection import GridSearchCV
params = dict(reduce_dim__n_components=[2, 5, 10],clf__C=[0.1, 10, 100])
grid_search = GridSearchCV(pipe, param_grid=params, return_train_score=True)

#OR: Individual steps may also be replaced as parameters, 
#and non-final steps may be ignored by setting them to None:
from sklearn.linear_model import LogisticRegression
params = dict(reduce_dim=[None, PCA(5), PCA(10)],
                clf=[SVC(), LogisticRegression()],
                clf__C=[0.1, 10, 100])
grid_search = GridSearchCV(pipe, param_grid=params, return_train_score=True)

#Then check the best parameter 
grid_search.fit(X, y)
#Estimator that was chosen by the search, 
#i.e. estimator which gave highest score (or smallest loss if specified)on the left out data
best_pipeline = grid_search.best_estimator_
best_pipeline.score(X_test, y_test) #0.9696969696969697
#best_params_ : Parameter setting that gave the best results on the hold out data.
>>> grid_search.best_params_
{'reduce_dim__n_components': 5, 'clf__C': 0.1}
#best_index_ : int,The index (of the cv_results_ arrays) which corresponds to the best candidate parameter setting.
grid_search.best_index_
#best_score_ : float,Mean cross-validated score of the best_estimator
grid_search.best_score_
#cv_results_: Full result grid of CV , time is in sec 
pd.DataFrame(grid_search.cv_results_)


##Complete Example 
http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#example-model-selection-grid-search-text-feature-extraction-py


##Using a Pipeline without cache enabled, 
#it is possible to inspect the original instance such as:
>>> from sklearn.datasets import load_digits
>>> digits = load_digits()
>>> pca1 = PCA()
>>> svm1 = SVC()
>>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])
>>> pipe.fit(digits.data, digits.target)
... 
Pipeline(memory=None,
         steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))])
>>> # The pca instance can be inspected directly
>>> print(pca1.components_)   #NO ERROR , ERROR if cacheing , 
    [[ -1.77484909e-19  ... 4.07058917e-18]]


#Enabling caching triggers a clone of the transformers before fitting. 
>>> cachedir = mkdtemp()
>>> pca2 = PCA()
>>> svm2 = SVC()
>>> cached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],
...                        memory=cachedir)
>>> cached_pipe.fit(digits.data, digits.target)
... 
 Pipeline(memory=...,
          steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))])
>>> print(cached_pipe.named_steps['reduce_dim'].components_) #NO ERROR 
... 
    [[ -1.77484909e-19  ... 4.07058917e-18]]
>>> # Remove the cache directory
>>> rmtree(cachedir)





###Scikit - STEP6.0:  Combining various transformers  - Use FeatureUnion 
# For transforming data, the transformers are applied in parallel, 
#and the sample vectors they output are concatenated end-to-end into larger vectors.



sklearn.pipeline.make_union(*transformers, **kwargs)[source]
    This is a shorthand for the FeatureUnion constructor; 
    they will be given names automatically based on their types. 
    *transformers : list of estimators
    n_jobs : int, optional
        Number of jobs to run in parallel (default 1).
 
#Example 
>>> from sklearn.decomposition import PCA, TruncatedSVD
>>> from sklearn.pipeline import make_union
>>> make_union(PCA(), TruncatedSVD())    
FeatureUnion(n_jobs=1,
       transformer_list=[('pca',
                          PCA(copy=True, iterated_power='auto',
                              n_components=None, random_state=None,
                              svd_solver='auto', tol=0.0, whiten=False)),
                         ('truncatedsvd',
                          TruncatedSVD(algorithm='randomized',
                          n_components=2, n_iter=5,
                          random_state=None, tol=0.0))],
       transformer_weights=None)

class sklearn.pipeline.FeatureUnion(transformer_list, n_jobs=1, transformer_weights=None)[source]
    Concatenates results of multiple transformer objects.
    This estimator applies a list of transformer objects in parallel 
    to the input data, then concatenates the results. 
    This is useful to combine several feature extraction mechanisms 
    into a single transformer.
        transformer_list : list of (string, transformer) tuples
            List of transformer objects to be applied to the data. 
        n_jobs : int, optional
            Number of jobs to run in parallel (default 1).
        transformer_weights : dict, optional
            Multiplicative weights for features per transformer. 
            Keys are transformer names, values the weights.
    #Methods
    fit(X[, y])             Fit all transformers using X. 
    fit_transform(X[, y])   Fit all transformers, transform the data and concatenate results. 
    get_feature_names()     Get feature names from all transformers. 
    get_params([deep])      Get parameters for this estimator. 
    set_params(**kwargs)    Set the parameters of this estimator. 
                            string = object
                            Parameters passed to the fit method of each step, 
                            eg: parameter 'p' for step with name 's' has key 's__p'.
                            (Note double underscore)
    transform(X)            Transform X separately by each transformer, concatenate results. 


#Example 
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest

iris = load_iris()
X, y = iris.data, iris.target
#Split the data 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# This dataset is way too high-dimensional. Better do PCA:
pca = PCA(n_components=2)

# Maybe some original features where good, too?
selection = SelectKBest(k=1)

# Build estimator from PCA and Univariate selection:
combined_features = FeatureUnion([("pca", pca), ("univ_select", selection)])

>>> sorted(combined_features.get_params().keys())
['n_jobs', 'pca', 'pca__copy', 'pca__iterated_power', 'pca__n_components', 
'pca__random_state', 'pca__svd_solver', 'pca__tol', 'pca__whiten', 'transformer_list'
, 'transformer_weights', 'univ_select', 'univ_select__k', 'univ_select__score_func']

# Set new value and Use combined features to transform dataset:
combined_features.set_params(univ_select__k=1)
X_features = combined_features.fit(X, y).transform(X)
>>> X.shape, X_features.shape
((150, 4), (150, 3))

#Or Do a gridsearch for best param of SVC 
svm = SVC(kernel="linear")
pipeline = Pipeline([("features", combined_features), ("svm", svm)])
param_grid = dict(features__pca__n_components=[1, 2, 3],
                  features__univ_select__k=[1, 2],
                  svm__C=[0.1, 1, 10])

grid_search = GridSearchCV(pipeline, param_grid=param_grid)
grid_search.fit(X, y)
best_pipeline = grid_search.best_estimator_
grid_search.best_params_
best_pipeline.score(X_test, y_test) #1.0




###Scikit - STEP7.0 - Only for classifier - Probability calibration
#When performing classification you often want not only to predict the class label, 
#but also obtain a probability of the respective label

#LogisticRegression returns well calibrated predictions by default as it directly optimizes log-loss. 
#In contrast, the other methods return biased probabilities
•GaussianNB tends to push probabilties to 0 or 1 
    (note the counts in the histograms). 
•RandomForestClassifier shows the opposite behavior: 
    the histograms show peaks at approximately 0.2 and 0.9 probability, 
    while probabilities close to 0 or 1 are very rare
•Linear Support Vector Classification (LinearSVC) 
    shows an even more sigmoid curve(S-curve)

##Solution - use CalibratedClassifierCV
sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method='sigmoid', cv=3)
    In case that cv='prefit' is passed to __init__, 
    it is assumed that base_estimator has been fitted already 
    and all data is used for calibration

#methods  
fit(X, y[, sample_weight])  Fit the calibrated model 
get_params([deep])          Get parameters for this estimator. 
predict(X)                  Predict the target of new samples. 
predict_proba(X)            Posterior probabilities of classification 
score(X, y[, sample_weight]) Returns the mean accuracy on the given test data and labels. 
set_params(\*\*params)      Set the parameters of this estimator. 


#Example 
import numpy as np

from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import log_loss

np.random.seed(0)

# Generate data
X, y = make_blobs(n_samples=1000, n_features=2, random_state=42,
                  cluster_std=5.0)
X_train, y_train = X[:600], y[:600]
X_valid, y_valid = X[600:800], y[600:800]
X_train_valid, y_train_valid = X[:800], y[:800]
X_test, y_test = X[800:], y[800:]

# Train uncalibrated random forest classifier on whole train and validation
# data and evaluate on test data
clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train_valid, y_train_valid)
clf_probs = clf.predict_proba(X_test)
score = log_loss(y_test, clf_probs)

# Train random forest classifier, calibrate on validation data and evaluate
# on test data
clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train, y_train)
clf_probs = clf.predict_proba(X_test)
sig_clf = CalibratedClassifierCV(clf, method="sigmoid", cv="prefit")
sig_clf.fit(X_valid, y_valid)
sig_clf_probs = sig_clf.predict_proba(X_test)
sig_score = log_loss(y_test, sig_clf_probs)



 
 
 
 

###Scikit - Generalized Linear Models - regression and classification
#generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables 
#that have error distribution models other than a normal distribution

#Below models are included 
#suffix CV means parameters are choosen by CrossValidation -for reducing overfitting 
#(there would be 'cv' argument in calling)
#Regularization - technique for solving overfitting by L1,L2 norms (probably along with CV to determin those parameters)

##Methods for all these models 
fit(X, y)                       Fit  model with coordinate descent 
get_params([deep])              Get parameters for this estimator. 
path(X, y[, l1_ratio, eps, n_alphas, …]) Compute elastic net path with coordinate descent 
predict(X)                      Predict using the linear model 
score(X, y[, sample_weight])    Returns the coefficient of determination R^2 of the prediction. 
set_params(**params)            Set the parameters of this estimator. 


#OLS
linear_model.LinearRegression([...])                Ordinary least squares Linear Regression. 

#LogisticRegression is for classification 
linear_model.LogisticRegression([penalty, ...])     Logistic Regression (aka logit, MaxEnt) classifier. 
linear_model.LogisticRegressionCV([Cs, ...])        Logistic Regression CV (aka logit, MaxEnt) classifier. 


#SGD(Stochastic gradient descent) has been successfully applied to large-scale and sparse machine learning problems 
#often encountered in text classification and natural language processing. 
#Given that the data is sparse, the classifiers in this module easily scale 
#to problems with more than 10^5 training examples and more than 10^5 features.
#cons: SGD is sensitive to feature scaling.
linear_model.SGDClassifier([loss, penalty, ...])    Linear classifiers (SVM, logistic regression, a.o.) with SGD(Stochastic gradient descent ) training. 
linear_model.SGDRegressor([loss, penalty, ...])     Linear model fitted by minimizing a regularized empirical loss with SGD 

#Perceptron:simple algorithm suitable for large scale learning
#slightly faster to train than SGD with the hinge loss 
#and that the resulting models are sparser.
linear_model.Perceptron([penalty, alpha, ...]) 

#Ridge:Ridge regression addresses some of the problems of Ordinary Least Squares 
#by imposing a penalty on the size of coefficients.
#the larger the value of alpha, the greater the amount of shrinkage 
#and thus the coefficients become more robust to collinearity(ie 2 or more features are corelated)
linear_model.Ridge([alpha, fit_intercept, ...]) L   inear least squares with l2 regularization. 
linear_model.RidgeClassifier([alpha, ...])          Classifier using Ridge regression. 
linear_model.RidgeClassifierCV([alphas, ...])       Ridge classifier with built-in cross-validation. 
linear_model.RidgeCV([alphas, ...])                 Ridge regression with built-in cross-validation. 
linear_model.BayesianRidge([n_iter, tol, ...])      Bayesian ridge regression 
#Automatic Relevance Determination (ARD)Regression is very similar to Bayesian Ridge Regression, 
#but can lead to sparser weights w 
linear_model.ARDRegression([n_iter, tol, ...])      Bayesian ARD regression. 


#ElasticNet:ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. 
#This combination allows for learning a sparse model where few of the weights 
#are non-zero like Lasso, while still maintaining the regularization properties of Ridge.
linear_model.ElasticNet([alpha, l1_ratio, ...])     Linear regression with combined L1 and L2 priors as regularizer. 
linear_model.ElasticNetCV([l1_ratio, eps, ...])     Elastic Net model with iterative fitting along a regularization path 

#lasso:The Lasso is a linear model that estimates sparse coefficients. 
#It is useful in some contexts due to its tendency to prefer solutions 
#with fewer parameter values, effectively reducing the number of variables 
#upon which the given solution is dependent.
#Lars:Least-angle regression (LARS) is also a regression algorithm for high-dimensional data
#cons: sensitive to noise 
linear_model.Lars([fit_intercept, verbose, ...])    Least Angle Regression model a.k.a. 
linear_model.LarsCV([fit_intercept, ...])           Cross-validated Least Angle Regression model 
linear_model.Lasso([alpha, fit_intercept, ...])     Linear Model trained with L1 prior as regularizer (aka the Lasso) 
linear_model.LassoCV([eps, n_alphas, ...])          Lasso linear model with iterative fitting along a regularization path 
linear_model.LassoLars([alpha, ...])                Lasso model fit with Least Angle Regression a.k.a. 
linear_model.LassoLarsCV([fit_intercept, ...])      Cross-validated Lasso, using the LARS algorithm 
linear_model.LassoLarsIC([criterion, ...])          Lasso model fit with Lars using BIC or AIC for model selection 

#OMP:algorithm for approximating the fit of a linear model 
#with constraints imposed on the number of non-zero coefficients 
linear_model.OrthogonalMatchingPursuit([...])       Orthogonal Matching Pursuit model (OMP) 
linear_model.OrthogonalMatchingPursuitCV([...])     Cross-validated Orthogonal Matching Pursuit model (OMP) 
linear_model.orthogonal_mp(X, y[, ...])             Orthogonal Matching Pursuit (OMP) 
linear_model.orthogonal_mp_gram(Gram, Xy[, ...])    Gram Orthogonal Matching Pursuit (OMP)

#The passive-aggressive algorithms are a family of algorithms for large-scale learning 
#with a regularization parameter C.
linear_model.PassiveAggressiveClassifier([...])     Passive Aggressive Classifier 
linear_model.PassiveAggressiveRegressor([C, ...])   Passive Aggressive Regressor 

#Multitask- that estimates sparse coefficients for multiple regression problems jointly: 
#Y is a 2D array, of shape (n_samples, n_tasks). 
linear_model.MultiTaskLasso([alpha, ...])           Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer 
linear_model.MultiTaskElasticNet([alpha, ...])      Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer 
linear_model.MultiTaskLassoCV([eps, ...])           Multi-task L1/L2 Lasso with built-in cross-validation. 
linear_model.MultiTaskElasticNetCV([...])           Multi-task L1/L2 ElasticNet with built-in cross-validation. 

#Randomized: The limitation of L1-based sparse models is that faced with a group of very correlated features, 
#they will select only one. 
#To mitigate this problem, it is possible to use randomization techniques, 
#reestimating the sparse model many times perturbing the design matrix or sub-sampling data 
#and counting how many times a given regressor is selected.
linear_model.RandomizedLasso([alpha, ...])          Randomized Lasso. 
linear_model.RandomizedLogisticRegression([...])    Randomized Logistic Regression 

#Robust to outliers and modeling errors 
#RANSAC is a non-deterministic algorithm producing only a reasonable result with a certain probability, 
#which is dependent on the number of iterations 
#HuberRegressor should be faster than RANSAC and Theil Sen unless the number of samples are very large,
# i.e n_samples >> n_features
#RANSAC is faster than Theil Sen and scales much better with the number of samples
#RANSAC will deal better with large outliers in the y direction (most common situation)
#Theil Sen will cope better with medium-size outliers in the X direction, 
#but this property will disappear in large dimensional settings.
#When in doubt, use RANSAC
linear_model.RANSACRegressor([...])                 RANSAC (RANdom SAmple Consensus) algorithm. 
linear_model.HuberRegressor([epsilon, ...])         Linear regression model that is robust to outliers. 
linear_model.TheilSenRegressor([...])               Theil-Sen Estimator: robust multivariate regression model. 

#misc routines in linear_model 
linear_model.lars_path(X, y[, Xy, Gram, ...])       Compute Least Angle Regression or Lasso path using LARS algorithm [1] 
linear_model.lasso_path(X, y[, eps, ...])           Compute Lasso path with coordinate descent 
linear_model.lasso_stability_path(X, y[, ...])      Stability path based on randomized Lasso estimates 
linear_model.logistic_regression_path(X, y)         Compute a Logistic Regression model for a list of regularization parameters. 



##Scikit - GLM - OLS 
sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, 
            copy_X=True, n_jobs=1)

#Example 
from sklearn import linear_model
clf = linear_model.LinearRegression()
clf.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
>>> clf.coef_
array([ 0.5,  0.5])

##Scikit - GLM - Ridge Regression
#Ridge regression addresses some of the problems of Ordinary Least Squares 
#by imposing a penalty on the size of coefficients(regularization)
#The ridge coefficients minimize a penalized residual sum of squares

sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)

#Example 
from sklearn import linear_model
clf = linear_model.Ridge (alpha = .5)
clf.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) 
>>> clf.coef_
array([ 0.34545455,  0.34545455])
>>> clf.intercept_ 
0.13636...

##Scikit - GLM - Ridge Regression with CV 
#Automatic Setting the regularization(minimizing over-fitting) parameter - alpha
#selects alphas which minimises MSE 
sklearn.linear_model.RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False)
    alphas : numpy array of shape [n_alphas]
            Array of alpha values to try. 
            Small positive values of alpha improve the conditioning of the problem 
            and reduce the variance of the estimates. 
            Alpha corresponds to C**-1 in other linear models 
            such as LogisticRegression or LinearSVC
#Example 
from sklearn import linear_model
clf = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
>>> clf.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       
RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
    normalize=False)
>>> clf.alpha_              #final alpha                         
0.1

##Scikit - GLM -  Lasso 
#linear model that estimates sparse coefficients(minimum number of coefficients)
#coordinate descent algorithm 
sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
    alpha : float, optional
            Constant that multiplies the L1 term. Defaults to 1.0. 
            alpha = 0 is equivalent to an ordinary least square, 
            solved by the LinearRegression object

#lasso_path() :computes the coefficients along the full path of possible values of alpha 
#The alpha parameter controls the degree of sparsity of the coefficients estimated.
#select alpha where MSE is minimum (usually small number , alpha)
from sklearn import linear_model
clf = linear_model.Lasso(alpha = 0.1)
>>> clf.fit([[0, 0], [1, 1]], [0, 1])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection='cyclic', tol=0.0001, warm_start=False)
>>> clf.predict([[1, 1]])
array([ 0.8])

##Scikit - GLM -  Least Angle Regression -  for high-dimensional data
#It is numerically efficient in contexts where p >> n 
#(i.e., when the number of dimensions/coefficients is significantly greater than the number of points)

sklearn.linear_model.LassoLars(alpha=1.0, fit_intercept=True, verbose=False, 
        normalize=True, precompute='auto', max_iter=500, eps=2.2204460492503131e-16, copy_X=True, fit_path=True, positive=False)
#Example 
from sklearn import linear_model
clf = linear_model.LassoLars(alpha=.1)
>>> clf.fit([[0, 0], [1, 1]], [0, 1])  
LassoLars(alpha=0.1, copy_X=True, eps=..., fit_intercept=True,
     fit_path=True, max_iter=500, normalize=True, positive=False,
     precompute='auto', verbose=False)
>>> clf.coef_    
array([ 0.717157...,  0.        ])



##Scikit - GLM -  Lasso(with LARS) with CV 
#For high-dimensional datasets with many collinear regressors, 
#LassoCV is  preferable. 
#LassoLarsCV has the advantage of exploring more relevant values of alpha parameter, 
#it is often faster than LassoCV for small samples 

sklearn.linear_model.LassoCV(eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, 
            normalize=False, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, positive=False, random_state=None, selection='cyclic')

sklearn.linear_model.LassoLarsCV(fit_intercept=True, verbose=False, max_iter=500, 
            normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=1, eps=2.2204460492503131e-16, copy_X=True, positive=False)



##Scikit - GLM -  Lasso(with LARS) with Information-criteria (aic, bic)based alpha selection
sklearn.linear_model.LassoLarsIC(criterion='aic', fit_intercept=True, verbose=False, 
    normalize=True, precompute='auto', max_iter=500, eps=2.2204460492503131e-16, copy_X=True, positive=False

#Example 
from sklearn import linear_model
clf = linear_model.LassoLarsIC(criterion='bic')
clf.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])
LassoLarsIC(copy_X=True, criterion='bic', eps=..., fit_intercept=True,
      max_iter=500, normalize=True, positive=False, precompute='auto',
      verbose=False)
>>> print(clf.coef_) 
[ 0.  -1.11...]


##Scikit - GLM -  Elastic Net 
#model is  trained with L1 and L2 prior as regularizer. 
#Resulting a sparse model(min # of coefficients), 
#and similar regularization(minimizing ovefitting) properties of Ridge. 

#control the convex combination of L1 and L2 using the l1_ratio parameter.

#Elastic-net is useful when there are multiple features which are correlated with one another. 
#Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.
sklearn.linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=True, 
    normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')

    
#ElasticNetCV can be used to set the parameters alpha (alpha) and l1_ratio (rho) by cross-validation
class sklearn.linear_model.ElasticNetCV(l1_ratio=0.5, eps=0.001, n_alphas=100, 
            alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=1, positive=False, random_state=None, selection='cyclic')

#Example 
from sklearn.linear_model import ElasticNetCV
from sklearn.datasets import make_regression

X, y = make_regression(n_features=2, random_state=0)
regr = ElasticNetCV(cv=5, random_state=0)
>>> regr.fit(X, y)
ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,
       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=1,
       normalize=False, positive=False, precompute='auto', random_state=0,
       selection='cyclic', tol=0.0001, verbose=0)
>>> print(regr.alpha_) 
0.19947279427
>>> print(regr.intercept_) 
0.398882965428
>>> print(regr.predict([[0, 0]])) 
[ 0.39888297]

            
       

##Scikit - GLM -  Multi-task Lasso - estimates sparse coefficients for multiple regression problems jointly
# here y is a 2D array, of shape (n_samples, n_tasks). 
#The constraint is that the selected features are the same for all the regression problems, also called tasks.
sklearn.linear_model.MultiTaskLasso(alpha=1.0, fit_intercept=True, normalize=False, 
    copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')

#Example 
from sklearn import linear_model
clf = linear_model.MultiTaskLasso(alpha=0.1)
>>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])
MultiTaskLasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
        normalize=False, random_state=None, selection='cyclic', tol=0.0001,
        warm_start=False)
>>> print(clf.coef_)
[[ 0.89393398  0.        ]
 [ 0.89393398  0.        ]]
>>> print(clf.intercept_)
[ 0.10606602  0.10606602]



##Scikit - GLM -  Bayesian Regression 
#when regularization parameters is not hand set, 
#but found based on data automatically 

sklearn.linear_model.BayesianRidge(n_iter=300, tol=0.001, alpha_1=1e-06, 
     alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False)

#Example 
from sklearn import linear_model
X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]
Y = [0., 1., 2., 3.]
clf = linear_model.BayesianRidge()
>>> clf.fit(X, Y)
BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,
       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,
       normalize=False, tol=0.001, verbose=False)

>>> clf.predict ([[1, 0.]])
array([ 0.50000013])

#The weights w of the model can be access:
>>> clf.coef_
array([ 0.49999993,  0.49999993])


##Scikit - GLM -  Logistic regression 
#linear model for classification rather than regression

sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, 
     fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, 
     solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)

#Case                                Solver
Small dataset or L1 penalty         'liblinear' 
Multinomial loss                    'lbfgs' or newton-cg' 
Large dataset                       'sag' or use SGDClassifier with 'log' loss.

#Methods 
decision_function(X)        Predict confidence scores for samples. 
densify()                   Convert coefficient matrix to dense array format. 
fit(X, y[, sample_weight])  Fit the model according to the given training data. 
get_params([deep])          Get parameters for this estimator. 
predict(X)                  Predict class labels for samples in X. 
predict_log_proba(X)        Log of probability estimates. 
predict_proba(X)            Probability estimates. 
score(X, y[, sample_weight]) Returns the mean accuracy on the given test data and labels. 
set_params(**params)        Set the parameters of this estimator. 
sparsify()                  Convert coefficient matrix to sparse format. 

#Example 
mport numpy as np
import matplotlib.pyplot as plt

from sklearn import linear_model, decomposition, datasets
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

logistic = linear_model.LogisticRegression()

pca = decomposition.PCA()
pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])

digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target

# Plot the PCA spectrum
pca.fit(X_digits)

plt.figure(1, figsize=(4, 3))
plt.clf()
plt.axes([.2, .2, .7, .7])
plt.plot(pca.explained_variance_, linewidth=2)
plt.axis('tight')
plt.xlabel('n_components')
plt.ylabel('explained_variance_')

# Prediction
n_components = [20, 40, 64]
Cs = np.logspace(-4, 4, 3)

# Parameters of pipelines can be set using '__' separated parameter names:
estimator = GridSearchCV(pipe,dict(pca__n_components=n_components, logistic__C=Cs))
estimator.fit(X_digits, y_digits)

plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,
            linestyle=':', label='n_components chosen')
plt.legend(prop=dict(size=12))
plt.show()



##Scikit - GLM -   Stochastic Gradient Descent - SGD - linera model 
#used  when the number of samples (and the number of features) is very large. 
#Use partial_fit()  allows only/out-of-core learning(ie many data points are in disk, not main memory)

sklearn.linear_model.SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, 
            l1_ratio=0.15, fit_intercept=True, n_iter=5, shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, class_weight=None, warm_start=False, average=False)
sklearn.linear_model.SGDRegressor(loss='squared_loss', penalty='l2', alpha=0.0001, 
            l1_ratio=0.15, fit_intercept=True, n_iter=5, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, warm_start=False, average=False)

#Methods
decision_function(X)    Predict confidence scores for samples. 
densify()               Convert coefficient matrix to dense array format. 
fit(X, y[, coef_init, intercept_init, …]) Fit linear model with Stochastic Gradient Descent. 
get_params([deep])      Get parameters for this estimator. 
partial_fit(X, y[, classes, sample_weight]) Fit linear model with Stochastic Gradient Descent. 
predict(X)              Predict class labels for samples in X. 
score(X, y[, sample_weight]) Returns the mean accuracy on the given test data and labels. 
set_params(*args, **kwargs)  
sparsify()              Convert coefficient matrix to sparse format. 

#Example 
>>> import numpy as np
>>> from sklearn import linear_model
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> Y = np.array([1, 1, 2, 2])
>>> clf = linear_model.SGDClassifier()
>>> clf.fit(X, Y)

#Example  
import numpy as np
from sklearn import linear_model
n_samples, n_features = 10, 5
np.random.seed(0)
y = np.random.randn(n_samples)
X = np.random.randn(n_samples, n_features)
clf = linear_model.SGDRegressor()
>>> clf.fit(X, y)
SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,
             fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',
             loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,
             random_state=None, shuffle=True, verbose=0, warm_start=False)


##Scikit - GLM -  Robust regressor 
    •RANSAC is faster, and scales much better with the number of samples
    •RANSAC will deal better with large outliers in the y direction (most common situation)
    •Theil Sen will cope better with medium-size outliers in the X direction, but this property will disappear in large dimensional settings.
#When in doubt, use RANSAC

#RANSAC: RANdom SAmple Consensus
# fits a model from random subsets of inliers from the complete data set

sklearn.linear_model.RANSACRegressor(base_estimator=None, min_samples=None, 
    residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, residual_metric=None, random_state=None)

#Theil-Sen estimator: generalized-median-based estimator
# uses a generalization of the median in multiple dimensions. 
#It is thus robust to multivariate outliers. 
#Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. 

sklearn.linear_model.TheilSenRegressor(fit_intercept=True, copy_X=True, 
        max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=1, verbose=False)

#Example of RANSAC 
import numpy as np
from matplotlib import pyplot as plt

from sklearn import linear_model, datasets

n_samples = 1000
n_outliers = 50
X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,
                                      n_informative=1, noise=10,
                                      coef=True, random_state=0)

# Add outlier data
np.random.seed(0)
X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))
y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)

# Fit line using all data
model = linear_model.LinearRegression()
model.fit(X, y)

# Robustly fit linear model with RANSAC algorithm
model_ransac = linear_model.RANSACRegressor(linear_model.LinearRegression())
model_ransac.fit(X, y)
inlier_mask = model_ransac.inlier_mask_
outlier_mask = np.logical_not(inlier_mask)

# Predict data of estimated models
line_X = np.arange(-5, 5)
line_y = model.predict(line_X[:, np.newaxis])
line_y_ransac = model_ransac.predict(line_X[:, np.newaxis])

# Compare estimated coefficients
print("Estimated coefficients (true, normal, RANSAC):")
print(coef, model.coef_, model_ransac.estimator_.coef_)

plt.plot(X[inlier_mask], y[inlier_mask], '.g', label='Inliers')
plt.plot(X[outlier_mask], y[outlier_mask], '.r', label='Outliers')
plt.plot(line_X, line_y, '-k', label='Linear regressor')
plt.plot(line_X, line_y_ransac, '-b', label='RANSAC regressor')
plt.legend(loc='lower right')
plt.show()




##Scikit - GLM -  Polynomial regression:
#extending linear models with higher degree basis functions
#for example ,terms with x^2, XY etc - these are still linear model
#use PolynomialFeatures

#Example 
from sklearn.preprocessing import PolynomialFeatures
import numpy as np
X = np.arange(6).reshape(3, 2)
>>> X
array([[0, 1],
       [2, 3],
       [4, 5]])
poly = PolynomialFeatures(degree=2)
>>> poly.fit_transform(X)
array([[  1.,   0.,   1.,   0.,   0.,   1.],
       [  1.,   2.,   3.,   4.,   6.,   9.],
       [  1.,   4.,   5.,  16.,  20.,  25.]])


The features of X have been transformed from 
#[x_1, x_2] to [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]

#Using Pipeline 

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import numpy as np
model = Pipeline([('poly', PolynomialFeatures(degree=3)),
                    ('linear', LinearRegression(fit_intercept=False))])
# fit to an order-3 polynomial data
x = np.arange(5)
y = 3 - 2 * x + x ** 2 - x ** 3
model = model.fit(x[:, np.newaxis], y)
>>> model.named_steps['linear'].coef_
array([ 3., -2.,  1., -1.])

#Use interaction_only=True when it's not necessary to include higher powers of any single feature, 
#but only interaction features that multiply together at most d distinct features. 

#Example with Perceptron
from sklearn.linear_model import Perceptron
from sklearn.preprocessing import PolynomialFeatures
import numpy as np
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = X[:, 0] ^ X[:, 1]
X = PolynomialFeatures(interaction_only=True).fit_transform(X)
>>> X
array([[ 1.,  0.,  0.,  0.],
       [ 1.,  0.,  1.,  0.],
       [ 1.,  1.,  0.,  0.],
       [ 1.,  1.,  1.,  1.]])
clf = Perceptron(fit_intercept=False, n_iter=10, shuffle=False).fit(X, y)
>>> clf.score(X, y)
1.0





###Scikit - Support Vector Machines - supervised learning methods - classification, regression and outliers detection

#Plot each data item(each row of X) as a point in n-dimensional space (where n is number of features (columns of X))
#with the value of each feature being the value of a particular coordinate.
#(these co-ordinates are known as Support Vectors)

#For example, for two features like Height and Hair length of an individual, 
#we plot these two variables in two dimensional space where each point has two co-ordinates 

#Next, find one line that splits these points 
#such that the distances from the closest point in each of the two groups will be farthest away.
#This line is classifier. 
#Depending on where the testing data lands on either side of the line, that's what class of the new data 

#In Scikit - different Kernel functions can be specified for the decision function
#Note If the number of features is much greater than the number of samples, 
#the method is likely to give poor performances

#scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray) 
#and sparse (any scipy.sparse)

#kernel function: It takes two inputs and returns a number interpreting how similar they are. 

#SVC and NuSVC are similar methods, but accept slightly different sets of parameters 
#C-Support Vector Classification.
class sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, 
        shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=None)
    kernel: 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable
    C : float, optional (default=1.0),Penalty parameter C of the error term

#Nu-Support Vector Classification.
class sklearn.svm.NuSVC(nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, 
        verbose=False, max_iter=-1, decision_function_shape=None, random_state=None)
    nu : float, optional (default=0.5),An upper bound on the fraction of training errors 
        and a lower bound of the fraction of support vectors. 
        Should be in the interval (0, 1].

#LinearSVC is another implementation of Support Vector Classification for the case of a linear kernel
class sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, 
        class_weight=None, verbose=0, random_state=None, max_iter=1000)
    penalty : string, 'l1' or 'l2' (default='l2')


#Example 
from sklearn import svm
X = [[0, 0], [1, 1], [5,5]]
y = [0, 1, 2]
clf = svm.SVC()
clf.fit(X, y)  
#predict 
>>>clf.predict([[2., 2.]])  #where does X=[2,2] get classify into? y= 1
array([1])

>>>clf.predict([[6., 6.]])
array([2])

#SVMs decision function depends on some subset of the training data, called the support vectors. 
#access those by  support_vectors_, support_ and n_support:

# get support vectors
>>> clf.support_vectors_
array([[ 0.,  0.],
       [ 1.,  1.]])
# get indices of support vectors
>>> clf.support_ 
array([0, 1]...)
# get number of support vectors for each class
>>> clf.n_support_ 
array([1, 1]...)


##Multi-class (multinomial) classification 
#where class is more than two (ie values of Y are more than two)
#Generally, classification estimators are binary, to convert to multi-class,
#use strategies 
1. One-vs.-rest (or one-vs.-all, OvA or OvR, one-against-all, OAA) 
    trains a single classifier per class, 
    with the samples of that class as positive samples and all other samples as negatives
2. One-vs.-one(OvO)
    trains K (K − 1) / 2 binary classifiers for a K-way multiclass problem; 
    each receives the samples of a pair of classes from the original training set, 
    and must learn to distinguish these two classes. 
    At prediction time, a voting scheme is applied: 
    all K (K − 1) / 2 classifiers are applied to an unseen sample 
    and the class that got the highest number of "+1" predictions gets predicted by the combined classifier

#SVC and NuSVC implement both these strageies 
X = [[0], [1], [2], [3]]
Y = [0, 1, 2, 3]
clf = svm.SVC(decision_function_shape='ovo') #one vs one 
>>> clf.fit(X, Y) 
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
    
#decision_function gives outputs from all classifiers 
#predict - gives the final class value 
dec = clf.decision_function([[1]])
>>> dec.shape[1] # 4 classes: 4*3/2 = 6
6
clf.decision_function_shape = "ovr"
dec = clf.decision_function([[1]])
>>> dec.shape[1] # 4 classes
4

#LinearSVC implements 'one-vs-the-rest' multi-class strategy, 
#thus training n_class models
lin_clf = svm.LinearSVC()
>>> lin_clf.fit(X, Y) 
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)
dec = lin_clf.decision_function([[1]])
>>> dec.shape[1]
4

##Scores and probabilities

#decision_function gives per-class(classifier) scores for each sample 
#(or a single score per sample in the binary case). 

#constructor option: probability=True,
#predict_proba() and predict_log_proba() gives probabilities of each sample by each classifiers 

 
##Unbalanced problems
# to give more importance to certain classes or certain individual samples 
#Use  class_weight and sample_weight 

#SVC (but not NuSVC) implement class_weight in the fit method. 
#It's a dictionary of the form {class_label : value}, 
#where value is a floating point number > 0 that sets the parameter C of class class_label to C * value.

#Example - Plot different SVM classifiers in the iris dataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset
y = iris.target

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)
lin_svc = svm.LinearSVC(C=C).fit(X, y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# title for the plots
titles = ['SVC with linear kernel',
          'LinearSVC (linear kernel)',
          'SVC with RBF kernel',
          'SVC with polynomial (degree 3) kernel']


for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):
    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    plt.subplot(2, 2, i + 1)
    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.xticks(())
    plt.yticks(())
    plt.title(titles[i])

plt.show()



##SVM-  Regression - use SVR, NuSVR and LinearSVR

sklearn.svm.SVR(kernel='rbf', degree=3, gamma='auto', coef0=0.0, tol=0.001, C=1.0, 
        epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)
        
sklearn.svm.NuSVR(nu=0.5, C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, 
        shrinking=True, tol=0.001, cache_size=200, verbose=False, max_iter=-1)

sklearn.svm.LinearSVR(epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', 
        fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000)
 
#Example 
from sklearn import svm
X = [[0, 0], [2, 2]]
y = [0.5, 2.5]
clf = svm.SVR()
>>> clf.fit(X, y) 
SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
>>> clf.predict([[1, 1]])
array([ 1.5])






###Scikit - KNN (K- Nearest Neighbors) - neighborSearch, classification and regression
http://scikit-learn.org/stable/modules/neighbors.html

#A algorithm that stores all available cases and classifies new cases 
#by a majority vote of its k neighbors. 

#neighbors measures 'near' by a distance function,
#- eg Euclidean, Manhattan, Minkowski and Hamming distance

#Euclidean, Manhattan, Minkowski distance functions are used for continuous function 
#Hamming distance for categorical variables. 

#If K = 1, then the case is simply assigned to the class of its nearest neighbor. 
#choosing K turns out to be a challenge while performing KNN modeling.

#scikit handle either Numpy arrays or scipy.sparse matrices as input. 
#For dense matrices, a large number of possible distance metrics are supported. 
#For sparse matrices, arbitrary Minkowski metrics are supported for searches

#if two neighbors, neighbor k+1 and k, have identical distances but different labels, 
#the results will depend on the ordering of the training data.


##Unsupervised Nearest Neighbors Search - NearestNeighbors
class sklearn.neighbors.NearestNeighbors(n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=1, **kwargs)[source]
    Unsupervised learner for implementing neighbor searches.
    n_neighbors : int, optional (default = 5)
        Number of neighbors to use by default for kneighbors queries.
    radius : float, optional (default = 1.0)
        Range of parameter space to use by default for radius_neighbors queries.
    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
        Algorithm used to compute the nearest neighbors:
        •'ball_tree' will use BallTree
        •'kd_tree' will use KDTree
        •'brute' will use a brute-force search.
        •'auto' will attempt to decide the most appropriate algorithm based on the values passed to fit method.
        Note: fitting on sparse input will override the setting of this parameter,
        using brute force.
    leaf_size : int, optional (default = 30)
        Leaf size passed to BallTree or KDTree. 
        This can affect the speed of the construction and query, 
        as well as the memory required to store the tree. 
        The optimal value depends on the nature of the problem.
    metric : string or callable, default 'minkowski'
        metric to use for distance computation
        •from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']
        •from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']
    p : integer, optional (default = 2)
        Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
    metric_params : dict, optional (default = None)
        Additional keyword arguments for the metric function.
    n_jobs : int, optional (default = 1)
        The number of parallel jobs to run for neighbors search. 
        If -1, then the number of jobs is set to the number of CPU cores. 
        Affects only kneighbors and kneighbors_graph methods.
    
#Methods
fit(X[, y])                 Fit the model using X as training data 
get_params([deep])          Get parameters for this estimator. 
set_params(**params)                          Set the parameters of this estimator. 
kneighbors([X, n_neighbors, return_distance]) 
    Finds the K-neighbors of a point. 
    Parameters:
    X : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric == 'precomputed'
        The query point or points. 
        If not provided, neighbors of each indexed point are returned. 
        In this case, the query point is not considered its own neighbor.
    n_neighbors : int
        Number of neighbors to get (default is the value passed to the constructor).
    return_distance : boolean, optional. Defaults to True.
        If False, distances will not be returned
    Returns:
    dist : array
        Array representing the lengths to points, 
        only present if return_distance=True
    ind : array
        Indices of the nearest points in the population matrix.
#Examples
import numpy as np
from sklearn.neighbors import NearestNeighbors
samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]
neigh = NearestNeighbors(2, 0.4)
>>> neigh.fit(samples)  
NearestNeighbors(...)
#find two neighbors, returns gives indexes  
>>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)
array([[2, 0]]...)
>>> nbrs = neigh.radius_neighbors([[0, 0, 1.3]], 0.4, return_distance=False)
>>> np.asarray(nbrs[0][0])
array(2)
#Example 
samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
from sklearn.neighbors import NearestNeighbors
neigh = NearestNeighbors(n_neighbors=1)
neigh.fit(samples) 
>>> print(neigh.kneighbors([[1., 1., 1.]])) 
(array([[ 0.5]]), array([[2]]...))  #(distance, indices)

#[1., 1., 1.] distance is [0.5] from element [2] (ie third element of samples)
#for multiple points 
 X = [[0., 1., 0.], [1., 0., 1.]]
>>> neigh.kneighbors(X, return_distance=False) 
array([[1],   #nearest point is 3rd element of samples 
       [2]]...)

       

kneighbors_graph([X, n_neighbors, mode])      Computes the (weighted) graph of k-Neighbors for points in X 
Returns:
A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]
    n_samples_fit is the number of samples in the fitted data 
    A[i, j] is assigned the weight of edge that connects i to j.
#Example 
X = [[0], [3], [1]]
from sklearn.neighbors import NearestNeighbors
neigh = NearestNeighbors(n_neighbors=2)
neigh.fit(X) 
A = neigh.kneighbors_graph(X)
>>> A.toarray()
array([[ 1.,  0.,  1.],
       [ 0.,  1.,  1.],
       [ 1.,  0.,  1.]])
       
       

radius_neighbors([X, radius, return_distance])
    Finds the neighbors within a given radius of a point or points. 
    radius : float
        Limiting distance of neighbors to return. (default is the value passed to the constructor).
    Returns:
    dist : array, shape (n_samples,) of arrays
        Array representing the distances to each point, 
        only present if return_distance=True.
    ind : array, shape (n_samples,) of arrays
        An array of arrays of indices of the approximate nearest points 
        from the population matrix that lie within a ball of size radius around the query points.
#Example 
import numpy as np
samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
from sklearn.neighbors import NearestNeighbors
neigh = NearestNeighbors(radius=1.6)
neigh.fit(samples) 
rng = neigh.radius_neighbors([[1., 1., 1.]])
>>> print(np.asarray(rng[0][0])) 
[ 1.5  0.5]
>>> print(np.asarray(rng[1][0])) 
[1 2]



radius_neighbors_graph([X, radius, mode])     
    Computes the (weighted) graph of Neighbors for points in X 
    Returns:
    A : sparse matrix in CSR format, shape = [n_samples, n_samples]
        A[i, j] is assigned the weight of edge that connects i to j.
#Example 
X = [[0], [3], [1]]
from sklearn.neighbors import NearestNeighbors
neigh = NearestNeighbors(radius=1.5)
neigh.fit(X) 
A = neigh.radius_neighbors_graph(X)
>>> A.toarray()
array([[ 1.,  0.,  1.],
       [ 0.,  1.,  0.],
       [ 1.,  0.,  1.]])

       
#Complete example 
from sklearn.neighbors import NearestNeighbors
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
>>> distances, indices = nbrs.kneighbors(X)
>>> indices                                           
array([[0, 1],    #K=2, hence nearest to index 0 and 1 
       [1, 0],
       [2, 1],
       [3, 4],
       [4, 3],
       [5, 4]]...)
>>> distances
array([[ 0.        ,  1.        ],   #distance from those indices 
       [ 0.        ,  1.        ],
       [ 0.        ,  1.41421356],
       [ 0.        ,  1.        ],
       [ 0.        ,  1.        ],
       [ 0.        ,  1.41421356]])


>>> nbrs.kneighbors_graph(X).toarray()
array([[ 1.,  1.,  0.,  0.,  0.,  0.],  #1 means nearest to that element of samples 
       [ 1.,  1.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  1.,  1.,  0.],
       [ 0.,  0.,  0.,  1.,  1.,  0.],
       [ 0.,  0.,  0.,  0.,  1.,  1.]])


       
       
##Nearest Neighbors Classification - supervised learning  

sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', 
        algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)
    Based on number of neighbors         
sklearn.neighbors.RadiusNeighborsClassifier(radius=1.0, weights='uniform', 
        algorithm='auto', leaf_size=30, p=2, metric='minkowski', outlier_label=None, metric_params=None, **kwargs)
    Based on radius 
    Parameters 
    weights : str or callable
        weight function used in prediction. 
        Possible values:
            •'uniform' : uniform weights. All points in each neighborhood are weighted equally.
            •'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
            •[callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.
        Uniform weights are used by default.
    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
    Algorithm used to compute the nearest neighbors:
        •'ball_tree' will use BallTree
        •'kd_tree' will use KDTree
        •'brute' will use a brute-force search.
        •'auto' will attempt to decide the most appropriate algorithm based on the values passed to fit method.
    Note: fitting on sparse input will override the setting of this parameter, using brute force.

#Methods
fit(X, y)           Fit the model using X as training data and y as target values 
get_params([deep])  Get parameters for this estimator. 
predict(X)          Predict the class labels for the provided data 
radius_neighbors([X, radius, return_distance]) Finds the neighbors within a given radius of a point or points. 
radius_neighbors_graph([X, radius, mode])      Computes the (weighted) graph of Neighbors for points in X 
score(X, y[, sample_weight])    Returns the mean accuracy on the given test data and labels. 
set_params(**params)            Set the parameters of this estimator. 

#Examples
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import RadiusNeighborsClassifier
>>> neigh = RadiusNeighborsClassifier(radius=1.0)
>>> neigh.fit(X, y) 
RadiusNeighborsClassifier(...)
>>> print(neigh.predict([[1.5]]))
[0]
#Example 
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsClassifier
>>> neigh = KNeighborsClassifier(n_neighbors=3)
>>> neigh.fit(X, y) 
KNeighborsClassifier(...)
>>> print(neigh.predict([[1.1]]))
[0]
>>> print(neigh.predict_proba([[0.9]]))
[[ 0.66666667  0.33333333]]


 
##Nearest Neighbors Regression

sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, weights='uniform', 
        algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)
    Based on number of neighbors 
sklearn.neighbors.RadiusNeighborsRegressor(radius=1.0, weights='uniform', 
        algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, **kwargs)
    Based on radius 
    Parameters 
    weights : str or callable
        weight function used in prediction. 
        Possible values:
            •'uniform' : uniform weights. All points in each neighborhood are weighted equally.
            •'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
            •[callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.
        Uniform weights are used by default.
    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
    Algorithm used to compute the nearest neighbors:
        •'ball_tree' will use BallTree
        •'kd_tree' will use KDTree
        •'brute' will use a brute-force search.
        •'auto' will attempt to decide the most appropriate algorithm based on the values passed to fit method.
    Note: fitting on sparse input will override the setting of this parameter, using brute force.

#Methods
fit(X, y)           Fit the model using X as training data and y as target values 
get_params([deep])  Get parameters for this estimator. 
predict(X)          Predict the class labels for the provided data 
radius_neighbors([X, radius, return_distance]) Finds the neighbors within a given radius of a point or points. 
radius_neighbors_graph([X, radius, mode])      Computes the (weighted) graph of Neighbors for points in X 
score(X, y[, sample_weight])    Returns the mean accuracy on the given test data and labels. 
set_params(**params)            Set the parameters of this estimator. 

#Example 
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import RadiusNeighborsRegressor
>>> neigh = RadiusNeighborsRegressor(radius=1.0)
>>> neigh.fit(X, y) 
RadiusNeighborsRegressor(...)
>>> print(neigh.predict([[1.5]]))
[ 0.5]
#Example 
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsRegressor
>>> neigh = KNeighborsRegressor(n_neighbors=2)
>>> neigh.fit(X, y) 
KNeighborsRegressor(...)
>>> print(neigh.predict([[1.5]]))
[ 0.5]



#Example - Demonstrate the resolution of a regression problem using a k-Nearest Neighbor and the interpolation of the target using both barycenter and constant weights
# Generate sample data
import numpy as np
import matplotlib.pyplot as plt
from sklearn import neighbors

np.random.seed(0)
X = np.sort(5 * np.random.rand(40, 1), axis=0)
T = np.linspace(0, 5, 500)[:, np.newaxis]
y = np.sin(X).ravel()

# Add noise to targets
y[::5] += 1 * (0.5 - np.random.rand(8))

# Fit regression model
n_neighbors = 5

for i, weights in enumerate(['uniform', 'distance']):
    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)
    y_ = knn.fit(X, y).predict(T)

    plt.subplot(2, 1, i + 1)
    plt.scatter(X, y, c='k', label='data')
    plt.plot(T, y_, c='g', label='prediction')
    plt.axis('tight')
    plt.legend()
    plt.title("KNeighborsRegressor (k = %i, weights = '%s')" % (n_neighbors,
                                                                weights))

plt.show()





 
###SciKit - Naive Bayes - classification - Bayes' theorem
http://scikit-learn.org/stable/modules/naive_bayes.html

#an assumption of independence between predictors/features 

#Gaussian Naive Bayes- Gaussian Naive Bayes algorithm for classification
class sklearn.naive_bayes.GaussianNB(priors=None)

#Methods
fit(X, y[, sample_weight])  Fit Gaussian Naive Bayes according to X, y 
get_params([deep])          Get parameters for this estimator. 
partial_fit(X, y[, classes, sample_weight]) Incremental fit on a batch of samples. 
predict(X)                  Perform classification on an array of test vectors X. 
predict_log_proba(X)        Return log-probability estimates for the test vector X. 
predict_proba(X)            Return probability estimates for the test vector X. 
score(X, y[, sample_weight]) Returns the mean accuracy on the given test data and labels. 
set_params(**params)        Set the parameters of this estimator. 



#Example 
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
Y = np.array([1, 1, 1, 2, 2, 2])
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X, Y)
>>> print(clf.predict([[-0.8, -1]]))
[1]
clf_pf = GaussianNB()
clf_pf.partial_fit(X, Y, np.unique(Y))
>>> print(clf_pf.predict([[-0.8, -1]]))
[1]


#Example with iris data 

from sklearn import datasets
iris = datasets.load_iris()
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)
>>> print("Number of mislabeled points out of a total %d points : %d"
    % (iris.data.shape[0],(iris.target != y_pred).sum()))
Number of mislabeled points out of a total 150 points : 6



##multiclass/Multinomial Naive Bayes
sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)
    Used in text classification (where the data are typically represented as word vector counts)
    alpha : float, optional (default=1.0),
        Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).

#Methods
fit(X, y[, sample_weight])  Fit Gaussian Naive Bayes according to X, y 
get_params([deep])          Get parameters for this estimator. 
partial_fit(X, y[, classes, sample_weight]) Incremental fit on a batch of samples. 
predict(X)                  Perform classification on an array of test vectors X. 
predict_log_proba(X)        Return log-probability estimates for the test vector X. 
predict_proba(X)            Return probability estimates for the test vector X. 
score(X, y[, sample_weight]) Returns the mean accuracy on the given test data and labels. 
set_params(**params)        Set the parameters of this estimator. 

#Exmaple 

import numpy as np
X = np.random.randint(5, size=(6, 100))   #6x100 random with 0 5 
y = np.array([1, 2, 3, 4, 5, 6])          #target for each row 
from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB()
clf.fit(X, y)
>>> print(clf.predict(X[2:3]))     #row index 2 of X
[3]


##Bernoulli Naive Bayes
sklearn.naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)
    For data that is distributed according to multivariate Bernoulli distributions; 
    i.e., there may be multiple features 
    but each one is assumed to be a binary-valued (Bernoulli, boolean) variable
    binarize : float or None, optional,
        Threshold for binarizing (mapping to booleans) of sample features. 
        If None, input is presumed to already consist of binary vectors.

#Methods
fit(X, y[, sample_weight])  Fit Gaussian Naive Bayes according to X, y 
get_params([deep])          Get parameters for this estimator. 
partial_fit(X, y[, classes, sample_weight]) Incremental fit on a batch of samples. 
predict(X)                  Perform classification on an array of test vectors X. 
predict_log_proba(X)        Return log-probability estimates for the test vector X. 
predict_proba(X)            Return probability estimates for the test vector X. 
score(X, y[, sample_weight]) Returns the mean accuracy on the given test data and labels. 
set_params(**params)        Set the parameters of this estimator. 

#Example 
import numpy as np
X = np.random.randint(2, size=(6, 100))
Y = np.array([1, 2, 3, 4, 4, 5])
from sklearn.naive_bayes import BernoulliNB
clf = BernoulliNB()
clf.fit(X, Y)
>>> print(clf.predict(X[2:3]))
[3]





###Scikit - Clustering 

#Method name            Parameters                  Scalability                               Usecase                                                                     Geometry (metric used)
K-Means                 number of clusters          Very large n_samples,                     General-purpose, even cluster size,flat geometry, not too many clusters     Distances between points 
                                                    medium n_clusters with MiniBatch code     
Affinity propagation    damping, sample preference  Not scalable with n_samples               Many clusters, uneven cluster size, non-flat geometry                       Graph distance (e.g. nearest-neighbor graph) 
Mean-shift              bandwidth                   Not scalable with n_samples               Many clusters, uneven cluster size, non-flat geometry                       Distances between points 
Spectral clustering     number of clusters          Medium n_samples, small n_clusters        Few clusters, even cluster size, non-flat geometry                          Graph distance (e.g. nearest-neighbor graph) 
Ward hierarchical       number of clusters          Large n_samples and n_clusters            Many clusters, possibly connectivity constraints                            Distances between points 
                                                                                              
Agglomerative           number of clusters,                                                   
                        linkage type, distance      Large n_samples and n_clusters            Many clusters, possibly connectivity constraints, non Euclidean distances   Any pairwise distance 
DBSCAN neighborhood     size                        Very large n_samples, medium n_clusters   Non-flat geometry, uneven cluster sizes                                     Distances between nearest points 
Gaussian mixtures       many                        Not scalable                              Flat geometry, good for density estimation                                  Mahalanobis distances to centers 
                                                                                              
Birch                   branching factor, threshold,                                          
                        optional global clusterer.  Large n_clusters and n_samples            Large dataset, outlier removal, data reduction.                             Euclidean distance between points 
                                                                                              


##Scikit - Clustering -  K-Means - unsupervised algorithm  for clustering problem
#http://scikit-learn.org/stable/modules/clustering.html

#to classify a given data set through a certain number of  clusters (assume k clusters). 
#Data points inside a cluster are homogeneous and heterogeneous to peer groups

#How to determine value of K:
#each cluster has its own centroid. 
#Sum of square of difference between centroid and the data points within a cluster 
#constitutes 'within sum of square' value for that cluster. 

#Also, when the sum of square values for all the clusters are added, 
#it becomes total 'within sum of square' value for the cluster solution.

#as the number of cluster increases, this value keeps on decreasing 
#but if you plot the result you may see that the sum of squared distance decreases 
#sharply up to some value of k(=optimum number of cluster), and then much more slowly after that. 


sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, 
        random_state=None, copy_x=True, n_jobs=1)        
    init : {'k-means++', 'random' or an ndarray}

#Attributes 
cluster_centers_ : array, [n_clusters, n_features]
    Coordinates of cluster centers
labels_ 
    Labels of each point
inertia_ : float. 
    Sum of distances of samples to their closest cluster center.

#Methods
fit(X[, y])          Compute k-means clustering. 
fit_predict(X[, y])  Compute cluster centers and predict cluster index for each sample. 
                      Returns:
                      labels : array, shape [n_samples,]
                        Index of the cluster each sample belongs to. 
fit_transform(X[, y]) Compute clustering and transform X to cluster-distance space. 
                      Returns:
                        X_new : array, shape [n_samples, k]
                            X transformed in the new space. 
get_params([deep])   Get parameters for this estimator. 
predict(X)           Predict the closest cluster each sample in X belongs to. 
                     Returns:
                     labels : array, shape [n_samples,]
                        Index of the cluster each sample belongs to.
score(X[, y])        Opposite of the value of X on the K-means objective. 
set_params(**params) Set the parameters of this estimator. 
transform(X)         Transform X to a cluster-distance space. 
                      Returns:
                        X_new : array, shape [n_samples, k]
                            X transformed in the new space. 
 
#Example 
>>> from sklearn.cluster import KMeans
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
                 [4, 2], [4, 4], [4, 0]])
>>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
>>> kmeans.labels_
array([0, 0, 0, 1, 1, 1], dtype=int32)
>>> kmeans.predict([[0, 0], [4, 4]])
array([0, 1], dtype=int32)
>>> kmeans.cluster_centers_
array([[ 1.,  2.],
       [ 4.,  2.]])

#Example - iris data clusters 

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


from sklearn.cluster import KMeans
from sklearn import datasets

np.random.seed(5)

centers = [[1, 1], [-1, -1], [1, -1]]
iris = datasets.load_iris()
X = iris.data
y = iris.target

estimators = {'k_means_iris_3': KMeans(n_clusters=3),
              'k_means_iris_8': KMeans(n_clusters=8),
              'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,
                                              init='random')}


fignum = 1
for name, est in estimators.items():
    fig = plt.figure(fignum, figsize=(4, 3))
    plt.clf()
    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

    plt.cla()
    est.fit(X)     #fit 
    labels = est.labels_

    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))

    ax.w_xaxis.set_ticklabels([])
    ax.w_yaxis.set_ticklabels([])
    ax.w_zaxis.set_ticklabels([])
    ax.set_xlabel('Petal width')
    ax.set_ylabel('Sepal length')
    ax.set_zlabel('Petal length')
    fignum = fignum + 1

# Plot the ground truth
fig = plt.figure(fignum, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

plt.cla()

for name, label in [('Setosa', 0),
                    ('Versicolour', 1),
                    ('Virginica', 2)]:
    ax.text3D(X[y == label, 3].mean(),
              X[y == label, 0].mean() + 1.5,
              X[y == label, 2].mean(), name,
              horizontalalignment='center',
              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
              
# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
ax.set_xlabel('Petal width')
ax.set_ylabel('Sepal length')
ax.set_zlabel('Petal length')
plt.show()




##Mini Batch K-Means - variant of the KMeans
#uses mini-batches to reduce the computation time
#MiniBatchKMeans converges faster than KMeans, but the quality of the results is reduced. 
#In practice this difference in quality can be quite small, 

sklearn.cluster.MiniBatchKMeans(n_clusters=8, init='k-means++', max_iter=100, batch_size=100, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, 
            init_size=None, n_init=3, reassignment_ratio=0.01)

#Attributes 
cluster_centers_ : array, [n_clusters, n_features],Coordinates of cluster centers
labels_ : Labels of each point
inertia_ : float. Sum of distances of samples to their closest cluster center.

#Methods
fit(X[, y])          Compute k-means clustering. 
fit_predict(X[, y])  Compute cluster centers and predict cluster index for each sample. 
                      Returns:
                      labels : array, shape [n_samples,]
                        Index of the cluster each sample belongs to. 
fit_transform(X[, y]) Compute clustering and transform X to cluster-distance space. 
                      Returns:
                        X_new : array, shape [n_samples, k]
                            X transformed in the new space. 
get_params([deep])   Get parameters for this estimator. 
predict(X)           Predict the closest cluster each sample in X belongs to. 
                     Returns:
                     labels : array, shape [n_samples,]
                        Index of the cluster each sample belongs to.
score(X[, y])        Opposite of the value of X on the K-means objective. 
set_params(**params) Set the parameters of this estimator. 
transform(X)         Transform X to a cluster-distance space. 
                      Returns:
                        X_new : array, shape [n_samples, k]
                            X transformed in the new space. 





##Example - timing comparison of K-means and Minibatchmeans 

import time

import numpy as np
import matplotlib.pyplot as plt

from sklearn.cluster import MiniBatchKMeans, KMeans
from sklearn.metrics.pairwise import pairwise_distances_argmin
from sklearn.datasets.samples_generator import make_blobs

# Generate sample data
np.random.seed(0)

batch_size = 45
centers = [[1, 1], [-1, -1], [1, -1]]
n_clusters = len(centers)
X, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)

# Compute clustering with Means

k_means = KMeans(init='k-means++', n_clusters=3, n_init=10)
t0 = time.time()
k_means.fit(X)
t_batch = time.time() - t0
k_means_labels = k_means.labels_
k_means_cluster_centers = k_means.cluster_centers_
k_means_labels_unique = np.unique(k_means_labels)

# Compute clustering with MiniBatchKMeans

mbk = MiniBatchKMeans(init='k-means++', n_clusters=3, batch_size=batch_size,
                      n_init=10, max_no_improvement=10, verbose=0)
t0 = time.time()
mbk.fit(X)
t_mini_batch = time.time() - t0
mbk_means_labels = mbk.labels_
mbk_means_cluster_centers = mbk.cluster_centers_
mbk_means_labels_unique = np.unique(mbk_means_labels)

# Plot result

fig = plt.figure(figsize=(8, 3))
fig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)
colors = ['#4EACC5', '#FF9C34', '#4E9A06']

# We want to have the same colors for the same cluster from the
# MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per
# closest one.

order = pairwise_distances_argmin(k_means_cluster_centers,
                                  mbk_means_cluster_centers)

# KMeans
ax = fig.add_subplot(1, 3, 1)
for k, col in zip(range(n_clusters), colors):
    my_members = k_means_labels == k
    cluster_center = k_means_cluster_centers[k]
    ax.plot(X[my_members, 0], X[my_members, 1], 'w',
            markerfacecolor=col, marker='.')
    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
            markeredgecolor='k', markersize=6)
ax.set_title('KMeans')
ax.set_xticks(())
ax.set_yticks(())
plt.text(-3.5, 1.8,  'train time: %.2fs\ninertia: %f' % (
    t_batch, k_means.inertia_))

# MiniBatchKMeans
ax = fig.add_subplot(1, 3, 2)
for k, col in zip(range(n_clusters), colors):
    my_members = mbk_means_labels == order[k]
    cluster_center = mbk_means_cluster_centers[order[k]]
    ax.plot(X[my_members, 0], X[my_members, 1], 'w',
            markerfacecolor=col, marker='.')
    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
            markeredgecolor='k', markersize=6)
ax.set_title('MiniBatchKMeans')
ax.set_xticks(())
ax.set_yticks(())
plt.text(-3.5, 1.8, 'train time: %.2fs\ninertia: %f' %
         (t_mini_batch, mbk.inertia_))

# Initialise the different array to all False
different = (mbk_means_labels == 4)
ax = fig.add_subplot(1, 3, 3)

for l in range(n_clusters):
    different += ((k_means_labels == k) != (mbk_means_labels == order[k]))

identic = np.logical_not(different)
ax.plot(X[identic, 0], X[identic, 1], 'w',
        markerfacecolor='#bbbbbb', marker='.')
ax.plot(X[different, 0], X[different, 1], 'w',
        markerfacecolor='m', marker='.')
ax.set_title('Difference')
ax.set_xticks(())
ax.set_yticks(())

plt.show()








###Scikit- Dimensionality reductions - PCA, FactorAnalysis,NMF, LDA, LSA etc 


##Scikit- Dimensionality reductions - PCA

#PCA is based on Singular value decomposition (generalization of eigenvalue decomposition, A*v=Lambda*v, v= eigenvector, lambda = eigenvalue )
#the singular value decomposition of an m × n   {\displaystyle m\times n}  m\times n real or complex matrix M    {\displaystyle \mathbf {M} }  \mathbf {M}  is a factorization of the form U Σ V ∗      {\displaystyle \mathbf {U\Sigma V^{*}} }  {\displaystyle \mathbf {U\Sigma V^{*}} }, where U    {\displaystyle \mathbf {U} }  \mathbf {U}  is an m × m   {\displaystyle m\times m}  m\times m real or complex unitary matrix, Σ    {\displaystyle \mathbf {\Sigma } }  \mathbf{\Sigma} is a m × n   {\displaystyle m\times n}  m\times n rectangular diagonal matrix with non-negative real numbers on the diagonal, and V    {\displaystyle \mathbf {V} }  \mathbf {V}  is an n × n   {\displaystyle n\times n}  n\times n real or complex unitary matrix. The diagonal entries σ i     {\displaystyle \sigma _{i}}  \sigma _{i} of Σ    {\displaystyle \mathbf {\Sigma } }  \mathbf{\Sigma} are known as the singular values of M    {\displaystyle \mathbf {M} }  \mathbf {M} . The columns of U    {\displaystyle \mathbf {U} }  \mathbf {U}  and the columns of V    {\displaystyle \mathbf {V} }  \mathbf {V}  are called the left-singular vectors and right-singular vectors of M    {\displaystyle \mathbf {M} }  \mathbf {M} , respectively
#M = U*Sigma*V_Conjugate 
#M = mXn matrix - real or complex 
#U = mXm  real or complex unitary matrix, called left singular vector 
#Sigma = diagonal matrix , each element is singular values of M 
#V = nXn, real or complex unitary matrix, called right singular vector 
#Unitary matrix = A*A_conjugate = A_conjugate * A = Identity matrix  
 
#Exact PCA and probabilistic interpretation

#PCA is used to decompose a multivariate dataset in a set of successive 
#orthogonal components that explain a maximum amount of the variance. 

#PCA is implemented as a transformer object that learns n components in its fit method, 
#and can be used on new data to project it on these components

#Note fit() must be called once!! and then use transform() as many times as required 
#Means don't use fit_transform()
#SVD suffers from a problem called 'sign indeterminancy', 
#which means the sign of the components_ and the output from transform depend on the algorithm and random state

class sklearn.decomposition.ProbabilisticPCA(n_components=None, copy=True, whiten=False)
    Additional layer on top of PCA that adds a probabilistic evaluation 
    of Principal component analysis (PCA)
class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)
    n_components : int, None or string
            Number of components to keep. 
            if n_components is not set all components are kept:
            n_components == min(n_samples, n_features)
            if n_components == 'mle', Minka's MLE is used to guess the dimension 
            if 0 < n_components < 1, select the number of components 
            such that the amount of variance that needs to be explained is greater than the percentage specified by n_components
    whiten : bool, optional
                When True (False by default) the components_ vectors are divided by n_samples times singular values to ensure uncorrelated outputs with unit component-wise variances.
                Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making there data respect some hard-wired assumptions
    svd_solver : string {'auto', 'full', 'arpack', 'randomized'}
        auto :
            the solver is selected by a default policy based on X.shape and n_components: 
            if the input data is larger than 500x500 and the number of components to extract 
            is lower than 80% of the smallest dimension of the data, t
            hen the more efficient 'randomized' method is enabled. 
            Otherwise the exact full SVD is computed and optionally truncated afterwards.
        full :
            run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing
        arpack :
            run SVD truncated to n_components calling ARPACK solver 
            via scipy.sparse.linalg.svds. It requires strictly 0 < n_components < X.shape[1]
        randomized :
            run randomized SVD by the method of Halko et al.
    #Attributes are 
    components_ : array, [n_components, n_features]
        Principal axes in feature space, representing the directions of maximum variance in the data.
    explained_variance_ratio_ : array, [n_components]
        Percentage of variance explained by each of the selected components. If n_components is not set then all components are stored and the sum of explained variances is equal to 1.0
    mean_ : array, [n_features]
        Per-feature empirical mean, estimated from the training set.
    n_components_ : int
        The estimated number of components. Relevant when n_components is set to 'mle' or a number between 0 and 1 to select using explained variance.
    noise_variance_ : float
        The estimated noise covariance following the Probabilistic PCA model
    
#Methods other than get_params() and fit() and score()
fit_transform(X[, y])   Fit the model with X and apply the dimensionality reduction on X. 
get_covariance()        Compute data covariance with the generative model. 
get_precision()         Compute data precision matrix with the generative model. 
inverse_transform(X)    Transform data back to its original space, i.e., 
score_samples(X)        Return the log-likelihood of each sample 
transform(X)            Apply the dimensionality reduction on X. 
                        X : array of shape (n_samples, n_features)
                        returns X_new : array, shape (n_samples, n_components)
            
##Model selection with Probabilistic PCA and Factor Analysis (FA)

#Script output:
best n_components by PCA CV = 10
best n_components by FactorAnalysis CV = 10
best n_components by PCA MLE = 10
best n_components by PCA CV = 40
best n_components by FactorAnalysis CV = 10
best n_components by PCA MLE = 38


#Code 

import numpy as np
import matplotlib.pyplot as plt
from scipy import linalg

from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.covariance import ShrunkCovariance, LedoitWolf
from sklearn.cross_validation import cross_val_score
from sklearn.grid_search import GridSearchCV

# Create the data

n_samples, n_features, rank = 1000, 50, 10
sigma = 1.
rng = np.random.RandomState(42)
U, _, _ = linalg.svd(rng.randn(n_features, n_features))
X = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)

# Adding homoscedastic noise
X_homo = X + sigma * rng.randn(n_samples, n_features)

# Adding heteroscedastic noise
sigmas = sigma * rng.rand(n_features) + sigma / 2.
X_hetero = X + rng.randn(n_samples, n_features) * sigmas

# Fit the models

n_components = np.arange(0, n_features, 5)  # options for n_components


def compute_scores(X):
    pca = PCA()
    fa = FactorAnalysis()

    pca_scores, fa_scores = [], []
    for n in n_components:
        pca.n_components = n
        fa.n_components = n
        pca_scores.append(np.mean(cross_val_score(pca, X)))
        fa_scores.append(np.mean(cross_val_score(fa, X)))

    return pca_scores, fa_scores


def shrunk_cov_score(X):
    shrinkages = np.logspace(-2, 0, 30)
    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages})
    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))


def lw_score(X):
    return np.mean(cross_val_score(LedoitWolf(), X))


for X, title in [(X_homo, 'Homoscedastic Noise'),
                 (X_hetero, 'Heteroscedastic Noise')]:
    pca_scores, fa_scores = compute_scores(X)
    n_components_pca = n_components[np.argmax(pca_scores)]
    n_components_fa = n_components[np.argmax(fa_scores)]

    pca = PCA(n_components='mle')
    pca.fit(X)
    n_components_pca_mle = pca.n_components_

    print("best n_components by PCA CV = %d" % n_components_pca)
    print("best n_components by FactorAnalysis CV = %d" % n_components_fa)
    print("best n_components by PCA MLE = %d" % n_components_pca_mle)

    plt.figure()
    plt.plot(n_components, pca_scores, 'b', label='PCA scores')
    plt.plot(n_components, fa_scores, 'r', label='FA scores')
    plt.axvline(rank, color='g', label='TRUTH: %d' % rank, linestyle='-')
    plt.axvline(n_components_pca, color='b',
                label='PCA CV: %d' % n_components_pca, linestyle='--')
    plt.axvline(n_components_fa, color='r',
                label='FactorAnalysis CV: %d' % n_components_fa, linestyle='--')
    plt.axvline(n_components_pca_mle, color='k',
                label='PCA MLE: %d' % n_components_pca_mle, linestyle='--')

    # compare with other covariance estimators
    plt.axhline(shrunk_cov_score(X), color='violet',
                label='Shrunk Covariance MLE', linestyle='-.')
    plt.axhline(lw_score(X), color='orange',
                label='LedoitWolf MLE' % n_components_pca_mle, linestyle='-.')

    plt.xlabel('nb of components')
    plt.ylabel('CV scores')
    plt.legend(loc='lower right')
    plt.title(title)

plt.show()

 
 
##Scikit - PCA - Incremental PCA - minibatch PCA
#plain PCA is not suitable of large dataset which can not be fit into main memory 
#Useful for large data set 
sklearn.decomposition.IncrementalPCA(n_components=None, whiten=False, copy=True, batch_size=None)
#Attributes and methods are similar to PCA 
#implement out-of-core Principal Component Analysis either by:
    •Using its partial_fit method on chunks of data fetched sequentially from the local hard drive or a network database.
    •Calling its fit method on a memory mapped file using numpy.memmap.



#Example 
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import load_iris
from sklearn.decomposition import PCA, IncrementalPCA

iris = load_iris()
X = iris.data
y = iris.target

n_components = 2
ipca = IncrementalPCA(n_components=n_components, batch_size=10)
X_ipca = ipca.fit_transform(X)

pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X)

for X_transformed, title in [(X_ipca, "Incremental PCA"), (X_pca, "PCA")]:
    plt.figure(figsize=(8, 8))
    for c, i, target_name in zip("rgb", [0, 1, 2], iris.target_names):
        plt.scatter(X_transformed[y == i, 0], X_transformed[y == i, 1],
                    c=c, label=target_name)

    if "Incremental" in title:
        err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()
        plt.title(title + " of iris dataset\nMean absolute unsigned error "
                  "%.6f" % err)
    else:
        plt.title(title + " of iris dataset")
    plt.legend(loc="best")
    plt.axis([-4, 4, -1.5, 1.5])

plt.show()


##Scikit - PCA - Approximate PCA - RandomizedPCA
#creates lower-dimensional data that preserves most of the variance, 
#by dropping the singular vector of components associated with lower singular values.

sklearn.decomposition.RandomizedPCA(n_components=None, copy=True, iterated_power=3, 
    whiten=False, random_state=None)
#Attributes and methods are similar to PCA 
#or use PCA with svd_solver='randomized' 

import numpy as np
from sklearn.decomposition import RandomizedPCA
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
pca = RandomizedPCA(n_components=2)
pca.fit(X)                 
>>> print(pca.explained_variance_ratio_) 
[ 0.99244...  0.00755...]

##Scikit - PCA - Kernel PCA
#an extension of PCA which achieves non-linear dimensionality reduction through the use of kernels

class sklearn.decomposition.KernelPCA(n_components=None, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None, alpha=1.0, fit_inverse_transform=False, eigen_solver='auto', tol=0, max_iter=None, remove_zero_eig=False, random_state=None, copy_X=True, n_jobs=1)
    kernel : 'linear' | 'poly' | 'rbf' | 'sigmoid' | 'cosine' | 'precomputed'
        Kernel. Default='linear'.
    gamma : float, default=1/n_features
        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels.
    degree : int, default=3
        Degree for poly kernels. Ignored by other kernels.
    coef0 : float, default=1
        Independent term in poly and sigmoid kernels. Ignored by other kernels.
    kernel_params : mapping of string to any, default=None
        Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels.
    alpha : int, default=1.0
        Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True).
    fit_inverse_transform : bool, default=False
        Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point)
#Attributes and methods are similar to PCA 

#Example 
import numpy as np
import matplotlib.pyplot as plt

from sklearn.decomposition import PCA, KernelPCA
from sklearn.datasets import make_circles

np.random.seed(0)

#Make a large circle containing a smaller circle in 2d.
X, y = make_circles(n_samples=400, factor=.3, noise=.05)

kpca = KernelPCA(kernel="rbf", fit_inverse_transform=True, gamma=10)
X_kpca = kpca.fit_transform(X)
X_back = kpca.inverse_transform(X_kpca)


##Scikit - PCA  for Sparse data -SparsePCA and MiniBatchSparsePCA
#SparsePCA time complexity : O(n ** 3)
#Mini-batch sparse PCA (MiniBatchSparsePCA) is a variant of SparsePCA that is faster but less accurate

class sklearn.decomposition.SparsePCA(n_components=None, alpha=1, ridge_alpha=0.01, max_iter=1000, tol=1e-08, method='lars', n_jobs=1, U_init=None, V_init=None, verbose=False, random_state=None)[source]
class sklearn.decomposition.MiniBatchSparsePCA(n_components=None, alpha=1, ridge_alpha=0.01, n_iter=100, callback=None, batch_size=3, verbose=False, shuffle=True, n_jobs=1, method='lars', random_state=None)[source]
#Attributes and methods are similar to PCA 


##Scikit - PCA - Truncated 
#TruncatedSVD implements a variant of singular value decomposition (SVD) 
#that only computes the k largest singular values, where k is a user-specified parameter.

#When truncated SVD is applied to term-document matrices 
#(as returned by CountVectorizer or TfidfVectorizer), 
#this transformation is known as latent semantic analysis (LSA), 

class sklearn.decomposition.TruncatedSVD(n_components=2, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)
#Methods
fit(X[, y])             Fit LSI model on training data X. 
fit_transform(X[, y])   Fit LSI model to X and perform dimensionality reduction on X. 
get_params([deep])      Get parameters for this estimator. 
inverse_transform(X)    Transform X back to its original space. 
set_params(**params)    Set the parameters of this estimator. 
transform(X)            Perform dimensionality reduction on X. 



>>> from sklearn.decomposition import TruncatedSVD
>>> from sklearn.random_projection import sparse_random_matrix
>>> X = sparse_random_matrix(100, 100, density=0.01, random_state=42)
>>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)
>>> svd.fit(X)  
TruncatedSVD(algorithm='randomized', n_components=5, n_iter=7,
        random_state=42, tol=0.0)
>>> print(svd.explained_variance_ratio_)  
[ 0.0606... 0.0584... 0.0497... 0.0434... 0.0372...]
>>> print(svd.explained_variance_ratio_.sum())  
0.249...
>>> print(svd.singular_values_)  
[ 2.5841... 2.5245... 2.3201... 2.1753... 2.0443...]




##Scikit - Dimensionality reduction(non PCA) - DictionaryLearning and SparseCoder

#dictionary learning applied on image patches has been shown to give good results
#in image processing tasks such as image completion, inpainting and denoising, 
#as well as for supervised recognition tasks.
class sklearn.decomposition.MiniBatchDictionaryLearning(n_components=None, alpha=1, n_iter=1000, fit_algorithm='lars', n_jobs=1, batch_size=3, shuffle=True, dict_init=None, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, verbose=False, split_sign=False, random_state=None)[source]
    Mini-batch dictionary learning
    implements partial_fit, which updates the dictionary 
    by iterating only once over a mini-batch
class sklearn.decomposition.DictionaryLearning(n_components=None, alpha=1, max_iter=1000, tol=1e-08, fit_algorithm='lars', transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, n_jobs=1, code_init=None, dict_init=None, verbose=False, split_sign=False, random_state=None)[source]
    a matrix factorization problem that amounts to finding a 
    (usually overcomplete) dictionary that will perform good 
    at sparsely encoding the fitted data.
    n_components : int,
        number of dictionary elements to extract
    fit_algorithm : {'lars', 'cd'}
    
    
#SparseCoder
#Finds a sparse representation of data against a fixed, precomputed dictionary
#Each row of the result is the solution to a sparse coding problem. 
#The goal is to find a sparse array code such that:
#X ~= code * dictionary

class sklearn.decomposition.SparseCoder(dictionary, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, n_jobs=1)
    finding a representation of the data as a linear combination of 
    as few dictionary atoms as possible
    dictionary : array, [n_components, n_features]
        The dictionary atoms used for sparse coding. 
        Lines are assumed to be normalized to unit norm.
    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}
        •Orthogonal matching pursuit (Orthogonal Matching Pursuit (OMP))
        •Least-angle regression (Least Angle Regression)
        •Lasso computed by least-angle regression
        •Lasso using coordinate descent (Lasso)
        •Thresholding
    #Methods
    fit(X[, y])             Do nothing and return the estimator unchanged 
    fit_transform(X[, y])   Fit to data, then transform it. 
    get_params([deep])      Get parameters for this estimator. 
    set_params(**params)    Set the parameters of this estimator. 
    transform(X)            Encode the data as a sparse combination of the dictionary atoms. 

    
##Scikit - Dimensionality reduction(non PCA) - Factor Analysis
#Factor analysis can produce similar components (the columns of its loading matrix) to PCA. 
#However, might not be whether they are orthogonal
#However, This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise

class sklearn.decomposition.FactorAnalysis(n_components=None, tol=0.01, copy=True, max_iter=1000, noise_variance_init=None, svd_method='randomized', iterated_power=3, random_state=0)
    #Methods
    fit(X[, y])             Fit the FactorAnalysis model to X using EM 
    fit_transform(X[, y])   Fit to data, then transform it. 
    get_covariance()        Compute data covariance with the FactorAnalysis model. 
    get_params([deep])      Get parameters for this estimator. 
    get_precision()         Compute data precision matrix with the FactorAnalysis model. 
    score(X[, y])           Compute the average log-likelihood of the samples 
    score_samples(X)        Compute the log-likelihood of each sample 
    set_params(**params)    Set the parameters of this estimator. 
    transform(X)            Apply dimensionality reduction to X using the model. 

    
    
##Scikit - Dimensionality reduction(non PCA) - Independent component analysis (ICA)
#PCA and factor analysis essentially assume components with  Gaussian distribtion with a low-rank covariance matrix
#ICA is used when  non-Gaussian distribution is assumed

#Note to use OCA,  whitening must be applied

#Typically, ICA is not used for reducing dimensionality 
#but for separating superimposed signals. 

#It is classically used to separate mixed signals (a problem known as blind source separation
#ICA can also be used as  linear decomposition that finds components with some sparsity:

class sklearn.decomposition.FastICA(n_components=None, algorithm='parallel', whiten=True, fun='logcosh', fun_args=None, max_iter=200, tol=0.0001, w_init=None, random_state=None)
    #Methods
    fit(X[, y])             Fit the model to X. 
    fit_transform(X[, y])   Fit the model and recover the sources from X. 
    get_params([deep])      Get parameters for this estimator. 
    inverse_transform(X[, copy]) Transform the sources back to the mixed data (apply mixing matrix). 
    set_params(**params)        Set the parameters of this estimator. 
    transform(X[, y, copy])     Recover the sources from X (apply the unmixing matrix). 

    
    
##Scikit - Dimensionality reduction(non PCA) - Non-negative matrix factorization (NMF or NNMF)
#NMF can be plugged in instead of PCA or its variants, 
#in the cases where the data matrix does not contain negative values

#Unlike PCA, the representation of a vector is obtained in an additive fashion, 
#by superimposing the components, without subtracting. 
#Such additive models are efficient for representing images and text

#It finds a decomposition of samples X into two matrices W and H of non-negative elements, 
#by optimizing the distance d between X and the matrix product WH


class sklearn.decomposition.NMF(n_components=None, init=None, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=200, random_state=None, alpha=0.0, l1_ratio=0.0, verbose=0, shuffle=False)
    init : 'random' | 'nndsvd' | 'nndsvda' | 'nndsvdar' | 'custom'
        Method used to initialize the procedure. 
        Default: 'nndsvd' if n_components < n_features, otherwise random. 
        Valid options:
            •'random': non-negative random matrices, scaled with:sqrt(X.mean() / n_components)
            •'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)initialization (better for sparseness)
            •'nndsvda': NNDSVD with zeros filled with the average of X(better when sparsity is not desired)
            •'nndsvdar': NNDSVD with zeros filled with small random values(generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired)
            •'custom': use custom matrices W and H
    solver : 'cd' | 'mu'
        Numerical solver to use: 'cd' is a Coordinate Descent solver. 
        'mu' is a Multiplicative Update solver.
    beta_loss : float or string, default 'frobenius'
        String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}
    #Methods
    fit(X[, y])             Fit the model to X. 
    fit_transform(X[, y])   Fit the model and recover the sources from X. 
    get_params([deep])      Get parameters for this estimator. 
    inverse_transform(X[, copy]) Transform the sources back to the mixed data (apply mixing matrix). 
    set_params(**params)        Set the parameters of this estimator. 
    transform(X[, y, copy])     Recover the sources from X (apply the unmixing matrix). 

#Example 
import numpy as np
X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
from sklearn.decomposition import NMF
model = NMF(n_components=2, init='random', random_state=0)
W = model.fit_transform(X)
H = model.components_
X_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])
W_new = model.transform(X_new)



##Scikit - Dimensionality reduction(non PCA) - Latent Dirichlet Allocation (LDA)

#Latent Dirichlet Allocation is a generative probabilistic model 
#for collections of discrete dataset such as text corpora. 
#It is also a topic model that is used for discovering abstract topics 
#from a collection of documents.

#When LatentDirichletAllocation is applied on a 'document-term' matrix, 
#the matrix will be decomposed into a 'topic-term' matrix and a 'document-topic' matrix. 
#While 'topic-term' matrix is stored as components_ in the model, 
#'document-topic' matrix can be calculated from transform method.

#LatentDirichletAllocation also implements partial_fit method. 
#This is used when data can be fetched sequentially.

class sklearn.decomposition.LatentDirichletAllocation(n_components=10, doc_topic_prior=None, topic_word_prior=None, learning_method=None, learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=1, verbose=0, random_state=None, n_topics=None)
    #Methods
    fit(X[, y])             Learn model for the data X with variational Bayes method. 
    fit_transform(X[, y])   Fit to data, then transform it. 
    get_params([deep])      Get parameters for this estimator. 
    partial_fit(X[, y])     Online VB with Mini-Batch update. 
    perplexity(X[, doc_topic_distr, sub_sampling]) Calculate approximate perplexity for data X. 
    score(X[, y])           Calculate approximate log-likelihood as score. 
    set_params(**params)    Set the parameters of this estimator. 
    transform(X)            Transform data X according to the fitted model. 

    
##Example
#a corpus of documents 
#and extract additive models of the topic structure of the corpus. 
#The output is a list of topics, each represented as a list of terms 
....
Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit
Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn
...

#code 
 
from __future__ import print_function
from time import time

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation
from sklearn.datasets import fetch_20newsgroups

n_samples = 2000
n_features = 1000
n_components = 10
n_top_words = 20


def print_top_words(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        message = "Topic #%d: " % topic_idx
        message += " ".join([feature_names[i]
                             for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(message)
    print()


# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics
# to filter out useless terms early on: the posts are stripped of headers,
# footers and quoted replies, and common English words, words occurring in
# only one document or in at least 95% of the documents are removed.

print("Loading dataset...")
t0 = time()
dataset = fetch_20newsgroups(shuffle=True, random_state=1,
                             remove=('headers', 'footers', 'quotes'))
data_samples = dataset.data[:n_samples]
print("done in %0.3fs." % (time() - t0))

# Use tf-idf features for NMF.
print("Extracting tf-idf features for NMF...")
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,
                                   max_features=n_features,
                                   stop_words='english')
t0 = time()
tfidf = tfidf_vectorizer.fit_transform(data_samples)
print("done in %0.3fs." % (time() - t0))

# Use tf (raw term count) features for LDA.
print("Extracting tf features for LDA...")
tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,
                                max_features=n_features,
                                stop_words='english')
t0 = time()
tf = tf_vectorizer.fit_transform(data_samples)
print("done in %0.3fs." % (time() - t0))
print()

# Fit the NMF model
print("Fitting the NMF model (Frobenius norm) with tf-idf features, "
      "n_samples=%d and n_features=%d..."
      % (n_samples, n_features))
t0 = time()
nmf = NMF(n_components=n_components, random_state=1,
          alpha=.1, l1_ratio=.5).fit(tfidf)
print("done in %0.3fs." % (time() - t0))

print("\nTopics in NMF model (Frobenius norm):")
tfidf_feature_names = tfidf_vectorizer.get_feature_names()
print_top_words(nmf, tfidf_feature_names, n_top_words)

# Fit the NMF model
print("Fitting the NMF model (generalized Kullback-Leibler divergence) with "
      "tf-idf features, n_samples=%d and n_features=%d..."
      % (n_samples, n_features))
t0 = time()
nmf = NMF(n_components=n_components, random_state=1,
          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,
          l1_ratio=.5).fit(tfidf)
print("done in %0.3fs." % (time() - t0))

print("\nTopics in NMF model (generalized Kullback-Leibler divergence):")
tfidf_feature_names = tfidf_vectorizer.get_feature_names()
print_top_words(nmf, tfidf_feature_names, n_top_words)

print("Fitting LDA models with tf features, "
      "n_samples=%d and n_features=%d..."
      % (n_samples, n_features))
lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,
                                learning_method='online',
                                learning_offset=50.,
                                random_state=0)
t0 = time()
lda.fit(tf)
print("done in %0.3fs." % (time() - t0))

print("\nTopics in LDA model:")
tf_feature_names = tf_vectorizer.get_feature_names()
print_top_words(lda, tf_feature_names, n_top_words)



###Scikit - Decision Trees - classification and regression
#Decision Trees (DTs) are a non-parametric supervised learning method 

#Simple to understand and to interpret. Trees can be visualised.
#Does not require preprocessing 
#cons: prone to overfitting 
#Solution- Consider performing dimensionality reduction (PCA, ICA, or Feature selection) 
#beforehand to give tree a better chance of finding features that are discriminative

class sklearn.tree.DecisionTreeClassifier(criterion='gini', 
        splitter='best', max_depth=None, 
        min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)
#Attributes 
classes_ : array of shape = [n_classes] or a list of such arrays
    The classes labels (single output problem), 
    or a list of arrays of class labels (multi-output problem).
feature_importances_ : array of shape = [n_features]
    The feature importances. The higher, the more important the feature
max_features_ : int,
    The inferred value of max_features.
tree_ : Tree object
    The underlying Tree object.
#Methods 
apply(X[, check_input])             Returns the index of the leaf that each sample is predicted as. 
decision_path(X[, check_input])     Return the decision path in the tree 
fit(X, y[, sample_weight, check_input, …]) Build a decision tree classifier from the training set (X, y). 
get_params([deep])                  Get parameters for this estimator. 
predict(X[, check_input])           Predict class or regression value for X. 
predict_log_proba(X)                Predict class log-probabilities of the input samples X. 
predict_proba(X[, check_input])     Predict class probabilities of the input samples X. 
score(X, y[, sample_weight])        Returns the mean accuracy on the given test data and labels. 
set_params(**params)                Set the parameters of this estimator. 

#Example 
from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
>>> clf.predict([[2., 2.]]) #predicted class
array([1])
#the probability of each class can be predicted
>>> clf.predict_proba([[2., 2.]])
array([[ 0.,  1.]])


#DecisionTreeClassifier is capable of both binary 
#(where the labels are [-1, 1]) classification 
#and multiclass (where the labels are [0, …, K-1]) classification.

from sklearn.datasets import load_iris
from sklearn import tree
iris = load_iris()
clf = tree.DecisionTreeClassifier()
clf = clf.fit(iris.data, iris.target)
>>> clf.predict(iris.data[:1, :])
array([0])
#the probability of each class can be predicted
>>> clf.predict_proba(iris.data[:1, :])
array([[ 1.,  0.,  0.]])

#check the decision Tree visualization 
#install graphvz from https://graphviz.gitlab.io/_pages/Download/Download_windows.html
#and 
$ pip install graphviz
#example 
import graphviz 
dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names=iris.feature_names,  
                         class_names=iris.target_names,  
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = graphviz.Source(dot_data)  
graph.render('iris') #'iris.pdf'








class sklearn.tree.DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort=False)
#Attributes 
classes_ : array of shape = [n_classes] or a list of such arrays
    The classes labels (single output problem), 
    or a list of arrays of class labels (multi-output problem).
feature_importances_ : array of shape = [n_features]
    The feature importances. The higher, the more important the feature
max_features_ : int,
    The inferred value of max_features.
tree_ : Tree object
    The underlying Tree object.
#Methods 
apply(X[, check_input])             Returns the index of the leaf that each sample is predicted as. 
decision_path(X[, check_input])     Return the decision path in the tree 
fit(X, y[, sample_weight, check_input, …]) Build a decision tree classifier from the training set (X, y). 
get_params([deep])                  Get parameters for this estimator. 
predict(X[, check_input])           Predict class or regression value for X. 
score(X, y[, sample_weight])        Returns the mean accuracy on the given test data and labels. 
set_params(**params)                Set the parameters of this estimator. 

#Example 
from sklearn import tree
X = [[0, 0], [2, 2]]
y = [0.5, 2.5]
clf = tree.DecisionTreeRegressor()
clf = clf.fit(X, y)
>>> clf.predict([[1, 1]])
array([ 0.5])

#Example 
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor
boston = load_boston()
regressor = DecisionTreeRegressor(random_state=0)
>>> cross_val_score(regressor, boston.data, boston.target, cv=10) #accuracy score from 10 CV folds 
array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,
        0.07..., 0.29..., 0.33..., -1.42..., -1.77...])

##Multi-output problems
#with several outputs to predict, 
#Y is a 2d array of size [n_samples, n_outputs].
#If a decision tree is fit on an output array Y of size [n_samples, n_outputs] 
#then the resulting estimator will:
#•Output n_output values upon predict;
#•Output a list of n_output arrays of class probabilities upon predict_proba.

#Example Regression 
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor

# Create a random dataset
rng = np.random.RandomState(1)
X = np.sort(200 * rng.rand(100, 1) - 100, axis=0)
y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T
y[::5, :] += (0.5 - rng.rand(20, 2))

# Fit regression model
regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
regr_3 = DecisionTreeRegressor(max_depth=8)
regr_1.fit(X, y)
regr_2.fit(X, y)
regr_3.fit(X, y)

# Predict
X_test = np.arange(-100.0, 100.0, 0.01)[:, np.newaxis]
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)
y_3 = regr_3.predict(X_test)

# Plot the results
plt.figure()
s = 50
s = 25
plt.scatter(y[:, 0], y[:, 1], c="navy", s=s,
            edgecolor="black", label="data")
plt.scatter(y_1[:, 0], y_1[:, 1], c="cornflowerblue", s=s,
            edgecolor="black", label="max_depth=2")
plt.scatter(y_2[:, 0], y_2[:, 1], c="red", s=s,
            edgecolor="black", label="max_depth=5")
plt.scatter(y_3[:, 0], y_3[:, 1], c="orange", s=s,
            edgecolor="black", label="max_depth=8")
plt.xlim([-6, 6])
plt.ylim([-6, 6])
plt.xlabel("target 1")
plt.ylabel("target 2")
plt.title("Multi-output Decision Tree Regression")
plt.legend(loc="best")
plt.show()

##Tips 
http://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use







###Scikit - Linear and Quadratic Discriminant Analysis - Classification and Dimensionality reduction
#they have closed-form solutions that can be easily computed, 
#are inherently multiclass, have proven to work well in practice 
#and have no hyperparameters to tune

class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001)[source]
    Linear Discriminant Analysis
    A classifier with a linear decision boundary, 
    generated by fitting class conditional densities to the data 
    and using Bayes' rule.
    #Dimensionalityreduction:
    The desired dimensionality can be set using the n_components constructor parameter
    and using 'transform()' to transform those 
    #Shrinkage
    Shrinkage is a tool to improve estimation of covariance matrices 
    in situations where the number of training samples is small compared 
    to the number of features. 
    In this scenario, the empirical sample covariance is a poor estimator. 
    Hence, Set shrinkage parameter to 'auto'. 
    But solver parameter should be  'lsqr' or 'eigen'.
#Methods
decision_function(X)    Predict confidence scores for samples. 
fit(X, y)               Fit LinearDiscriminantAnalysis model according to the given training data and parameters. 
fit_transform(X[, y])   Fit to data, then transform it. 
get_params([deep])      Get parameters for this estimator. 
predict(X)              Predict class labels for samples in X. 
predict_log_proba(X)    Estimate log probability. 
predict_proba(X)        Estimate probability. 
score(X, y[, sample_weight]) Returns the mean accuracy on the given test data and labels. 
set_params(**params)    Set the parameters of this estimator. 
transform(X)            Project data to maximize class separation. 


#Example 
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
y = np.array([1, 1, 1, 2, 2, 2])
clf = LinearDiscriminantAnalysis()
>>> clf.fit(X, y)
LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,
              solver='svd', store_covariance=False, tol=0.0001)
>>> print(clf.predict([[-0.8, -1]])) #predict class 
[1]




class sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=False, tol=0.0001, store_covariances=None)[source]
    Quadratic Discriminant Analysis
    A classifier with a quadratic decision boundary, 
    generated by fitting class conditional densities to the data 
    and using Bayes' rule.

#Methods
decision_function(X)    Predict confidence scores for samples. 
fit(X, y)               Fit LinearDiscriminantAnalysis model according to the given training data and parameters. 
get_params([deep])      Get parameters for this estimator. 
predict(X)              Predict class labels for samples in X. 
predict_log_proba(X)    Estimate log probability. 
predict_proba(X)        Estimate probability. 
score(X, y[, sample_weight]) Returns the mean accuracy on the given test data and labels. 
set_params(**params)    Set the parameters of this estimator. 

#Example 
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
y = np.array([1, 1, 1, 2, 2, 2])
clf = QuadraticDiscriminantAnalysis()
>>> clf.fit(X, y)
QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False,
                              store_covariances=None, tol=0.0001)
>>> print(clf.predict([[-0.8, -1]]))
[1]





###Scikit - Kernel ridge regression - Regression 

#Kernel ridge regression (KRR) combines 
#Ridge Regression (linear least squares with l2-norm regularization)
#with the kernel trick. 
    
#It thus learns a linear function in the space induced by the respective kernel and the data. 
#For non-linear kernels, this corresponds to a non-linear function in the original space.

#The form of the model learned by KernelRidge is identical 
#to support vector regression (SVR). 
#However, different loss functions are used: 
#KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, 
#both combined with l2 regularization. 

#In contrast to SVR, fitting KernelRidge can be done in closed-form 
#and is typically faster for medium-sized datasets. 
#On the other hand, the learned model is non-sparse 
#and thus slower than SVR, which learns a sparse model for epsilon > 0, at prediction-time.

class sklearn.kernel_ridge.KernelRidge(alpha=1, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None)[source]¶
    Parameters:
    alpha : {float, array-like}, shape = [n_targets]
        Small positive values of alpha improve the conditioning of the problem 
        and reduce the variance of the estimates. 
        Alpha corresponds to (2*C)^-1 in other linear models 
        such as LogisticRegression or LinearSVC. 
        If an array is passed, penalties are assumed to be specific 
        to the targets. Hence they must correspond in number.
    kernel : string or callable, default='linear'
        Kernel mapping used internally. 
        A callable should accept two arguments 
        and the keyword arguments passed to this object as kernel_params, 
        and should return a floating point number.
        Check predefined kernel from 
        >>> sklearn.metrics.pairwise.kernel_metrics()
    gamma : float, default=None
        Gamma parameter for the RBF, laplacian, polynomial, 
        exponential chi2 and sigmoid kernels. 
    degree : float, default=3
        Degree of the polynomial kernel. 
        Ignored by other kernels.
    coef0 : float, default=1
        Zero coefficient for polynomial and sigmoid kernels. 
        Ignored by other kernels.
#Methods
fit(X[, y, sample_weight])  Fit Kernel Ridge regression model 
get_params([deep])          Get parameters for this estimator. 
predict(X)                  Predict using the kernel ridge model 
score(X, y[, sample_weight]) Returns the coefficient of determination R^2 of the prediction. 
set_params(**params)        Set the parameters of this estimator. 

#Example 
from sklearn.kernel_ridge import KernelRidge
import numpy as np
n_samples, n_features = 10, 5
rng = np.random.RandomState(0)
y = rng.randn(n_samples)
X = rng.randn(n_samples, n_features)
clf = KernelRidge(alpha=1.0)
>>> clf.fit(X, y) 
KernelRidge(alpha=1.0, coef0=1, degree=3, gamma=None, kernel='linear',
            kernel_params=None)

 
 
 
 
 
###Scikit - Neural network models (supervised) - Multi-layer Perceptron
#Use keras for large database (Keras support TensorFlow/mxnet)

#The advantages of Multi-layer Perceptron are:
    •Capability to learn non-linear models.
    •Capability to learn models in real-time (on-line learning) 
     using partial_fit.
#The disadvantages of Multi-layer Perceptron (MLP) include:
    •MLP with hidden layers have a non-convex loss function 
     where there exists more than one local minimum. 
     Therefore different random weight initializations can lead to different validation accuracy.
    •MLP requires tuning a number of hyperparameters 
     such as the number of hidden neurons, layers, and iterations.
    •MLP is sensitive to feature scaling.

#Tips 
http://scikit-learn.org/stable/modules/neural_networks_supervised.html#mlp-tips

#Both MLPRegressor and MLPClassifier use parameter alpha for regularization (L2 regularization) term 
#which helps in avoiding overfitting by penalizing weights with large magnitudes
class sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)[source]
    Multi-layer Perceptron regressor.
class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), 
        activation='relu', solver='adam', alpha=0.0001, 
        batch_size='auto', learning_rate='constant', 
        learning_rate_init=0.001, power_t=0.5, max_iter=200, 
        shuffle=True, random_state=None, tol=0.0001, verbose=False, 
        warm_start=False, momentum=0.9, nesterovs_momentum=True, 
        early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)[source]
    Multi-layer Perceptron classifier.
    This model optimizes the log-loss function using LBFGS 
    or stochastic gradient descent.
    Parameters:
    hidden_layer_sizes : tuple, length = n_layers - 2, default (100,) ie one hidden layer 
        The ith element represents the number of neurons 
        in the ith hidden layer.
    activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'
        Activation function for the hidden layer.
        •'identity', no-op activation, useful to implement linear bottleneck, returns f(x) = x
        •'logistic', the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).
        •'tanh', the hyperbolic tan function, returns f(x) = tanh(x).
        •'relu', the rectified linear unit function, returns f(x) = max(0, x)
    solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'
        The solver for weight optimization.
        •'lbfgs' is an optimizer in the family of quasi-Newton methods.
        •'sgd' refers to stochastic gradient descent.
        •'adam' refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba
        The default solver 'adam' works pretty well on relatively large datasets 
        (with thousands of training samples or more) in terms of both training time and validation score. 
        For small datasets, however, 'lbfgs' can converge faster 
        and perform better.
    alpha : float, optional, default 0.0001
        L2 penalty (regularization term) parameter.
    batch_size : int, optional, default 'auto'
        Size of minibatches for stochastic optimizers. 
        If the solver is 'lbfgs', the classifier will not use minibatch. 
        When set to 'auto', batch_size=min(200, n_samples)
    learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'
        Learning rate schedule for weight updates.
        •'constant' is a constant learning rate given by 'learning_rate_init'.
        •'invscaling' gradually decreases the learning rate learning_rate_ at each time step 't' using an inverse scaling exponent of 'power_t'. effective_learning_rate = learning_rate_init / pow(t, power_t)
        •'adaptive' keeps the learning rate constant to 'learning_rate_init' as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if 'early_stopping' is on, the current learning rate is divided by 5.
        Only used when solver='sgd'.
    learning_rate_init : double, optional, default 0.001
        The initial learning rate used. 
        It controls the step-size in updating the weights. 
        Only used when solver='sgd' or 'adam'.
    power_t : double, optional, default 0.5
        The exponent for inverse scaling learning rate. 
        It is used in updating effective learning rate when the learning_rate is set to 'invscaling'. Only used when solver='sgd'.
    max_iter : int, optional, default 200
        Maximum number of iterations. 
        The solver iterates until convergence (determined by 'tol') 
        or this number of iterations. 
        For stochastic solvers ('sgd', 'adam'), 
        note that this determines the number of epochs 
        (how many times each data point will be used), 
        not the number of gradient steps.
    shuffle : bool, optional, default True
        Whether to shuffle samples in each iteration. 
        Only used when solver='sgd' or 'adam'.

#Attributes:
classes_ : array or list of array of shape (n_classes,)
    Class labels for each output.
loss_ : float
    The current loss computed with the loss function.
coefs_ : list, length n_layers - 1
    The ith element in the list represents the 
    weight matrix corresponding to layer i.
intercepts_ : list, length n_layers - 1
    The ith element in the list represents 
    the bias vector corresponding to layer i + 1.
n_iter_ : int,
    The number of iterations the solver has ran.
n_layers_ : int
    Number of layers.
n_outputs_ : int
    Number of outputs.
out_activation_ : string
    Name of the output activation function.
 
#Methods
fit(X, y)           Fit the model to data matrix X and target(s) y. 
get_params([deep])  Get parameters for this estimator. 
predict(X)          Predict using the multi-layer perceptron classifier 
predict_log_proba(X) Return the log of probability estimates. (only for classifier)
predict_proba(X)    Probability estimates. (only for classifier)
score(X, y[, sample_weight]) Returns the mean accuracy on the given test data and labels. 
set_params(**params)    Set the parameters of this estimator. 

#MLP trains on two arrays: array X of size (n_samples, n_features), 
#which holds the training samples represented as floating point feature vectors; 
#and array y of size (n_samples,), which holds the target values (class labels) for the training samples:

from sklearn.neural_network import MLPClassifier
X = [[0., 0.], [1., 1.]]
y = [0, 1]
>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                hidden_layer_sizes=(5, 2), random_state=1)

>>> clf.fit(X, y)                         
MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
       beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(5, 2), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,
       warm_start=False)

#predict labels for new samples:
>>> clf.predict([[2., 2.], [-1., -2.]])
array([1, 0])

#MLP can fit a non-linear model to the training data. 
#clf.coefs_ contains the weight matrices that constitute the model parameters:
>>>>>> [coef.shape for coef in clf.coefs_]
[(2, 5), (5, 2), (2, 1)]

#Currently, MLPClassifier supports only the Cross-Entropy loss function, 
#which allows probability estimates by running the predict_proba method.
#giving a vector of probability estimates P(y|x) per sample x:
>>> clf.predict_proba([[2., 2.], [1., 2.]])  
array([[  1.967...e-04,   9.998...-01],
       [  1.967...e-04,   9.998...-01]])
       
#MLPClassifier supports multi-class classification 
#by applying Softmax as the output function.

#the model supports multi-label classification 
#in which a sample can belong to more than one class. 
#For each class, the raw output passes through the logistic function. 
#Values larger or equal to 0.5 are rounded to 1, otherwise to 0. 

X = [[0., 0.], [1., 1.]]
y = [[0, 1], [1, 1]]
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                    hidden_layer_sizes=(15,), random_state=1)

>>> clf.fit(X, y)                         
MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
       beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(15,), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,
       warm_start=False)
#For a predicted output of a sample, 
#the indices where the value is 1 represents 
#the assigned classes of that sample:
>>> clf.predict([[1., 2.]])
array([[1, 1]])
>>> clf.predict([[0., 0.]])
array([[0, 1]])


##Example MNIST 
print(__doc__)
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_mldata
from sklearn.neural_network import MLPClassifier
mnist = fetch_mldata("MNIST original")
# rescale the data, use the traditional train/test split
X, y = mnist.data / 255., mnist.target
X_train, X_test = X[:60000], X[60000:]
y_train, y_test = y[:60000], y[60000:]
# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)
mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,
                    solver='sgd', verbose=10, tol=1e-4, random_state=1,
                    learning_rate_init=.1)
mlp.fit(X_train, y_train)
print("Training set score: %f" % mlp.score(X_train, y_train))
print("Test set score: %f" % mlp.score(X_test, y_test))
fig, axes = plt.subplots(4, 4)
# use global min / max to ensure all weights are shown on the same scale
vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()
for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):
    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,
               vmax=.5 * vmax)
    ax.set_xticks(())
    ax.set_yticks(())
plt.show()



###Scikit - Isotonic regression 
#Fitting to only increasing(or decreasing) data (must) by a piecewise linear function 

class sklearn.isotonic.IsotonicRegression(y_min=None, y_max=None, increasing=True, out_of_bounds=’nan’)

#Methods
fit(X, y[, sample_weight])  Fit the model using X, y as training data. 
fit_transform(X[, y])       Fit to data, then transform it. 
                            X : array-like, shape=(n_samples,)
                                Training data.
                            y : array-like, shape=(n_samples,)
                                Training target.
get_params([deep])          Get parameters for this estimator. 
predict(T)                  Predict new data by linear interpolation. 
score(X, y[, sample_weight]) Returns the coefficient of determination R^2 of the prediction. 
set_params(**params)        Set the parameters of this estimator. 
transform(T)                Transform new data by linear interpolation 

#Example 
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection

from sklearn.linear_model import LinearRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.utils import check_random_state

n = 100
x = np.arange(n)
rs = check_random_state(0)
y = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))

# Fit IsotonicRegression and LinearRegression models

ir = IsotonicRegression()
y_ = ir.fit_transform(x, y)

lr = LinearRegression()
lr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression

# Plot result
segments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]
lc = LineCollection(segments, zorder=0)
lc.set_array(np.ones(len(y)))
lc.set_linewidths(0.5 * np.ones(n))

fig = plt.figure()
plt.plot(x, y, 'r.', markersize=12)
plt.plot(x, y_, 'g.-', markersize=12)
plt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')
plt.gca().add_collection(lc)
plt.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')
plt.title('Isotonic regression')
plt.show()


















###Scikit External projects - scikit_pandas - Quick Intro 
#bridge for scikit-learn pipelines and pandas data frame 
#with dedicated transformers.

$ pip install sklearn-pandas

##scikit-pandas - Main use case 
    •DataFrameMapper, a class for mapping pandas data frame columns to different sklearn transformations
    •cross_val_score, similar to sklearn.cross_validation.cross_val_score but working on pandas DataFrames

#Note 
#some transformers expect a 1-D input (the label-oriented ones) 
#while some others, like OneHotEncoder or Imputer, expect 2D input, 
#with the shape [n_samples, n_features].
#while some(eg StandardScaler) takes both 1D and 2D 

##Transformations required 
For categorical input 
 (prefix-Label - takes 1D)
 (without prefix Label - takes 2D)   
 with string multiclass/binary y and x 
    Use LabelBinarizer() to convert to 0/1 for binary OR one hot encoding(becomes multilabel for y) for multiclass
    Use LabelEncoder() to convert to 0...No_Of_classes
 with number multiclass/binary 
   Scikit takes directly for y 
   for X,       
    Use LabelEncoder() to convert to 0...No_Of_classes
    Use LabelBinarizer() to convert to 0/1 for binary OR one hot encoding(becomes multilabel) for multiclass
    Use OneHotEncoder() to convert to one hot encoding
 Use MultiLabelBinarizer() to convert from string/numeric multilabel to proper format 
For float/int, it better to normalize by StandardScaler()(Not Normalizer() as Normalizer is meant with l1/l2 norms)
For sparse data, use MaxAbsScaler()
Data with many outliers, use RobustScaler
To convert a numeric feature to binary 0/1, use Binarizer  
For text feature, transform using TfidfVectorizer (=CountVectorizer + TfidfTransformer)


#Example 

from sklearn_pandas import DataFrameMapper, cross_val_score

import pandas as pd
import numpy as np
import sklearn.preprocessing, sklearn.decomposition, \
    sklearn.linear_model, sklearn.pipeline, sklearn.metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn_pandas import DataFrameMapper, cross_val_score


data = pd.DataFrame({'pet':      ['cat', 'dog', 'dog', 'fish', 'cat', 'dog', 'cat', 'fish'],
                      'children': [4., 6, 3, 3, 2, 3, 5, 4],
                      'salary':   [90, 24, 44, 27, 32, 59, 36, 27]})
                      
#Example of LabelBinarizer and StandardScaler
#Note LabelBinarizer takes 1D, hence 'pet 
#StandardScaler takes 2D, hence ['children']
mapper = DataFrameMapper([
    ('pet', sklearn.preprocessing.LabelBinarizer()),
     (['children'], sklearn.preprocessing.StandardScaler())
 ])
>>> np.round(mapper.fit_transform(data.copy()), 2)
array([[ 1.  ,  0.  ,  0.  ,  0.21],
       [ 0.  ,  1.  ,  0.  ,  1.88],
       [ 0.  ,  1.  ,  0.  , -0.63],
       [ 0.  ,  0.  ,  1.  , -0.63],
       [ 1.  ,  0.  ,  0.  , -1.46],
       [ 0.  ,  1.  ,  0.  , -0.63],
       [ 1.  ,  0.  ,  0.  ,  1.04],
       [ 0.  ,  0.  ,  1.  ,  0.21]])
       
#Example of LabelEncoder 
mapper = DataFrameMapper([
     ('pet', sklearn.preprocessing.LabelEncoder()),
     (['children'], sklearn.preprocessing.StandardScaler())
 ])
>>> np.round(mapper.fit_transform(data.copy()), 2)
array([[ 0.  ,  0.21],
       [ 1.  ,  1.88],
       [ 1.  , -0.63],
       [ 2.  , -0.63],
       [ 0.  , -1.46],
       [ 1.  , -0.63],
       [ 0.  ,  1.04],
       [ 2.  ,  0.21]])
#Create another DF with pet as numerical Categorical 
df2 = pd.DataFrame(np.round(mapper.fit_transform(data.copy()), 2))
df2.columns=['pet', 'children']
>>> df2
   pet  children
0  0.0      0.21
1  1.0      1.88
2  1.0     -0.63
3  2.0     -0.63
4  0.0     -1.46
5  1.0     -0.63
6  0.0      1.04
7  2.0      0.21
#Example of LabelBinarizer with numeric categorical 
mapper = DataFrameMapper([
    ('pet', sklearn.preprocessing.LabelBinarizer()),
     (['children'], sklearn.preprocessing.StandardScaler())
 ])
>>> np.round(mapper.fit_transform(df2.copy()), 2)
array([[ 1.  ,  0.  ,  0.  ,  0.21],
       [ 0.  ,  1.  ,  0.  ,  1.88],
       [ 0.  ,  1.  ,  0.  , -0.63],
       [ 0.  ,  0.  ,  1.  , -0.63],
       [ 1.  ,  0.  ,  0.  , -1.46],
       [ 0.  ,  1.  ,  0.  , -0.63],
       [ 1.  ,  0.  ,  0.  ,  1.04],
       [ 0.  ,  0.  ,  1.  ,  0.21]])
#Example of MultiLabelBinarizer
#MultiLabelBinarizer takes 1D or 2D(for multilabel)
mapper = DataFrameMapper([
     ('pet', sklearn.preprocessing.MultiLabelBinarizer()),
     (['children'], sklearn.preprocessing.StandardScaler())
 ])
>>> np.round(mapper.fit_transform(data.copy()), 2)
array([[ 1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
         1.  ,  0.21],
       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,
         0.  ,  1.88],
       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,
         0.  , -0.63],
       [ 0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  1.  ,  0.  ,  1.  ,
         0.  , -0.63],
       [ 1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
         1.  , -1.46],
       [ 0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,
         0.  , -0.63],
       [ 1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
         1.  ,  1.04],
       [ 0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  1.  ,  0.  ,  1.  ,
         0.  ,  0.21]])

#Example of OneHotEncoder
#Note OneHotEncoder takes only numeric categorical and takes 2D 
mapper = DataFrameMapper([
     ('pet', sklearn.preprocessing.OneHotEncoder()),
     (['children'], sklearn.preprocessing.StandardScaler())
 ])
>>> np.round(mapper.fit_transform(data.copy()), 2)
ValueError: pet: could not convert string to float: 'fish'

#Example of Binarizer
#Binarizertakes only numeric and 2D 
mapper = DataFrameMapper([
    ('pet', sklearn.preprocessing.Binarizer()),
     (['children'], sklearn.preprocessing.StandardScaler())
 ])
>>> np.round(mapper.fit_transform(data.copy()), 2)
ValueError: pet: could not convert string to float: 'fish'

mapper = DataFrameMapper([
     (['pet'], sklearn.preprocessing.OneHotEncoder()),
     (['children'], sklearn.preprocessing.Binarizer())
 ])
>>> np.round(mapper.fit_transform(df2.copy()), 2)
array([[1., 0., 0., 1.],
       [0., 1., 0., 1.],
       [0., 1., 0., 0.],
       [0., 0., 1., 0.],
       [1., 0., 0., 0.],
       [0., 1., 0., 0.],
       [1., 0., 0., 1.],
       [0., 0., 1., 1.]])
       

##Detailed example 
#mapper contains list of tuples, 
#each element is ( column_name, transformer, dict_of_options)
mapper = DataFrameMapper([
    ('pet', sklearn.preprocessing.LabelBinarizer()),
     (['children'], sklearn.preprocessing.StandardScaler())
 ])
>>> np.round(mapper.fit_transform(data.copy()), 2)
array([[ 1.  ,  0.  ,  0.  ,  0.21],
       [ 0.  ,  1.  ,  0.  ,  1.88],
       [ 0.  ,  1.  ,  0.  , -0.63],
       [ 0.  ,  0.  ,  1.  , -0.63],
       [ 1.  ,  0.  ,  0.  , -1.46],
       [ 0.  ,  1.  ,  0.  , -0.63],
       [ 1.  ,  0.  ,  0.  ,  1.04],
       [ 0.  ,  0.  ,  1.  ,  0.21]])
 
##transforming new sample 
#Note transform takes DF 
>> sample = pd.DataFrame({'pet': ['cat'], 'children': [5.]})
>>> np.round(mapper.transform(sample), 2)
array([[ 1.  ,  0.  ,  0.  ,  1.04]])
 
##Output features names
>>> mapper.transformed_names_
['pet_cat', 'pet_dog', 'pet_fish', 'children']
 
##Custom column names for transformed features
mapper_alias = DataFrameMapper([
    (['children'], sklearn.preprocessing.StandardScaler(),
     {'alias': 'children_scaled'})
])
_ = mapper_alias.fit_transform(data.copy())
>>> mapper_alias.transformed_names_
['children_scaled']
 
##Passing Series/DataFrames to the transformers
#By default the transformers are passed a numpy array of the selected columns as input.
#To pass a dataframe/series to the transformers to handle custom cases 
#use input_df=True:
from sklearn.base import TransformerMixin
class DateEncoder(TransformerMixin):
   def fit(self, X, y=None):
       return self
   def transform(self, X):
       dt = X.dt
       return pd.concat([dt.year, dt.month, dt.day], axis=1)


dates_df = pd.DataFrame( {'dates': pd.date_range('2015-10-30', '2015-11-02')})
mapper_dates = DataFrameMapper([
                    ('dates', DateEncoder())
                ], input_df=True)
>>> mapper_dates.fit_transform(dates_df)
array([[2015,   10,   30],
       [2015,   10,   31],
       [2015,   11,    1],
       [2015,   11,    2]])
       
#or specify this option per group of columns 
mapper_dates = DataFrameMapper([
                ('dates', DateEncoder(), {'input_df': True})
            ])
>>> mapper_dates.fit_transform(dates_df)
array([[2015,   10,   30],
       [2015,   10,   31],
       [2015,   11,    1],
       [2015,   11,    2]])
 
##Outputting a dataframe
#By default the output of the dataframe mapper is a numpy array. 
#Or us  df_out 
#Note this does not work together with the default=True or sparse=True arguments to the mapper.
mapper_df = DataFrameMapper([
            ('pet', sklearn.preprocessing.LabelBinarizer()),
            (['children'], sklearn.preprocessing.StandardScaler())
        ], df_out=True)
>>> np.round(mapper_df.fit_transform(data.copy()), 2)
   pet_cat  pet_dog  pet_fish  children
0      1.0      0.0       0.0      0.21
1      0.0      1.0       0.0      1.88
2      0.0      1.0       0.0     -0.63
3      0.0      0.0       1.0     -0.63
4      1.0      0.0       0.0     -1.46
5      0.0      1.0       0.0     -0.63
6      1.0      0.0       0.0      1.04
7      0.0      0.0       1.0      0.21
 
##Transform Multiple Columns together 
#Transformations may require multiple input columns. 
#In these cases, the column names can be specified in a list:
mapper2 = DataFrameMapper([
        (['children', 'salary'], sklearn.decomposition.PCA(1))
    ])
>>> np.round(mapper2.fit_transform(data.copy()), 1)
array([[ 47.6],
       [-18.4],
       [  1.6],
       [-15.4],
       [-10.4],
       [ 16.6],
       [ -6.4],
       [-15.4]])
 
##Multiple transformers for the same column
mapper3 = DataFrameMapper([
            (['age'], [sklearn.preprocessing.Imputer(),
                       sklearn.preprocessing.StandardScaler()])])
data_3 = pd.DataFrame({'age': [1, np.nan, 3]})
>>> mapper3.fit_transform(data_3)
array([[-1.22474487],
       [ 0.        ],
       [ 1.22474487]])
 
##Columns that don't need any transformation - use None 
#Only columns that are listed in the DataFrameMapper are kept. 
mapper3 = DataFrameMapper([
        ('pet', sklearn.preprocessing.LabelBinarizer()),
        ('children', None)
    ])
>>> np.round(mapper3.fit_transform(data.copy()))
array([[ 1.,  0.,  0.,  4.],
       [ 0.,  1.,  0.,  6.],
       [ 0.,  1.,  0.,  3.],
       [ 0.,  0.,  1.,  3.],
       [ 1.,  0.,  0.,  2.],
       [ 0.,  1.,  0.,  3.],
       [ 1.,  0.,  0.,  5.],
       [ 0.,  0.,  1.,  4.]])
 
##Applying a default transformer
#A default transformer can be applied to columns not explicitly selected passing it 
#Use default = transformer 
#Using default=False (the default) drops unselected columns. 
#Using default=None pass the unselected columns unchanged.
mapper4 = DataFrameMapper([
            ('pet', sklearn.preprocessing.LabelBinarizer()),
            ('children', None)
        ], default=sklearn.preprocessing.StandardScaler())
>>> np.round(mapper4.fit_transform(data.copy()), 1)
array([[ 1. ,  0. ,  0. ,  4. ,  2.3],
       [ 0. ,  1. ,  0. ,  6. , -0.9],
       [ 0. ,  1. ,  0. ,  3. ,  0.1],
       [ 0. ,  0. ,  1. ,  3. , -0.7],
       [ 1. ,  0. ,  0. ,  2. , -0.5],
       [ 0. ,  1. ,  0. ,  3. ,  0.8],
       [ 1. ,  0. ,  0. ,  5. , -0.3],
       [ 0. ,  0. ,  1. ,  4. , -0.7]])

 
##Same transformer for the multiple columns
#To simplify this process, use gen_features function 
#which accepts a list of columns and feature transformer class(or list of classes), 
#and generates a feature definition, acceptable by DataFrameMapper.

from sklearn_pandas import gen_features
feature_def = gen_features(
        columns=['col1', 'col2', 'col3'],
        classes=[sklearn.preprocessing.LabelEncoder]
    )
>>> feature_def
[('col1', [LabelEncoder()]), ('col2', [LabelEncoder()]), ('col3', [LabelEncoder()])]
mapper5 = DataFrameMapper(feature_def)
data5 = pd.DataFrame({
        'col1': ['yes', 'no', 'yes'],
        'col2': [True, False, False],
        'col3': ['one', 'two', 'three']
    })
>>> mapper5.fit_transform(data5)
array([[1, 1, 0],
       [0, 0, 2],
       [1, 0, 1]])
       
       
#If it is required to override some of transformer parameters, 
#then a dict with 'class' key and transformer parameters should be provided. 
feature_def = gen_features(
        columns=[['col1'], ['col2'], ['col3']],
        classes=[{'class': sklearn.preprocessing.Imputer, 'strategy': 'most_frequent'}]
    )
mapper6 = DataFrameMapper(feature_def)
data6 = pd.DataFrame({
        'col1': [None, 1, 1, 2, 3],
        'col2': [True, False, None, None, True],
        'col3': [0, 0, 0, None, None]
    })
>>> mapper6.fit_transform(data6)
array([[ 1.,  1.,  0.],
       [ 1.,  0.,  0.],
       [ 1.,  1.,  0.],
       [ 2.,  1.,  0.],
       [ 3.,  1.,  0.]])
 
##Feature selection and other supervised transformations
#DataFrameMapper supports transformers(ie feature selection) that require both X and y arguments. 

#Example - Treating the 'pet' column as the target, 
#we will select the column that best predicts it.

from sklearn.feature_selection import SelectKBest, chi2
mapper_fs = DataFrameMapper([
        (['children','salary'], SelectKBest(chi2, k=1))
    ])
#'children','salary' is converted to one column 
>>> mapper_fs.fit_transform(data[['children','salary']], data['pet'])
array([[ 90.],
       [ 24.],
       [ 44.],
       [ 27.],
       [ 32.],
       [ 59.],
       [ 36.],
       [ 27.]])
>>> mapper_fs.transformed_names_
['children_salary']

##Working with sparse features
#A DataFrameMapper will return a dense feature array by default. 
#Setting sparse=True in the mapper will return a sparse array 
#whenever any of the extracted features is sparse. 

>>> mapper5 = DataFrameMapper([
        ('pet', CountVectorizer()),
    ], sparse=True)
>>> type(mapper5.fit_transform(data))
<class 'scipy.sparse.csr.csr_matrix'>
 
##Cross-Validation with pandas DataFrame 
pipe = sklearn.pipeline.Pipeline([
        ('featurize', mapper),
        ('lm', sklearn.linear_model.LinearRegression())])
>>> np.round(cross_val_score(pipe, X=data.copy(), y=data.salary, scoring='r2'), 2)
array([ -1.09,  -5.3 , -15.38])
 
 
##CategoricalImputer
#scikit-learn Imputer transformer currently only works with numbers, 
#sklearn-pandas provides CategoricalImputer with strings, 
#substituting null values with the most frequent value in that column.

from sklearn_pandas import CategoricalImputer
data = np.array(['a', 'b', 'b', np.nan], dtype=object)
imputer = CategoricalImputer()
>>> imputer.fit_transform(data)
array(['a', 'b', 'b', 'b'], dtype=object)

##Example 
#Let's go through an example from Kaggle, the Titanic dataset. 
#The task here is to predict who will survive on Titanic, 
#based on a subset of whole dataset.


from __future__ import division
import csv as csv
import numpy as np

import pandas as pd

from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import KFold, StratifiedKFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.grid_search import GridSearchCV
from sklearn.preprocessing import LabelEncoder, LabelBinarizer

from sklearn_pandas import DataFrameMapper

df_train = pd.read_csv('titanic_train.csv', header = 0, index_col = 'PassengerId')
df_test = pd.read_csv('titanic_test.csv', header = 0, index_col = 'PassengerId')

#axis=0 , means vertical stacking 
df = pd.concat([df_train, df_test], keys=["train", "test"])

#Setting up 

df['Title'] = df['Name'].apply(lambda c: c[c.index(',') + 2 : c.index('.')])
df['LastName'] = df['Name'].apply(lambda n: n[0:n.index(',')])
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df.loc[df['Embarked'].isnull(), 'Embarked'] = df['Embarked'].mode()[0]
df.loc[df['Fare'].isnull(), 'Fare'] = df['Fare'].mode()[0]
df['FamilyID'] = df['LastName'] + ':' + df['FamilySize'].apply(str)
df.loc[df['FamilySize'] <= 2, 'FamilyID'] = 'Small_Family'

df['AgeOriginallyNaN'] = df['Age'].isnull().astype(int)
medians_by_title = pd.DataFrame(df.groupby('Title')['Age'].median()) \
  .rename(columns = {'Age': 'AgeFilledMedianByTitle'})
df = df.merge(medians_by_title, left_on = 'Title', right_index = True) \
  .sort_index(level = 0).sort_index(level = 1)
  
#after processing , split 
df_train = df.ix['train']
df_test  = df.ix['test']


#to create dummy variables out of categorical ones. 
#In Scikit ,algorithms accept only float variables.

def featurize(features):
  transformations = [
                      ('Embarked', LabelBinarizer()),
                      ('Fare', None),
                      ('Parch', None),
                      ('Pclass', LabelBinarizer()),
                      ('Sex', LabelBinarizer()),
                      ('SibSp', None),                                       
                      ('Title', LabelBinarizer()),
                      ('FamilySize', None),
                      ('FamilyID', LabelBinarizer()),
                      ('AgeOriginallyNaN', None),
                      ('AgeFilledMedianByTitle', None)]

  return DataFrameMapper(filter(lambda x: x[0] in df.columns, transformations))
 
#Create pipeline 
features = ['Sex', 'Title', 'FamilySize', 'AgeFilledMedianByTitle',
            'Embarked', 'Pclass', 'FamilyID', 'AgeOriginallyNaN']

pipeline = Pipeline([('featurize', featurize(features)), ('forest', RandomForestClassifier())])

#fit 
#put the pandas dataframe X and y directly, without explicitly transforming into numpy array
X = df_train[df_train.columns.drop('Survived')]
y = df_train['Survived']
model = pipeline.fit(X = X, y = y)

#prediction
prediction = model.predict(df_test)

#Modifying model
simple_pipeline = Pipeline([('featurize', featurize(['Sex'])), ('lm', LogisticRegression())])
model = simple_pipeline.fit(X = X, y = y).predict(df_test)

#Cross validation- to validate our model. 

>>> cross_val_score(pipeline, X, y, 'accuracy') #3-fold cross validation.
array([ 0.74747475,  0.8013468 ,  0.82491582])

#other CV tsrategy 
stratified_kfold = StratifiedKFold(df_train['Survived'] == 1, n_folds = 10)
cross_val_score(simple_pipeline, X, y, 'accuracy', cv = stratified_kfold)

#Grid Search
features = ['Sex', 'Title', 'FamilySize', 'AgeFilledMedianByTitle',
            'Embarked', 'Pclass', 'FamilyID', 'AgeOriginallyNaN']

pipeline = Pipeline([('featurize', featurize(features)), ('forest', RandomForestClassifier(n_estimators = 10))])

pipeline.fit(X, y).predict(df_train)

param_grid = dict(forest__n_estimators = [2, 16, 32], forest__criterion = ['gini', 'entropy'])

grid_search = GridSearchCV(pipeline, param_grid=param_grid, scoring='accuracy')

best_pipeline = grid_search.fit(X, y).best_estimator_
best_pipeline.get_params()['forest']

#Or with other CV 
grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv = stratified_kfold, scoring='accuracy')
best_pipeline = grid_search.fit(X, y).best_estimator_
best_pipeline.get_params()['forest']







###Scikit External projects - scikit-plot - Quick Intro 
#scikit-plot A visualization library for quick and easy generation of common plots in data analysis and machine learning
$ pip install scikit-plot 

#Example 
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
X, y = load_digits(return_X_y=True)
from sklearn.ensemble import RandomForestClassifier
random_forest_clf = RandomForestClassifier(n_estimators=5, max_depth=5, random_state=1)
#prediction 
from sklearn.model_selection import cross_val_predict
predictions = cross_val_predict(random_forest_clf, X, y)

#Set normalize=True so the values displayed in [0,1]
import scikitplot as skplt
skplt.metrics.plot_confusion_matrix(y, predictions, normalize=True)
plt.show()

#Example with keras 
# Import what's needed for the Functions API
import matplotlib.pyplot as plt
import scikitplot as skplt
# This is a Keras classifier. 
#We'll generate probabilities on the test set.
keras_clf.fit(X_train, y_train, batch_size=64, nb_epoch=10, verbose=2)
probas = keras_clf.predict_proba(X_test, batch_size=64)
# Now plot.
skplt.metrics.plot_precision_recall_curve(y_test, probas)
plt.show()


##scikit-plot - Metrics Module (API Reference)
#Confusion metrics
#Decide  cut-off for distinguishing persons with from those without disease. 
#The cut-off determines 
    the clinical sensitivity (fraction of true positives to all with disease) 
    and specificity (fraction of true negatives to all without disease). 

#Chanding cut-off, you will get other values 
#for true positives and negatives and false positives and negatives,

                    Disease    
            +                       –
    +   True positive (TP)     False positive (FP)     Positive predictive value (PPV)=TP / (TP + FP)
Test                           
    –   False negative (FN)    True negative (TN)
        All with disease       All without disease
            = TP + FN          = FP + TN
        Sensitivity            Specificity
        = TP / (TP + FN)       = TN / (TN + FP)


scikitplot.metrics.plot_confusion_matrix(y_true, y_pred, labels=None, true_labels=None, pred_labels=None, title=None, normalize=False, hide_zeros=False, x_tick_rotation=0, ax=None, figsize=None, cmap='Blues', title_fontsize='large', text_fontsize='medium')
    Generates confusion matrix plot from predictions and true labels

#Example 
import scikitplot as skplt
rf = RandomForestClassifier()
rf = rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)
plt.show()


scikitplot.metrics.plot_roc_curve(y_true, y_probas, title='ROC Curves', curves=('micro', 'macro', 'each_class'), ax=None, figsize=None, cmap='nipy_spectral', title_fontsize='large', text_fontsize='medium')
    Generates the ROC curves from labels and predicted scores/probabilities
    The ROC curve is the plot between sensitivity and (1- specificity). 
    ROC (Receiver operating characteristic) curve area(AUC) should be one 
    •.90-1 = excellent (A)
    •.80-.90 = good (B)
    •.70-.80 = fair (C)
    •.60-.70 = poor (D)
    •.50-.60 = fail (F)
    Gini = 2*AUC – 1
    Gini above 60% is a good model

    
#Example 
import scikitplot as skplt
nb = GaussianNB()
nb = nb.fit(X_train, y_train)
y_probas = nb.predict_proba(X_test)
skplt.metrics.plot_roc_curve(y_test, y_probas)
plt.show()

#Multiclass 
from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from scipy import interp

# Import some data to play with
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Binarize the output
y = label_binarize(y, classes=[0, 1, 2])
n_classes = y.shape[1]

# Add noisy features to make the problem harder
random_state = np.random.RandomState(0)
n_samples, n_features = X.shape
X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]

# shuffle and split training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
                                                    random_state=0)

# Learn to predict each class against the other
classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,
                                 random_state=random_state))
y_score = classifier.fit(X_train, y_train).decision_function(X_test)

y_score = nb.decision_function(X_test)
# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])




scikitplot.metrics.plot_ks_statistic(y_true, y_probas, title='KS Statistic Plot', ax=None, figsize=None, title_fontsize='large', text_fontsize='medium')
    Generates the KS Statistic plot from labels 
    and scores/probabilities
    K-S or Kolmogorov-Smirnov chart measures performance of classification models. 
    K-S is a measure of the degree of separation between the positive and negative distributions. 
    The K-S is 100 for best case 
    KS score is found from max seperation in graph between positive and negative 
    
#Example
import scikitplot as skplt
lr = LogisticRegression()
lr = lr.fit(X_train, y_train)
y_probas = lr.predict_proba(X_test)
skplt.metrics.plot_ks_statistic(y_test, y_probas)
plt.show()




scikitplot.metrics.plot_precision_recall_curve(y_true, y_probas, title='Precision-Recall Curve', curves=('micro', 'each_class'), ax=None, figsize=None, cmap='nipy_spectral', title_fontsize='large', text_fontsize='medium')
    Generates the Precision Recall Curve from labels and probabilities
    By changing cutoff, we get a series of recall and precision
    Draw graph with 
    • The x-axis showing recall (= sensitivity = TP / (TP + FN)) 
    • The y-axis showing precision (= positive predictive value = TP / (TP + FP))
    Precision-recall curves are often zigzag curves frequently going up and down
    The perfect test has no overlap of results for persons with and without disease, respectively. 
    The perfect test is thus able to discriminate between persons with and without disease 
    with 100 % sensitivity (= recall), 100 % specificity and 100 % precision (= positive predictive value). 
    

#Example
import scikitplot as skplt
nb = GaussianNB()
nb.fit(X_train, y_train)
y_probas = nb.predict_proba(X_test)
skplt.metrics.plot_precision_recall_curve(y_test, y_probas)
plt.show()

y_score = classifier.decision_function(X_test)
#Compute the average precision score
from sklearn.metrics import average_precision_score
average_precision = average_precision_score(y_test, y_score)
#Precision-Recall points 
from sklearn.metrics import precision_recall_curve
precision, recall, _ = precision_recall_curve(y_test, y_score)

#for Multillabel  
from sklearn.preprocessing import label_binarize
# Use label_binarize to be multi-label like settings
Y = label_binarize(y, classes=[0, 1, 2])
n_classes = Y.shape[1]
# Split into training and test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5,
                                                    random_state=random_state)
# We use OneVsRestClassifier for multi-label prediction
from sklearn.multiclass import OneVsRestClassifier
# Run classifier
classifier = OneVsRestClassifier(svm.LinearSVC(random_state=random_state))
classifier.fit(X_train, Y_train)
y_score = classifier.decision_function(X_test)
#The average precision score in multi-label settings
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
# For each class
precision = dict()
recall = dict()
average_precision = dict()
for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],y_score[:, i])
    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])

# A "micro-average": quantifying score on all classes jointly
precision["micro"], recall["micro"], _ = precision_recall_curve(Y_test.ravel(), y_score.ravel())
average_precision["micro"] = average_precision_score(Y_test, y_score,average="micro")
print('Average precision score, micro-averaged over all classes: {0:0.2f}'
      .format(average_precision["micro"]))






scikitplot.metrics.plot_silhouette(X, cluster_labels, title='Silhouette Analysis', metric='euclidean', copy=True, ax=None, figsize=None, cmap='nipy_spectral', title_fontsize='large', text_fontsize='medium')
    Plots silhouette analysis of clusters provided.
        X (array-like, shape (n_samples, n_features)) 
            Data to cluster, where n_samples is the number of samples 
            and n_features is the number of features.
        cluster_labels (array-like, shape (n_samples,)) 
            Cluster label for each sample.
    Silhouette analysis can be used to study the separation distance between the resulting clusters
    Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.
    silhouette analysis is used to choose an optimal value for n_clusters
    
#Example
import scikitplot as skplt
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, random_state=1)
cluster_labels = kmeans.fit_predict(X)
# The silhouette_score gives the average value for all the samples.
# This gives a perspective into the density and separation of the formed
# clusters
silhouette_avg = silhouette_score(X, cluster_labels)
# Compute the silhouette scores for each sample
sample_silhouette_values = silhouette_samples(X, cluster_labels)  

skplt.metrics.plot_silhouette(X, cluster_labels)
plt.show()





scikitplot.metrics.plot_calibration_curve(y_true, probas_list, clf_names=None, n_bins=10, title='Calibration plots (Reliability Curves)', ax=None, figsize=None, cmap='nipy_spectral', title_fontsize='large', text_fontsize='medium')
    Plots calibration curves for a set of classifier probability estimates.
    Plotting the calibration curves of a classifier is useful 
    for determining whether or not you can interpret their predicted probabilities directly 
    as as confidence level. 
    For instance, a well-calibrated binary classifier should classify the samples 
    such that for samples to which it gave a score of 0.8, 
    around 80% should actually be from the positive class.
    This function currently only works for binary classification.
    Parameters:	
        y_true (array-like, shape (n_samples)) – Ground truth (correct) target values.
        probas_list (list of array-like, shape (n_samples, 2) or (n_samples,)) – A list containing the outputs of binary classifiers' predict_proba() method or decision_function() method.
        
#Example
import scikitplot as skplt
rf = RandomForestClassifier()
lr = LogisticRegression()
nb = GaussianNB()
svm = LinearSVC()
rf_probas = rf.fit(X_train, y_train).predict_proba(X_test)
lr_probas = lr.fit(X_train, y_train).predict_proba(X_test)
nb_probas = nb.fit(X_train, y_train).predict_proba(X_test)
svm_scores = svm.fit(X_train, y_train).decision_function(X_test)
probas_list = [rf_probas, lr_probas, nb_probas, svm_scores]
clf_names = ['Random Forest', 'Logistic Regression','Gaussian Naive Bayes', 'Support Vector Machine']
skplt.metrics.plot_calibration_curve(y_test,probas_list,clf_names)
plt.show()


#Step 1 : Calculate probability for each observation
#Step 2 : Rank these probabilities in decreasing order.
#Step 3 : Build 10 groups having almost 10% of the observations.
#         Count how many to class 0(xn) and how many 1(yn) on sorted probabilities
#         and each group population 10% (ie 10% is baseline positive from random model)
#         Sum total xn(xT) and total yn(yT)
#Step 4 : Calculate the response rate 
#            %right = yn/yT 
#            %wrong = xn/xT 
#            lift@group = %right/%population 
#            lift = cummulative %right/cummulative %population
#            cummulative %right  (=gain)
#            cummulative %population 
#Cumulative Gain chart is the graph of Cumulative %Right and Cummulative %Population with x=1..no of groups
#Lift curve is the plot of lift and 100% with x=1,... noofgroups. 
#Note that for a random model, this always stays flat at 100%          
          
          
          

scikitplot.metrics.plot_cumulative_gain(y_true, y_probas, title='Cumulative Gains Curve', ax=None, figsize=None, title_fontsize='large', text_fontsize='medium')
    Generates the Cumulative Gains Plot 
    from labels and scores/probabilities
    The cumulative gains chart is used to determine the effectiveness of a binary classifier. 
    only for binary classification.
    The greater the area between the  curve and the baseline, the better the model
    Parameters:	
        y_true (array-like, shape (n_samples)) – Ground truth (correct) target values.
        y_probas (array-like, shape (n_samples, n_classes)) – Prediction probabilities for each class returned by a classifier.
        
        
#Example
import scikitplot as skplt
lr = LogisticRegression()
lr = lr.fit(X_train, y_train)
y_probas = lr.predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, y_probas)
plt.show()



scikitplot.metrics.plot_lift_curve(y_true, y_probas, title='Lift Curve', ax=None, figsize=None, title_fontsize='large', text_fontsize='medium')
    Generates the Lift Curve from labels and scores/probabilities
    The lift curve is used to determine the effectiveness of a binary classifier. 
    The greater the area between the  curve and the baseline, the better the model
    lift > 1 means the results from the predictive model are better than random
    Parameters:	
        y_true (array-like, shape (n_samples)) – Ground truth (correct) target values.
        y_probas (array-like, shape (n_samples, n_classes)) – Prediction probabilities for each class returned by a classifier.
#Example
import scikitplot as skplt
lr = LogisticRegression()
lr = lr.fit(X_train, y_train)
y_probas = lr.predict_proba(X_test)
skplt.metrics.plot_lift_curve(y_test, y_probas)
plt.show()



##Scikit-plot - Estimators Module (API Reference)
scikitplot.estimators.plot_learning_curve(clf, X, y, title='Learning Curve', cv=None, shuffle=False, random_state=None, train_sizes=None, n_jobs=1, scoring=None, ax=None, figsize=None, title_fontsize='large', text_fontsize='medium')
    Generates a plot of the train and test learning curves for a classifier.
    Parameters:	
        clf – Classifier instance that implements fit and predict methods.
        X (array-like, shape (n_samples, n_features)) – Training vector, where n_samples is the number of samples and n_features is the number of features.
        y (array-like, shape (n_samples) or (n_samples, n_features)) – Target relative to X for classification or regression; None for unsupervised learning.
        
        
#Example 
import scikitplot as skplt
rf = RandomForestClassifier()
skplt.estimators.plot_learning_curve(rf, X, y)
plt.show()

cv = someCV 
from sklearn.model_selection import learning_curve
train_sizes, train_scores, test_scores = learning_curve(
        rf, X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5))
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)
#Plot of train_scores_mean +/- train_scores_std wrt train_sizes
#Plot of test_scores_mean +/- test_scores_std wrt train_sizes



scikitplot.estimators.plot_feature_importances(clf, title='Feature Importance', feature_names=None, max_num_features=20, order='descending', x_tick_rotation=0, ax=None, figsize=None, title_fontsize='large', text_fontsize='medium')
    Generates a plot of a classifier's feature importances.
    Higher the value for a feature- the better 
    Parameters:	
        clf – Classifier instance that has a feature_importances_ attribute, e.g. sklearn.ensemble.RandomForestClassifier or xgboost.XGBClassifier.
        title (string, optional) – Title of the generated plot. Defaults to 'Feature importances'.
        feature_names (None, list of string, optional) – Determines the feature names used to plot the feature importances. If None, feature names will be numbered.

#Example
import scikitplot as skplt
rf = RandomForestClassifier()
rf.fit(X, y)
skplt.estimators.plot_feature_importances(
    rf, feature_names=['petal length', 'petal width',
                       'sepal length', 'sepal width'])
plt.show()




##Scikit-plot - Clusterer Module (API Reference)
scikitplot.cluster.plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1, show_cluster_time=True, ax=None, figsize=None, title_fontsize='large', text_fontsize='medium')
    Plots elbow curve of different values of K for KMeans clustering.
    Plot between sum of squared errors (SSE) and x-axis=various k values 
    Choose k value which gives optimum SSE 
    Parameters:	
        clf – Clusterer instance that implements fit,fit_predict, and score methods, and an n_clusters hyperparameter. e.g. sklearn.cluster.KMeans instance
        X (array-like, shape (n_samples, n_features)) – Data to cluster, where n_samples is the number of samples and n_features is the number of features.
#Example
import scikitplot as skplt
kmeans = KMeans(random_state=1)
skplt.cluster.plot_elbow_curve(kmeans, cluster_ranges=range(1, 30))
plt.show()




##Scikit-plot - Decomposition Module (API Reference)
 scikitplot.decomposition.plot_pca_component_variance(clf, title='PCA Component Explained Variances', target_explained_variance=0.75, ax=None, figsize=None, title_fontsize='large', text_fontsize='medium')
    Plots PCA components' explained variance ratios. 
    Plot each component's explained variance ratio 
    = components's variance/sumtotal of all components variance 
    Parameters:	
        clf – PCA instance that has the explained_variance_ratio_ attribute.
#Example
import scikitplot as skplt
pca = PCA(random_state=1)
pca.fit(X)
skplt.decomposition.plot_pca_component_variance(pca)
plt.show()
#scikit-learn 
import numpy as np
from sklearn.decomposition import PCA
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
pca = PCA(n_components=2)
pca.fit(X)
>>> print(pca.explained_variance_ratio_)  
[ 0.99244...  0.00755...]
>>> print(pca.singular_values_)  
[ 6.30061...  0.54980...]




scikitplot.decomposition.plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None, figsize=None, cmap='Spectral', title_fontsize='large', text_fontsize='medium')
    Plots the 2-dimensional projection of PCA on a given dataset.
    Parameters:	
        clf – Fitted PCA instance that can transform given data set into 2 dimensions.
        X (array-like, shape (n_samples, n_features)) – Feature set to project, where n_samples is the number of samples and n_features is the number of features.
        y (array-like, shape (n_samples) or (n_samples, n_features)) – Target relative to X for labeling.
#Example
import scikitplot as skplt
pca = PCA(random_state=1)
pca.fit(X)
skplt.decomposition.plot_pca_2d_projection(pca, X, y)
plt.show()
        
        
        
        
        


###Scikit - Keras wrapper - DNN  

##Quick Intro - Keras Sequential model

#The Sequential model is a linear stack of layers.
#layers are Dense(every neuron connected to every other)
#then Activation and repeat of this combination 

#General terminology  
Steps: number of times the training loop in learning algorithm 
       will run to update the parameters in the model. 
       In each loop, one batch is taken 
Epoch :the number of times same data set is used for 
       extracting batches to feed the learning algorithm.
       
Batch size :the size of the chunk of data used in each loop  of the learning algorithm. 
pool_size:  size of the  pooling windows eg (2,2)
strides:    (dx,dy) amount to jump over while applying pool 
padding:    rows, columns zeros to add in the begining and end 
cropping:   rows, columns zeros to trim in the begining and end 
filters:    the dimensionality of the output space (i.e. the number output of filters).
kernel_size: (x,y) specifying the length of the convolution/pooling window.

        
##Layers 
Dense           Densely-connected NN layer
Activation      Applies an activation function to an output(softmax,elu,selu,softplus,softsign,relu,tanh,sigmoid,hard_sigmoid,linear)
LeakyReLU       Leaky version of a Rectified Linear Unit
PReLU           Parametric Rectified Linear Unit
ELU             Exponential Linear Unit.
ThresholdedReLU Thresholded Rectified Linear Unit.
Dropout         Randomly setting a fraction rate of input units to zero (solution of overfitting)
Flatten         Flattens the input
Reshape         Reshapes an output to a certain shape.
Permute         Permutes the dimensions of the input
RepeatVector    Repeats the input n times.
Lambda          Apply arbitrary function on inputs 
ActivityRegularization  Layer that applies an update to the cost function based input activity.
Masking         Masks a sequence by using a mask value to skip timesteps.
Conv1D          Convolvs kernel over inputs - 1D 
Conv2D
Conv3D
SeparableConv2D
Conv2DTranspose     Deconvolution
Cropping1D          Cropping layer for 1D input
Cropping2D
Cropping3D
UpSampling1D        Upsampling layer for 1D inputs(Repeats each temporal N times)
UpSampling2D
UpSampling3D
ZeroPadding1D       Zero-padding layer for 1D input
ZeroPadding2D
ZeroPadding3D
#pooling is the application of a moving window across a 2D input space, 
#where the maximum/average value within that window is the output - downsampling 
MaxPooling1D        Max pooling operation for 1D input 
MaxPooling2D
MaxPooling3D
AveragePooling1D    Average pooling for 1D input
AveragePooling2D
AveragePooling3D
#Global max pooling = ordinary max pooling layer with pool size equals to the size of the input 
GlobalMaxPooling1D      Global max pooling for 1D input 
GlobalMaxPooling2D
GlobalMaxPooling3D
GlobalAveragePooling1D
GlobalAveragePooling2D
GlobalAveragePooling2D
LocallyConnected1D      Similar to Conv1D layer, except that weights are unshared
LocallyConnected2D
SimpleRNN           Fully-connected Recurrent NN where the output is to be fed back to input
GRU                 Gated Recurrent Unit
LSTM                Long-Short Term Memory layer
ConvLSTM2D          Convolutional LSTM    
Embedding           Turns positive integers (indexes) into dense vectors of fixed size
Add                 Layer that adds a list of inputs
Subtract            Layer that subtracts two inputs
Multiply            Layer that multiplies (element-wise) a list of inputs.
Average             Layer that averages a list of inputs.
Maximum             Layer that computes the maximum (element-wise) a list of inputs.
Concatenate         Layer that concatenates a list of inputs.
Dot                 Layer that computes a dot product between samples in two tensors.
BatchNormalization  Normalize the activations of the previous layer at each batch
GaussianNoise       Apply additive zero-centered Gaussian noise(overfitting)
GaussianDropout     Apply multiplicative 1-centered Gaussian noise.(overfitting)
AlphaDropout        Applies Alpha Dropout to the input.
TimeDistributed     Applies a layer to every temporal slice of an input
Bidirectional       Bidirectional wrapper for RNNs.


#Example 
from keras.models import Sequential
from keras.layers import Dense, Activation

model = Sequential([
    Dense(32, input_shape=(784,)),
    Activation('relu'),
    Dense(10),
    Activation('softmax'),
])
#OR 
model = Sequential()
model.add(Dense(32, input_dim=784))
model.add(Activation('relu'))

##Specifying the input shape
Type1:Pass an input_shape argument to the first layer.
      This is a shape tuple (a tuple of integers or None entries, 
      where None indicates that any positive integer may be expected). 
      In input_shape, the batch dimension is not included.
Type2:Some 2D layers, such as Dense, support the specification 
      of their input shape via the argument input_dim, 
      and some 3D temporal layers support the arguments input_dim 
      and input_length.
Type2:If you ever need to specify a fixed batch size for your inputs 
      (this is useful for stateful recurrent networks), 
      you can pass a batch_size argument to a layer. 
      If you pass both batch_size=32 and input_shape=(6, 8) to a layer,
      it will then expect every batch of inputs 
      to have the batch shape (32, 6, 8).

#Below two are equivalents 
model = Sequential()
model.add(Dense(32, input_shape=(784,)))
#Or 
model = Sequential()
model.add(Dense(32, input_dim=784))


##Compilation
#Before training a model, compile with 
1.An optimizer. 
  This could be the string identifier of an existing optimizer 
  (such as rmsprop or adagrad), or an instance of the Optimizer class. 
  https://keras.io/optimizers/
2.A loss function. 
  This is the objective that the model will try to minimize. 
  It can be the string identifier of an existing loss function 
  (such as categorical_crossentropy or mse), 
  or it can be an objective function. 
  https://keras.io/losses/
3.A list of metrics. 
  For any classification problem you will want to set this 
  to metrics=['accuracy']. 
  A metric could be the string identifier of an existing metric 
  or a custom metric function.
  https://keras.io/metrics/

# For a multi-class classification problem
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# For a binary classification problem
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# For a mean squared error regression problem
model.compile(optimizer='rmsprop',loss='mse')

##Training
#Numpy arrays of input data and labels. 

##For a single-input model with 2 classes (binary classification):
model = Sequential()
model.add(Dense(32, activation='relu', input_dim=100))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Generate dummy data
import numpy as np
data = np.random.random((1000, 100))
labels = np.random.randint(2, size=(1000, 1))

# Train the model, iterating on the data in batches of 32 samples
model.fit(data, labels, epochs=10, batch_size=32)

##For a single-input model with 10 classes (categorical classification):
model = Sequential()
model.add(Dense(32, activation='relu', input_dim=100))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Generate dummy data
import numpy as np
data = np.random.random((1000, 100))
labels = np.random.randint(10, size=(1000, 1))

# Convert labels to categorical one-hot encoding
one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)

# Train the model, iterating on the data in batches of 32 samples
model.fit(data, one_hot_labels, epochs=10, batch_size=32)


##Example - Multilayer Perceptron (MLP) for multi-class softmax classification:

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD

# Generate dummy data
import numpy as np
x_train = np.random.random((1000, 20))
y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)
x_test = np.random.random((100, 20))
y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)

model = Sequential()
# Dense(64) is a fully-connected layer with 64 hidden units.
# in the first layer, you must specify the expected input data shape:
# here, 20-dimensional vectors.
model.add(Dense(64, activation='relu', input_dim=20))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])

model.fit(x_train, y_train,
          epochs=20,
          batch_size=128)
score = model.evaluate(x_test, y_test, batch_size=128)

##Example - MLP for binary classification:

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout

# Generate dummy data
x_train = np.random.random((1000, 20))
y_train = np.random.randint(2, size=(1000, 1))
x_test = np.random.random((100, 20))
y_test = np.random.randint(2, size=(100, 1))

model = Sequential()
model.add(Dense(64, input_dim=20, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train,
          epochs=20,
          batch_size=128)
score = model.evaluate(x_test, y_test, batch_size=128)

##Example - VGG-like convnet:

import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import SGD

# Generate dummy data
x_train = np.random.random((100, 100, 100, 3))
y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)
x_test = np.random.random((20, 100, 100, 3))
y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)

model = Sequential()
# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.
# this applies 32 convolution filters of size 3x3 each.
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd)

model.fit(x_train, y_train, batch_size=32, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=32)

##Example - Sequence classification with LSTM:

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import LSTM

model = Sequential()
model.add(Embedding(max_features, output_dim=256))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=16, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=16)

##Example - Sequence classification with 1D convolutions:

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D

model = Sequential()
model.add(Conv1D(64, 3, activation='relu', input_shape=(seq_length, 100)))
model.add(Conv1D(64, 3, activation='relu'))
model.add(MaxPooling1D(3))
model.add(Conv1D(128, 3, activation='relu'))
model.add(Conv1D(128, 3, activation='relu'))
model.add(GlobalAveragePooling1D())
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train, batch_size=16, epochs=10)
score = model.evaluate(x_test, y_test, batch_size=16)

##Example - Stacked LSTM for sequence classification
#making the model capable of learning higher-level temporal representations.

from keras.models import Sequential
from keras.layers import LSTM, Dense
import numpy as np

data_dim = 16
timesteps = 8
num_classes = 10

# expected input data shape: (batch_size, timesteps, data_dim)
model = Sequential()
model.add(LSTM(32, return_sequences=True,
               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32
model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32
model.add(LSTM(32))  # return a single vector of dimension 32
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# Generate dummy training data
x_train = np.random.random((1000, timesteps, data_dim))
y_train = np.random.random((1000, num_classes))

# Generate dummy validation data
x_val = np.random.random((100, timesteps, data_dim))
y_val = np.random.random((100, num_classes))

model.fit(x_train, y_train,
          batch_size=64, epochs=5,
          validation_data=(x_val, y_val))

##Example - Same stacked LSTM model, rendered "stateful"
#A stateful recurrent model is one for which the internal states 
#(memories) obtained after processing a batch of samples are reused 
#as initial states for the samples of the next batch. 
#This allows to process longer sequences 
#while keeping computational complexity manageable.


from keras.models import Sequential
from keras.layers import LSTM, Dense
import numpy as np

data_dim = 16
timesteps = 8
num_classes = 10
batch_size = 32

# Expected input batch shape: (batch_size, timesteps, data_dim)
# Note that we have to provide the full batch_input_shape since the network is stateful.
# the sample of index i in batch k is the follow-up for the sample i in batch k-1.
model = Sequential()
model.add(LSTM(32, return_sequences=True, stateful=True,
               batch_input_shape=(batch_size, timesteps, data_dim)))
model.add(LSTM(32, return_sequences=True, stateful=True))
model.add(LSTM(32, stateful=True))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# Generate dummy training data
x_train = np.random.random((batch_size * 10, timesteps, data_dim))
y_train = np.random.random((batch_size * 10, num_classes))

# Generate dummy validation data
x_val = np.random.random((batch_size * 3, timesteps, data_dim))
y_val = np.random.random((batch_size * 3, num_classes))

model.fit(x_train, y_train,
          batch_size=batch_size, epochs=5, shuffle=False,
          validation_data=(x_val, y_val))

          
          
          
          
##All models and layers  are callable
#takes input tensor  and outputs output tensor 
from keras.layers import Input, Dense
from keras.models import Model

# This returns a tensor
inputs = Input(shape=(784,))

# a layer instance is callable on a tensor, and returns a tensor
x = Dense(64, activation='relu')(inputs)
x = Dense(64, activation='relu')(x)
predictions = Dense(10, activation='softmax')(x)

#with model instance 
x = Input(shape=(784,))
# This works, and returns the 10-way softmax we defined above.
y = model(x)



##Kears - Wrappers for the Scikit-Learn API - only Sequential Keras models (single-input only)
keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params), which implements the Scikit-Learn classifier interface,
keras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params), which implements the Scikit-Learn regressor interface.
    build_fn: model class instance or a function returing model 
    sk_params: model parameters & fitting parameters
               sk_params takes both model parameters and fitting parameters (check keras  documentation)
               sk_params  accepts parameters for calling fit, predict, predict_proba, and score methods (e.g., epochs, batch_size)
               (check keras  documentation)
               fitting (predicting) parameters are selected in the following order:
                    Values passed to the dictionary arguments of fit, predict, predict_proba, and score methods
                    Values passed to sk_params
                    The default values of the keras.models.Sequential fit, predict, predict_proba and score methods
               When using scikit-learn's grid_search API, 
               legal tunable parameters are those 
               you could pass to sk_params, 
               including fitting parameters. 
               For example, you could use grid_search 
               to search for the best batch_size or epochs 
               as well as the model parameters.

##Example of build_fn 
def create_model():
	...
	return model

model = KerasClassifier(build_fn=create_model)

#With few sk_params 

def create_model():
	...
	return model

model = KerasClassifier(build_fn=create_model, epochs=10)

#with args of build_fn 
def create_model(dropout_rate=0.0):
	...
	return model

model = KerasClassifier(build_fn=create_model, dropout_rate=0.2)


               
               
##Example- keras API(load,save etc) with pipeline 
#- for below data, check kaggle-DSG17-qualifier.zip/scripts 
#under data/DSG-17(not found)
##Data fields
•media_id - identifiant of the song listened by the user
•album_id - identifiant of the album of the song
•media_duration - duration of the song
•user_gender -  gender of the user
•user_id -  anonymized id of the user
•context_type - type of content where the song was listened: playlist, album ...
•release_date - release date of the song with the format YYYYMMDD
•ts_listen - timestamp of the listening in UNIX time
•platform_name - type of os
•platform_family - type of device
•user_age - age of the user
•listen_type - if the songs was listened in a flow or not
•artist_id - identifiant of the artist of the song
•genre_id - identifiant of the genre of the song
•is_listened - 1 if the track was listened, 0 otherwise

##fit.py 
import os

from keras import backend as K
from keras import callbacks
from keras import layers
from keras import models
from keras.wrappers.scikit_learn import KerasClassifier
import pandas as pd
import tensorflow as tf
from sklearn import metrics
from sklearn import pipeline
from sklearn import preprocessing
from sklearn.externals import joblib


# Load data
X_train = pd.read_csv('data/X_train.csv')
y_train = pd.read_csv('data/y_train.csv')['is_listened']

# Use Tenserflow backend
sess = tf.Session()
K.set_session(sess)


def model():
    model = models.Sequential([
        layers.Dense(64, input_dim=X_train.shape[1], activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    return model


early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=1, verbose=0, mode='auto')

pipe = pipeline.Pipeline([
    ('rescale', preprocessing.StandardScaler()),
    ('nn', KerasClassifier(build_fn=model, nb_epoch=10, batch_size=128,
                           validation_split=0.2, callbacks=[early_stopping]))
])


pipe.fit(X_train.values, y_train.values)

directory = os.path.dirname(os.path.realpath(__file__))
model_step = pipe.steps.pop(-1)[1]
joblib.dump(pipe, os.path.join(directory, 'pipeline.pkl'))
models.save_model(model_step.model, os.path.join(directory, 'model.h5'))



##predict.py 
import os

from keras import models
import pandas as pd
from sklearn.externals import joblib


X_test = pd.read_csv('data/X_test.csv')
y_test = pd.read_csv('data/y_test.csv')

directory = os.path.dirname(os.path.realpath(__file__))
pipe = joblib.load(os.path.join(directory, 'pipeline.pkl'))
model = models.load_model(os.path.join(directory, 'model.h5'))
pipe.steps.append(('nn', model))

pred = pipe.predict_proba(X_test)[:, 0]

submission = pd.DataFrame(data={
    'sample_id': y_test['sample_id'].astype(int),
    'is_listened': pred
}).sort_values('sample_id')

submission.to_csv(os.path.join(directory, 'submission_keras.csv'), index=False)

##Example - Keras with GridSearch 
## Use scikit-learn to grid search the batch size and epochs
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model():
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, verbose=0)
# define the grid search parameters
batch_size = [10, 20, 40, 60, 80, 100]
epochs = [10, 50, 100]
param_grid = dict(batch_size=batch_size, epochs=epochs)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))




## Tune the Training Optimization Algorithm
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model(optimizer='adam'):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
param_grid = dict(optimizer=optimizer)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

##Tune Learning Rate and Momentum
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.optimizers import SGD
# Function to create model, required for KerasClassifier
def create_model(learn_rate=0.01, momentum=0):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	optimizer = SGD(lr=learn_rate, momentum=momentum)
	model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]
momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]
param_grid = dict(learn_rate=learn_rate, momentum=momentum)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))


##Tune Network Weight Initialization
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model(init_mode='uniform'):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, kernel_initializer=init_mode, activation='relu'))
	model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']
param_grid = dict(init_mode=init_mode)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))



##Tune the Neuron Activation Function
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model(activation='relu'):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation=activation))
	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']
param_grid = dict(activation=activation)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))



##une Dropout Regularization
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.constraints import maxnorm
# Function to create model, required for KerasClassifier
def create_model(dropout_rate=0.0, weight_constraint=0):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))
	model.add(Dropout(dropout_rate))
	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
weight_constraint = [1, 2, 3, 4, 5]
dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))


##Tune the Number of Neurons in the Hidden Layer
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.constraints import maxnorm
# Function to create model, required for KerasClassifier
def create_model(neurons=1):
	# create model
	model = Sequential()
	model.add(Dense(neurons, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(4)))
	model.add(Dropout(0.2))
	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
neurons = [1, 5, 10, 15, 20, 25, 30]
param_grid = dict(neurons=neurons)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))









###Scikit - XBoost - eXtreme Gradient Boosting
#Xboost - http://www.picnet.com.au/blogs/guido/post/2016/09/22/xgboost-windows-x64-binaries-for-download/
$ git clone https://github.com/dmlc/xgboost.git xgboost_install_dir
$ copy libxgboost.dll (downloaded from above link) into the xgboost_install_dir\python-package\xgboost\ directory
$ cd xgboost_install_dir\python-package\
$ python setup.py install

#test
import xgboost
xr = xgboost.XGBRegressor()

##Xboost interface 
class xgboost.DMatrix(data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None, nthread=None)
    DMatrix is a internal data structure that used by XGBoost which is optimized for both memory efficiency and training speed. 
    You can construct DMatrix from numpy.arrays
    save_binary(fname, silent=True)
        Save DMatrix to an XGBoost buffer
        
class xgboost.Booster(params=None, cache=(), model_file=None)
    Booster is the model of xgboost, 
    that contains low level routines for training, prediction and evaluation
    Takes DMatrix in fit, predict 
    load_model(fname)
        Load the model from a file.
    predict(data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False)
        Predict with data
    get_score(fmap='', importance_type='weight')
        Get feature importance of each feature
    dump_model(fout, fmap='', with_stats=False)
        Dump model into a text file.
    eval(data, name='eval', iteration=0)
        Evaluate the model on mat
xgboost.train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None, learning_rates=None)
    Train a booster with dtrain(DMatrix) and given parameters
    Return : a trained booster model(Booster)
    evals_result (dict) – 
        This dictionary stores the evaluation results of all the items in watchlist. 
        Example: with a watchlist containing [(dtest,’eval’), (dtrain,’train’)] 
        and a parameter containing (‘eval_metric’: ‘logloss’) 
        Returns: 
        {‘train’: {‘logloss’: [‘0.48253’, ‘0.35953’]},
        ‘eval’: {‘logloss’: [‘0.480385’, ‘0.357756’]}}
xgboost.cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None, metrics=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, shuffle=True)
    Cross-validation with given parameters.
    Returns:evaluation history
 

##Data Interface
import xgboost as xgb

#The XGBoost python module is able to load data from:
    •libsvm txt format file
    •Numpy 2D array, and
    •xgboost binary buffer file.

#To load a libsvm text file or a XGBoost binary file into DMatrix:
dtrain = xgb.DMatrix('train.svm.txt')
dtest = xgb.DMatrix('test.svm.buffer')

#To load a numpy array into DMatrix:
data = np.random.rand(5, 10)  # 5 entities, each contains 10 features
label = np.random.randint(2, size=5)  # binary target
dtrain = xgb.DMatrix(data, label=label)

#To load a scpiy.sparse array into DMatrix:
csr = scipy.sparse.csr_matrix((dat, (row, col)))
dtrain = xgb.DMatrix(csr)

#Saving DMatrix into a XGBoost binary file will make loading faster:
dtrain = xgb.DMatrix('train.svm.txt')
dtrain.save_binary('train.buffer')

#Missing values can be replaced by a default value in the DMatrix constructor:
dtrain = xgb.DMatrix(data, label=label, missing=-999.0)

#Weights can be set when needed:
w = np.random.rand(5, 1)
dtrain = xgb.DMatrix(data, label=label, missing=-999.0, weight=w)


##Setting Parameters
#Booster parameters
param = {'max_depth': 2, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'}
param['nthread'] = 4
param['eval_metric'] = 'auc'

#You can also specify multiple eval metrics:
param['eval_metric'] = ['auc', 'ams@0']

# alternatively:
# plst = param.items()
# plst += [('eval_metric', 'ams@0')]

#Specify validations set to watch performance
evallist = [(dtest, 'eval'), (dtrain, 'train')]


#Training-Training a model requires a parameter list and data set.
num_round = 10
bst = xgb.train(plst, dtrain, num_round, evallist)

#After training, the model can be saved.
bst.save_model('0001.model')

#The model and its feature map can also be dumped to a text file.
# dump model
bst.dump_model('dump.raw.txt')
# dump model with feature map
bst.dump_model('dump.raw.txt', 'featmap.txt')

#A saved model can be loaded as follows:
bst = xgb.Booster({'nthread': 4})  # init model
bst.load_model('model.bin')  # load data


##Early Stopping
xgb.train(..., evals=evals, early_stopping_rounds=10)

#Prediction
#A model that has been trained or loaded can perform predictions on data sets.
# 7 entities, each contains 10 features
data = np.random.rand(7, 10)
dtest = xgb.DMatrix(data)
ypred = bst.predict(dtest)

#If early stopping is enabled during training, 
#you can get predictions from the best iteration with bst.best_ntree_limit:
ypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)



##Plotting
#To plot importance
xgb.plot_importance(bst)

#To plot the output tree via matplotlib
xgb.plot_tree(bst, num_trees=2)

#When you use IPython, you can use the to_graphviz function
xgb.to_graphviz(bst, num_trees=2)


    
    
##sklearn Interface 
class xgboost.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)
class xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)
    Check http://xgboost.readthedocs.io/en/latest/parameter.html
    max_depth : int
        Maximum tree depth for base learners.
    learning_rate : float
        Boosting learning rate (xgb's 'eta')
    n_estimators : int
        Number of boosted trees to fit.
    silent : boolean
        Whether to print messages while running boosting.
    objective : string or callable
        Specify the learning task and the corresponding learning objective or a custom objective function to be used 
        •[default=reg:linear]
            'reg:linear' –linear regression
            'reg:logistic' –logistic regression
            'binary:logistic' –logistic regression for binary classification, output probability
            'binary:logitraw' –logistic regression for binary classification, output score before logistic transformation
            'count:poisson' –poisson regression for count data, output mean of poisson distribution◾max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization)
            'multi:softmax' –set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)
            'multi:softprob' –same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata, nclass matrix. The result contains predicted probability of each data point belonging to each class.
            'rank:pairwise' –set XGBoost to do ranking task by minimizing the pairwise loss
            'reg:gamma' –gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed
            'reg:tweedie' –Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed.
    booster: string
        Specify which booster to use: gbtree, gblinear or dart.
        gbtree and dart use tree based model while gblinear uses linear function.
    reg_alpha : float (xgb’s alpha)
        L1 regularization term on weights
    reg_lambda : float (xgb’s lambda)
        L2 regularization term on weights
    #Methods 
    fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)
        eval_metric:default according to objective
        (rmse for regression, and error for classification, mean average precision for ranking )
        ◦User can add multiple evaluation metrics via list 
          'rmse': root mean square error
          'mae': mean absolute error
          'logloss': negative log-likelihood
          'error': Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.
          'error@t': a different than 0.5 binary classification threshold value could be specified by providing a numerical value through 't'.
          'merror': Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases).
          'mlogloss': Multiclass logloss
          'auc': Area under the curve for ranking evaluation.
          'ndcg':Normalized Discounted Cumulative Gain
          'map':Mean average precision
          'ndcg@n','map@n': n can be assigned as an integer to cut off the top positions in the lists for evaluation.
          'ndcg-','map-','ndcg@n-','map@n-': In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding '-' in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. training repeatedly
          'poisson-nloglik': negative log-likelihood for Poisson regression
          'gamma-nloglik': negative log-likelihood for gamma regression
          'gamma-deviance': residual deviance for gamma regression
          'tweedie-nloglik': negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter)
    evals_result()
        Return the evaluation results.
        #Example 
        param_dist = {'objective':'binary:logistic', 'n_estimators':2}
        clf = xgb.XGBClassifier(**param_dist)
        clf.fit(X_train, y_train,eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric='logloss', verbose=True)
        >>> evals_result = clf.evals_result()
            {'validation_0': {'logloss': ['0.604835', '0.531479']},
            'validation_1': {'logloss': ['0.41965', '0.17686']}}
            
xgboost.to_graphviz(booster, fmap='', num_trees=0, rankdir='UT', yes_color='#0000FF', no_color='#FF0000', **kwargs)
    Convert specified tree to graphviz instance
xgboost.plot_tree(booster, fmap='', num_trees=0, rankdir='UT', ax=None, **kwargs)
    Plot specified tree.      
xgboost.plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance', xlabel='F score', ylabel='Features', importance_type='weight', max_num_features=None, grid=True, show_values=True, **kwargs)
    Plot importance based on fitted trees.
        •booster (Booster, XGBModel or dict) – Booster or XGBModel instance, 

##Check parameter tuning 
https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
 


##With sklearn 
import pickle
import xgboost as xgb

import numpy as np
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, mean_squared_error
from sklearn.datasets import load_iris, load_digits, load_boston

rng = np.random.RandomState(31337)

print("Zeros and Ones from the Digits dataset: binary classification")
digits = load_digits(2)
y = digits['target']
X = digits['data']
kf = KFold(n_splits=2, shuffle=True, random_state=rng)
for train_index, test_index in kf.split(X):
    xgb_model = xgb.XGBClassifier().fit(X[train_index], y[train_index])
    predictions = xgb_model.predict(X[test_index])
    actuals = y[test_index]
    print(confusion_matrix(actuals, predictions))

print("Iris: multiclass classification")
iris = load_iris()
y = iris['target']
X = iris['data']
kf = KFold(n_splits=2, shuffle=True, random_state=rng)
for train_index, test_index in kf.split(X):
    xgb_model = xgb.XGBClassifier().fit(X[train_index], y[train_index])
    predictions = xgb_model.predict(X[test_index])
    actuals = y[test_index]
    print(confusion_matrix(actuals, predictions))

print("Boston Housing: regression")
boston = load_boston()
y = boston['target']
X = boston['data']
kf = KFold(n_splits=2, shuffle=True, random_state=rng)
for train_index, test_index in kf.split(X):
    xgb_model = xgb.XGBRegressor().fit(X[train_index], y[train_index])
    predictions = xgb_model.predict(X[test_index])
    actuals = y[test_index]
    print(mean_squared_error(actuals, predictions))

print("Parameter optimization")
y = boston['target']
X = boston['data']
xgb_model = xgb.XGBRegressor()
clf = GridSearchCV(xgb_model,{'max_depth': [2,4,6],'n_estimators': [50,100,200]}, verbose=1)
clf.fit(X,y)
print(clf.best_score_)
print(clf.best_params_)

# The sklearn API models are picklable
print("Pickling sklearn API models")
# must open in binary format to pickle
pickle.dump(clf, open("best_boston.pkl", "wb"))
clf2 = pickle.load(open("best_boston.pkl", "rb"))
print(np.allclose(clf.predict(X), clf2.predict(X)))

# Early-stopping

X = digits['data']
y = digits['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
clf = xgb.XGBClassifier()
clf.fit(X_train, y_train, early_stopping_rounds=10, eval_metric="auc",
        eval_set=[(X_test, y_test)])










###Scikit - LightGBM - a Gradient Boosting Model 
#Boosting is a sequential technique which works on the principle of ensemble. 
#It combines a set of weak learners and delivers improved prediction accuracy

#For Windows users, 
#VC runtime is needed if Visual Studio (2015 or 2017) is not installed.
$ pip install lightgbm


class LGBMRegressor(boosting_type="gbdt", num_leaves=31, max_depth=-1,
                 learning_rate=0.1, n_estimators=100,
                 subsample_for_bin=200000, objective=None, class_weight=None,
                 min_split_gain=0., min_child_weight=1e-3, min_child_samples=20,
                 subsample=1., subsample_freq=1, colsample_bytree=1.,
                 reg_alpha=0., reg_lambda=0., random_state=None,
                 n_jobs=-1, silent=True, **kwargs)
class LGBMClassifier(boosting_type="gbdt", num_leaves=31, max_depth=-1,
                 learning_rate=0.1, n_estimators=100,
                 subsample_for_bin=200000, objective=None, class_weight=None,
                 min_split_gain=0., min_child_weight=1e-3, min_child_samples=20,
                 subsample=1., subsample_freq=1, colsample_bytree=1.,
                 reg_alpha=0., reg_lambda=0., random_state=None,
                 n_jobs=-1, silent=True, **kwargs):
        Parameters
        ----------
        boosting_type : string, optional (default="gbdt")
            'gbdt', traditional Gradient Boosting Decision Tree.
            'dart', Dropouts meet Multiple Additive Regression Trees.
            'goss', Gradient-based One-Side Sampling.
            'rf', Random Forest.
        num_leaves : int, optional (default=31)
            Maximum tree leaves for base learners.
        max_depth : int, optional (default=-1)
            Maximum tree depth for base learners, -1 means no limit.
        learning_rate : float, optional (default=0.1)
            Boosting learning rate.
        n_estimators : int, optional (default=100)
            Number of boosted trees to fit.
        subsample_for_bin : int, optional (default=50000)
            Number of samples for constructing bins.
        objective : string, callable or None, optional (default=None)
            Specify the learning task and the corresponding learning objective or
            default: 'regression' for LGBMRegressor, 'binary' 
            or 'multiclass' for LGBMClassifier, 
            'lambdarank' for LGBMRanker.
        class_weight : dict, 'balanced' or None, optional (default=None)
            Weights associated with classes in the form {class_label: weight}.
            Use this parameter only for multi-class classification task;
            for binary classification task you may use is_unbalance or scale_pos_weight parameters.
            The 'balanced' mode uses the values of y to automatically adjust weights
            inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).
            If None, all classes are supposed to have weight one.
            Note that these weights will be multiplied with sample_weight (passed through the fit method)
            if sample_weight is specified.
        min_split_gain : float, optional (default=0.)
            Minimum loss reduction required to make a further partition on a leaf node of the tree.
        min_child_weight : float, optional (default=1e-3)
            Minimum sum of instance weight(hessian) needed in a child(leaf).
        min_child_samples : int, optional (default=20)
            Minimum number of data need in a child(leaf).
        subsample : float, optional (default=1.)
            Subsample ratio of the training instance.
        subsample_freq : int, optional (default=1)
            Frequence of subsample, <=0 means no enable.
        colsample_bytree : float, optional (default=1.)
            Subsample ratio of columns when constructing each tree.
        reg_alpha : float, optional (default=0.)
            L1 regularization term on weights.
        reg_lambda : float, optional (default=0.)
            L2 regularization term on weights.
        random_state : int or None, optional (default=None)
            Random number seed.
            Will use default seeds in c++ code if set to None.
        n_jobs : int, optional (default=-1)
            Number of parallel threads.
        silent : bool, optional (default=True)
            Whether to print messages while running boosting.
        **kwargs : other parameters
            Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.

        Attributes
        ----------
        n_features_ : int
            The number of features of fitted model.
        classes_ : array of shape = [n_classes]
            The class label array (only for classification problem).
        n_classes_ : int
            The number of classes (only for classification problem).
        best_score_ : dict or None
            The best score of fitted model.
        best_iteration_ : int or None
            The best iteration of fitted model if early_stopping_rounds has been specified.
        objective_ : string or callable
            The concrete objective used while fitting this model.
        booster_ : Booster
            The underlying Booster of this model.
        evals_result_ : dict or None
            The evaluation results if early_stopping_rounds has been specified.
        feature_importances_ : array of shape = [n_features]
            The feature importances (the higher, the more important the feature).

        Note
        ----
        A custom objective function can be provided for the objective
        parameter. In this case, it should have the signature
        objective(y_true, y_pred) -> grad, hess or
        objective(y_true, y_pred, group) -> grad, hess:

            y_true: array-like of shape = [n_samples]
                The target values.
            y_pred: array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)
                The predicted values.
            group: array-like
                Group/query data, used for ranking task.
            grad: array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)
                The value of the gradient for each sample point.
            hess: array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)
                The value of the second derivative for each sample point.

        For multi-class task, the y_pred is group by class_id first, then group by row_id.
        If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]
        and you should group grad and hess in this way as well.
#Methods 
fit(X, y,sample_weight=None, init_score=None, group=None,
            eval_set=None, eval_names=None, eval_sample_weight=None,
            eval_class_weight=None, eval_init_score=None, eval_group=None,
            eval_metric=None, early_stopping_rounds=None, verbose=True,
            feature_name='auto', categorical_feature='auto', callbacks=None):
        Parameters
        ----------
        X : array-like or sparse matrix of shape = [n_samples, n_features]
            Input feature matrix.
        y : array-like of shape = [n_samples]
            The target values (class labels in classification, 
            real numbers in regression).
        sample_weight : array-like of shape = [n_samples] or None, optional (default=None)
            Weights of training data.
        init_score : array-like of shape = [n_samples] or None, optional (default=None)
            Init score of training data.
        group : array-like of shape = [n_samples] or None, optional (default=None)
            Group data of training data.
        eval_set : list or None, optional (default=None)
            A list of (X, y) tuple pairs to use as a validation sets for early-stopping.
        eval_names : list of strings or None, optional (default=None)
            Names of eval_set.
        eval_sample_weight : list of arrays or None, optional (default=None)
            Weights of eval data.
        eval_class_weight : list or None, optional (default=None)
            Class weights of eval data.
        eval_init_score : list of arrays or None, optional (default=None)
            Init score of eval data.
        eval_group : list of arrays or None, optional (default=None)
            Group data of eval data.
        eval_metric : string, list of strings, callable or None, optional (default=None)
            If string, it should be a built-in evaluation metric to use.
            If callable, it should be a custom evaluation metric, see note for more details.
        early_stopping_rounds : int or None, optional (default=None)
            Activates early stopping. The model will train until the validation score stops improving.
            Validation error needs to decrease at least every early_stopping_rounds round(s)
            to continue training.
        verbose : bool, optional (default=True)
            If True and an evaluation set is used, writes the evaluation progress.
        feature_name : list of strings or 'auto', optional (default="auto")
            Feature names.
            If 'auto' and data is pandas DataFrame, data columns names are used.
        categorical_feature : list of strings or int, or 'auto', optional (default="auto")
            Categorical features.
            If list of int, interpreted as indices.
            If list of strings, interpreted as feature names (need to specify feature_name as well).
            If 'auto' and data is pandas DataFrame, pandas categorical columns are used.
        callbacks : list of callback functions or None, optional (default=None)
            List of callback functions that are applied at each iteration.
            See Callbacks in Python API for more information.
        Returns
        -------
        self : object
            Returns self.

predict(X, raw_score=False, num_iteration=0):
    Return the predicted value for each sample.
        Parameters
        ----------
        X : array-like or sparse matrix of shape = [n_samples, n_features]
            Input features matrix.
        raw_score : bool, optional (default=False)
            Whether to predict raw scores.
        num_iteration : int, optional (default=0)
            Limit number of iterations in the prediction; defaults to 0 (use all trees).
        Returns
        -------
        predicted_result : array-like of shape = [n_samples] or shape = [n_samples, n_classes]
            The predicted values.

predict_proba(self, X, raw_score=False, num_iteration=0):
    Return the predicted probability for each class for each sample.
        Parameters
        ----------
        X : array-like or sparse matrix of shape = [n_samples, n_features]
            Input features matrix.
        raw_score : bool, optional (default=False)
            Whether to predict raw scores.
        num_iteration : int, optional (default=0)
            Limit number of iterations in the prediction; defaults to 0 (use all trees).

        Returns
        -------
        predicted_probability : array-like of shape = [n_samples, n_classes]
            The predicted probability for each class for each sample.

    

##Example - Regression 
# coding: utf-8
# pylint: disable = invalid-name, C0111
import lightgbm as lgb
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV

# load or create your dataset
print('Load data...')
df_train = pd.read_csv('data/regression.train', header=None, sep='\t')
df_test = pd.read_csv('data/regression.test', header=None, sep='\t')

y_train = df_train[0].values
y_test = df_test[0].values
X_train = df_train.drop(0, axis=1).values
X_test = df_test.drop(0, axis=1).values

print('Start training...')
# train
gbm = lgb.LGBMRegressor(objective='regression',
                        num_leaves=31,
                        learning_rate=0.05,
                        n_estimators=20)
gbm.fit(X_train, y_train,
        eval_set=[(X_test, y_test)],
        eval_metric='l1',
        early_stopping_rounds=5)

print('Start predicting...')
# predict
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)
# eval
print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)

# feature importances
print('Feature importances:', list(gbm.feature_importances_))

# other scikit-learn modules
estimator = lgb.LGBMRegressor(num_leaves=31)

param_grid = {
    'learning_rate': [0.01, 0.1, 1],
    'n_estimators': [20, 40]
}

gbm = GridSearchCV(estimator, param_grid)

gbm.fit(X_train, y_train)

print('Best parameters found by grid search are:', gbm.best_params_)




##Example - Classification using native API of LightGBM

import json
import lightgbm as lgb
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error

try:
    import cPickle as pickle
except:
    import pickle

# load or create your dataset
print('Load data...')
df_train = pd.read_csv('data/binary.train', header=None, sep='\t')
df_test = pd.read_csv('data/binary.test', header=None, sep='\t')
W_train = pd.read_csv('data/binary.train.weight', header=None)[0]
W_test = pd.read_csv('data/binary.test.weight', header=None)[0]

y_train = df_train[0].values
y_test = df_test[0].values
X_train = df_train.drop(0, axis=1).values
X_test = df_test.drop(0, axis=1).values

num_train, num_feature = X_train.shape

# create dataset for lightgbm
# if you want to re-use data, remember to set free_raw_data=False
lgb_train = lgb.Dataset(X_train, y_train,weight=W_train, free_raw_data=False)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, weight=W_test, free_raw_data=False)

# specify your configurations as a dict
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': 0
}

# generate a feature name
feature_name = ['feature_' + str(col) for col in range(num_feature)]

print('Start training...')
# feature_name and categorical_feature
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                valid_sets=lgb_train,  # eval training data
                feature_name=feature_name,
                categorical_feature=[21])

# check feature name
print('Finish first 10 rounds...')
print('7th feature name is:', repr(lgb_train.feature_name[6]))

# save model to file
gbm.save_model('model.txt')

# dump model to json (and save to file)
print('Dump model to JSON...')
model_json = gbm.dump_model()

with open('model.json', 'w+') as f:
    json.dump(model_json, f, indent=4)

# feature names
print('Feature names:', gbm.feature_name())

# feature importances
print('Feature importances:', list(gbm.feature_importance()))

# load model to predict
print('Load model to predict')
bst = lgb.Booster(model_file='model.txt')
# can only predict with the best iteration (or the saving iteration)
y_pred = bst.predict(X_test)
# eval with loaded model
print('The rmse of loaded model\'s prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)

# dump model with pickle
with open('model.pkl', 'wb') as fout:
    pickle.dump(gbm, fout)
# load model with pickle to predict
with open('model.pkl', 'rb') as fin:
    pkl_bst = pickle.load(fin)
# can predict with any iteration when loaded in pickle way
y_pred = pkl_bst.predict(X_test, num_iteration=7)
# eval with loaded model
print('The rmse of pickled model\'s prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)

# continue training
# init_model accepts:
# 1. model file name
# 2. Booster()
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model='model.txt',
                valid_sets=lgb_eval)

print('Finish 10 - 20 rounds with model file...')

# decay learning rates
# learning_rates accepts:
# 1. list/tuple with length = num_boost_round
# 2. function(curr_iter)
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                learning_rates=lambda iter: 0.05 * (0.99 ** iter),
                valid_sets=lgb_eval)

print('Finish 20 - 30 rounds with decay learning rates...')

# change other parameters during training
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                valid_sets=lgb_eval,
                callbacks=[lgb.reset_parameter(bagging_fraction=[0.7] * 5 + [0.6] * 5)])

print('Finish 30 - 40 rounds with changing bagging_fraction...')


# self-defined objective function
# f(preds: array, train_data: Dataset) -> grad: array, hess: array
# log likelihood loss
def loglikelood(preds, train_data):
    labels = train_data.get_label()
    preds = 1. / (1. + np.exp(-preds))
    grad = preds - labels
    hess = preds * (1. - preds)
    return grad, hess


# self-defined eval metric
# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool
# binary error
def binary_error(preds, train_data):
    labels = train_data.get_label()
    return 'error', np.mean(labels != (preds > 0.5)), False


gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                fobj=loglikelood,
                feval=binary_error,
                valid_sets=lgb_eval)

print('Finish 40 - 50 rounds with self-defined objective function and eval metric...')

print('Start a new training job...')


# callback
def reset_metrics():
    def callback(env):
        lgb_eval_new = lgb.Dataset(X_test, y_test, reference=lgb_train)
        if env.iteration - env.begin_iteration == 5:
            print('Add a new valid dataset at iteration 5...')
            env.model.add_valid(lgb_eval_new, 'new valid')
    callback.before_iteration = True
    callback.order = 0
    return callback


gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                valid_sets=lgb_train,
                callbacks=[reset_metrics()])

print('Finish first 10 rounds with callback function...')











###Scikit External projects - auto_ml - Quick Intro 
#Automated machine learning for production and analytics, 
#built on scikit-learn and related projects. 
#Trains a pipeline wth all the standard machine learning steps. 
#Tuned for prediction speed and ease of transfer to production environments.

$ pip install auto_ml

##Interface 
class Predictor(type_of_estimator, column_descriptions, verbose=True, name=None)
    Parameters:	
        type_of_estimator ('regressor' or 'classifier')
            Whether you want a classifier or regressor
        column_descriptions (dictionary)
            'column_name' : 'type '
            where type  ['categorical', 'output', 'nlp', 'date', 'ignore'] 
            All columns are assumed to be continuous unless labeled otherwise.      
        
ml_predictor.train(raw_training_data, user_input_func=None, optimize_final_model=None, write_gs_param_results_to_file=True, perform_feature_selection=None, verbose=True, X_test=None, y_test=None, ml_for_analytics=True, take_log_of_y=None, model_names=None, perform_feature_scaling=None, calibrate_final_model=False, _scorer=None, scoring=None, verify_features=False, training_params=None, grid_search_params=None, compare_all_models=False, cv=2, feature_learning=False, fl_data=None, optimize_feature_learning=False, train_uncertainty_model=False, uncertainty_data=None, uncertainty_delta=None, uncertainty_delta_units=None, calibrate_uncertainty=False, uncertainty_calibration_settings=None, uncertainty_calibration_data=None, uncertainty_delta_direction=None, advanced_analytics=None, analytics_config=None, prediction_intervals=None, predict_intervals=None, ensemble_config=None, trained_transformation_pipeline=None, transformed_X=None, transformed_y=None, return_transformation_pipeline=False, X_test_already_transformed=False, skip_feature_responses=None, prediction_interval_params=None):
    raw_training_data 
        (DataFrame, or a list of dictionaries)
        where each dictionary represents a row of data. 
        Each row should have both the training features, and the output value we are trying to predict.) 
    user_input_func (function) – [default- None] 
        A function that will be called as the first step in the pipeline, 
        for both training and predictions. 
        The function will be passed the entire X dataset. 
        The function must not alter the order or length of the X dataset, 
        and must return the entire X dataset. 
    optimize_final_model (Boolean) – [default- False] 
        Whether or not to perform GridSearchCV on the final model. 
        True increases computation time significantly, 
        but will likely increase accuracy.
    perform_feature_selection (Boolean) 
        [default- True for large datasets (> 100,000 rows), 
        False for small datasets] 
        Whether or not to run feature selection before training the final model. 
        Feature selection means picking only the most useful features, 
        and tends to combat overfitting as well.
    verbose – [default- True] 
    ml_for_analytics (Boolean) – [default- True] 
        Whether or not to print out results 
        for which features the trained model found useful. 
    take_log_of_y (Boolean) – [default- None] 
        For regression problems, 
        accuracy is sometimes improved by taking the natural log of y values during training, 
        so they all exist on a comparable scale.
    model_names (list of strings) 
        [default- relevant 'GradientBoosting'] Which model(s) to try. 
        from scikit-learn are ['ARDRegression', 'AdaBoostClassifier', 'AdaBoostRegressor', 'BayesianRidge', 'ElasticNet', 'ExtraTreesClassifier', 'ExtraTreesRegressor', 'GradientBoostingClassifier', 'GradientBoostingRegressor', 'Lasso', 'LassoLars', 'LinearRegression', 'LogisticRegression', 'MiniBatchKMeans', 'OrthogonalMatchingPursuit', 'PassiveAggressiveClassifier', 'PassiveAggressiveRegressor', 'Perceptron', 'RANSACRegressor', 'RandomForestClassifier', 'RandomForestRegressor', 'Ridge', 'RidgeClassifier', 'SGDClassifier', 'SGDRegressor']. 
        Other options : eg model_names=['DeepLearningClassifier']
            DeepLearningClassifier and DeepLearningRegressor(tensorflow,keras)
            XGBClassifier and XGBRegressor
            LGBMClassifer and LGBMRegressor
            CatBoostClassifier and CatBoostRegressor    
    perform_feature_scaling – [default- True] 
        Whether to scale values, roughly to the range of {-1, 1}. 
        Scaling values is highly recommended for deep learning. 
        auto_ml has it's own custom scaler that is relatively robust to outliers.
    calibrate_final_model – [default- False] 
        Whether to calibrate the probability predictions coming 
        from the final trained classifier. 
        Usefulness depends on your scoring metric, and model. 
        If True, you must pass in values for X_test and y_test as well. 
        This is the dataset we will calibrate the model to. 
        Note that this means you cannot use this 
        as your test dataset once the model has been calibrated to them.
    verify_features – [default- False] 
        Allows you to verify that all the same features are present 
        in a prediction dataset as the training datset. 
        False by default because it increases serialized model size by around 1MB
        In order to check whether a prediction dataset 
        has the same features, invoke 
        trained_ml_pipeline.named_steps['final_model']. verify_features(prediction_data). 
    cv – [default- 2] 
        How many folds of cross-validation to perform. 
        The default of 2 works well for very large datasets. 
        It speeds up training speed, and helps combat overfitting. 
        However, for smaller datasets, cv of 3, or even up to 9, 
        might make more sense, if you're ok with the trade-off in training speed.
    feature_learning – [default- False] 
        Whether or not to use Deep Learning to learn features from the data. 
        The learned features are then predicted 
        for every row in the training data, and fed into a final model 
        (by default, gradient boosting) to turn those features 
        and the original features into the most accurate predictions possible. 
        If True, you must pass in fl_data as well. 
    fl_data – [default- None] 
        If feature_learning=True, then this is the dataset 
        we will fit the deep learning model on. 
        This dataset should be different than your df_train dataset.
    prediction_intervals – [default- False] 
        In addition to predicting a single value, 
        regressors can return upper and lower bounds for that prediction 
        If you pass True, we will return the 95th and 5th percentile 
        (the range we'd expect 90% of values to fall within) 
        when you get predicted intervals. 
        If you pass in two float values between 0 and 1, 
        we will return those particular predicted percentiles 
        At prediction time, 
        call ml_predictor.predict_intervals() not  ml_predictor.predict()
    compare_all_models - [default=False]
        True means many scikit Models would be compared 
        If False, by default uses GradientBoostingRegressor or GradientBoostingClassifier
    scoring=None
        Scoring string from sklearn.metrics 
        Default is 'accuracy_score' for classifcation
        Uses internal Scorer 
    optimize_feature_learning=False
    train_uncertainty_model=False    
    calibrate_uncertainty=False
	return_transformation_pipeline=False
	X_test_already_transformed=False
    
ml_predictor.train_categorical_ensemble(data, categorical_column, default_category='most_frequently_occurring_category', min_category_size=5)
    categorical_column 
        The column of data that holds the category you want 
        to train each model on. 
        If you want to train a model for each market you operate in, 
        categorical_column='market_name'.
    default_category – [default- most frequently occurring category in the training data] 
        When getting predictions for a category that was not in our training data, 
        which category should we use? 
        By default, uses the largest category from the training data. 
        Can also take on the value '_RAISE_ERROR', which will raise an error.
    min_category_size – [default- 5] 
        The minimum size of a category in the training data. 
        If a category has fewer than this number of observations, 
        we will not train a model for it but would be part of Default category 

    
ml_predictor.predict(prediction_data)
    prediction_data 
        A single dictionary, or a DataFrame, or list of dictionaries. 
    Return type:	
        list of predicted values, of the same length and order as the prediction_rows passed in. 
        If a single dictionary is passed in, 
        the return value will be the predicted value, not nested in a list (so just a single number or predicted class).

ml_predictor.predict_proba(prediction_data)
    Return type:	Only works for 'classifier' estimators. 
        each row in the returned list will now itself be a list, of length (number of categories in training data) 
        The items in this row's list will represent the probability of each category.
        
ml_predictor.score(X_test, y_test, advanced_scoring=True, verbose=2)
    verbose – [default- 2] 
    Return type:	
        number representing the trained estimator's score on the validation data.
        
ml_predictor.predict_intervals(prediction_data, return_type=None)
    return_type 
        [default- dict for single prediction, list of lists for multiple predictions] 
        Accepted values are 'df', 'list', 'dict'. 
        If 'df', we will return a pandas DataFrame, 
        with the columns [prediction, prediction_lower, prediction_median, prediction_upper]. 
        If 'list', we will return a single (non-nested) list for single predictions, 
        and a list of lists for batch predictions. 
        If 'dict', we will return a single (non-nested) dictionary for single predictions, 
        and a list of dictionaries for batch predictions.
        
ml_predictor.save(file_name='auto_ml_saved_pipeline.dill', verbose=True)
    This function will serialize the trained pipeline to disk

auto_ml.utils_models.load_ml_model(file_name)
    Returns saved pipeline 
    
##few important attribute of Predictor
self._scorer = scoring
self.scoring = 'accuracy_score'
   
    

##core example 
from auto_ml import Predictor
from auto_ml.utils import get_boston_dataset
from auto_ml.utils_models import load_ml_model

# Load data
df_train, df_test = get_boston_dataset()

# Tell auto_ml which column is 'output'
# Also note columns that aren't purely numerical
# Examples include ['nlp', 'date', 'categorical', 'ignore']
column_descriptions = {
  'MEDV': 'output'
  , 'CHAS': 'categorical'
}

ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)

ml_predictor.train(df_train)

# Score the model on test data
test_score = ml_predictor.score(df_test, df_test.MEDV)

# auto_ml is specifically tuned for running in production
# It can get predictions on an individual row (passed in as a dictionary)
# A single prediction like this takes ~1 millisecond
# Here we will demonstrate saving the trained model, and loading it again
file_name = ml_predictor.save()

trained_model = load_ml_model(file_name)

# .predict and .predict_proba take in either:
# A pandas DataFrame
# A list of dictionaries
# A single dictionary (optimized for speed in production evironments)
predictions = trained_model.predict(df_test)
print(predictions)
#All params in the model 
trained_model.get_params()
#Final model params 
>>> trained_model.get_params()['final_model__model']
GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,
             max_leaf_nodes=None, min_impurity_decrease=0.0,
             min_impurity_split=None, min_samples_leaf=1,
             min_samples_split=2, min_weight_fraction_leaf=0.0,
             n_estimators=35, presort=False, random_state=None,
             subsample=1.0, verbose=0, warm_start=True)




##Training data format
1.Must either be a pandas DataFrame, 
  or a list filled with python dictionaries.
2.The non-header-row objects can be 'sparse'. 
  Dict:You only have to include attributes on each object 
  that actually apply to that row. 
  Recomended: pass None or nan  if you have missing values
  
##Header row information - column_descriptions
#each column is a numerical column, unless you specify otherwise using one of the types noted below.
'output'        Output column, must 
'categorical'   By default, any string value in cloumn is taken as Categoorical 
                For numerucal category, use this type 
'nlp'           For NLP text data, encoded using TF-IDF, along with some other feature engineering (count of some aggregations like total capital letters, puncutation characters, smiley faces, etc., as well as a sentiment prediction of that text).
'ignore'        This column of data will be ignored.
'date'          Creates new features like day_of_week, or minutes_into_day, etc. 
                Then the original date field will be removed from the training data 

    
##Auto_ml: Using machine learning for analytics - ml_for_analytics=True 
from auto_ml import Predictor

col_desc_dictionary = {col_to_predict: 'output', state_code: 'categorical'}
# Can pass in type_of_estimator='regressor' as well
ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=col_desc_dictionary)
ml_predictor.train(df, ml_for_analytics=True)

# Prints analytics data 

# Where new_data is a single dictionary, or a DataFrame
ml_predictor.predict(new_data)


##ml_for_analytics : Interpreting Results

#regression based models - coefficient for each feature. 
#Note that by default, features are scaled to roughly the range of [0,1].
#=> all else equal, we'd expect a change 
#from being the smallest to the largest value 
#on this particular variable will lead to [coefficient] 
#impact on our output variable.

#random-forest-esque models - amount of variance 
#that is explained by this feature. 
#These values will, in total, sum up to 1.


##ml_for_analytics :Interpreting Predicted Probability Buckets for Classifiers
#Order the predicted probabilities from lowest to highest. 
#We bucket those sorted predictions into 10 buckets

#for each bucket, report what the average predicted probability was, 
#and what the actual event occurrence was, 
#along with what the max and min predicted probabilities were for that bucket.



##ml_for_analytics : Feature Responses for non-linear models
#we take a portion of our training dataset (10k rows, by default, though user-configurable). 
#Then, for each column, we find that column's standard deviation. 
#Holding all else constant, we then increment all values in that column by one standard deviation. 
#We get predictions for all rows, 
#and compare them to our baseline predictions. 
#The feature_response is how much the output predictions responded 
#to this change in the feature. 

#We repeat the process twice for each column, 
#once incrementing by one std, and once decrementing by one std.

#The output is how much the output variable responded to these one std increments and decrements to each feature. 
#The value we report is the average across all predictions. 



##Categorical Ensembling
#If a categorical feature is driving force behind the model 
#eg 'market_name'(with values US, UK, India etc 
#auto_ml will automatically train one model for each category of that feature  
#use 
ml_predictor.train_categorical_ensemble(df_train, categorical_column='market_name').

#Then predict , but mention feature  category 
test_data = {'feature1': 1,'feature2': 1, 'market_name': 'India'} 
trained_ml_predictor.predict(test_data)

#You can specify a default category
#Default category would be used if no matching category is found 
#And if no of samples in any category is less than min_category_size



##Deep Learning in auto_ml
#(Keras and TensorFlow )installed on your machine. 
#model_names='DeepLearningRegressor' 
#or model_names='DeepLearningClassifier'` 
#when invoking ml_predictor.train().



##Feature Learning
#Deep Learning is great at learning important features from your data. 
#Gradient boosting is great at turning features into accurate predictions, 
#but it doesn't do any feature learning.

#feature_learning=True, fl_data=some_dataframe to .train(), 
#train a deep learning model on your fl_data. 
#get 10 most useful features and append these features on X_train to train 

#feature learning is supported in  regression and binary classification only 
ml_predictor.train(df_train, feature_learning=True, fl_data=df_fl_data)










###Scikit External projects - auto-sklearn - Quick Intro 
#An automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator
#Note for windows 
  
    
    
    
    
    
    
###Scikit External projects - TPOT - Quick Intro 
#TPOT An automated machine learning toolkit 
#that optimizes a series of scikit-learn operators 
#to design a machine learning pipeline, 
#including data and feature preprocessors as well as the estimators. 
#Works as a drop-in replacement for a scikit-learn estimator.

$ pip install deap update_checker tqdm stopit
$ pip install pywin32
#Xboost 
conda install py-xgboost

#genetic programming
#for TPOT-MDR configuration
$ pip install scikit-mdr skrebate

$pip install tpot



##Classification
class tpot.TPOTClassifier(generations=100, population_size=100,
                          offspring_size=None, mutation_rate=0.9,
                          crossover_rate=0.1,
                          scoring='accuracy', cv=5,
                          subsample=1.0, n_jobs=1,
                          max_time_mins=None, max_eval_time_mins=5,
                          random_state=None, config_dict=None,
                          warm_start=False,
                          memory=None,
                          periodic_checkpoint_folder=None,
                          verbosity=0,
                          disable_update_check=False)
    Automated machine learning for supervised classification tasks.
    The TPOTClassifier performs an intelligent search over machine learning pipelines that can contain supervised classification models, preprocessors, feature selection techniques, and any other estimator or transformer that follows the scikit-learn API. 
    The TPOTClassifier will also search over the hyperparameters of all objects in the pipeline.
    By default, TPOTClassifier will search over a broad range of supervised classification algorithms, transformers, and their parameters. 
    However, the algorithms, transformers, and hyperparameters that the TPOTClassifier searches over can be fully customized using the config_dict parameter.
    generations: int, optional (default=100)
        Number of iterations to the run pipeline optimization process. Must be a positive number.
        Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline.
        TPOT will evaluate population_size + generations × offspring_size pipelines in total. 
    population_size: int, optional (default=100)
        Number of individuals to retain in the genetic programming population every generation. Must be a positive number.
        Generally, TPOT will work better when you give it more individuals with which to optimize the pipeline. 
    offspring_size: int, optional (default=100)
        Number of offspring to produce in each genetic programming generation. Must be a positive number. 
    mutation_rate: float, optional (default=0.9)
        Mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the GP algorithm how many pipelines to apply random changes to every generation.
        mutation_rate + crossover_rate cannot exceed 1.0.
        We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms. 
    crossover_rate: float, optional (default=0.1)
        Crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the genetic programming algorithm how many pipelines to "breed" every generation.
        mutation_rate + crossover_rate cannot exceed 1.0.
        We recommend using the default parameter unless you understand how the crossover rate affects GP algorithms. 
    scoring: string or callable, optional (default='accuracy')
        Function used to evaluate the quality of a given pipeline for the classification problem. The following built-in scoring functions can be used:
        'accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss','precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc'
        If you would like to use a custom scorer, you can pass the callable object/function with signature scorer(estimator, X, y).
        If you would like to use a metric function, you can pass the callable function to this parameter with the signature score_func(y_true, y_pred). TPOT assumes that any function with "error" or "loss" in the function name is meant to be minimized, whereas any other functions will be maximized. This scoring type was deprecated in version 0.9.1 and will be removed in version 0.11.
    cv: int, cross-validation generator, or an iterable, optional (default=5)
        Cross-validation strategy used when evaluating pipelines.
        Possible inputs:
            integer, to specify the number of folds in a StratifiedKFold,
            An object to be used as a cross-validation generator, or
            An iterable yielding train/test splits.
    subsample: float, optional (default=1.0)
        Fraction of training samples that are used during the TPOT optimization process. Must be in the range (0.0, 1.0].
        Setting subsample=0.5 tells TPOT to use a random subsample of half of the training data. This subsample will remain the same during the entire pipeline optimization process. 
    n_jobs: integer, optional (default=1)
        Number of processes to use in parallel for evaluating pipelines during the TPOT optimization process.
        Setting n_jobs=-1 will use as many cores as available on the computer. Beware that using multiple processes on the same machine may cause memory issues for large datasets 
    max_time_mins: integer or None, optional (default=None)
        How many minutes TPOT has to optimize the pipeline.
        If not None, this setting will override the generations parameter and allow TPOT to run until max_time_mins minutes elapse. 
    max_eval_time_mins: integer, optional (default=5)
        How many minutes TPOT has to evaluate a single pipeline.
        Setting this parameter to higher values will allow TPOT to evaluate more complex pipelines, but will also allow TPOT to run longer. Use this parameter to help prevent TPOT from wasting time on evaluating time-consuming pipelines. 
    random_state: integer or None, optional (default=None)
        The seed of the pseudo random number generator used in TPOT.
        Use this parameter to make sure that TPOT will give you the same results each time you run it against the same data set with that seed. 
    config_dict: Python dictionary, string, or None, optional (default=None)
        A configuration dictionary for customizing the operators and parameters that TPOT searches in the optimization process.
        Possible inputs are:
            Python dictionary, TPOT will use your custom configuration,
            string 'TPOT light', TPOT will use a built-in configuration with only fast models and preprocessors, or
            string 'TPOT MDR', TPOT will use a built-in configuration specialized for genomic studies, or
            string 'TPOT sparse': TPOT will use a configuration dictionary with a one-hot encoder and the operators normally included in TPOT that also support sparse matrices, or
            None, TPOT will use the default TPOTClassifier configuration.
    warm_start: boolean, optional (default=False)
        Flag indicating whether the TPOT instance will reuse the population from previous calls to fit().
        Setting warm_start=True can be useful for running TPOT for a short time on a dataset, checking the results, then resuming the TPOT run from where it left off. 
    memory: a sklearn.external.joblib.Memory object or string, optional (default=None)
        If supplied, pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. More details about memory caching in [scikit-learn documentation](http://scikit-learn.org/stable/modules/pipeline.html#caching-transformers-avoid-repeated-computation)
        Possible inputs are:
            String 'auto': TPOT uses memory caching with a temporary directory and cleans it up upon shutdown, or
            Path of a caching directory, TPOT uses memory caching with the provided directory and TPOT does NOT clean the caching directory up upon shutdown, or
            Memory object, TPOT uses the instance of sklearn.external.joblib.Memory for memory caching and TPOT does NOT clean the caching directory up upon shutdown, or
            None, TPOT does not use memory caching.
    periodic_checkpoint_folder: path string, optional (default: None)
        If supplied, a folder in which TPOT will periodically save the best pipeline so far while optimizing.
        Currently once per generation but not more often than once per 30 seconds.
        Useful in multiple cases:
            Sudden death before TPOT could save optimized pipeline
            Track its progress
            Grab pipelines while it's still optimizing
    early_stop: integer, optional (default: None)
        How many generations TPOT checks whether there is no improvement in optimization process.
        Ends the optimization process if there is no improvement in the given number of generations. 
    verbosity: integer, optional (default=0)
        How much information TPOT communicates while it's running.
        Possible inputs are:
            0, TPOT will print nothing,
            1, TPOT will print minimal information,
            2, TPOT will print more information and provide a progress bar, or
            3, TPOT will print everything and provide a progress bar.
    disable_update_check: boolean, optional (default=False)
        Flag indicating whether the TPOT version checker should be disabled.
        The update checker will tell you when a new version of TPOT has been released. 
    Attributes: 	fitted_pipeline_: scikit-learn Pipeline object
        The best pipeline that TPOT discovered during the pipeline optimization process, fitted on the entire training dataset. 
    pareto_front_fitted_pipelines_: Python dictionary
        Dictionary containing the all pipelines on the TPOT Pareto front, where the key is the string representation of the pipeline and the value is the corresponding pipeline fitted on the entire training dataset.
        The TPOT Pareto front provides a trade-off between pipeline complexity (i.e., the number of steps in the pipeline) and the predictive performance of the pipeline.
        Note: pareto_front_fitted_pipelines_ is only available when verbosity=3. 
    evaluated_individuals_: Python dictionary
        Dictionary containing all pipelines that were evaluated during the pipeline optimization process, where the key is the string representation of the pipeline and the value is a tuple containing (# of steps in pipeline, accuracy metric for the pipeline).
        This attribute is primarily for internal use, but may be useful for looking at the other pipelines that TPOT evaluated. 

#Functions
fit(features, classes, sample_weight=None, groups=None)
    Run the TPOT optimization process on the given training data.
    Uses genetic programming to optimize a machine learning pipeline 
    that maximizes the score on the provided features and target. 
    This pipeline optimization procedure uses internal k-fold cross-validaton 
    to avoid overfitting on the provided data. 
    At the end of the pipeline optimization procedure, 
    the best pipeline is then trained on the entire set of provided samples.
    Parameters: 	
    features: array-like {n_samples, n_features}
        Feature matrix
        TPOT and all scikit-learn algorithms assume that the features will be numerical and there will be no missing values. As such, when a feature matrix is provided to TPOT, all missing values will automatically be replaced (i.e., imputed) using median value imputation.
        If you wish to use a different imputation strategy than median imputation, please make sure to apply imputation to your feature set prior to passing it to TPOT. 
    classes: array-like {n_samples}
        List of class labels for prediction 
    sample_weight: array-like {n_samples}, optional
        Per-sample weights. Higher weights force TPOT to put more emphasis on those points. 
    groups: array-like, with shape {n_samples, }, optional
        Group labels for the samples used when performing cross-validation.
        This parameter should only be used in conjunction with sklearn's Group cross-validation functions, such as sklearn.model_selection.GroupKFold. 
    Returns: 	
    self: object
        Returns a copy of the fitted TPOT object 
    
predict(features)
    Use the optimized pipeline to predict the classes for a feature set.
    Parameters: 	
    features: array-like {n_samples, n_features}
        Feature matrix 
    Returns: 	
    predictions: array-like {n_samples}
        Predicted classes for the samples in the feature matrix 
        
        
predict_proba(features)
    Use the optimized pipeline to estimate the class probabilities for a feature set.
    Note: This function will only work for pipelines whose final classifier supports the predict_proba function. 
    TPOT will raise an error otherwise.
    Parameters: 	
    features: array-like {n_samples, n_features}
        Feature matrix 
    Returns: 	
    predictions: array-like {n_samples, n_classes}
        The class probabilities of the input samples 
    
    
score(testing_features, testing_classes)
    Returns the optimized pipeline's score on the given testing data using the user-specified scoring function.
    The default scoring function for TPOTClassifier is 'accuracy'.
    Parameters: 	
    testing_features: array-like {n_samples, n_features}
        Feature matrix of the testing set 
    testing_classes: array-like {n_samples}
        List of class labels for prediction in the testing set 
    Returns: 	
    accuracy_score: float
        The estimated test set accuracy according to the user-specified scoring function. 

    
export(output_file_name)
    Export the optimized pipeline as Python code.
    Parameters: 	
    output_file_name: string
        String containing the path and file name of the desired output file 
    Returns: 	Does not return anything
    
    
##Example
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)
tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_mnist_pipeline.py')




##Regression
class tpot.TPOTRegressor(generations=100, population_size=100,
                         offspring_size=None, mutation_rate=0.9,
                         crossover_rate=0.1,
                         scoring='neg_mean_squared_error', cv=5,
                         subsample=1.0, n_jobs=1,
                         max_time_mins=None, max_eval_time_mins=5,
                         random_state=None, config_dict=None,
                         warm_start=False,
                         memory=None,
                         periodic_checkpoint_folder=None,
                         verbosity=0,
                         disable_update_check=False)
    Automated machine learning for supervised regression tasks.
    Parameters: 	
    generations: int, optional (default=100)
        Number of iterations to the run pipeline optimization process. Must be a positive number.
        Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline.
        TPOT will evaluate population_size + generations × offspring_size pipelines in total. 
    population_size: int, optional (default=100)
        Number of individuals to retain in the genetic programming population every generation. Must be a positive number.
        Generally, TPOT will work better when you give it more individuals with which to optimize the pipeline. 
    offspring_size: int, optional (default=100)
        Number of offspring to produce in each genetic programming generation. Must be a positive number. 
    mutation_rate: float, optional (default=0.9)
        Mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the GP algorithm how many pipelines to apply random changes to every generation.
        mutation_rate + crossover_rate cannot exceed 1.0.
        We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms. 
    crossover_rate: float, optional (default=0.1)
        Crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the genetic programming algorithm how many pipelines to "breed" every generation.
        mutation_rate + crossover_rate cannot exceed 1.0.
        We recommend using the default parameter unless you understand how the crossover rate affects GP algorithms. 
    scoring: string or callable, optional (default='neg_mean_squared_error')
        Function used to evaluate the quality of a given pipeline for the regression problem. The following built-in scoring functions can be used:
        'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'r2'
        Note that we recommend using the neg version of mean squared error and related metrics so TPOT will minimize (instead of maximize) the metric.
        If you would like to use a custom scorer, you can pass the callable object/function with signature scorer(estimator, X, y).
        If you would like to use a metric function, you can pass the callable function to this parameter with the signature score_func(y_true, y_pred). TPOT assumes that any function with "error" or "loss" in the function name is meant to be minimized, whereas any other functions will be maximized. This scoring type was deprecated in version 0.9.1 and will be removed in version 0.11.
        See the section on scoring functions for more details. 
    cv: int, cross-validation generator, or an iterable, optional (default=5)
        Cross-validation strategy used when evaluating pipelines.
        Possible inputs:
            integer, to specify the number of folds in a KFold,
            An object to be used as a cross-validation generator, or
            An iterable yielding train/test splits.
    subsample: float, optional (default=1.0)
        Fraction of training samples that are used during the TPOT optimization process. Must be in the range (0.0, 1.0].
        Setting subsample=0.5 tells TPOT to use a random subsample of half of the training data. This subsample will remain the same during the entire pipeline optimization process. 
    n_jobs: integer, optional (default=1)
        Number of processes to use in parallel for evaluating pipelines during the TPOT optimization process.
        Setting n_jobs=-1 will use as many cores as available on the computer. Beware that using multiple processes on the same machine may cause memory issues for large datasets 
    max_time_mins: integer or None, optional (default=None)
        How many minutes TPOT has to optimize the pipeline.
        If not None, this setting will override the generations parameter and allow TPOT to run until max_time_mins minutes elapse. 
    max_eval_time_mins: integer, optional (default=5)
        How many minutes TPOT has to evaluate a single pipeline.
        Setting this parameter to higher values will allow TPOT to evaluate more complex pipelines, but will also allow TPOT to run longer. Use this parameter to help prevent TPOT from wasting time on evaluating time-consuming pipelines. 
    random_state: integer or None, optional (default=None)
        The seed of the pseudo random number generator used in TPOT.
        Use this parameter to make sure that TPOT will give you the same results each time you run it against the same data set with that seed. 
    config_dict: Python dictionary, string, or None, optional (default=None)
        A configuration dictionary for customizing the operators and parameters that TPOT searches in the optimization process.
        Possible inputs are:
            Python dictionary, TPOT will use your custom configuration,
            string 'TPOT light', TPOT will use a built-in configuration with only fast models and preprocessors, or
            string 'TPOT MDR', TPOT will use a built-in configuration specialized for genomic studies, or
            string 'TPOT sparse': TPOT will use a configuration dictionary with a one-hot encoder and the operators normally included in TPOT that also support sparse matrices, or
            None, TPOT will use the default TPOTRegressor configuration.
        See the built-in configurations section for the list of configurations included with TPOT, and the custom configuration section for more information and examples of how to create your own TPOT configurations. 
    warm_start: boolean, optional (default=False)
        Flag indicating whether the TPOT instance will reuse the population from previous calls to fit().
        Setting warm_start=True can be useful for running TPOT for a short time on a dataset, checking the results, then resuming the TPOT run from where it left off. 
    memory: a sklearn.external.joblib.Memory object or string, optional (default=None)
        If supplied, pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. More details about memory caching in [scikit-learn documentation](http://scikit-learn.org/stable/modules/pipeline.html#caching-transformers-avoid-repeated-computation)
        Possible inputs are:
            String 'auto': TPOT uses memory caching with a temporary directory and cleans it up upon shutdown, or
            Path of a caching directory, TPOT uses memory caching with the provided directory and TPOT does NOT clean the caching directory up upon shutdown, or
            Memory object, TPOT uses the instance of sklearn.external.joblib.Memory for memory caching and TPOT does NOT clean the caching directory up upon shutdown, or
            None, TPOT does not use memory caching.
    periodic_checkpoint_folder: path string, optional (default: None)
        If supplied, a folder in which TPOT will periodically save the best pipeline so far while optimizing.
        Currently once per generation but not more often than once per 30 seconds.
        Useful in multiple cases:
            Sudden death before TPOT could save optimized pipeline
            Track its progress
            Grab pipelines while it's still optimizing
    early_stop: integer, optional (default: None)
        How many generations TPOT checks whether there is no improvement in optimization process.
        Ends the optimization process if there is no improvement in the given number of generations. 
    verbosity: integer, optional (default=0)
        How much information TPOT communicates while it's running.
        Possible inputs are:
            0, TPOT will print nothing,
            1, TPOT will print minimal information,
            2, TPOT will print more information and provide a progress bar, or
            3, TPOT will print everything and provide a progress bar.
    disable_update_check: boolean, optional (default=False)
        Flag indicating whether the TPOT version checker should be disabled.
        The update checker will tell you when a new version of TPOT has been released. 
    Attributes: 	
    fitted_pipeline_: scikit-learn Pipeline object
        The best pipeline that TPOT discovered during the pipeline optimization process, fitted on the entire training dataset. 
    pareto_front_fitted_pipelines_: Python dictionary
        Dictionary containing the all pipelines on the TPOT Pareto front, where the key is the string representation of the pipeline and the value is the corresponding pipeline fitted on the entire training dataset.
        The TPOT Pareto front provides a trade-off between pipeline complexity (i.e., the number of steps in the pipeline) and the predictive performance of the pipeline.
        Note: _pareto_front_fitted_pipelines is only available when verbosity=3. 
    evaluated_individuals_: Python dictionary
        Dictionary containing all pipelines that were evaluated during the pipeline optimization process, where the key is the string representation of the pipeline and the value is a tuple containing (# of steps in pipeline, accuracy metric for the pipeline).
        This attribute is primarily for internal use, but may be useful for looking at the other pipelines that TPOT evaluated. 
    Functions
    fit(features, target[, sample_weight, groups]) 	Run the TPOT optimization process on the given training data.
    predict(features) 	                        Use the optimized pipeline to predict the target values for a feature set.
    score(testing_features, testing_target) 	Returns the optimized pipeline's score on the given testing data using the user-specified scoring function.
    export(output_file_name) 	                Export the optimized pipeline as Python code.

##Example
from tpot import TPOTRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
digits = load_boston()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)
tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_boston_pipeline.py')



##Example - Iris flower classification
from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_iris_pipeline.py')  #get the pipeline code 

#eg : generated file: tpot_iris_pipeline.py
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1),
                     tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_target, testing_target = \
    train_test_split(features, tpot_data['class'], random_state=42)

exported_pipeline = make_pipeline(
    Normalizer(),
    GaussianNB()
)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)




##Example - MNIST digit recognition
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_mnist_pipeline.py')#get the pipeline code 

#eg : generated file: tpot_mnist_pipeline.py
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1),
                     tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_target, testing_target = \
    train_test_split(features, tpot_data['class'], random_state=42)

exported_pipeline = KNeighborsClassifier(n_neighbors=6, weights="distance")

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)



##Example - Boston housing prices modeling
from tpot import TPOTRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

housing = load_boston()
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,
                                                    train_size=0.75, test_size=0.25)

tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_boston_pipeline.py')#get the pipeline code 

#eg : generated file: tpot_boston_pipeline.py
import numpy as np

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1),
                     tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_target, testing_target = \
    train_test_split(features, tpot_data['class'], random_state=42)

exported_pipeline = GradientBoostingRegressor(alpha=0.85, learning_rate=0.1, loss="ls",
                                              max_features=0.9, min_samples_leaf=5,
                                              min_samples_split=6)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)




##Example - TPOT tutorial on the Titanic dataset

# Import required libraries
from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split
import pandas as pd 
import numpy as np

# Load the data
titanic = pd.read_csv('data/titanic_train.csv')
>>> titanic.head(5)
	PassengerId 	Survived 	Pclass 	Name 	Sex 	Age 	SibSp 	Parch 	Ticket 	Fare 	Cabin 	Embarked
0 	1 	0 	3 	Braund, Mr. Owen Harris 	male 	22.0 	1 	0 	A/5 21171 	7.2500 	NaN 	S
1 	2 	1 	1 	Cumings, Mrs. John Bradley (Florence Briggs Th... 	female 	38.0 	1 	0 	PC 17599 	71.2833 	C85 	C
2 	3 	1 	3 	Heikkinen, Miss. Laina 	female 	26.0 	0 	0 	STON/O2. 3101282 	7.9250 	NaN 	S
3 	4 	1 	1 	Futrelle, Mrs. Jacques Heath (Lily May Peel) 	female 	35.0 	1 	0 	113803 	53.1000 	C123 	S
4 	5 	0 	3 	Allen, Mr. William Henry 	male 	35.0 	0 	0 	373450 	8.0500 	NaN 	S

#Data Exploration
>>> titanic.groupby('Sex').Survived.value_counts()
Sex     Survived
female  1           233
        0            81
male    0           468
        1           109
Name: Survived, dtype: int64

>>> titanic.groupby(['Pclass','Sex']).Survived.value_counts()
Pclass  Sex     Survived
1       female  1            91
                0             3
        male    0            77
                1            45
2       female  1            70
                0             6
        male    0            91
                1            17
3       female  0            72
                1            72
        male    0           300
                1            47
Name: Survived, dtype: int64

id = pd.crosstab([titanic.Pclass, titanic.Sex], titanic.Survived.astype(float))
>>> id.div(id.sum(1).astype(float), 0)
	Survived 	0.0 	1.0
Pclass 	Sex 		
1 	female 	0.031915 	0.968085
male 	0.631148 	0.368852
2 	female 	0.078947 	0.921053
male 	0.842593 	0.157407
3 	female 	0.500000 	0.500000
male 	0.864553 	0.135447

#Data Munging
#The first and most important step in using TPOT on any data set 
#is to rename the target class/response variable to class.
titanic.rename(columns={'Survived': 'class'}, inplace=True)
#At present, TPOT requires all the data to be in numerical format. 
#Categorical:  Name, Sex, Ticket, Cabin and Embarked.
>>> titanic.dtypes
PassengerId      int64
class            int64
Pclass           int64
Name            object
Sex             object
Age            float64
SibSp            int64
Parch            int64
Ticket          object
Fare           float64
Cabin           object
Embarked        object
dtype: object

#number of levels that each of the five categorical variables have.
for cat in ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']:
    print("Number of levels in category '{0}': \b {1:2.2f} ".format(cat, titanic[cat].unique().size))
#output 
Number of levels in category 'Name':  891.00 
Number of levels in category 'Sex':  2.00 
Number of levels in category 'Ticket':  681.00 
Number of levels in category 'Cabin':  148.00 
Number of levels in category 'Embarked':  4.00 

#Find level 
for cat in ['Sex', 'Embarked']:
    print("Levels for catgeory '{0}': {1}".format(cat, titanic[cat].unique()))
#output 
Levels for catgeory 'Sex': ['male' 'female']
Levels for catgeory 'Embarked': ['S' 'C' 'Q' nan]

#code these levels manually into numerical values. 
#For nan i.e. the missing values, replace them with a placeholder value (-999). 
titanic['Sex'] = titanic['Sex'].map({'male':0,'female':1})
titanic['Embarked'] = titanic['Embarked'].map({'S':0,'C':1,'Q':2})
titanic = titanic.fillna(-999)
>>> pd.isnull(titanic).any()
PassengerId    False
class          False
Pclass         False
Name           False
Sex            False
Age            False
SibSp          False
Parch          False
Ticket         False
Fare           False
Cabin          False
Embarked       False
dtype: bool

#Drop - Name and Ticket as they  have so many levels
#For Cabin, encode the levels as digits using Scikit-learn's MultiLabelBinarizer and treat them as new features.

from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
CabinTrans = mlb.fit_transform([{str(val)} for val in titanic['Cabin'].values])
>>> CabinTrans
array([[1, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [1, 0, 0, ..., 0, 0, 0],
       ..., 
       [1, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [1, 0, 0, ..., 0, 0, 0]])

titanic_new = titanic.drop(['Name','Ticket','Cabin','class'], axis=1)
assert (len(titanic['Cabin'].unique()) == len(mlb.classes_)), "Not Equal" #check correct encoding done
titanic_new = np.hstack((titanic_new.values,CabinTrans))
>>> np.isnan(titanic_new).any()
False
#sklearn only understands ndarray 
titanic_class = titanic['class'].values


##Data Analysis using TPOT

training_indices, validation_indices = training_indices, testing_indices = train_test_split(titanic.index, stratify = titanic_class, train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(verbosity=2, max_time_mins=2, max_eval_time_mins=0.04, population_size=40)
tpot.fit(titanic_new[training_indices], titanic_class[training_indices])
tpot.score(titanic_new[validation_indices], titanic.loc[validation_indices, 'class'].values)
#0.8340807174887892
tpot.export('tpot_titanic_pipeline.py')



# %load tpot_titanic_pipeline.py
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)
exported_pipeline = RandomForestClassifier(bootstrap=False, max_features=0.4, min_samples_leaf=1, min_samples_split=9)
exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)


##Make predictions on the submission data
# Read in the submission dataset
titanic_sub = pd.read_csv('data/titanic_test.csv')
titanic_sub.describe()

#The most important step here is to check 
#for new levels in the categorical variables of the submission dataset that are absent in the training set. 
#We identify them and set them to our placeholder value of '-999', 
for var in ['Cabin']: #,'Name','Ticket']:
    new = list(set(titanic_sub[var]) - set(titanic[var]))
    titanic_sub.ix[titanic_sub[var].isin(new), var] = -999
    

titanic_sub['Sex'] = titanic_sub['Sex'].map({'male':0,'female':1})
titanic_sub['Embarked'] = titanic_sub['Embarked'].map({'S':0,'C':1,'Q':2})
titanic_sub = titanic_sub.fillna(-999)
>>> pd.isnull(titanic_sub).any()
PassengerId    False
Pclass         False
Name           False
Sex            False
Age            False
SibSp          False
Parch          False
Ticket         False
Fare           False
Cabin          False
Embarked       False
dtype: bool

from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
SubCabinTrans = mlb.fit([{str(val)} for val in titanic['Cabin'].values]).transform([{str(val)} for val in titanic_sub['Cabin'].values])
titanic_sub = titanic_sub.drop(['Name','Ticket','Cabin'], axis=1)

# Form the new submission data set
titanic_sub_new = np.hstack((titanic_sub.values,SubCabinTrans))
# Ensure equal number of features in both the final training and submission dataset
assert (titanic_new.shape[1] == titanic_sub_new.shape[1]), "Not Equal"

# Generate the predictions
submission = tpot.predict(titanic_sub_new)

# Create the submission file
final = pd.DataFrame({'PassengerId': titanic_sub['PassengerId'], 'Survived': submission})
final.to_csv('data/submission.csv', index = False)









###Scikit External projects - sparkit-learn - Quick Intro 
#Scikit-learn API and functionality for PySpark's distributed modelling.

#Requirements
    Python 2.7.x or 3.4.x
    Spark[>=1.3.0]
    NumPy[>=1.9.0]
    SciPy[>=0.14.0]
    Scikit-learn[>=0.16]

#Does not work in scikit=0.19.2 
#use scikit=0.18.2 
$ pip install scikit-learn==0.18.2
#Create virtualenv 
$ pip install virtualenv 

$ virtualenv --system-site-packages  sparkit
#sparkit created at current dir 
$ sparkit\Scripts\activate
$ pip install scikit-learn==0.18.2
$ python 

#then deactivate 
$ deactivate


#pyspark depends on py4j 
#use py4j from spark 
$ set SPARK_HOME=<path to Spark home>
$ set PYTHONPATH=%SPARK_HOME%/python/lib/py4j-0.10.4-src.zip;%SPARK_HOME%/python/;%PYTHONPATH%

$ pyspark --master local[4] --driver-memory 2G

#RDD consists of many partitions, 
#with each partition on a core of node in cluster 

#One Partition executes one Task 

#One program consists of many stages 

#A stage is consisting of many parallel tasks 
#where one task is on one  partition 

#stage is uniquely identified by  id . 
#When a stage is created, 
#DAGScheduler increments internal counter  nextStageId  to track the number of stage submissions.

#stage can only work on the partitions of a single RDD (identified by  rdd ), 
#but can be associated with many other dependent parent stages (via internal field  parents ), 
#with the boundary of a stage marked by shuffle dependencies.       
        
#Submitting a stage can therefore trigger execution of a series of dependent parent stages   

#Display means 
[Stage7:===========>                              (14174 + 5) / 62500]        
        
#Stage 7: shows the stage you are in now, 
#(14174 + 5) / 62500] is (numCompletedTasks + numActiveTasks) / totalNumOfTasksInThisStage 
    
    
##splearn Reference 
splearn.rdd.block(rdd, bsize=-1, dtype=None):
    Block an RDD
    Parameters
    ----------
    rdd : RDD
        RDD of data points to block into either numpy arrays,
        scipy sparse matrices, or pandas data frames.
        Type of data point will be automatically inferred
        and blocked accordingly.
    bsize : int, optional, default None
        Size of each block (number of elements), if None all data points
        from each partition will be combined in a block.
    Returns
    -------
    rdd : ArrayRDD or TupleRDD or DictRDD
        The transformed rdd with added functionality
        
        
splearn.rdd.ArrayRDD(rdd, bsize=-1, dtype=np.ndarray, noblock=False)
    A distributed array data structure.
    Stores distributed numpy.arrays. It provides a transparent interface to the
    underlying pyspark.rdd.RDD and also extends it with numpy.array like
    methods.
    Parameters
    ----------
    rdd : pyspark.rdd.RDD
        A parallelized data container
    bsize : {int, None, False} default to None
        The number of entries to block together. If None one block will be
        created in every partition. If False, no blocking executed. Useful when
        casting already blocked rdds.
    Attributes
    ----------
    partitions : int
        The number of partitions in the rdd
    blocks : int
        The number of blocks present in the rdd
    size
    ndim
    _rdd : pyspark.rdd.RDD
        The underlying distributed data container        
    #base:BlockRDD and ArrayLikeRDDMixin attributes ,methods 
    size
    ndim
    shape : tuple
        The dimensionality of the dataset
    self[index] 
        one index or slice, return ArrayRDD
    len(self)
    toarray()
    tolist()
    unblock()
        Flattens the blocks
    transform(fn, dtype=None, *args, **kwargs) 
        maps each element with fn(each_ndarray,*args, **kwargs) 
        fn can be any numpy.methods(*args, **kwargs)
    sum(axis=None)
    mean(axis=None)
    #Methods - elementwise  
    self * other 
    self + other
    self - other
    self / other
    self // other
    self ** other 
    self % other 
    tosparse() 
        Returns SparseRDD containing scipy.parse.csr_matrix 
    flatten()
    min(axis=None)
    max(axis=None)
    prod(axis=None)
    dot(other)
    add(other)
    subtract(other)
    multiply(other)
    divide(other)
    power(other)
    floor_divide(other)
    true_divide(other)
    mod(other)
    fmod(other)
    remainder(other)

 
            
class SparseRDD(rdd, bsize=-1, dtype=sp.spmatrix, noblock=False)
    #base:BlockRDD and ArrayLikeRDDMixin attributes ,methods 
    size
    ndim
    shape : tuple
        The dimensionality of the dataset
    self[index] # one index or slice, return ArrayRDD
    len(self)
    toarray()
    tolist()
    unblock()
        Flattens the blocks
    transform(fn, dtype=None, *args, **kwargs) 
        maps each element with fn(ndarray,*args, **kwargs) 
    sum(self, axis=None)
    mean(self, axis=None)
    #methods 
    todense()
    dot(other)
    min(axis=None)
    max(axis=None)

        
class DictRDD(rdd, columns=None, bsize=-1, dtype=None, noblock=False):
    Distributed named tuple data structure.

    The tuple is stored as a tuple of numpy.arrays in each block. It works like
    a column based data structure, each column can be transformed and accessed
    independently and identified by a column name. Very useful for training:
    In splearn the fit methods expects a DictRDD with 'X', 'y' and an optional
    'w' columns.
        'X' - training data,
        'y' - labels,
        'w' - weights

    Parameters
    ----------
    rdd : pyspark.rdd.RDD
        A parallelized data container
    bsize : {int, None, False} default to None
        The number of entries to block together. If None, one block will be
        created in every partition. If False, no blocking executed. Useful when
        casting already blocked rdds.

    Attributes
    ----------
    columns : tuple
        Name of columns in the rdd as a tuple
    partitions : int
        The number of partitions in the rdd
    blocks : int
        The number of blocks present in the rdd
    shape : tuple
        The dimensionality of the dataset
    _rdd : pyspark.rdd.RDD
        The underlying distributed data container

    #base:BlockRDD and ArrayLikeRDDMixin attributes ,methods 
    size
    ndim
    shape : tuple
        The dimensionality of the dataset
    len(self)
    toarray()
    tolist()
    #Methods 
    get_params()
    ix(key)
        Returns the selected blocks defined by index parameter.
        Parameter
        ---------
        index : int or slice
            The key of the block or the range of the blocks.
        Returns
        -------
        block : DictRDD
            The selected block(s)        
    self[index] 
        Access a specified block.
        Parameters
        ----------
        key : int or slice
            The key of the block or the range of the blocks.

        Returns
        -------
        block : ArrayRDD or TupleRDD
            The selected block(s).
    self in key 
        checks for containment 
    unblock()
        Flattens the blocks
    transform(fn, column=None, dtype=None):
        Execute a transformation on a column or columns. Returns the modified
        DictRDD.
        Parameters
        ----------
        f : function
            The function to execute on the columns.
        column : {str, list or None}
            The column(s) to transform. If None is specified the method is
            equivalent to map.
        column : {str, list or None}
            The dtype of the column(s) to transform.
        Returns
        -------
        result : DictRDD
            DictRDD with transformed column(s).
            
##splearn: Spark version of scikit-learn 
splearn.feature_extraction
    SparkDictVectorizer
splearn.feature_extraction.text
    SparkCountVectorizer
    SparkHashingVectorizer
    SparkTfidfTransformer
splearn.decomposition
    SparkTruncatedSVD
splearn.feature_selection
    SparkVarianceThreshold
splearn.linear_model
    SparkLinearRegression
    SparkLogisticRegression
    SparkSGDClassifier
splearn.neighbors    
    SparkLSHForest
splearn.cluster
    SparkKMeans
splearn.preprocessing
    SparkLabelEncoder
splearn.svm
    SparkLinearSVC
splearn.grid_search
    SparkGridSearchCV
splearn.naive_bayes 
    SparkGaussianNB
    SparkMultinomialNB
    SparkBernoulliNB
splearn.pipeline    
    SparkPipeline
    SparkFeatureUnion
    make_sparkunion(*transformers)            
            

##splearn : ArrayRDD:
#A numpy.array like distributed array
#Note for 1D ndarray, only axis=0 is possible 
#for 2D nd array, axis=0, columnwise and axis=1, rowwise 

from splearn.rdd import ArrayRDD

data = range(20)
# PySpark RDD with 2 partitions
#each partition would be processed by 
rdd = sc.parallelize(data, 2) # each partition with 10 elements
>>> rdd.collect()
[Stage 0:>                                                          (0 + 0) / 2]
[Stage 0:>                                                          (0 + 2) / 2]
[Stage 0:=============================>                             (1 + 1) / 2]

[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]


# ArrayRDD : stores np.ndarray ie list of ndarray
#here each ndarray is 1D, hence only axis=0 is possible 
# each partition will contain blocks with 5 elements
#bsize=number of elements
# 4 blocks of ndarray, each block=5 elements, 2 partition, each partition=2 blocks
#So it stores list of ndarray 
X = ArrayRDD(rdd, bsize=5) 

>>> X.partitions
2
>>>
>>> X.collect()
[Stage 4:>                                                          (0 + 2) / 2]
[Stage 4:=============================>                             (1 + 1) / 2]

[array([0, 1, 2, 3, 4]), array([5, 6, 7, 8, 9]), array([10, 11, 12, 13, 14]), ar
ray([15, 16, 17, 18, 19])]
>>> X.bsize
5 
>>> X[0].collect()  #note collect would convert to list , hence list of 1 element 
[Stage 7:>                                                          (0 + 2) / 2]
[Stage 7:=============================>                             (1 + 1) / 2]
[array([0, 1, 2, 3, 4])]
>>> X[0,0] #eventhough X is min 2D, but this is not allowed 
>>> X[0].collect()[0][0] #now gives single element 
0


>>> X.min()
[Stage 25:>                                                         (0 + 2) / 2]
[Stage 25:=============================>                            (1 + 1) / 2]

0
>>> X.min(axis=0)
[Stage 26:>                                                         (0 + 2) / 2]
[Stage 26:=============================>                            (1 + 1) / 2]

0
>>> X.min(axis=1)
ERROR as ndarray is 1D 

>>> X.transform(lambda x:x.min()).collect()
[Stage 28:>                                                         (0 + 0) / 2]
[Stage 28:>                                                         (0 + 2) / 2]
[Stage 28:=============================>                            (1 + 1) / 2]

[0, 5, 10, 15]


##ArrayRDD : Basic operations:

len(X) # 20 - number of elements in the whole dataset
X.blocks # 4 - number of blocks
X.shape # (20,) - the shape of the whole dataset

X # returns an ArrayRDD
# <class 'splearn.rdd.ArrayRDD'> from PythonRDD...

X.dtype # returns the type of the blocks
# numpy.ndarray

X.collect() # get the dataset
# [array([0, 1, 2, 3, 4]),
#  array([5, 6, 7, 8, 9]),
#  array([10, 11, 12, 13, 14]),
#  array([15, 16, 17, 18, 19])]

X[1].collect() # indexing
# [array([5, 6, 7, 8, 9])]

X[1] # also returns an ArrayRDD!

X[1::2].collect() # slicing
# [array([5, 6, 7, 8, 9]),
#  array([15, 16, 17, 18, 19])]

X[1::2] # returns an ArrayRDD as well

X.tolist() # returns the dataset as a list
# [0, 1, 2, ... 17, 18, 19]
X.toarray() # returns the dataset as a numpy.array
# array([ 0,  1,  2, ... 17, 18, 19])

# pyspark.rdd operations will still work
X.getNumPartitions() # 2 - number of partitions



##splearn : SparseRDD
#blocks are sparse matrices - scipy.sparse matrices. 
#Usually the SparseRDD is created by splearn's transformators, 
#but one can instantiate too.

# generate a SparseRDD from a text using SparkCountVectorizer
from splearn.rdd import *
from sklearn.feature_extraction.tests.test_text import ALL_FOOD_DOCS
>>> ALL_FOOD_DOCS
(u'the pizza pizza beer copyright',


# ArrayRDD created from the raw data
X = ArrayRDD(sc.parallelize(ALL_FOOD_DOCS, 4), 2) #4 partions and 2 blocks 
X.collect()
# [array([u'the pizza pizza beer copyright',
#         u'the pizza burger beer copyright'], dtype='<U31'),
#  array([u'the the pizza beer beer copyright',
#         u'the burger beer beer copyright'], dtype='<U33'),
#  array([u'the coke burger coke copyright',
#         u'the coke burger burger'], dtype='<U30'),
#  array([u'the salad celeri copyright',
#         u'the salad salad sparkling water copyright'], dtype='<U41'),
#  array([u'the the celeri celeri copyright',
#         u'the tomato tomato salad water'], dtype='<U31'),
#  array([u'the tomato salad water copyright'], dtype='<U32')]

# Feature extraction executed
from splearn.feature_extraction.text import SparkCountVectorizer
vect = SparkCountVectorizer() #only works in scikit-learn==0.18.2
X = vect.fit_transform(X)
# and we have a SparseRDD
X
# <class 'splearn.rdd.SparseRDD'> from PythonRDD...

# it's type is the scipy.sparse's general parent
X.dtype
# scipy.sparse.base.spmatrix
>>> X.shape
[Stage 8:>                                                          (0 + 4) / 4]
[Stage 8:============================================>              (3 + 1) / 4]

(11, 11)

# slicing works just like in ArrayRDDs
X[2:4].collect()
# [<2x11 sparse matrix of type '<type 'numpy.int64'>'
#   with 7 stored elements in Compressed Sparse Row format>,
#  <2x11 sparse matrix of type '<type 'numpy.int64'>'
#   with 9 stored elements in Compressed Sparse Row format>]

# general mathematical operations are available
X.sum(), X.mean(), X.max(), X.min()
# (55, 0.45454545454545453, 2, 0)

# even with axis parameters provided
X.sum(axis=1)
# matrix([[5],
#         [5],
#         [6],
#         [5],
#         [5],
#         [4],
#         [4],
#         [6],
#         [5],
#         [5],
#         [5]])

# It can be transformed to dense ArrayRDD
X.todense()
# <class 'splearn.rdd.ArrayRDD'> from PythonRDD...
X.todense().collect()
# [array([[1, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0],
#         [1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0]]),
#  array([[2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0],
#         [2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]]),
#  array([[0, 1, 0, 2, 1, 0, 0, 0, 1, 0, 0],
#         [0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0]]),
#  array([[0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0],
#         [0, 0, 0, 0, 1, 0, 2, 1, 1, 0, 1]]),
#  array([[0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0],
#         [0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1]]),
#  array([[0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1]])]

# One can instantiate SparseRDD manually too:
import scipy as sp
sparse = sc.parallelize(np.array([sp.eye(2).tocsr()]*20), 2)
sparse = SparseRDD(sparse, bsize=5)
sparse
# <class 'splearn.rdd.SparseRDD'> from PythonRDD...

sparse.collect()
# [<10x2 sparse matrix of type '<type 'numpy.float64'>'
#   with 10 stored elements in Compressed Sparse Row format>,
#  <10x2 sparse matrix of type '<type 'numpy.float64'>'
#   with 10 stored elements in Compressed Sparse Row format>,
#  <10x2 sparse matrix of type '<type 'numpy.float64'>'
#   with 10 stored elements in Compressed Sparse Row format>,
#  <10x2 sparse matrix of type '<type 'numpy.float64'>'
#   with 10 stored elements in Compressed Sparse Row format>]



##splearn : DictRDD:
#A column based data format, each column with it's own type.

from splearn.rdd import DictRDD

X = range(20)
y = list(range(2)) * 10
# PySpark RDD with 2 partitions
X_rdd = sc.parallelize(X, 2) # each partition with 10 elements
y_rdd = sc.parallelize(y, 2) # each partition with 10 elements
# DictRDD
#total elements of X_rdd, y_rdd = 2*20 = part(2)*blocks(4)*bsize(5)
# each partition will contain blocks with 5 elements
Z = DictRDD((X_rdd, y_rdd),
            columns=('X', 'y'),
            bsize=5,
            dtype=[np.ndarray, np.ndarray]) # 4 blocks, 2/partition
# if no dtype is provided, the type of the blocks will be determined
# automatically

# or:
import numpy as np

data = np.array([range(20), [0,1]*10]).T
rdd = sc.parallelize(data, 2) #data is 2D 
Z = DictRDD(rdd,
            columns=('X', 'y'),
            bsize=5,
            dtype=[np.ndarray, np.ndarray])

##DictRDD:Basic operations:

len(Z) # 8 - number of blocks
Z.columns # returns ('X', 'y')
Z.dtype # returns the types in correct order
# [numpy.ndarray, numpy.ndarray]

Z # returns a DictRDD
#<class 'splearn.rdd.DictRDD'> from PythonRDD...

Z.collect()
# [(array([0, 1, 2, 3, 4]), array([0, 1, 0, 1, 0])),
#  (array([5, 6, 7, 8, 9]), array([1, 0, 1, 0, 1])),
#  (array([10, 11, 12, 13, 14]), array([0, 1, 0, 1, 0])),
#  (array([15, 16, 17, 18, 19]), array([1, 0, 1, 0, 1]))]

Z[:, 'y'] # column select - returns an ArrayRDD
Z[:, 'y'].collect()
# [array([0, 1, 0, 1, 0]),
#  array([1, 0, 1, 0, 1]),
#  array([0, 1, 0, 1, 0]),
#  array([1, 0, 1, 0, 1])]

Z[:-1, ['X', 'y']] # slicing - DictRDD
Z[:-1, ['X', 'y']].collect()
# [(array([0, 1, 2, 3, 4]), array([0, 1, 0, 1, 0])),
#  (array([5, 6, 7, 8, 9]), array([1, 0, 1, 0, 1])),
#  (array([10, 11, 12, 13, 14]), array([0, 1, 0, 1, 0]))]


##splearn- SparkCountVectorizer

from splearn.rdd import ArrayRDD
from splearn.feature_extraction.text import SparkCountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.feature_extraction.tests.test_text import ALL_FOOD_DOCS
X = list(ALL_FOOD_DOCS)# list of texts

X_rdd = ArrayRDD(sc.parallelize(X, 4))  # sc is SparkContext, 
local = CountVectorizer()
dist = SparkCountVectorizer()

result_local = local.fit_transform(X)
result_dist = dist.fit_transform(X_rdd)  # SparseRDD, 2D



##splearn- SparkHashingVectorizer

from splearn.rdd import ArrayRDD
from splearn.feature_extraction.text import SparkHashingVectorizer
from sklearn.feature_extraction.text import HashingVectorizer


local = HashingVectorizer()
dist = SparkHashingVectorizer()

result_local = local.fit_transform(X)
result_dist = dist.fit_transform(X_rdd)  # SparseRDD

##splearn- SparkTfidfTransformer

from splearn.rdd import ArrayRDD
from splearn.feature_extraction.text import SparkHashingVectorizer
from splearn.feature_extraction.text import SparkTfidfTransformer
from splearn.pipeline import SparkPipeline

from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline


local_pipeline = Pipeline((
    ('vect', HashingVectorizer()),
    ('tfidf', TfidfTransformer())
))
dist_pipeline = SparkPipeline((
    ('vect', SparkHashingVectorizer()),
    ('tfidf', SparkTfidfTransformer())
))

result_local = local_pipeline.fit_transform(X)
result_dist = dist_pipeline.fit_transform(X_rdd)  # SparseRDD



##splearn- Distributed Classifiers

from splearn.rdd import DictRDD
from splearn.feature_extraction.text import SparkHashingVectorizer
from splearn.feature_extraction.text import SparkTfidfTransformer
from splearn.svm import SparkLinearSVC
from splearn.pipeline import SparkPipeline

from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

from sklearn.feature_extraction.tests.test_text import ALL_FOOD_DOCS
X = list(ALL_FOOD_DOCS)# list of texts
y = np.random.randint(0,2,size=len(ALL_FOOD_DOCS)  # list of labels
X_rdd = sc.parallelize(X, 4)
y_rdd = sc.parallelize(y, 4)
Z = DictRDD((X_rdd, y_rdd),
            columns=('X', 'y'),
            dtype=[np.ndarray, np.ndarray])

local_pipeline = Pipeline((
    ('vect', HashingVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', LinearSVC())
))
dist_pipeline = SparkPipeline((
    ('vect', SparkHashingVectorizer()),
    ('tfidf', SparkTfidfTransformer()),
    ('clf', SparkLinearSVC())
))

local_pipeline.fit(X, y)
dist_pipeline.fit(Z, clf__classes=np.unique(y))

y_pred_local = local_pipeline.predict(X)
y_pred_dist = dist_pipeline.predict(Z[:, 'X'])
from sklearn.metrics import confusion_matrix
confusion_matrix(y, y_pred_local)
confusion_matrix(y, y_pred_dist)
from sklearn.metrics import accuracy_score



##splearn- Distributed Model Selection

from splearn.rdd import DictRDD
from splearn.grid_search import SparkGridSearchCV
from splearn.naive_bayes import SparkMultinomialNB

from sklearn.grid_search import GridSearchCV
from sklearn.naive_bayes import MultinomialNB

from sklearn.feature_extraction.tests.test_text import ALL_FOOD_DOCS
X = list(ALL_FOOD_DOCS)# list of texts
y = np.random.randint(0,2,size=len(ALL_FOOD_DOCS)  # list of labels

X_rdd = sc.parallelize(X, 4)
y_rdd = sc.parallelize(y, 4)
Z = DictRDD((X_rdd, y_rdd),
            columns=('X', 'y'),
            dtype=[np.ndarray, np.ndarray])

parameters = {'alpha': [0.1, 1, 10]}
fit_params = {'classes': np.unique(y)}

local_estimator = MultinomialNB()
local_grid = GridSearchCV(estimator=local_estimator,
                          param_grid=parameters)

estimator = SparkMultinomialNB()
grid = SparkGridSearchCV(estimator=estimator,
                         param_grid=parameters,
                         fit_params=fit_params)

local_grid.fit(X, y)
grid.fit(Z)

##splearn- SparkPipline 

import numpy as np
from sklearn.base import BaseEstimator
from splearn.rdd import DictRDD
from splearn.linear_model.base import SparkLinearRegression
from splearn.pipeline import SparkPipeline

class Noiser(BaseEstimator):
    def __init__(self, random_seed=42):
        np.random.seed(random_seed)
    def fit(self, Z):
        return self
    def transform(self, Z):
        f = lambda X: X + np.random.rand(*X.shape)
        if isinstance(Z, DictRDD):
            return Z.transform(f, column='X')
        else:
            return Z.transform(f)

X = np.arange(100)[:, np.newaxis] #(100,) converted to (100,1)
y = np.arange(100)
X_rdd = sc.parallelize(X)
y_rdd = sc.parallelize(y)
rdd = X_rdd.zip(y_rdd)
Z = DictRDD(rdd, ('X', 'y'), 25)

pipe = SparkPipeline([('noise', Noiser()),
                      ('reg', SparkLinearRegression())])
>>> pipe.fit(Z)
SparkPipeline(steps=[
    ('noise', Noiser(random_seed=None)),
    ('reg', SparkLinearRegression(copy_X=True,
                                  fit_intercept=True,
                                  n_jobs=1,
                                  normalize=False)
    )])

>>> pipe.predict(Z[:, 'X']).collect()
[array([ 1.51878876, 2.50336579, 3.20260105, 4.41610508, 5.52531787]),
 array([ 5.56329829, 6.54787532, 7.24711057, 8.46061461, 9.5698274 ]),
 array([ 9.60780781, 10.59238484, 11.2916201, 12.50512413, 13.61433693]),
 array([ 13.65231734, 14.63689437, 15.33612963, 16.54963366, 17.65884645])]

 
###Scikit External projects - spark-sklearn  - Quick Intro 
#Spark-sklearn focuses on problems that have a small amount of data and that can be run in parallel.
#For large , use Scikit-learn

#has GridSearchCV, exact drop in replacement of sklearn.GridSearchCV

$ pip install spark-sklearn


spark_sklearn.GridSearchCV(sc, estimator, param_grid, scoring=None, fit_params=None,
                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,
                 pre_dispatch='2*n_jobs', error_score='raise', return_train_score=True
    Drop-in replacement for scikit GridSearchCV
    
    
   
    
##spark-sklearn - Example 
from sklearn import svm, model_selection, datasets
from spark_sklearn import GridSearchCV
iris = datasets.load_iris()
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svr = svm.SVC()
clf_spark = GridSearchCV(sc, svr, parameters)
clf_sk = model_selection.GridSearchCV(svr, parameters)
>>> clf_spark.fit(iris.data, iris.target)
[Stage 9:>                                                         (0 + 0) / 12]
[Stage 9:>                                                         (0 + 4) / 12]
[Stage 9:==============>                                           (3 + 4) / 12]
[Stage 9:===================>                                      (4 + 4) / 12]
[Stage 9:=============================>                            (6 + 4) / 12]
[Stage 9:=================================>                        (7 + 4) / 12]
[Stage 9:===========================================>              (9 + 3) / 12]
[Stage 9:===============================================>         (10 + 2) / 12]
[Stage 9:====================================================>    (11 + 1) / 12]

GridSearchCV(cv=None, error_score='raise',
       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False),
       fit_params={}, iid=True, n_jobs=1,
       param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')},
       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
       sc=<pyspark.context.SparkContext object at 0x000000AD92898A58>,
       scoring=None, verbose=0)
       
>>> clf_sk.fit(iris.data, iris.target)
GridSearchCV(cv=None, error_score='raise',
       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False),
       fit_params={}, iid=True, n_jobs=1,
       param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')},
       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
       scoring=None, verbose=0)
       
>>> clf_sk.score(iris.data, iris.target), clf_spark.score(iris.data, iris.target)
(0.9933333333333333, 0.9933333333333333)


##spark-sklearn : Reference 


class spark_sklearn.Converter(sc)
    Class for converting between scikit-learn and Spark ML models
    Parameters:
        sc – SparkContext 
    toPandas(df)
        Similar to the Spark DataFrame built-in toPandas() method, 
        but it handles MLlib Vector columns differently. 
        It converts MLlib Vectors into rows of scipy.sparse.csr_matrix,
        which is generally friendlier for PyData tools like scikit-learn.
        Parameters:
            df – Spark DataFrame 
        Returns:
            Pandas dataframe 
    toSKLearn(model)
        Convert a Spark MLlib model from the Pipelines API (spark.ml) 
        to a scikit-learn model. 
        Currently supported models: 
            - pyspark.ml.classification.LogisticRegressionModel 
            - pyspark.ml.regression.LinearRegressionModel
        Parameters:
            model – Spark ML model 
        Returns:
            scikit-learn model with equivalent predictive behavior.
            Currently, parameters or arguments for training are not copied. 
    toSpark(model)
        Convert a scikit-learn model to a Spark ML model from the Pipelines API (spark.ml). 
        Currently supported models: 
            - sklearn.linear_model.LogisticRegression (binary classification only, not multiclass) 
            - sklearn.linear_model.LinearRegression
        Parameters:
            model – scikit-learn model 
        Returns:
            Spark ML model with equivalent predictive behavior. 
            Currently, parameters or arguments for training are not copied. 
            
            

class spark_sklearn.GridSearchCV(sc, estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score='raise', return_train_score=True)
    Bases: sklearn.model_selection._search.BaseSearchCV
    Exhaustive search over specified parameter values for an estimator.
    estimator : estimator object.
        This is assumed to implement the scikit-learn estimator interface. 
        Either estimator needs to provide a score function, 
        or scoring must be passed.
    param_grid : dict or list of dictionaries
        Dictionary with parameters names (string) as keys 
        and lists of parameter settings to try as values, 
        or a list of such dictionaries, in which case the grids spanned 
        by each dictionary in the list are explored. 
        This enables searching over any sequence of parameter settings.
    scoring : string, callable or None, default=None
        A string  or a scorer callable object / function 
        with signature scorer(estimator, X, y). 
    fit_params : dict, optional
        Parameters to pass to the fit method.
    n_jobs : int, default=1
        Number of jobs to run in parallel.
    iid : boolean, default=True
        If True, the data is assumed to be identically distributed 
        across the folds, and the loss minimized is the total loss 
        per sample, and not the mean loss across the folds.
    cv : int, cross-validation generator or an iterable, optional
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are:
        •None, to use the default 3-fold cross validation,
        •integer, to specify the number of folds in a (Stratified)KFold,
        •An object to be used as a cross-validation generator.
        •An iterable yielding train, test splits.

#Example 
from sklearn import svm, datasets
from spark_sklearn.grid_search import GridSearchCV
from spark_sklearn.util import createLocalSparkSession
sc = createLocalSparkSession().sparkContext
iris = datasets.load_iris()
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svr = svm.SVC()
clf = GridSearchCV(sc, svr, parameters)
clf.fit(iris.data, iris.target)
>>> sorted(clf.cv_results_.keys())




spark_sklearn.gapply(grouped_data, func, schema, *cols)
    Given a spark dataframe grouped by key columns key1, key2,..., keyn, 
    invokes func((key1, key2, ..., keyn), values) 
    or func(key, values) if single key 
    Note values = pandas.DataFrame of columns selected by 'cols', 
    if cols is empty, By default all other columns 
    func returns a pandas.DataFrame of the specified 'schema' of type StructType 

#Example 
import pandas as pd
from pyspark.sql import SparkSession
from spark_sklearn.group_apply import gapply
from spark_sklearn.util import createLocalSparkSession
spark = createLocalSparkSession()
df = (spark
    .createDataFrame([Row(course="dotNET", year=2012, earnings=10000),
                      Row(course="Java",   year=2012, earnings=20000),
                      Row(course="dotNET", year=2012, earnings=5000),
                      Row(course="dotNET", year=2013, earnings=48000),
                      Row(course="Java",   year=2013, earnings=30000)])
    .select("course", "year", "earnings")) 
    
def yearlyMedian(_, vals):  #vals = pandas DF 
    all_years = set(vals['year'])
    # Note that interpolation is performed, so we need to cast back to int.
    yearly_median = [(year, int(vals['earnings'][vals['year'] == year].median()))
                     for year in all_years]
    return pd.DataFrame.from_records(yearly_median) #data : ndarray (structured dtype), list of tuples, dict, or DataFrame
    
    
newSchema = StructType().add("year", LongType()).add("median_earnings", LongType())
>>> gapply(df.groupBy("course"), yearlyMedian, newSchema).orderBy("median_earnings").show()
+------+----+---------------+
|course|year|median_earnings|
+------+----+---------------+
|dotNET|2012|           7500|
|  Java|2012|          20000|
|  Java|2013|          30000|
|dotNET|2013|          48000|
+------+----+---------------+


def twoKeyYearlyMedian(_, vals):
    return pd.DataFrame.from_records([(int(vals["earnings"].median()),)])

newSchema = StructType([df.schema["earnings"]])
>>> gapply(df.groupBy("course", "year"), twoKeyYearlyMedian, newSchema, "earnings").orderBy("earnings").show()
+------+----+--------+
|course|year|earnings|
+------+----+--------+
|dotNET|2012|    7500|
|  Java|2012|   20000|
|  Java|2013|   30000|
|dotNET|2013|   48000|
+------+----+--------+
>>> spark.stop(); SparkSession._instantiatedContext = None
 
@@@
##spark_sklearn: Keyed Models

class spark_sklearn.keyed_models.KeyedEstimator(sklearnEstimator=None, 
            keyCols=['key'], xCol='features', outputCol='output', yCol=None, estimatorType=None)¶
    Provides an interface for training per-key scikit-learn estimators
    Don’t use “estimator” as a column name.
    For every unique keyCols value, the remaining columns are aggregated and used to train the scikit-learn estimator.
    estimatorType inference is conducted as follows: 
    if yCol is specified, then this is assumed to be of "predictor" type, 
    else a "transformer" or a "clusterer", 
    depending on the estimator having the transform() or fit_predict() attributes, 
    with "clusterer" being chosen in case both attributes are present.
    sklearnEstimator – An instance of a scikit-learn estimator, 
            with parameters configured as desired for each user.
    •keyCols – Key column names list used to group data 
            to which models are applied, 
            where order implies lexicographical importance.
    •xCol – Name of column of input features 
            used for training and transformation/prediction.
    •yCol – Specifies name of label column for regression or classification pipelines. 
            Required for predictors, must be unspecified or None for transformers.
    #methods 
    fit(dataset, params=None)
        Fits a model to the input dataset with optional parameters.
        Returns KeyedModel
        
        
class spark_sklearn.keyed_models.KeyedModel(sklearnEstimator=None, keyCols=None, xCol=None, outputCol=None, yCol=None, estimatorType=None, keyedSklearnEstimators=None, outputType=None)
    Bases: pyspark.ml.base.Model
    Wraps fitted scikit-learn estimators 
    - at transformation time transforms the input for each key 
    using a key-specific model
    Attributes/Methods 
        keyedModels
            Returns a DataFrame with columns keyCols (where each key is unique) 
            and the column "estimator" containing the fitted scikit-learn estimator 
            (instance of SparkSklearnEstimator). 
        sklearnEstimatorType
            the estimator type of this keyed model 
        transform(dataset, params=None)
            Transforms the input dataset with optional parameters.
            Parameters:
                •dataset – input dataset, pyspark.sql.DataFrame
                •params – an optional param map that overrides embedded params.
            Returns:
                transformed dataset
 

class spark_sklearn.keyed_models.SparkSklearnEstimator(estimator)
    SparkSklearnEstimator is a wrapper for containing scikit-learn estimators in dataframes 
    Any method called on the estimator this object wraps 
    may be called on the wrapper instead
    Attributes 
        estimator
            The underlying estimator 
      
class spark_sklearn.CSRVectorUDT
    Bases: pyspark.sql.types.UserDefinedType
    SQL user-defined type (UDT) for scipy.sparse.csr_matrix (vectors only, not matrices).
        

##Example - key is user ID, 
#estimation per key(userID) , must be contained in one machine 

from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from pyspark.ml.linalg import Vectors, Matrices, MatrixUDT
from pyspark.sql.functions import udf
from pyspark.sql import SparkSession
from spark_sklearn.util import createLocalSparkSession
from spark_sklearn.keyed_models import KeyedEstimator
spark = createLocalSparkSession()
df = spark.createDataFrame([(user,
                             Vectors.dense([i, i ** 2, i ** 3]),
                             0.0 + user + i + 2 * i ** 2 + 3 * i ** 3)
                            for user in range(3) for i in range(5)])
df = df.toDF("key", "features", "y")
>>> df.where("5 < y and y < 10").sort("key", "y").show()
+---+-------------+---+
|key|     features|  y|
+---+-------------+---+
|  0|[1.0,1.0,1.0]|6.0|
|  1|[1.0,1.0,1.0]|7.0|
|  2|[1.0,1.0,1.0]|8.0|
+---+-------------+---+
>>> km = KeyedEstimator(sklearnEstimator=LinearRegression(), yCol="y").fit(df)
def printFloat(x):
    return "{:.2f}".format(round(x, 2))

def printModel(model):
    coef = "[" + ", ".join(map(printFloat, model.coef_)) + "]"
    intercept = printFloat(model.intercept_)
    return "intercept: {} coef: {}".format(intercept, coef)

>>> km.keyedModels.columns
['key', 'estimator']   #estimator is sklearn estimators(wrapped)
>>> mls = udf(printModel)
>>> km.keyedModels.select("key", mls("estimator").alias("linear fit") ).sort("key").show(truncate=False)
+---+----------------------------------------+
|key|linear fit                              |
+---+----------------------------------------+
|0  |intercept: 0.00 coef: [1.00, 2.00, 3.00]|
|1  |intercept: 1.00 coef: [1.00, 2.00, 3.00]|
|2  |intercept: 2.00 coef: [1.00, 2.00, 3.00]|
+---+----------------------------------------+


#Apply to test data 
input = spark.createDataFrame([(0, Vectors.dense(3, 1, -1))]).toDF("key", "features")
>>> km.transform(input).withColumn("output", udf(printFloat)("output")).show()
+---+--------------+------+
|key|      features|output|
+---+--------------+------+
|  0|[3.0,1.0,-1.0]|  2.00|
+---+--------------+------+

#to perform key-based clustering. 
#The most common use case would require just fitting our model.

km = KeyedEstimator(sklearnEstimator=KMeans(random_state=0, n_clusters=2)).fit(df)
def getCentroids(kmeans):
    n_clusters, n_features = kmeans.cluster_centers_.shape
    return Matrices.dense(n_clusters, n_features, kmeans.cluster_centers_.reshape(-1))

centroids = udf(getCentroids, MatrixUDT())("estimator").alias("centroids")
>>> km.keyedModels.select("key", centroids).sort("key").show()
+---+--------------------+
|key|           centroids|
+---+--------------------+
|  0|4.0   64.0  3.5  ...|
|  1|4.0   64.0  3.5  ...|
|  2|4.0   64.0  3.5  ...|
+---+--------------------+

#In the case of KMeans, we can also predict cluster labels, 
>>> km.transform(input).withColumnRenamed("output", "cluster label").show()
+---+--------------+-------------+
|key|      features|cluster label|
+---+--------------+-------------+
|  0|[3.0,1.0,-1.0]|            1|
+---+--------------+-------------+
>>> spark.stop(); SparkSession._instantiatedContext = None # clear hidden SparkContext for reuse







###Scikit External projects - Surprise  - scikit for recommender systems
#need numpy, and a C compiler
$ pip install scikit-surprise

#Recommender systems :seeks to predict the "rating" or "preference" that a user would give to an item
#generally data format is 
user id | item id | rating | timestamp.

#Recommender systems are often based on Collaborative Filtering (CF)
#which relies only on past user behavior—for example,
#their previous transactions or product ratings—and does not require the creation
#of explicit profiles. 
#Notably, CF techniques do not require domain knowledge
#and avoid the need for extensive data collection


#CF Approach-1: Neighborhood methods are centered on computing the relationships between
#items or alternatively, between users

#CF Approach-2: Latent factor models, such as singular value decomposition (SVD), comprise
#an alternative approach by transforming both items and users to the same
#latent factor space, thus making them directly comparable


#Typical CF data exhibit large user and item effects—systematic tendencies for
#some users to give higher ratings than others—and for some items to receive
#higher ratings than others. 
#It is customary to adjust the data by accounting for
#these effects, which we encapsulate within the baseline estimates

##scikit-surprise : Rawid and inner_id 
Raw ids are ids as defined in a rating file or in a pandas dataframe. 
    They can be strings or numbers. 
    If read from file, then they are  strings
    predict() uses raw ids 
On trainset creation, each raw id is mapped to a unique integer called inner id,
    Trainset has to/fro conversions 

##scikit-surprise : Dataset loading and dumping 
class surprise.reader.Reader(name=None, line_format=u'user item rating',
        sep=None, rating_scale=(1, 5), skip_lines=0)
    To read a file of lines with  line_format
    eg to read 
        user | item | rating | timestamp. 
    Reader(line_format='user item rating timestamp', sep='|')
    Parameters 
    name (string, optional) – 
        For builtin datasets , Accepted values are ‘ml-100k’, ‘ml-1m’, and ‘jester’. Default is None.


class surprise.dataset.Dataset(reader)
    Use class methods to create this file 
    folds()
        Generator function to iterate over the folds of the Dataset.
    #all these methods Return DatasetAutoFolds  
    classmethod load_builtin(name=u'ml-100k')
        Load a built-in dataset.
        Parameters:	
            name (string) – The name of the built-in dataset to load. 
            Accepted values are ‘ml-100k’, ‘ml-1m’, and ‘jester’. 
            Default is ‘ml-100k’.
    classmethod load_from_df(df, reader)
        Load a dataset from a pandas dataframe.
    classmethod load_from_file(file_path, reader)
        Load a dataset from a (custom) file.
    classmethod load_from_folds(folds_files, reader)
        Load a dataset where folds (for cross-validation) are predefined by some files.
        Where a dataset is already split into predefined folds, 
        such as the movielens-100k dataset which defines files u1.base, u1.test, u2.base, u2.test, etc…
        Parameters:
            folds_files (iterable of tuples) – The list of the folds. A fold is a tuple of the form (path_to_train_file, path_to_test_file).
  
  
  
class surprise.dataset.DatasetAutoFolds(ratings_file=None, reader=None, df=None)
    build_full_trainset()
        Do not split the dataset into folds 
        and just return a trainset as is, built from the whole dataset.
        Returns:	
           The Trainset.  
    split(n_folds=5, shuffle=True)
        Split the dataset into folds for future cross-validation.
  
  
class surprise.Trainset(ur, ir, n_users, n_items, n_ratings, rating_scale, offset, raw2inner_id_users, raw2inner_id_items)
    Create by DatasetAutoFolds.build_full_trainset() or Dataset.folds
    It is used by the fit() method of every prediction algorithm.
    #Attributes/Methods 
    ur
        defaultdict of list – The users ratings. 
        user inner ids : [(item_inner_id, rating),...]
    ir
        defaultdict of list – The items ratings. 
        item inner ids: [(user_inner_id, rating),...]
    n_users
        Total number of users 
    n_items
        Total number of items 
    n_ratings
        Total number of ratings 
    rating_scale
        tuple – The minimum and maximal rating of the rating scale.
    global_mean
        The mean of all ratings μ
    all_items()
        Generator function to iterate over all items.
        Yields:	Inner id of items.
    all_ratings()
        Generator function to iterate over all ratings.
        Yields:	A tuple (uid, iid, rating) where ids are inner ids 
    all_users()
        Generator function to iterate over all users.
        Yields:	Inner id of users.
    build_anti_testset(fill=None)
        Return a list of ratings that can be used as a testset in the test() method.
        The ratings are all the ratings that are not in the trainset, 
        i.e. all the ratings rui where the user u is known, 
        the item i is known, but the rating rui is not in the trainset. 
        Parameters:	
            fill (float) – The value to fill unknown ratings. 
                If None the global mean of all ratings global_mean will be used.
        Returns:	
            A list of tuples (uid, iid, fill) where ids are raw ids.
    build_testset()
        Return a list of ratings that can be used as a testset in the test() method.
        The ratings are all the ratings that are in the trainset, 
        i.e. all the ratings returned by the all_ratings() generator. 
        This is useful in cases where you want to to test 
        your algorithm on the trainset.
    global_mean
        Return the mean of all ratings.
        It’s only computed once.
    knows_item(iid)
        Indicate if the item is part of the trainset.
        An item is part of the trainset if the item was rated at least once.
        Parameters:	iid (int) – The (inner) item id.
        Returns:	True if item is part of the trainset, else False.
    knows_user(uid)
        Indicate if the user is part of the trainset.
        A user is part of the trainset if the user has at least one rating.
        Parameters:	uid (int) – The (inner) user id. See this note.
        Returns:	True if user is part of the trainset, else False.
    to_inner_iid(riid)
        Convert an item raw id to an inner id.
        Parameters:	riid (str) – The item raw id.
        Returns:	The item inner id.
        Return type:	int
        Raises:	ValueError – When item is not part of the trainset.
    to_inner_uid(ruid)
        Convert a user raw id to an inner id.
        Parameters:	ruid (str) – The user raw id.
        Returns:	The user inner id.
        Return type:	int
    to_raw_iid(iiid)
        Convert an item inner id to a raw id.
        Parameters:	iiid (int) – The item inner id.
        Returns:	The item raw id.
        Return type:	str
    to_raw_uid(iuid)
        Convert a user inner id to a raw id.
        Parameters:	iuid (int) – The user inner id.
        Returns:	The user raw id.
        Return type:	str

    
surprise.dump.dump(file_name, predictions=None, algo=None, verbose=0)
    To serialize a list of prediction and/or an algorithm on drive.
    What is dumped is a dictionary with keys 'predictions' and 'algo'.
    Parameters:	
        file_name (str) – The name (with full path) specifying where to dump the predictions.
        predictions (list of Prediction) – The predictions to dump.
        algo (Algorithm, optional) – The algorithm to dump.
        verbose (int) – Level of verbosity. If 1, then a message indicates that the dumping went successfully. Default is 0.

surprise.dump.load(file_name)
    To deserialize a list of prediction and/or an algorithm that were dumped on drive using dump().
    Parameters:	file_name (str) – The path of the file from which the algorithm is to be loaded
    Returns:	A tuple (predictions, algo) 
        where predictions is a list of Prediction objects 
        and algo is an Algorithm object. Depending on what was dumped, some of these may be None.

    
##scikit-surprise : prediction_algorithms package - surprise.prediction_algorithms.

class surprise.prediction_algorithms.algo_base.AlgoBase(**kwargs)
    Abstract class for all Prediction algorithm classes 
    Keyword Arguments:
     	baseline_options (dict, optional) 
            If the algorithm needs to compute a baseline estimate, 
            the baseline_options parameter is used to configure 
            how they are computed. 
    compute_baselines()
        Compute users and items baselines.
        For algorithms using Pearson baseline similarty or the BaselineOnly algorithm.
        Returns:	A tuple (bu, bi), 
            which are users and items baselines.
    compute_similarities()
        Build the similarity matrix.
        For algorithms using a similarity measure, such as the k-NN algorithms.
        Returns:	The similarity matrix.
    fit(trainset)
        Train an algorithm on a given training set.
        Parameters:	trainset (Trainset) – A training set, as returned by the folds method.
        Returns:	self
    get_neighbors(iid, k)
        Return the k nearest neighbors of iid, 
        which is the inner id of a user or an item, 
        depending on the user_based field of sim_options 
        Returns:	
            The list of the k (inner) ids of the closest users (or items) to iid.
    predict(uid, iid, r_ui=None, clip=True, verbose=False)
        Compute the rating prediction for given user and item.
        Parameters:	
            uid – (Raw) id of the user. See this note.
            iid – (Raw) id of the item. See this note.
            r_ui (float) – The true rating rui,Optional, default is None.
            clip (bool) – Whether to clip the estimation into the rating scale
            Returns:	Prediction instance 
    test(testset, verbose=False)
        Test the algorithm on given testset, 
        i.e. estimate all the ratings in the given testset.
        Parameters:	
            testset – A test set, as returned by a cross-validation itertor or by the build_testset() method.
            verbose (bool) – Whether to print details for each predictions. Default is False.
        Returns:	
            A list of Prediction objects
            
            
    
class surprise.prediction_algorithms.predictions.Prediction
    A named tuple for storing the results of a prediction.
    Parameters:	
        uid – The (raw) user id. 
        iid – The (raw) item id. 
        r_ui (float) – The true rating rui
        est (float) – The estimated rating r^ui.
        details (dict) – Stores additional details about the prediction that might be useful for later analysis.
  
exception surprise.prediction_algorithms.predictions.PredictionImpossible
    Exception raised when a prediction is impossible.
    When raised, the estimation r^ui is set to the global mean of all ratings μ.


#The available prediction algorithms are:
random_pred.NormalPredictor() 	
    Algorithm predicting a random rating 
    based on the distribution of the training set, which is assumed to be normal.
baseline_only.BaselineOnly(bsl_options={}) 	
    Algorithm predicting the baseline estimate for given user and item.
    
#knn based - check parameters http://surprise.readthedocs.io/en/stable/knn_inspired.html
knns.KNNBasic(k=40, min_k=1, sim_options={}, **kwargs) 	
    A basic collaborative filtering algorithm.
knns.KNNWithMeans(k=40, min_k=1, sim_options={}, **kwargs))
    A basic collaborative filtering algorithm, 
    taking into account the mean ratings of each user.
knns.KNNWithZScore(k=40, min_k=1, sim_options={}, **kwargs)
    A basic collaborative filtering algorithm, 
    taking into account the z-score normalization of each user.
knns.KNNBaseline(k=40, min_k=1, sim_options={}, bsl_options={}) 	
    A basic collaborative filtering algorithm taking into account a baseline rating.
    Parameters:
        k (int) – The (max) number of neighbors to take into account for aggregation Default is 40.
        min_k (int) – The minimum number of neighbors to take into account for aggregation. 
            If there are not enough neighbors, the neighbor aggregation is set to zero (so the prediction ends up being equivalent to the baseline). Default is 1.
        sim_options (dict) – A dictionary of options for the similarity measure
        
#matrix facorization based, check params - http://surprise.readthedocs.io/en/stable/matrix_factorization.html
matrix_factorization.SVD(n_factors=100, n_epochs=20, biased=True, init_mean=0,
                 init_std_dev=.1, lr_all=.005,
                 reg_all=.02, lr_bu=None, lr_bi=None, lr_pu=None, lr_qi=None,
                 reg_bu=None, reg_bi=None, reg_pu=None, reg_qi=None,
                 random_state=None, verbose=False) 	
    The famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize.
matrix_factorization.SVDpp(n_factors=20, n_epochs=20, init_mean=0, init_std_dev=.1,
                 lr_all=.007, reg_all=.02, lr_bu=None, lr_bi=None, lr_pu=None,
                 lr_qi=None, lr_yj=None, reg_bu=None, reg_bi=None, reg_pu=None,
                 reg_qi=None, reg_yj=None, random_state=None, verbose=False) 	
    The SVD++ algorithm, an extension of SVD taking into account implicit ratings.
matrix_factorization.NMF(n_factors=15, n_epochs=50, biased=False, reg_pu=.06,
                 reg_qi=.06, reg_bu=.02, reg_bi=.02, lr_bu=.005, lr_bi=.005,
                 init_low=0, init_high=1, random_state=None, verbose=False) 	
    A collaborative filtering algorithm based on Non-negative Matrix Factorization.

#Others 
slope_one.SlopeOne() 	
    A simple yet accurate collaborative filtering algorithm.
co_clustering.CoClustering(n_cltr_u=3, n_cltr_i=3, n_epochs=20, random_state=None,
                 verbose=False) 	
    A collaborative filtering algorithm based on co-clustering.
    Check params http://surprise.readthedocs.io/en/stable/co_clustering.html
    
    
    
##scikit-surprise : The model_selection package

surprise.model_selection.split.train_test_split(data, test_size=0.2, 
            train_size=None, random_state=None, shuffle=True)
    Split a dataset into trainset and testset.
    
surprise.model_selection.validation.cross_validate(algo, data, 
            measures=[u'rmse', u'mae'], cv=None, return_train_measures=False, n_jobs=-1, pre_dispatch=u'2*n_jobs', verbose=False)
    Run a cross validation procedure for a given algorithm, 
    reporting accuracy measures and computation times.
    Parameters:	
        algo (AlgoBase) – The algorithm to evaluate.
        data (Dataset) – The dataset on which to evaluate the algorithm.
        measures (list of string) – The performance measures to compute. 
                Allowed names are function names as defined in the accuracy module. Default is ['rmse', 'mae'].
        cv (cross-validation iterator, int or None) – Determines how the data parameter will be split 
                (i.e. how trainsets and testsets will be defined). If an int is passed, KFold is used with the appropriate n_splits parameter. If None, KFold is used with n_splits=5.
    Returns:	
        A dict with the following keys:
                'test_*' where * corresponds to a lower-case accuracy measure, 
                        e.g. 'test_rmse': numpy array with accuracy values for each testset.
                'train_*' where * corresponds to a lower-case accuracy measure, 
                        e.g. 'train_rmse': numpy array with accuracy values for each trainset. Only available if return_train_measures is True.
                'fit_time': numpy array with the training time in seconds for each split.
                'test_time': numpy array with the testing time in seconds for each split.
 
 
class surprise.model_selection.search.GridSearchCV(algo_class, param_grid, measures=[u'rmse', u'mae'], cv=None, refit=False, return_train_measures=False, n_jobs=-1, pre_dispatch=u'2*n_jobs', joblib_verbose=0)
    The GridSearchCV class computes accuracy metrics for an algorithm
    on various combinations of parameters, over a cross-validation procedure. 
    This is useful for finiding the best set of parameters for a prediction algorithm. 
    It is analogous to GridSearchCV from scikit-learn.
    Parameters:	
        algo_class (AlgoBase) – The class of the algorithm to evaluate.
        param_grid (dict) – Dictionary with algorithm parameters as keys and list of values as keys. All combinations will be evaluated with desired algorithm. Dict parameters such as sim_options require special treatment, see this note.
        measures (list of string) – The performance measures to compute. Allowed names are function names as defined in the accuracy module. Default is ['rmse', 'mae'].
        cv (cross-validation iterator, int or None) – Determines how the data parameter will be split (i.e. how trainsets and testsets will be defined). If an int is passed, KFold is used with the appropriate n_splits parameter. If None, KFold is used with n_splits=5.
    Attributes:
        best_estimator
            dict of AlgoBase – Using an accuracy measure as key, get the algorithm that gave the best accuracy results for the chosen measure, averaged over all splits.
        best_score
            dict of floats – Using an accuracy measure as key, get the best average score achieved for that measure.
        best_params
            dict of dicts – Using an accuracy measure as key, get the parameters combination that gave the best accuracy results for the chosen measure (on average).
        best_index
            dict of ints – Using an accuracy measure as key, get the index that can be used with cv_results that achieved the highest accuracy for that measure (on average).
        cv_results
            dict of arrays – A dict that contains accuracy measures over all splits, as well as train and test time for each parameter combination. Can be imported into a pandas DataFrame (see example).
    Methods 
        fit(data)
            Runs the fit() method of the algorithm for all parameter combination, over different splits given by the cv parameter.
            Parameters:	data (Dataset) – The dataset on which to evaluate the algorithm, in parallel.
        predict(*args)
            Call predict() on the estimator with the best found parameters (according the the refit parameter). See AlgoBase.predict().
            Only available if refit is not False.
        test(testset, verbose=False)
            Call test() on the estimator with the best found parameters (according the the refit parameter). See AlgoBase.test().
            Only available if refit is not False.
    
#Below CV classes have split method 
split(data)
    Generator function to iterate over trainsets and testsets.    
    Parameters:	data (Dataset) – The data containing ratings that will be devided into trainsets and testsets.
    Yields:	tuple of (trainset, testset)
 
class surprise.model_selection.split.KFold(n_splits=5, random_state=None, shuffle=True)
    A basic cross-validation iterator.
class surprise.model_selection.split.LeaveOneOut(n_splits=5, random_state=None)
    Cross-validation iterator where each user has exactly one rating in the testset.
class surprise.model_selection.split.PredefinedKFold
    A cross-validation iterator to when a dataset has been loaded with the load_from_folds method.
class surprise.model_selection.split.RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)
    Repeated KFold cross validator.
class surprise.model_selection.split.ShuffleSplit(n_splits=5, test_size=0.2, train_size=None, random_state=None, shuffle=True)
    A basic cross-validation iterator with random trainsets and testsets.
    
##scikit-surprise : similarities module- surprise.similarities.
cosine() 	        Compute the cosine similarity between all pairs of users (or items).
msd() 	            Compute the Mean Squared Difference similarity between all pairs of users (or items).
pearson() 	        Compute the Pearson correlation coefficient between all pairs of users (or items).
pearson_baseline() 	Compute the (shrunk) Pearson correlation coefficient between all pairs of users (or items) using baselines for centering instead of means.


##scikit-surprise : accuracy module
surprise.accuracy.fcp(predictions, verbose=True)
    Compute FCP (Fraction of Concordant Pairs).
surprise.accuracy.mae(predictions, verbose=True)
    Compute MAE (Mean Absolute Error).
surprise.accuracy.rmse(predictions, verbose=True)
    Compute RMSE (Root Mean Squared Error).   

    
    
##scikit-surprise : Automatic cross-validation
from surprise import SVD
from surprise import Dataset
from surprise.model_selection import cross_validate

# Load the movielens-100k dataset (download it if needed).
data = Dataset.load_builtin('ml-100k') #surprise.dataset.DatasetAutoFolds

# Use the famous SVD algorithm.
algo = SVD()

# Run 5-fold cross-validation and print results.
>>> cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
Evaluating RMSE, MAE of algorithm SVD on 5 split(s).

                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std
RMSE (testset)    0.9357  0.9382  0.9362  0.9326  0.9340  0.9353  0.0019
MAE (testset)     0.7377  0.7375  0.7404  0.7329  0.7348  0.7367  0.0026
Fit time          20.25   20.19   19.98   19.58   9.83    17.97   4.08
Test time         2.73    2.23    1.83    1.55    0.27    1.72    0.83
{'test_mae': array([0.73774188, 0.73752532, 0.74040574, 0.73288515, 0.73475746])
, 'test_time': (2.7344119548797607, 2.234405755996704, 1.8281505107879639, 1.546
8945503234863, 0.26563143730163574), 'fit_time': (20.250277996063232, 20.1877806
18667603, 19.984647750854492, 19.57839798927307, 9.828259944915771), 'test_rmse'
: array([0.93572685, 0.93816543, 0.93623375, 0.93258228, 0.9339612 ])}


##scikit-surprise : Train-test split and the fit() method

from surprise import SVD
from surprise import Dataset
from surprise import accuracy
from surprise.model_selection import train_test_split
# Load the movielens-100k dataset (download it if needed),
data = Dataset.load_builtin('ml-100k')
# sample random trainset and testset
# test set is made of 25% of the ratings.
trainset, testset = train_test_split(data, test_size=.25)
# We'll use the famous SVD algorithm.
algo = SVD()
# Train the algorithm on the trainset, and predict ratings for the testset
algo.fit(trainset)
predictions = algo.test(testset)
# Then compute RMSE
>>> accuracy.rmse(predictions)
RMSE: 0.9411

##scikit-surprise : Train on a whole trainset and the predict() method
from surprise import KNNBasic
from surprise import Dataset
# Load the movielens-100k dataset
data = Dataset.load_builtin('ml-100k')
# Retrieve the trainset.
trainset = data.build_full_trainset()
# Build an algorithm, and train it.
algo = KNNBasic()
algo.fit(trainset)

#predict takes raw id which is string for file based loading 
uid = str(196)  # raw user id (as in the ratings file). They are **strings**!
iid = str(302)  # raw item id (as in the ratings file). They are **strings**!
# get a prediction for specific users and items.
pred = algo.predict(uid, iid, r_ui=4, verbose=True)
>>> pred 
user: 196        item: 302        r_ui = 4.00   est = 4.06   {'actual_k': 40, 'was_impossible': False}


##scikit-surprise : Use a custom dataset- load from file 
from surprise import BaselineOnly
from surprise import Dataset
from surprise import Reader
from surprise.model_selection import cross_validate
# path to dataset file
file_path = os.path.expanduser('~/.surprise_data/ml-100k/ml-100k/u.data')
# As we're loading a custom dataset, we need to define a reader. In the
# movielens-100k dataset, each line has the following format:
# 'user item rating timestamp', separated by '\t' characters.
reader = Reader(line_format='user item rating timestamp', sep='\t')
data = Dataset.load_from_file(file_path, reader=reader)
# We can now use this dataset as we please, e.g. calling cross_validate
cross_validate(BaselineOnly(), data, verbose=True)
    
    
##scikit-surprise : Use a custom dataset- To load from a pandas dataframe, 

import pandas as pd
from surprise import NormalPredictor
from surprise import Dataset
from surprise import Reader
from surprise.model_selection import cross_validate
# Creation of the dataframe. Column names are irrelevant.
ratings_dict = {'itemID': [1, 1, 1, 2, 2],
                'userID': [9, 32, 2, 45, 'user_foo'],
                'rating': [3, 2, 4, 3, 1]}
df = pd.DataFrame(ratings_dict)
# A reader is still needed but only the rating_scale param is requiered.
reader = Reader(rating_scale=(1, 5))
# The columns must correspond to user id, item id and ratings (in that order).
data = Dataset.load_from_df(df[['userID', 'itemID', 'rating']], reader)
# We can now use this dataset as we please, e.g. calling cross_validate
cross_validate(NormalPredictor(), data, cv=2)

##scikit-surprise : Use cross-validation iterators

from surprise import SVD
from surprise import Dataset
from surprise import accuracy
from surprise.model_selection import KFold
# Load the movielens-100k dataset
data = Dataset.load_builtin('ml-100k')
# define a cross-validation iterator
kf = KFold(n_splits=3)
algo = SVD()
for trainset, testset in kf.split(data):
    # train and test algorithm.
    algo.fit(trainset)
    predictions = algo.test(testset)
    # Compute and print Root Mean Squared Error
    accuracy.rmse(predictions, verbose=True)
#Result could be, e.g.:
RMSE: 0.9374
RMSE: 0.9476
RMSE: 0.9478

##scikit-surprise : when the folds are already predefined by some files

from surprise import SVD
from surprise import Dataset
from surprise import Reader
from surprise import accuracy
from surprise.model_selection import PredefinedKFold
# path to dataset folder
files_dir = os.path.expanduser('~/.surprise_data/ml-100k/ml-100k/')
# This time, we'll use the built-in reader.
reader = Reader('ml-100k')
# folds_files is a list of tuples containing file paths:
# [(u1.base, u1.test), (u2.base, u2.test), ... (u5.base, u5.test)]
train_file = files_dir + 'u%d.base'
test_file = files_dir + 'u%d.test'
folds_files = [(train_file % i, test_file % i) for i in (1, 2, 3, 4, 5)]
data = Dataset.load_from_folds(folds_files, reader=reader)
pkf = PredefinedKFold()
algo = SVD()
for trainset, testset in pkf.split(data):
    # train and test algorithm.
    algo.fit(trainset)
    predictions = algo.test(testset)
    # Compute and print Root Mean Squared Error
    accuracy.rmse(predictions, verbose=True)

    
##scikit-surprise : Tune algorithm parameters with GridSearchCV
from surprise import SVD
from surprise import Dataset
from surprise.model_selection import GridSearchCV
# Use movielens-100K
data = Dataset.load_builtin('ml-100k')
param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],
              'reg_all': [0.4, 0.6]}
gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)
gs.fit(data)
# best RMSE score
print(gs.best_score['rmse'])
# combination of parameters that gave the best RMSE score
print(gs.best_params['rmse'])
#Result:
0.961300130118
{'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}

# We can now use the algorithm that yields the best rmse:
algo = gs.best_estimator['rmse']
algo.fit(data.build_full_trainset())
results_df = pd.DataFrame.from_dict(gs.cv_results)

#Dictionary parameters - bsl_options and sim_options 
#bsl_options is for Algo paramters 
#sim_options is for similarity paramters 
param_grid = {'k': [10, 20],
              'sim_options': {'name': ['msd', 'cosine'],
                              'min_support': [1, 5],
                              'user_based': [False]}
              }
#for example for the KNNBaseline algorithm:
param_grid = {'bsl_options': {'method': ['als', 'sgd'],
                              'reg': [1, 2]},
              'k': [2, 3],
              'sim_options': {'name': ['msd', 'cosine'],
                              'min_support': [1, 5],
                              'user_based': [False]}
              }


##scikit-surprise : Using prediction algorithms
from surprise import KNNBasic
algo = KNNBasic()

##Using prediction algorithms: Baselines estimates configuration
#For algorithms using baselines in another objective function (e.g. the SVD algorithm)
#Baselines can be estimated in two different ways:
    Using Stochastic Gradient Descent (SGD).
    Using Alternating Least Squares (ALS).

bsl_options (dict)
    Options for baseline estimates 
    parameter passed at the creation of an algorithm
        'method' : 'als' (default) and 'sgd'. 
    For ALS:
        'reg_i': The regularization parameter for items. Corresponding to λ2, Default is 10.
        'reg_u': The regularization parameter for users. Corresponding to λ3,Default is 15.
        'n_epochs': The number of iteration of the ALS procedure. Default is 10. Note that in [Kor10], what is described is a single iteration ALS process.
    For SGD:
        'reg': The regularization parameter of the cost function that is optimized, corresponding to λ1 and then λ5, Default is 0.02.
        'learning_rate': The learning rate of SGD, corresponding to γ  ,Default is 0.005.
        'n_epochs': The number of iteration of the SGD procedure. Default is 20.
    ForALS and SGD 
        user and item biases (bu and bi) are initialized to zero.

#Example 
print('Using ALS')
bsl_options = {'method': 'als',
               'n_epochs': 5,
               'reg_u': 12,
               'reg_i': 5
               }
algo = BaselineOnly(bsl_options=bsl_options)

print('Using SGD')
bsl_options = {'method': 'sgd',
               'learning_rate': .00005,
               }
algo = BaselineOnly(bsl_options=bsl_options)

#Note that some similarity measures may use baselines, 
#such as the pearson_baseline similarity. 
bsl_options = {'method': 'als',
               'n_epochs': 20,
               }
sim_options = {'name': 'pearson_baseline'}
algo = KNNBasic(bsl_options=bsl_options, sim_options=sim_options)

##Using prediction algorithms: Similarity measure configuration
#Many algorithms use a similarity measure to estimate a rating. 

sim_options(dict): argument at the creation of an algorithm. 
    'name': The name of the similarity to use, as defined in the similarities module. Default is 'MSD'.
    'user_based': Whether similarities will be computed between users or between items. 
                  This has a huge impact on the performance of a prediction algorithm. Default is True.
    'min_support': The minimum number of common items (when 'user_based' is 'True') 
                   or minimum number of common users (when 'user_based' is 'False') for the similarity not to be zero. 
    'shrinkage': Shrinkage parameter to apply (only relevant for pearson_baseline similarity). Default is 100.

#Example 
sim_options = {'name': 'cosine',
               'user_based': False  # compute  similarities between items
               }
algo = KNNBasic(sim_options=sim_options)

sim_options = {'name': 'pearson_baseline',
               'shrinkage': 0  # no shrinkage
               }
algo = KNNBasic(sim_options=sim_options)


##scikit-surprise : How to get the top-N recommendations for each user

from collections import defaultdict
from surprise import SVD
from surprise import Dataset
def get_top_n(predictions, n=10):
    '''Return the top-N recommendation for each user from a set of predictions.
    Args:
        predictions(list of Prediction objects): The list of predictions, as
            returned by the test method of an algorithm.
        n(int): The number of recommendation to output for each user. Default
            is 10.
    Returns:
    A dict where keys are user (raw) ids and values are lists of tuples:
        [(raw item id, rating estimation), ...] of size n.
    '''
    # First map the predictions to each user.
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))
    # Then sort the predictions for each user and retrieve the k highest ones.
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]
    return top_n
# First train an SVD algorithm on the movielens dataset.
data = Dataset.load_builtin('ml-100k')
trainset = data.build_full_trainset()
algo = SVD()
algo.fit(trainset)
# Than predict ratings for all pairs (u, i) that are NOT in the training set.
testset = trainset.build_anti_testset()
predictions = algo.test(testset)
top_n = get_top_n(predictions, n=10)
# Print the recommended items for each user
for uid, user_ratings in top_n.items():
    print(uid, [iid for (iid, _) in user_ratings])
    
    
    
##scikit-surprise : How to compute precision@k and recall@k
#Precision@k=|{Recommended items that are relevant}|/|{Recommended items}|
#Recall@k=|{Recommended items that are relevant}|/|{Relevant items}|

from collections import defaultdict
from surprise import Dataset
from surprise import SVD
from surprise.model_selection import KFold
def precision_recall_at_k(predictions, k=10, threshold=3.5):
    '''Return precision and recall at k metrics for each user.'''
    # First map the predictions to each user.
    user_est_true = defaultdict(list)
    for uid, _, true_r, est, _ in predictions:
        user_est_true[uid].append((est, true_r))
    precisions = dict()
    recalls = dict()
    for uid, user_ratings in user_est_true.items():
        # Sort user ratings by estimated value
        user_ratings.sort(key=lambda x: x[0], reverse=True)
        # Number of relevant items
        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)
        # Number of recommended items in top k
        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])
        # Number of relevant and recommended items in top k
        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))
                              for (est, true_r) in user_ratings[:k])
        # Precision@K: Proportion of recommended items that are relevant
        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1
        # Recall@K: Proportion of relevant items that are recommended
        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1
    return precisions, recalls
data = Dataset.load_builtin('ml-100k')
kf = KFold(n_splits=5)
algo = SVD()
for trainset, testset in kf.split(data):
    algo.fit(trainset)
    predictions = algo.test(testset)
    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)
    # Precision and recall can then be averaged over all users
    print(sum(prec for prec in precisions.values()) / len(precisions))
    print(sum(rec for rec in recalls.values()) / len(recalls))
    
    
##scikit-surprise : How to get the k nearest neighbors of a user (or item)

import io  # needed because of weird encoding of u.item file
from surprise import KNNBaseline
from surprise import Dataset
from surprise import get_dataset_dir
def read_item_names():
    """Read the u.item file from MovieLens 100-k dataset and return two
    mappings to convert raw ids into movie names and movie names into raw ids.
    """
    file_name = get_dataset_dir() + '/ml-100k/ml-100k/u.item'
    rid_to_name = {}
    name_to_rid = {}
    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:
        for line in f:
            line = line.split('|')
            rid_to_name[line[0]] = line[1]
            name_to_rid[line[1]] = line[0]
    return rid_to_name, name_to_rid
# First, train the algortihm to compute the similarities between items
data = Dataset.load_builtin('ml-100k')
trainset = data.build_full_trainset()
sim_options = {'name': 'pearson_baseline', 'user_based': False}
algo = KNNBaseline(sim_options=sim_options)
algo.fit(trainset)
# Read the mappings raw id <-> movie name
rid_to_name, name_to_rid = read_item_names()
# Retrieve inner id of the movie Toy Story
toy_story_raw_id = name_to_rid['Toy Story (1995)']
toy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)
# Retrieve inner ids of the nearest neighbors of Toy Story.
toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)
# Convert inner ids of the neighbors into names.
toy_story_neighbors = (algo.trainset.to_raw_iid(inner_id)
                       for inner_id in toy_story_neighbors)
toy_story_neighbors = (rid_to_name[rid]
                       for rid in toy_story_neighbors)
print()
print('The 10 nearest neighbors of Toy Story are:')
for movie in toy_story_neighbors:
    print(movie)
    

##scikit-surprise : How to serialize an algorithm

import os
from surprise import SVD
from surprise import Dataset
from surprise import dump
data = Dataset.load_builtin('ml-100k')
trainset = data.build_full_trainset()
algo = SVD()
algo.fit(trainset)
# Compute predictions of the 'original' algorithm.
predictions = algo.test(trainset.build_testset())
# Dump algorithm and reload it.
file_name = os.path.expanduser('~/dump_file')
dump.dump(file_name, algo=algo)
_, loaded_algo = dump.load(file_name)
# We now ensure that the algo is still the same by checking the predictions.
predictions_loaded_algo = loaded_algo.test(trainset.build_testset())
assert predictions == predictions_loaded_algo
print('Predictions are the same')



##scikit-surprise : Analysis of the KNNBasic algorithm
                                  
import pickle
import os
import pandas as pd
from surprise import KNNBasic
from surprise import Dataset                                                     
from surprise import Reader                                                      
from surprise import dump
from surprise.accuracy import rmse
# We will train and test on the u1.base and u1.test files of the movielens-100k dataset.
# if you haven't already, you need to download the movielens-100k dataset
# You can do it manually, or by running:
#Dataset.load_builtin('ml-100k')
# Now, let's load the dataset
train_file = os.path.expanduser('~') + '/.surprise_data/ml-100k/ml-100k/u1.base'
test_file = os.path.expanduser('~') + '/.surprise_data/ml-100k/ml-100k/u1.test'
data = Dataset.load_from_folds([(train_file, test_file)], Reader('ml-100k'))
                
# We'll use a basic nearest neighbor approach, where similarities are computed
# between users.
algo = KNNBasic()                                                       
for trainset, testset in data.folds(): 
    algo.train(trainset)                             
    predictions = algo.test(testset)
    rmse(predictions)                                                                               
    dump.dump('./dump_file', predictions, algo)
    

# The dump has been saved and we can now use it whenever we want.
# Let's load it and see what we can do
predictions, algo = dump.load('./dump_file')
trainset = algo.trainset
print('algo: {0}, k = {1}, min_k = {2}'.format(algo.__class__.__name__, algo.k, algo.min_k))

# Let's build a pandas dataframe with all the predictions
def get_Iu(uid):
    """Return the number of items rated by given user
    
    Args:
        uid: The raw id of the user.
    Returns:
        The number of items rated by the user.
    """
    
    try:
        return len(trainset.ur[trainset.to_inner_uid(uid)])
    except ValueError:  # user was not part of the trainset
        return 0
    
def get_Ui(iid):
    """Return the number of users that have rated given item
    
    Args:
        iid: The raw id of the item.
    Returns:
        The number of users that have rated the item.
    """
    
    try:
        return len(trainset.ir[trainset.to_inner_iid(iid)])
    except ValueError:  # item was not part of the trainset
        return 0
df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])    
df['Iu'] = df.uid.apply(get_Iu)
df['Ui'] = df.iid.apply(get_Ui)
df['err'] = abs(df.est - df.rui)
df.head()
	uid 	iid 	rui 	est 	details 	Iu 	Ui 	err
0 	1 	6 	5.0 	3.468613 	{'actual_k': 20, 'was_impossible': False} 	135 	20 	1.531387
1 	1 	10 	3.0 	3.866290 	{'actual_k': 40, 'was_impossible': False} 	135 	73 	0.866290
2 	1 	12 	5.0 	4.538194 	{'actual_k': 40, 'was_impossible': False} 	135 	211 	0.461806
3 	1 	14 	5.0 	4.235741 	{'actual_k': 40, 'was_impossible': False} 	135 	140 	0.764259
4 	1 	17 	3.0 	3.228002 	{'actual_k': 40, 'was_impossible': False} 	135 	72 	0.228002
best_predictions = df.sort_values(by='err')[:10]
worst_predictions = df.sort_values(by='err')[-10:]
# Let's take a look at the best predictions of the algorithm
>>> best_predictions
	uid 	iid 	rui 	est 	details 	Iu 	Ui 	err
272 	5 	439 	1.0 	1.0 	{'actual_k': 3, 'was_impossible': False} 	91 	3 	0.0
886 	13 	314 	1.0 	1.0 	{'actual_k': 2, 'was_impossible': False} 	373 	2 	0.0
156 	2 	314 	1.0 	1.0 	{'actual_k': 2, 'was_impossible': False} 	40 	2 	0.0
926 	13 	437 	1.0 	1.0 	{'actual_k': 3, 'was_impossible': False} 	373 	3 	0.0
9276 	206 	314 	1.0 	1.0 	{'actual_k': 1, 'was_impossible': False} 	33 	2 	0.0
19118 	405 	437 	1.0 	1.0 	{'actual_k': 3, 'was_impossible': False} 	582 	3 	0.0
8032 	181 	1334 	1.0 	1.0 	{'actual_k': 1, 'was_impossible': False} 	218 	1 	0.0
8041 	181 	1354 	1.0 	1.0 	{'actual_k': 1, 'was_impossible': False} 	218 	1 	0.0
9202 	201 	1424 	3.0 	3.0 	{'actual_k': 1, 'was_impossible': False} 	215 	1 	0.0
3018 	60 	1123 	4.0 	4.0 	{'actual_k': 1, 'was_impossible': False} 	119 	1 	0.0
#Try running the same algorithm with a value of min_k equal to 10
#This means that if there are less than 10 neighbors, the prediction is set to the mean of all ratings. 
#You'll see your accuracy decrease!


# Now, let's look at the prediction with the biggest error
>>> worst_predictions
	uid 	iid 	rui 	est 	details 	Iu 	Ui 	err
9406 	208 	302 	1.0 	4.308447 	{'actual_k': 40, 'was_impossible': False} 	11 	245 	3.308447
19089 	405 	169 	1.0 	4.364728 	{'actual_k': 40, 'was_impossible': False} 	582 	97 	3.364728
19785 	436 	132 	1.0 	4.365369 	{'actual_k': 40, 'was_impossible': False} 	126 	200 	3.365369
157 	2 	315 	1.0 	4.381308 	{'actual_k': 40, 'was_impossible': False} 	40 	136 	3.381308
8503 	193 	56 	1.0 	4.386478 	{'actual_k': 40, 'was_impossible': False} 	61 	312 	3.386478
5531 	113 	976 	5.0 	1.610771 	{'actual_k': 7, 'was_impossible': False} 	31 	7 	3.389229
7917 	181 	408 	1.0 	4.421499 	{'actual_k': 40, 'was_impossible': False} 	218 	93 	3.421499
7390 	167 	169 	1.0 	4.664991 	{'actual_k': 40, 'was_impossible': False} 	38 	97 	3.664991
7412 	167 	1306 	5.0 	1.000000 	{'actual_k': 1, 'was_impossible': False} 	38 	1 	4.000000
5553 	114 	1104 	5.0 	1.000000 	{'actual_k': 1, 'was_impossible': False} 	27 	1 	4.000000

#These are situations where baseline estimates would be quite helpful, in order to deal with highly biased users (and items).
from collections import Counter
import matplotlib.pyplot as plt
import matplotlib
%matplotlib notebook
matplotlib.style.use('ggplot')
counter = Counter([r for (_, r) in trainset.ir[trainset.to_inner_iid('302')]])
pd.DataFrame.from_dict(counter, orient='index').plot(kind='bar', legend=False)
plt.xlabel('Rating value')
plt.ylabel('Number of users')
plt.title('Number of users having rated item 302')




##scikit-surprise : Comparison of two algorithms
                                 
import pickle
import os
import pandas as pd
from surprise import SVD
from surprise import KNNBasic
from surprise import Dataset                                                     
from surprise import Reader                                                      
from surprise import dump
from surprise.accuracy import rmse
# We will train and test on the u1.base and u1.test files of the movielens-100k dataset.
# if you haven't already, you need to download the movielens-100k dataset
# You can do it manually, or by running:
#Dataset.load_builtin('ml-100k')
# Now, let's load the dataset
train_file = os.path.expanduser('~') + '/.surprise_data/ml-100k/ml-100k/u1.base'
test_file = os.path.expanduser('~') + '/.surprise_data/ml-100k/ml-100k/u1.test'
data = Dataset.load_from_folds([(train_file, test_file)], Reader('ml-100k'))
                
# We'll use the well-known SVD algorithm and a basic nearest neighbors approach.
algo_svd = SVD()                                                       
algo_knn = KNNBasic()
for trainset, testset in data.folds(): 
    algo_svd.train(trainset)                             
    predictions_svd = algo_svd.test(testset)
    
    algo_knn.train(trainset)
    predictions_knn = algo_knn.test(testset)
    
    rmse(predictions_svd)
    rmse(predictions_knn)                                                                           
    
    dump.dump('./dump_SVD', predictions_svd, algo_svd)
    dump.dump('./dump_KNN', predictions_knn, algo_knn)
#Computing the msd similarity matrix...
#Done computing similarity matrix.
#RMSE: 0.9500
#RMSE: 0.9889
# The dumps have been saved and we can now use them whenever we want.
predictions_svd, algo_svd = dump.load('./dump_SVD')
predictions_knn, algo_knn = dump.load('./dump_KNN')
df_svd = pd.DataFrame(predictions_svd, columns=['uid', 'iid', 'rui', 'est', 'details'])    
df_knn = pd.DataFrame(predictions_knn, columns=['uid', 'iid', 'rui', 'est', 'details'])    
df_svd['err'] = abs(df_svd.est - df_svd.rui)
df_knn['err'] = abs(df_knn.est - df_knn.rui)
#We now have two dataframes with the all the predictions for each algorithm. The cool thing is that, as both algorithm have been tested on the same testset, the indexes of the two dataframes are the same!
>>> df_svd.head()
	uid 	iid 	rui 	est 	details 	err
0 	1 	6 	5.0 	3.300098 	{'was_impossible': False} 	1.699902
1 	1 	10 	3.0 	3.673812 	{'was_impossible': False} 	0.673812
2 	1 	12 	5.0 	4.746471 	{'was_impossible': False} 	0.253529
3 	1 	14 	5.0 	4.031972 	{'was_impossible': False} 	0.968028
4 	1 	17 	3.0 	2.744526 	{'was_impossible': False} 	0.255474
>>> df_knn.head()
	uid 	iid 	rui 	est 	details 	err
0 	1 	6 	5.0 	3.468613 	{'was_impossible': False, 'actual_k': 20} 	1.531387
1 	1 	10 	3.0 	3.866290 	{'was_impossible': False, 'actual_k': 40} 	0.866290
2 	1 	12 	5.0 	4.538194 	{'was_impossible': False, 'actual_k': 40} 	0.461806
3 	1 	14 	5.0 	4.235741 	{'was_impossible': False, 'actual_k': 40} 	0.764259
4 	1 	17 	3.0 	3.228002 	{'was_impossible': False, 'actual_k': 40} 	0.228002
# Let's check how good are the KNN predictions when the SVD has a huge error:
>>> df_knn[df_svd.err >= 3.5]
	uid 	iid 	rui 	est 	details 	err
4198 	89 	221 	1.0 	3.766186 	{'was_impossible': False, 'actual_k': 40} 	2.766186
7385 	167 	48 	1.0 	4.259556 	{'was_impossible': False, 'actual_k': 40} 	3.259556
7390 	167 	169 	1.0 	4.664991 	{'was_impossible': False, 'actual_k': 40} 	3.664991
13972 	295 	183 	1.0 	4.202611 	{'was_impossible': False, 'actual_k': 40} 	3.202611
15306 	312 	265 	1.0 	4.131875 	{'was_impossible': False, 'actual_k': 40} 	3.131875
19140 	405 	575 	5.0 	2.410506 	{'was_impossible': False, 'actual_k': 36} 	2.589494
# Well... Not much better.
# Now, let's look at the predictions of SVD on the 10 worst predictions for KNN
>>> df_svd.iloc[df_knn.sort_values(by='err')[-10:].index]
	uid 	iid 	rui 	est 	details 	err
9406 	208 	302 	1.0 	4.326863 	{'was_impossible': False} 	3.326863
19089 	405 	169 	1.0 	3.039247 	{'was_impossible': False} 	2.039247
19785 	436 	132 	1.0 	4.243595 	{'was_impossible': False} 	3.243595
157 	2 	315 	1.0 	4.260016 	{'was_impossible': False} 	3.260016
8503 	193 	56 	1.0 	4.129742 	{'was_impossible': False} 	3.129742
5531 	113 	976 	5.0 	3.320028 	{'was_impossible': False} 	1.679972
7917 	181 	408 	1.0 	2.962557 	{'was_impossible': False} 	1.962557
7390 	167 	169 	1.0 	4.720954 	{'was_impossible': False} 	3.720954
7412 	167 	1306 	5.0 	3.350088 	{'was_impossible': False} 	1.649912
5553 	114 	1104 	5.0 	3.070772 	{'was_impossible': False} 	1.929228
# How different are the predictions from both algorithms ?
# Let's count the number of predictions for each rating value
import matplotlib.pyplot as plt
import matplotlib
%matplotlib notebook
matplotlib.style.use('ggplot')
figure, (ax1, ax2) = plt.subplots(1, 2)
df_svd.est.plot(kind='hist', title='SVD', ax=ax1)
df_knn.est.plot(kind='hist', title='KNN', ax=ax2)
# As expected, one of the drawbacks of the NN algorithms is that their predictions are often
# quite concentrated around the mean. The SVD algorithm seems more confortable predicting extreme rating values.
<matplotlib.axes._subplots.AxesSubplot at 0x7ff756a46ef0>
# Question: when a user has rated only a small number of items (less than 10), which algorithm
# gives the best predictions on average?
def get_Iu(uid):
    """Return the number of items rated by given user
    
    Args:
        uid: The raw id of the user.
    Returns:
        The number of items rated by the user.
    """
    
    try:
        return len(trainset.ur[trainset.to_inner_uid(uid)])
    except ValueError:  # user was not part of the trainset
        return 0
    
df_knn['Iu'] = df_knn.uid.apply(get_Iu)
df_svd['Iu'] = df_svd.uid.apply(get_Iu)
>>> df_knn[df_knn.Iu < 10].err.mean(), df_svd[df_svd.Iu < 10].err.mean()
(1.0382962702232326, 1.0130235152149263)
    